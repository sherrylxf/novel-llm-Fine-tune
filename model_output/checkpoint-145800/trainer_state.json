{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.999385412943358,
  "eval_steps": 500,
  "global_step": 145800,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00020571951686771463,
      "grad_norm": 0.11723920702934265,
      "learning_rate": 4.1142857142857144e-07,
      "loss": 2.5765,
      "step": 10
    },
    {
      "epoch": 0.00041143903373542926,
      "grad_norm": 0.10819897800683975,
      "learning_rate": 8.685714285714287e-07,
      "loss": 2.6273,
      "step": 20
    },
    {
      "epoch": 0.0006171585506031439,
      "grad_norm": 0.10171341896057129,
      "learning_rate": 1.3257142857142858e-06,
      "loss": 2.5857,
      "step": 30
    },
    {
      "epoch": 0.0008228780674708585,
      "grad_norm": 0.12171831727027893,
      "learning_rate": 1.782857142857143e-06,
      "loss": 2.5638,
      "step": 40
    },
    {
      "epoch": 0.0010285975843385732,
      "grad_norm": 0.1022997498512268,
      "learning_rate": 2.24e-06,
      "loss": 2.6276,
      "step": 50
    },
    {
      "epoch": 0.0012343171012062879,
      "grad_norm": 0.10861861705780029,
      "learning_rate": 2.697142857142857e-06,
      "loss": 2.5926,
      "step": 60
    },
    {
      "epoch": 0.0014400366180740024,
      "grad_norm": 0.10572745651006699,
      "learning_rate": 3.1542857142857145e-06,
      "loss": 2.588,
      "step": 70
    },
    {
      "epoch": 0.001645756134941717,
      "grad_norm": 0.10482380539178848,
      "learning_rate": 3.611428571428572e-06,
      "loss": 2.5893,
      "step": 80
    },
    {
      "epoch": 0.0018514756518094317,
      "grad_norm": 0.12083689868450165,
      "learning_rate": 4.068571428571428e-06,
      "loss": 2.578,
      "step": 90
    },
    {
      "epoch": 0.0020571951686771464,
      "grad_norm": 0.12175009399652481,
      "learning_rate": 4.525714285714286e-06,
      "loss": 2.6117,
      "step": 100
    },
    {
      "epoch": 0.002262914685544861,
      "grad_norm": 0.12388705462217331,
      "learning_rate": 4.982857142857143e-06,
      "loss": 2.5677,
      "step": 110
    },
    {
      "epoch": 0.0024686342024125758,
      "grad_norm": 0.14431540668010712,
      "learning_rate": 5.44e-06,
      "loss": 2.5597,
      "step": 120
    },
    {
      "epoch": 0.0026743537192802903,
      "grad_norm": 0.12917178869247437,
      "learning_rate": 5.897142857142857e-06,
      "loss": 2.5725,
      "step": 130
    },
    {
      "epoch": 0.0028800732361480047,
      "grad_norm": 0.12794196605682373,
      "learning_rate": 6.354285714285714e-06,
      "loss": 2.5767,
      "step": 140
    },
    {
      "epoch": 0.0030857927530157196,
      "grad_norm": 0.14654043316841125,
      "learning_rate": 6.811428571428572e-06,
      "loss": 2.6312,
      "step": 150
    },
    {
      "epoch": 0.003291512269883434,
      "grad_norm": 0.15278342366218567,
      "learning_rate": 7.268571428571429e-06,
      "loss": 2.5805,
      "step": 160
    },
    {
      "epoch": 0.003497231786751149,
      "grad_norm": 0.14850081503391266,
      "learning_rate": 7.725714285714286e-06,
      "loss": 2.57,
      "step": 170
    },
    {
      "epoch": 0.0037029513036188635,
      "grad_norm": 0.16402722895145416,
      "learning_rate": 8.182857142857143e-06,
      "loss": 2.5544,
      "step": 180
    },
    {
      "epoch": 0.003908670820486578,
      "grad_norm": 0.1747300773859024,
      "learning_rate": 8.64e-06,
      "loss": 2.6004,
      "step": 190
    },
    {
      "epoch": 0.004114390337354293,
      "grad_norm": 0.18536114692687988,
      "learning_rate": 9.097142857142858e-06,
      "loss": 2.5487,
      "step": 200
    },
    {
      "epoch": 0.004320109854222007,
      "grad_norm": 0.1899721771478653,
      "learning_rate": 9.554285714285715e-06,
      "loss": 2.5245,
      "step": 210
    },
    {
      "epoch": 0.004525829371089722,
      "grad_norm": 0.1981816291809082,
      "learning_rate": 1.0011428571428572e-05,
      "loss": 2.4825,
      "step": 220
    },
    {
      "epoch": 0.004731548887957436,
      "grad_norm": 0.21598902344703674,
      "learning_rate": 1.046857142857143e-05,
      "loss": 2.5463,
      "step": 230
    },
    {
      "epoch": 0.0049372684048251516,
      "grad_norm": 0.22860758006572723,
      "learning_rate": 1.0925714285714287e-05,
      "loss": 2.5307,
      "step": 240
    },
    {
      "epoch": 0.005142987921692866,
      "grad_norm": 0.24212728440761566,
      "learning_rate": 1.1382857142857144e-05,
      "loss": 2.5891,
      "step": 250
    },
    {
      "epoch": 0.0053487074385605805,
      "grad_norm": 0.21083825826644897,
      "learning_rate": 1.1840000000000002e-05,
      "loss": 2.6122,
      "step": 260
    },
    {
      "epoch": 0.005554426955428295,
      "grad_norm": 0.25286516547203064,
      "learning_rate": 1.2297142857142858e-05,
      "loss": 2.6119,
      "step": 270
    },
    {
      "epoch": 0.0057601464722960094,
      "grad_norm": 0.2722053527832031,
      "learning_rate": 1.2754285714285717e-05,
      "loss": 2.5838,
      "step": 280
    },
    {
      "epoch": 0.005965865989163725,
      "grad_norm": 0.24169377982616425,
      "learning_rate": 1.3211428571428572e-05,
      "loss": 2.5508,
      "step": 290
    },
    {
      "epoch": 0.006171585506031439,
      "grad_norm": 0.2432468682527542,
      "learning_rate": 1.3668571428571431e-05,
      "loss": 2.5214,
      "step": 300
    },
    {
      "epoch": 0.006377305022899154,
      "grad_norm": 0.2621643543243408,
      "learning_rate": 1.4125714285714287e-05,
      "loss": 2.577,
      "step": 310
    },
    {
      "epoch": 0.006583024539766868,
      "grad_norm": 0.2962479591369629,
      "learning_rate": 1.4582857142857143e-05,
      "loss": 2.551,
      "step": 320
    },
    {
      "epoch": 0.006788744056634583,
      "grad_norm": 0.259345680475235,
      "learning_rate": 1.5040000000000002e-05,
      "loss": 2.5511,
      "step": 330
    },
    {
      "epoch": 0.006994463573502298,
      "grad_norm": 0.2904846966266632,
      "learning_rate": 1.5497142857142857e-05,
      "loss": 2.5266,
      "step": 340
    },
    {
      "epoch": 0.0072001830903700124,
      "grad_norm": 0.2882314622402191,
      "learning_rate": 1.5954285714285716e-05,
      "loss": 2.5548,
      "step": 350
    },
    {
      "epoch": 0.007405902607237727,
      "grad_norm": 0.26546376943588257,
      "learning_rate": 1.6411428571428572e-05,
      "loss": 2.5827,
      "step": 360
    },
    {
      "epoch": 0.007611622124105441,
      "grad_norm": 0.28620293736457825,
      "learning_rate": 1.6868571428571428e-05,
      "loss": 2.583,
      "step": 370
    },
    {
      "epoch": 0.007817341640973157,
      "grad_norm": 0.3655603229999542,
      "learning_rate": 1.7325714285714287e-05,
      "loss": 2.5824,
      "step": 380
    },
    {
      "epoch": 0.00802306115784087,
      "grad_norm": 0.2881571054458618,
      "learning_rate": 1.7782857142857142e-05,
      "loss": 2.6645,
      "step": 390
    },
    {
      "epoch": 0.008228780674708586,
      "grad_norm": 0.2977347671985626,
      "learning_rate": 1.824e-05,
      "loss": 2.5447,
      "step": 400
    },
    {
      "epoch": 0.0084345001915763,
      "grad_norm": 0.2831588685512543,
      "learning_rate": 1.8697142857142857e-05,
      "loss": 2.5628,
      "step": 410
    },
    {
      "epoch": 0.008640219708444015,
      "grad_norm": 0.2715007960796356,
      "learning_rate": 1.9154285714285716e-05,
      "loss": 2.5204,
      "step": 420
    },
    {
      "epoch": 0.00884593922531173,
      "grad_norm": 0.321534663438797,
      "learning_rate": 1.9611428571428572e-05,
      "loss": 2.5363,
      "step": 430
    },
    {
      "epoch": 0.009051658742179444,
      "grad_norm": 0.2688504755496979,
      "learning_rate": 2.0068571428571428e-05,
      "loss": 2.4558,
      "step": 440
    },
    {
      "epoch": 0.009257378259047159,
      "grad_norm": 0.28142493963241577,
      "learning_rate": 2.0525714285714287e-05,
      "loss": 2.5246,
      "step": 450
    },
    {
      "epoch": 0.009463097775914872,
      "grad_norm": 0.3118290901184082,
      "learning_rate": 2.0982857142857142e-05,
      "loss": 2.5958,
      "step": 460
    },
    {
      "epoch": 0.009668817292782588,
      "grad_norm": 0.3309347629547119,
      "learning_rate": 2.144e-05,
      "loss": 2.5626,
      "step": 470
    },
    {
      "epoch": 0.009874536809650303,
      "grad_norm": 0.2995477020740509,
      "learning_rate": 2.1897142857142857e-05,
      "loss": 2.5593,
      "step": 480
    },
    {
      "epoch": 0.010080256326518017,
      "grad_norm": 0.2918110191822052,
      "learning_rate": 2.2354285714285716e-05,
      "loss": 2.5736,
      "step": 490
    },
    {
      "epoch": 0.010285975843385732,
      "grad_norm": 0.3254440128803253,
      "learning_rate": 2.2811428571428572e-05,
      "loss": 2.5631,
      "step": 500
    },
    {
      "epoch": 0.010491695360253446,
      "grad_norm": 0.3554801046848297,
      "learning_rate": 2.3268571428571427e-05,
      "loss": 2.6015,
      "step": 510
    },
    {
      "epoch": 0.010697414877121161,
      "grad_norm": 0.3089503347873688,
      "learning_rate": 2.3725714285714287e-05,
      "loss": 2.5766,
      "step": 520
    },
    {
      "epoch": 0.010903134393988876,
      "grad_norm": 0.3235666751861572,
      "learning_rate": 2.4182857142857142e-05,
      "loss": 2.499,
      "step": 530
    },
    {
      "epoch": 0.01110885391085659,
      "grad_norm": 0.3122659921646118,
      "learning_rate": 2.464e-05,
      "loss": 2.4652,
      "step": 540
    },
    {
      "epoch": 0.011314573427724305,
      "grad_norm": 0.3169405460357666,
      "learning_rate": 2.5097142857142857e-05,
      "loss": 2.5077,
      "step": 550
    },
    {
      "epoch": 0.011520292944592019,
      "grad_norm": 0.32708439230918884,
      "learning_rate": 2.5554285714285713e-05,
      "loss": 2.503,
      "step": 560
    },
    {
      "epoch": 0.011726012461459734,
      "grad_norm": 0.3887191414833069,
      "learning_rate": 2.6011428571428568e-05,
      "loss": 2.468,
      "step": 570
    },
    {
      "epoch": 0.01193173197832745,
      "grad_norm": 0.30616307258605957,
      "learning_rate": 2.646857142857143e-05,
      "loss": 2.56,
      "step": 580
    },
    {
      "epoch": 0.012137451495195163,
      "grad_norm": 0.32355132699012756,
      "learning_rate": 2.6925714285714286e-05,
      "loss": 2.5909,
      "step": 590
    },
    {
      "epoch": 0.012343171012062878,
      "grad_norm": 0.32195964455604553,
      "learning_rate": 2.7382857142857142e-05,
      "loss": 2.522,
      "step": 600
    },
    {
      "epoch": 0.012548890528930592,
      "grad_norm": 0.42187297344207764,
      "learning_rate": 2.7839999999999998e-05,
      "loss": 2.5165,
      "step": 610
    },
    {
      "epoch": 0.012754610045798307,
      "grad_norm": 0.34033921360969543,
      "learning_rate": 2.829714285714286e-05,
      "loss": 2.5385,
      "step": 620
    },
    {
      "epoch": 0.012960329562666023,
      "grad_norm": 0.3440949022769928,
      "learning_rate": 2.8754285714285716e-05,
      "loss": 2.5353,
      "step": 630
    },
    {
      "epoch": 0.013166049079533736,
      "grad_norm": 0.31835660338401794,
      "learning_rate": 2.921142857142857e-05,
      "loss": 2.5319,
      "step": 640
    },
    {
      "epoch": 0.013371768596401452,
      "grad_norm": 0.35179322957992554,
      "learning_rate": 2.966857142857143e-05,
      "loss": 2.5763,
      "step": 650
    },
    {
      "epoch": 0.013577488113269165,
      "grad_norm": 0.3011491894721985,
      "learning_rate": 3.0125714285714286e-05,
      "loss": 2.5292,
      "step": 660
    },
    {
      "epoch": 0.01378320763013688,
      "grad_norm": 0.2989351749420166,
      "learning_rate": 3.0582857142857145e-05,
      "loss": 2.5023,
      "step": 670
    },
    {
      "epoch": 0.013988927147004596,
      "grad_norm": 0.34612178802490234,
      "learning_rate": 3.104e-05,
      "loss": 2.6045,
      "step": 680
    },
    {
      "epoch": 0.01419464666387231,
      "grad_norm": 0.32505080103874207,
      "learning_rate": 3.149714285714286e-05,
      "loss": 2.5317,
      "step": 690
    },
    {
      "epoch": 0.014400366180740025,
      "grad_norm": 0.3336101770401001,
      "learning_rate": 3.195428571428571e-05,
      "loss": 2.5369,
      "step": 700
    },
    {
      "epoch": 0.014606085697607738,
      "grad_norm": 0.34131908416748047,
      "learning_rate": 3.241142857142857e-05,
      "loss": 2.5249,
      "step": 710
    },
    {
      "epoch": 0.014811805214475454,
      "grad_norm": 0.33473384380340576,
      "learning_rate": 3.286857142857143e-05,
      "loss": 2.5175,
      "step": 720
    },
    {
      "epoch": 0.01501752473134317,
      "grad_norm": 0.3393643796443939,
      "learning_rate": 3.3325714285714286e-05,
      "loss": 2.529,
      "step": 730
    },
    {
      "epoch": 0.015223244248210883,
      "grad_norm": 0.33983391523361206,
      "learning_rate": 3.378285714285714e-05,
      "loss": 2.519,
      "step": 740
    },
    {
      "epoch": 0.015428963765078598,
      "grad_norm": 0.3332687020301819,
      "learning_rate": 3.424e-05,
      "loss": 2.54,
      "step": 750
    },
    {
      "epoch": 0.015634683281946313,
      "grad_norm": 0.3072340786457062,
      "learning_rate": 3.469714285714286e-05,
      "loss": 2.5096,
      "step": 760
    },
    {
      "epoch": 0.015840402798814025,
      "grad_norm": 0.3788318634033203,
      "learning_rate": 3.5154285714285716e-05,
      "loss": 2.5854,
      "step": 770
    },
    {
      "epoch": 0.01604612231568174,
      "grad_norm": 0.3477576971054077,
      "learning_rate": 3.561142857142857e-05,
      "loss": 2.5123,
      "step": 780
    },
    {
      "epoch": 0.016251841832549456,
      "grad_norm": 0.3099963366985321,
      "learning_rate": 3.606857142857143e-05,
      "loss": 2.5287,
      "step": 790
    },
    {
      "epoch": 0.01645756134941717,
      "grad_norm": 0.35012322664260864,
      "learning_rate": 3.652571428571428e-05,
      "loss": 2.5524,
      "step": 800
    },
    {
      "epoch": 0.016663280866284887,
      "grad_norm": 0.3261586129665375,
      "learning_rate": 3.6982857142857145e-05,
      "loss": 2.5688,
      "step": 810
    },
    {
      "epoch": 0.0168690003831526,
      "grad_norm": 0.3626594841480255,
      "learning_rate": 3.744e-05,
      "loss": 2.5612,
      "step": 820
    },
    {
      "epoch": 0.017074719900020314,
      "grad_norm": 0.32280728220939636,
      "learning_rate": 3.7897142857142856e-05,
      "loss": 2.5259,
      "step": 830
    },
    {
      "epoch": 0.01728043941688803,
      "grad_norm": 0.359967976808548,
      "learning_rate": 3.835428571428571e-05,
      "loss": 2.5135,
      "step": 840
    },
    {
      "epoch": 0.017486158933755745,
      "grad_norm": 0.3138636648654938,
      "learning_rate": 3.881142857142857e-05,
      "loss": 2.5485,
      "step": 850
    },
    {
      "epoch": 0.01769187845062346,
      "grad_norm": 0.33920300006866455,
      "learning_rate": 3.926857142857143e-05,
      "loss": 2.4826,
      "step": 860
    },
    {
      "epoch": 0.01789759796749117,
      "grad_norm": 0.33083805441856384,
      "learning_rate": 3.9725714285714286e-05,
      "loss": 2.5427,
      "step": 870
    },
    {
      "epoch": 0.018103317484358887,
      "grad_norm": 0.37247738242149353,
      "learning_rate": 4.018285714285714e-05,
      "loss": 2.4968,
      "step": 880
    },
    {
      "epoch": 0.018309037001226602,
      "grad_norm": 0.3095572292804718,
      "learning_rate": 4.064e-05,
      "loss": 2.5625,
      "step": 890
    },
    {
      "epoch": 0.018514756518094318,
      "grad_norm": 0.3150281012058258,
      "learning_rate": 4.109714285714286e-05,
      "loss": 2.4796,
      "step": 900
    },
    {
      "epoch": 0.018720476034962033,
      "grad_norm": 0.3211440145969391,
      "learning_rate": 4.1554285714285715e-05,
      "loss": 2.5636,
      "step": 910
    },
    {
      "epoch": 0.018926195551829745,
      "grad_norm": 0.3775373697280884,
      "learning_rate": 4.201142857142857e-05,
      "loss": 2.5596,
      "step": 920
    },
    {
      "epoch": 0.01913191506869746,
      "grad_norm": 0.3039627969264984,
      "learning_rate": 4.2468571428571427e-05,
      "loss": 2.4859,
      "step": 930
    },
    {
      "epoch": 0.019337634585565176,
      "grad_norm": 0.3200564384460449,
      "learning_rate": 4.292571428571429e-05,
      "loss": 2.4805,
      "step": 940
    },
    {
      "epoch": 0.01954335410243289,
      "grad_norm": 0.3150968551635742,
      "learning_rate": 4.3382857142857145e-05,
      "loss": 2.5199,
      "step": 950
    },
    {
      "epoch": 0.019749073619300606,
      "grad_norm": 0.33617937564849854,
      "learning_rate": 4.384e-05,
      "loss": 2.5447,
      "step": 960
    },
    {
      "epoch": 0.019954793136168318,
      "grad_norm": 0.3163326680660248,
      "learning_rate": 4.4297142857142856e-05,
      "loss": 2.5181,
      "step": 970
    },
    {
      "epoch": 0.020160512653036033,
      "grad_norm": 0.3165050745010376,
      "learning_rate": 4.475428571428572e-05,
      "loss": 2.485,
      "step": 980
    },
    {
      "epoch": 0.02036623216990375,
      "grad_norm": 0.3377610146999359,
      "learning_rate": 4.5211428571428574e-05,
      "loss": 2.539,
      "step": 990
    },
    {
      "epoch": 0.020571951686771464,
      "grad_norm": 0.33185505867004395,
      "learning_rate": 4.566857142857143e-05,
      "loss": 2.5296,
      "step": 1000
    },
    {
      "epoch": 0.02077767120363918,
      "grad_norm": 0.30468517541885376,
      "learning_rate": 4.6125714285714286e-05,
      "loss": 2.4795,
      "step": 1010
    },
    {
      "epoch": 0.02098339072050689,
      "grad_norm": 0.32255879044532776,
      "learning_rate": 4.658285714285715e-05,
      "loss": 2.5149,
      "step": 1020
    },
    {
      "epoch": 0.021189110237374607,
      "grad_norm": 0.3443335294723511,
      "learning_rate": 4.7040000000000004e-05,
      "loss": 2.5261,
      "step": 1030
    },
    {
      "epoch": 0.021394829754242322,
      "grad_norm": 0.3386564552783966,
      "learning_rate": 4.749714285714286e-05,
      "loss": 2.5552,
      "step": 1040
    },
    {
      "epoch": 0.021600549271110037,
      "grad_norm": 0.327467143535614,
      "learning_rate": 4.7954285714285715e-05,
      "loss": 2.5743,
      "step": 1050
    },
    {
      "epoch": 0.021806268787977753,
      "grad_norm": 0.32254528999328613,
      "learning_rate": 4.841142857142858e-05,
      "loss": 2.5706,
      "step": 1060
    },
    {
      "epoch": 0.022011988304845465,
      "grad_norm": 0.33831486105918884,
      "learning_rate": 4.886857142857143e-05,
      "loss": 2.5601,
      "step": 1070
    },
    {
      "epoch": 0.02221770782171318,
      "grad_norm": 0.3264665901660919,
      "learning_rate": 4.932571428571429e-05,
      "loss": 2.5354,
      "step": 1080
    },
    {
      "epoch": 0.022423427338580895,
      "grad_norm": 0.32340008020401,
      "learning_rate": 4.9782857142857144e-05,
      "loss": 2.5452,
      "step": 1090
    },
    {
      "epoch": 0.02262914685544861,
      "grad_norm": 0.3217698931694031,
      "learning_rate": 5.024e-05,
      "loss": 2.5205,
      "step": 1100
    },
    {
      "epoch": 0.022834866372316326,
      "grad_norm": 0.32789334654808044,
      "learning_rate": 5.0697142857142856e-05,
      "loss": 2.5948,
      "step": 1110
    },
    {
      "epoch": 0.023040585889184038,
      "grad_norm": 0.3006053566932678,
      "learning_rate": 5.115428571428572e-05,
      "loss": 2.5368,
      "step": 1120
    },
    {
      "epoch": 0.023246305406051753,
      "grad_norm": 0.35527634620666504,
      "learning_rate": 5.1611428571428574e-05,
      "loss": 2.581,
      "step": 1130
    },
    {
      "epoch": 0.02345202492291947,
      "grad_norm": 0.3349999189376831,
      "learning_rate": 5.2068571428571436e-05,
      "loss": 2.5201,
      "step": 1140
    },
    {
      "epoch": 0.023657744439787184,
      "grad_norm": 0.28828904032707214,
      "learning_rate": 5.252571428571429e-05,
      "loss": 2.4908,
      "step": 1150
    },
    {
      "epoch": 0.0238634639566549,
      "grad_norm": 0.31872066855430603,
      "learning_rate": 5.298285714285715e-05,
      "loss": 2.5752,
      "step": 1160
    },
    {
      "epoch": 0.02406918347352261,
      "grad_norm": 0.3054904043674469,
      "learning_rate": 5.344e-05,
      "loss": 2.5471,
      "step": 1170
    },
    {
      "epoch": 0.024274902990390326,
      "grad_norm": 0.3284400999546051,
      "learning_rate": 5.389714285714286e-05,
      "loss": 2.5263,
      "step": 1180
    },
    {
      "epoch": 0.02448062250725804,
      "grad_norm": 0.36349359154701233,
      "learning_rate": 5.4354285714285715e-05,
      "loss": 2.486,
      "step": 1190
    },
    {
      "epoch": 0.024686342024125757,
      "grad_norm": 0.3349460959434509,
      "learning_rate": 5.481142857142857e-05,
      "loss": 2.5027,
      "step": 1200
    },
    {
      "epoch": 0.024892061540993472,
      "grad_norm": 0.29655909538269043,
      "learning_rate": 5.526857142857144e-05,
      "loss": 2.5098,
      "step": 1210
    },
    {
      "epoch": 0.025097781057861184,
      "grad_norm": 0.35215821862220764,
      "learning_rate": 5.5725714285714295e-05,
      "loss": 2.4993,
      "step": 1220
    },
    {
      "epoch": 0.0253035005747289,
      "grad_norm": 0.30572250485420227,
      "learning_rate": 5.618285714285715e-05,
      "loss": 2.5311,
      "step": 1230
    },
    {
      "epoch": 0.025509220091596615,
      "grad_norm": 0.33785006403923035,
      "learning_rate": 5.6640000000000007e-05,
      "loss": 2.5253,
      "step": 1240
    },
    {
      "epoch": 0.02571493960846433,
      "grad_norm": 0.38817498087882996,
      "learning_rate": 5.709714285714286e-05,
      "loss": 2.5098,
      "step": 1250
    },
    {
      "epoch": 0.025920659125332045,
      "grad_norm": 0.343476802110672,
      "learning_rate": 5.755428571428572e-05,
      "loss": 2.5551,
      "step": 1260
    },
    {
      "epoch": 0.026126378642199757,
      "grad_norm": 0.3102760314941406,
      "learning_rate": 5.8011428571428574e-05,
      "loss": 2.5743,
      "step": 1270
    },
    {
      "epoch": 0.026332098159067473,
      "grad_norm": 0.32857102155685425,
      "learning_rate": 5.846857142857143e-05,
      "loss": 2.5403,
      "step": 1280
    },
    {
      "epoch": 0.026537817675935188,
      "grad_norm": 0.31715676188468933,
      "learning_rate": 5.8925714285714285e-05,
      "loss": 2.4705,
      "step": 1290
    },
    {
      "epoch": 0.026743537192802903,
      "grad_norm": 0.3371421694755554,
      "learning_rate": 5.9382857142857154e-05,
      "loss": 2.5627,
      "step": 1300
    },
    {
      "epoch": 0.02694925670967062,
      "grad_norm": 0.2966505289077759,
      "learning_rate": 5.984000000000001e-05,
      "loss": 2.491,
      "step": 1310
    },
    {
      "epoch": 0.02715497622653833,
      "grad_norm": 0.36444127559661865,
      "learning_rate": 6.0297142857142865e-05,
      "loss": 2.5091,
      "step": 1320
    },
    {
      "epoch": 0.027360695743406046,
      "grad_norm": 0.32078829407691956,
      "learning_rate": 6.075428571428572e-05,
      "loss": 2.5055,
      "step": 1330
    },
    {
      "epoch": 0.02756641526027376,
      "grad_norm": 0.33358657360076904,
      "learning_rate": 6.121142857142857e-05,
      "loss": 2.5275,
      "step": 1340
    },
    {
      "epoch": 0.027772134777141477,
      "grad_norm": 0.3095657229423523,
      "learning_rate": 6.166857142857143e-05,
      "loss": 2.5252,
      "step": 1350
    },
    {
      "epoch": 0.027977854294009192,
      "grad_norm": 0.28753548860549927,
      "learning_rate": 6.212571428571428e-05,
      "loss": 2.4693,
      "step": 1360
    },
    {
      "epoch": 0.028183573810876904,
      "grad_norm": 0.3226066529750824,
      "learning_rate": 6.258285714285714e-05,
      "loss": 2.5466,
      "step": 1370
    },
    {
      "epoch": 0.02838929332774462,
      "grad_norm": 0.32896688580513,
      "learning_rate": 6.303999999999999e-05,
      "loss": 2.4953,
      "step": 1380
    },
    {
      "epoch": 0.028595012844612334,
      "grad_norm": 0.3101474642753601,
      "learning_rate": 6.349714285714286e-05,
      "loss": 2.5118,
      "step": 1390
    },
    {
      "epoch": 0.02880073236148005,
      "grad_norm": 0.305423766374588,
      "learning_rate": 6.395428571428572e-05,
      "loss": 2.524,
      "step": 1400
    },
    {
      "epoch": 0.029006451878347765,
      "grad_norm": 0.32656368613243103,
      "learning_rate": 6.441142857142858e-05,
      "loss": 2.4838,
      "step": 1410
    },
    {
      "epoch": 0.029212171395215477,
      "grad_norm": 0.28374189138412476,
      "learning_rate": 6.486857142857143e-05,
      "loss": 2.5131,
      "step": 1420
    },
    {
      "epoch": 0.029417890912083192,
      "grad_norm": 0.3076235353946686,
      "learning_rate": 6.532571428571429e-05,
      "loss": 2.5161,
      "step": 1430
    },
    {
      "epoch": 0.029623610428950908,
      "grad_norm": 0.32458940148353577,
      "learning_rate": 6.578285714285714e-05,
      "loss": 2.5873,
      "step": 1440
    },
    {
      "epoch": 0.029829329945818623,
      "grad_norm": 0.2991335093975067,
      "learning_rate": 6.624e-05,
      "loss": 2.5532,
      "step": 1450
    },
    {
      "epoch": 0.03003504946268634,
      "grad_norm": 0.2992364764213562,
      "learning_rate": 6.669714285714285e-05,
      "loss": 2.5055,
      "step": 1460
    },
    {
      "epoch": 0.03024076897955405,
      "grad_norm": 0.32036253809928894,
      "learning_rate": 6.715428571428571e-05,
      "loss": 2.5827,
      "step": 1470
    },
    {
      "epoch": 0.030446488496421766,
      "grad_norm": 0.3195453882217407,
      "learning_rate": 6.761142857142858e-05,
      "loss": 2.5558,
      "step": 1480
    },
    {
      "epoch": 0.03065220801328948,
      "grad_norm": 0.3112785816192627,
      "learning_rate": 6.806857142857144e-05,
      "loss": 2.4655,
      "step": 1490
    },
    {
      "epoch": 0.030857927530157196,
      "grad_norm": 0.33897483348846436,
      "learning_rate": 6.852571428571429e-05,
      "loss": 2.528,
      "step": 1500
    },
    {
      "epoch": 0.03106364704702491,
      "grad_norm": 0.312196284532547,
      "learning_rate": 6.898285714285715e-05,
      "loss": 2.5346,
      "step": 1510
    },
    {
      "epoch": 0.03126936656389263,
      "grad_norm": 0.4629119336605072,
      "learning_rate": 6.944e-05,
      "loss": 2.5062,
      "step": 1520
    },
    {
      "epoch": 0.03147508608076034,
      "grad_norm": 0.30187559127807617,
      "learning_rate": 6.989714285714286e-05,
      "loss": 2.5051,
      "step": 1530
    },
    {
      "epoch": 0.03168080559762805,
      "grad_norm": 0.36943569779396057,
      "learning_rate": 7.035428571428571e-05,
      "loss": 2.5162,
      "step": 1540
    },
    {
      "epoch": 0.031886525114495766,
      "grad_norm": 0.31255921721458435,
      "learning_rate": 7.081142857142857e-05,
      "loss": 2.5761,
      "step": 1550
    },
    {
      "epoch": 0.03209224463136348,
      "grad_norm": 0.2837419807910919,
      "learning_rate": 7.126857142857144e-05,
      "loss": 2.5374,
      "step": 1560
    },
    {
      "epoch": 0.0322979641482312,
      "grad_norm": 0.2901482880115509,
      "learning_rate": 7.172571428571428e-05,
      "loss": 2.5503,
      "step": 1570
    },
    {
      "epoch": 0.03250368366509891,
      "grad_norm": 0.2968418300151825,
      "learning_rate": 7.218285714285715e-05,
      "loss": 2.4231,
      "step": 1580
    },
    {
      "epoch": 0.03270940318196663,
      "grad_norm": 0.3029636740684509,
      "learning_rate": 7.264000000000001e-05,
      "loss": 2.4478,
      "step": 1590
    },
    {
      "epoch": 0.03291512269883434,
      "grad_norm": 0.33517521619796753,
      "learning_rate": 7.309714285714286e-05,
      "loss": 2.4906,
      "step": 1600
    },
    {
      "epoch": 0.03312084221570206,
      "grad_norm": 0.3458508253097534,
      "learning_rate": 7.355428571428572e-05,
      "loss": 2.5221,
      "step": 1610
    },
    {
      "epoch": 0.03332656173256977,
      "grad_norm": 0.3335268497467041,
      "learning_rate": 7.401142857142857e-05,
      "loss": 2.4967,
      "step": 1620
    },
    {
      "epoch": 0.03353228124943749,
      "grad_norm": 0.2994150221347809,
      "learning_rate": 7.446857142857143e-05,
      "loss": 2.47,
      "step": 1630
    },
    {
      "epoch": 0.0337380007663052,
      "grad_norm": 0.29934969544410706,
      "learning_rate": 7.49257142857143e-05,
      "loss": 2.568,
      "step": 1640
    },
    {
      "epoch": 0.03394372028317291,
      "grad_norm": 0.29936474561691284,
      "learning_rate": 7.538285714285714e-05,
      "loss": 2.5255,
      "step": 1650
    },
    {
      "epoch": 0.03414943980004063,
      "grad_norm": 0.3287421464920044,
      "learning_rate": 7.584e-05,
      "loss": 2.4773,
      "step": 1660
    },
    {
      "epoch": 0.03435515931690834,
      "grad_norm": 0.3024856150150299,
      "learning_rate": 7.629714285714285e-05,
      "loss": 2.534,
      "step": 1670
    },
    {
      "epoch": 0.03456087883377606,
      "grad_norm": 0.2898547947406769,
      "learning_rate": 7.675428571428572e-05,
      "loss": 2.4918,
      "step": 1680
    },
    {
      "epoch": 0.034766598350643774,
      "grad_norm": 0.3177925646305084,
      "learning_rate": 7.721142857142858e-05,
      "loss": 2.5162,
      "step": 1690
    },
    {
      "epoch": 0.03497231786751149,
      "grad_norm": 0.30977025628089905,
      "learning_rate": 7.766857142857143e-05,
      "loss": 2.5759,
      "step": 1700
    },
    {
      "epoch": 0.035178037384379204,
      "grad_norm": 0.3115711808204651,
      "learning_rate": 7.812571428571429e-05,
      "loss": 2.5497,
      "step": 1710
    },
    {
      "epoch": 0.03538375690124692,
      "grad_norm": 0.2921033501625061,
      "learning_rate": 7.858285714285715e-05,
      "loss": 2.559,
      "step": 1720
    },
    {
      "epoch": 0.035589476418114635,
      "grad_norm": 0.3132830262184143,
      "learning_rate": 7.904e-05,
      "loss": 2.5271,
      "step": 1730
    },
    {
      "epoch": 0.03579519593498234,
      "grad_norm": 0.30644145607948303,
      "learning_rate": 7.949714285714286e-05,
      "loss": 2.4588,
      "step": 1740
    },
    {
      "epoch": 0.03600091545185006,
      "grad_norm": 0.29191291332244873,
      "learning_rate": 7.995428571428571e-05,
      "loss": 2.4578,
      "step": 1750
    },
    {
      "epoch": 0.036206634968717774,
      "grad_norm": 0.31037986278533936,
      "learning_rate": 8.041142857142858e-05,
      "loss": 2.4724,
      "step": 1760
    },
    {
      "epoch": 0.03641235448558549,
      "grad_norm": 0.32569774985313416,
      "learning_rate": 8.086857142857144e-05,
      "loss": 2.5054,
      "step": 1770
    },
    {
      "epoch": 0.036618074002453205,
      "grad_norm": 0.43980228900909424,
      "learning_rate": 8.132571428571429e-05,
      "loss": 2.4972,
      "step": 1780
    },
    {
      "epoch": 0.03682379351932092,
      "grad_norm": 0.30360984802246094,
      "learning_rate": 8.178285714285715e-05,
      "loss": 2.5015,
      "step": 1790
    },
    {
      "epoch": 0.037029513036188635,
      "grad_norm": 0.28729474544525146,
      "learning_rate": 8.224000000000001e-05,
      "loss": 2.5612,
      "step": 1800
    },
    {
      "epoch": 0.03723523255305635,
      "grad_norm": 0.27559468150138855,
      "learning_rate": 8.269714285714286e-05,
      "loss": 2.5543,
      "step": 1810
    },
    {
      "epoch": 0.037440952069924066,
      "grad_norm": 0.3418911099433899,
      "learning_rate": 8.315428571428572e-05,
      "loss": 2.5289,
      "step": 1820
    },
    {
      "epoch": 0.03764667158679178,
      "grad_norm": 0.2885597050189972,
      "learning_rate": 8.361142857142857e-05,
      "loss": 2.4969,
      "step": 1830
    },
    {
      "epoch": 0.03785239110365949,
      "grad_norm": 0.29658347368240356,
      "learning_rate": 8.406857142857143e-05,
      "loss": 2.4964,
      "step": 1840
    },
    {
      "epoch": 0.038058110620527205,
      "grad_norm": 0.29976120591163635,
      "learning_rate": 8.452571428571428e-05,
      "loss": 2.4542,
      "step": 1850
    },
    {
      "epoch": 0.03826383013739492,
      "grad_norm": 0.3160189986228943,
      "learning_rate": 8.498285714285715e-05,
      "loss": 2.5826,
      "step": 1860
    },
    {
      "epoch": 0.038469549654262636,
      "grad_norm": 0.31914493441581726,
      "learning_rate": 8.544000000000001e-05,
      "loss": 2.5265,
      "step": 1870
    },
    {
      "epoch": 0.03867526917113035,
      "grad_norm": 0.29900065064430237,
      "learning_rate": 8.589714285714287e-05,
      "loss": 2.511,
      "step": 1880
    },
    {
      "epoch": 0.038880988687998067,
      "grad_norm": 0.3178728520870209,
      "learning_rate": 8.635428571428572e-05,
      "loss": 2.5746,
      "step": 1890
    },
    {
      "epoch": 0.03908670820486578,
      "grad_norm": 0.29385942220687866,
      "learning_rate": 8.681142857142858e-05,
      "loss": 2.5144,
      "step": 1900
    },
    {
      "epoch": 0.0392924277217335,
      "grad_norm": 0.3382028043270111,
      "learning_rate": 8.726857142857143e-05,
      "loss": 2.5247,
      "step": 1910
    },
    {
      "epoch": 0.03949814723860121,
      "grad_norm": 0.330148309469223,
      "learning_rate": 8.77257142857143e-05,
      "loss": 2.4636,
      "step": 1920
    },
    {
      "epoch": 0.03970386675546893,
      "grad_norm": 0.31473585963249207,
      "learning_rate": 8.818285714285714e-05,
      "loss": 2.5469,
      "step": 1930
    },
    {
      "epoch": 0.039909586272336636,
      "grad_norm": 0.305902898311615,
      "learning_rate": 8.864e-05,
      "loss": 2.5015,
      "step": 1940
    },
    {
      "epoch": 0.04011530578920435,
      "grad_norm": 0.3192905783653259,
      "learning_rate": 8.909714285714285e-05,
      "loss": 2.5071,
      "step": 1950
    },
    {
      "epoch": 0.04032102530607207,
      "grad_norm": 0.31977176666259766,
      "learning_rate": 8.955428571428573e-05,
      "loss": 2.4759,
      "step": 1960
    },
    {
      "epoch": 0.04052674482293978,
      "grad_norm": 0.2988787293434143,
      "learning_rate": 9.001142857142858e-05,
      "loss": 2.515,
      "step": 1970
    },
    {
      "epoch": 0.0407324643398075,
      "grad_norm": 0.29134494066238403,
      "learning_rate": 9.046857142857144e-05,
      "loss": 2.5242,
      "step": 1980
    },
    {
      "epoch": 0.04093818385667521,
      "grad_norm": 0.304279625415802,
      "learning_rate": 9.092571428571429e-05,
      "loss": 2.5694,
      "step": 1990
    },
    {
      "epoch": 0.04114390337354293,
      "grad_norm": 0.3000682294368744,
      "learning_rate": 9.138285714285715e-05,
      "loss": 2.5245,
      "step": 2000
    },
    {
      "epoch": 0.041349622890410644,
      "grad_norm": 0.2934725880622864,
      "learning_rate": 9.184e-05,
      "loss": 2.5199,
      "step": 2010
    },
    {
      "epoch": 0.04155534240727836,
      "grad_norm": 0.311893492937088,
      "learning_rate": 9.229714285714286e-05,
      "loss": 2.492,
      "step": 2020
    },
    {
      "epoch": 0.041761061924146074,
      "grad_norm": 0.28983816504478455,
      "learning_rate": 9.275428571428571e-05,
      "loss": 2.4617,
      "step": 2030
    },
    {
      "epoch": 0.04196678144101378,
      "grad_norm": 0.31212255358695984,
      "learning_rate": 9.321142857142858e-05,
      "loss": 2.4797,
      "step": 2040
    },
    {
      "epoch": 0.0421725009578815,
      "grad_norm": 0.2661963701248169,
      "learning_rate": 9.366857142857142e-05,
      "loss": 2.4483,
      "step": 2050
    },
    {
      "epoch": 0.04237822047474921,
      "grad_norm": 0.31590497493743896,
      "learning_rate": 9.41257142857143e-05,
      "loss": 2.5585,
      "step": 2060
    },
    {
      "epoch": 0.04258393999161693,
      "grad_norm": 0.2850225865840912,
      "learning_rate": 9.458285714285715e-05,
      "loss": 2.4792,
      "step": 2070
    },
    {
      "epoch": 0.042789659508484644,
      "grad_norm": 0.26103854179382324,
      "learning_rate": 9.504000000000001e-05,
      "loss": 2.5048,
      "step": 2080
    },
    {
      "epoch": 0.04299537902535236,
      "grad_norm": 0.29273802042007446,
      "learning_rate": 9.549714285714286e-05,
      "loss": 2.4949,
      "step": 2090
    },
    {
      "epoch": 0.043201098542220075,
      "grad_norm": 0.3212519586086273,
      "learning_rate": 9.595428571428572e-05,
      "loss": 2.5651,
      "step": 2100
    },
    {
      "epoch": 0.04340681805908779,
      "grad_norm": 0.29035982489585876,
      "learning_rate": 9.641142857142857e-05,
      "loss": 2.5311,
      "step": 2110
    },
    {
      "epoch": 0.043612537575955505,
      "grad_norm": 0.3061845004558563,
      "learning_rate": 9.686857142857143e-05,
      "loss": 2.5354,
      "step": 2120
    },
    {
      "epoch": 0.04381825709282322,
      "grad_norm": 0.32481759786605835,
      "learning_rate": 9.732571428571428e-05,
      "loss": 2.537,
      "step": 2130
    },
    {
      "epoch": 0.04402397660969093,
      "grad_norm": 0.2911372184753418,
      "learning_rate": 9.778285714285715e-05,
      "loss": 2.5582,
      "step": 2140
    },
    {
      "epoch": 0.044229696126558644,
      "grad_norm": 0.32849109172821045,
      "learning_rate": 9.824000000000001e-05,
      "loss": 2.5322,
      "step": 2150
    },
    {
      "epoch": 0.04443541564342636,
      "grad_norm": 0.3011954426765442,
      "learning_rate": 9.869714285714287e-05,
      "loss": 2.5383,
      "step": 2160
    },
    {
      "epoch": 0.044641135160294075,
      "grad_norm": 0.2623154819011688,
      "learning_rate": 9.915428571428572e-05,
      "loss": 2.4812,
      "step": 2170
    },
    {
      "epoch": 0.04484685467716179,
      "grad_norm": 0.3015471398830414,
      "learning_rate": 9.961142857142858e-05,
      "loss": 2.5286,
      "step": 2180
    },
    {
      "epoch": 0.045052574194029506,
      "grad_norm": 0.288231760263443,
      "learning_rate": 0.00010006857142857142,
      "loss": 2.4982,
      "step": 2190
    },
    {
      "epoch": 0.04525829371089722,
      "grad_norm": 0.28427621722221375,
      "learning_rate": 0.00010052571428571429,
      "loss": 2.517,
      "step": 2200
    },
    {
      "epoch": 0.045464013227764936,
      "grad_norm": 0.31620001792907715,
      "learning_rate": 0.00010098285714285716,
      "loss": 2.5083,
      "step": 2210
    },
    {
      "epoch": 0.04566973274463265,
      "grad_norm": 0.288861483335495,
      "learning_rate": 0.00010144,
      "loss": 2.5617,
      "step": 2220
    },
    {
      "epoch": 0.04587545226150037,
      "grad_norm": 0.3077877163887024,
      "learning_rate": 0.00010189714285714287,
      "loss": 2.5257,
      "step": 2230
    },
    {
      "epoch": 0.046081171778368075,
      "grad_norm": 0.29118239879608154,
      "learning_rate": 0.00010235428571428572,
      "loss": 2.4956,
      "step": 2240
    },
    {
      "epoch": 0.04628689129523579,
      "grad_norm": 0.31068137288093567,
      "learning_rate": 0.00010281142857142858,
      "loss": 2.5626,
      "step": 2250
    },
    {
      "epoch": 0.046492610812103506,
      "grad_norm": 0.3217317759990692,
      "learning_rate": 0.00010326857142857143,
      "loss": 2.5082,
      "step": 2260
    },
    {
      "epoch": 0.04669833032897122,
      "grad_norm": 0.26497289538383484,
      "learning_rate": 0.00010372571428571429,
      "loss": 2.5737,
      "step": 2270
    },
    {
      "epoch": 0.04690404984583894,
      "grad_norm": 0.2698274552822113,
      "learning_rate": 0.00010418285714285714,
      "loss": 2.4843,
      "step": 2280
    },
    {
      "epoch": 0.04710976936270665,
      "grad_norm": 0.29156428575515747,
      "learning_rate": 0.00010464,
      "loss": 2.5335,
      "step": 2290
    },
    {
      "epoch": 0.04731548887957437,
      "grad_norm": 0.3002912104129791,
      "learning_rate": 0.00010509714285714288,
      "loss": 2.5412,
      "step": 2300
    },
    {
      "epoch": 0.04752120839644208,
      "grad_norm": 0.30021965503692627,
      "learning_rate": 0.00010555428571428571,
      "loss": 2.5096,
      "step": 2310
    },
    {
      "epoch": 0.0477269279133098,
      "grad_norm": 0.31332898139953613,
      "learning_rate": 0.00010601142857142859,
      "loss": 2.4739,
      "step": 2320
    },
    {
      "epoch": 0.047932647430177514,
      "grad_norm": 0.272695392370224,
      "learning_rate": 0.00010646857142857142,
      "loss": 2.5445,
      "step": 2330
    },
    {
      "epoch": 0.04813836694704522,
      "grad_norm": 0.28729090094566345,
      "learning_rate": 0.0001069257142857143,
      "loss": 2.5762,
      "step": 2340
    },
    {
      "epoch": 0.04834408646391294,
      "grad_norm": 0.26336172223091125,
      "learning_rate": 0.00010738285714285714,
      "loss": 2.5271,
      "step": 2350
    },
    {
      "epoch": 0.04854980598078065,
      "grad_norm": 0.2743348181247711,
      "learning_rate": 0.00010784000000000001,
      "loss": 2.5369,
      "step": 2360
    },
    {
      "epoch": 0.04875552549764837,
      "grad_norm": 0.29264959692955017,
      "learning_rate": 0.00010829714285714285,
      "loss": 2.5109,
      "step": 2370
    },
    {
      "epoch": 0.04896124501451608,
      "grad_norm": 0.2916426658630371,
      "learning_rate": 0.00010875428571428572,
      "loss": 2.5329,
      "step": 2380
    },
    {
      "epoch": 0.0491669645313838,
      "grad_norm": 0.2928306758403778,
      "learning_rate": 0.00010921142857142858,
      "loss": 2.5542,
      "step": 2390
    },
    {
      "epoch": 0.049372684048251514,
      "grad_norm": 0.3035091459751129,
      "learning_rate": 0.00010966857142857143,
      "loss": 2.4856,
      "step": 2400
    },
    {
      "epoch": 0.04957840356511923,
      "grad_norm": 0.34391582012176514,
      "learning_rate": 0.0001101257142857143,
      "loss": 2.5255,
      "step": 2410
    },
    {
      "epoch": 0.049784123081986945,
      "grad_norm": 0.3942328095436096,
      "learning_rate": 0.00011058285714285715,
      "loss": 2.5173,
      "step": 2420
    },
    {
      "epoch": 0.04998984259885466,
      "grad_norm": 0.2852119505405426,
      "learning_rate": 0.00011104000000000001,
      "loss": 2.5149,
      "step": 2430
    },
    {
      "epoch": 0.05019556211572237,
      "grad_norm": 0.28125420212745667,
      "learning_rate": 0.00011149714285714286,
      "loss": 2.4698,
      "step": 2440
    },
    {
      "epoch": 0.050401281632590084,
      "grad_norm": 0.30547937750816345,
      "learning_rate": 0.00011195428571428572,
      "loss": 2.4727,
      "step": 2450
    },
    {
      "epoch": 0.0506070011494578,
      "grad_norm": 0.3154851198196411,
      "learning_rate": 0.00011241142857142857,
      "loss": 2.5547,
      "step": 2460
    },
    {
      "epoch": 0.050812720666325514,
      "grad_norm": 0.2967433035373688,
      "learning_rate": 0.00011286857142857143,
      "loss": 2.5794,
      "step": 2470
    },
    {
      "epoch": 0.05101844018319323,
      "grad_norm": 0.2725524604320526,
      "learning_rate": 0.0001133257142857143,
      "loss": 2.4998,
      "step": 2480
    },
    {
      "epoch": 0.051224159700060945,
      "grad_norm": 0.3192947208881378,
      "learning_rate": 0.00011378285714285714,
      "loss": 2.5,
      "step": 2490
    },
    {
      "epoch": 0.05142987921692866,
      "grad_norm": 0.3489823639392853,
      "learning_rate": 0.00011424000000000002,
      "loss": 2.5203,
      "step": 2500
    },
    {
      "epoch": 0.051635598733796376,
      "grad_norm": 0.27212220430374146,
      "learning_rate": 0.00011469714285714285,
      "loss": 2.5243,
      "step": 2510
    },
    {
      "epoch": 0.05184131825066409,
      "grad_norm": 0.3138796389102936,
      "learning_rate": 0.00011515428571428573,
      "loss": 2.5714,
      "step": 2520
    },
    {
      "epoch": 0.052047037767531806,
      "grad_norm": 0.2890970706939697,
      "learning_rate": 0.00011561142857142856,
      "loss": 2.4629,
      "step": 2530
    },
    {
      "epoch": 0.052252757284399515,
      "grad_norm": 0.2641656696796417,
      "learning_rate": 0.00011606857142857144,
      "loss": 2.4881,
      "step": 2540
    },
    {
      "epoch": 0.05245847680126723,
      "grad_norm": 0.3238428235054016,
      "learning_rate": 0.00011652571428571429,
      "loss": 2.535,
      "step": 2550
    },
    {
      "epoch": 0.052664196318134945,
      "grad_norm": 0.27158233523368835,
      "learning_rate": 0.00011698285714285715,
      "loss": 2.4742,
      "step": 2560
    },
    {
      "epoch": 0.05286991583500266,
      "grad_norm": 0.3046453297138214,
      "learning_rate": 0.00011744000000000001,
      "loss": 2.5002,
      "step": 2570
    },
    {
      "epoch": 0.053075635351870376,
      "grad_norm": 0.3280419409275055,
      "learning_rate": 0.00011789714285714286,
      "loss": 2.4969,
      "step": 2580
    },
    {
      "epoch": 0.05328135486873809,
      "grad_norm": 0.27405238151550293,
      "learning_rate": 0.00011835428571428573,
      "loss": 2.4936,
      "step": 2590
    },
    {
      "epoch": 0.05348707438560581,
      "grad_norm": 0.26534733176231384,
      "learning_rate": 0.00011881142857142857,
      "loss": 2.5058,
      "step": 2600
    },
    {
      "epoch": 0.05369279390247352,
      "grad_norm": 0.27784010767936707,
      "learning_rate": 0.00011926857142857144,
      "loss": 2.5192,
      "step": 2610
    },
    {
      "epoch": 0.05389851341934124,
      "grad_norm": 0.27642911672592163,
      "learning_rate": 0.00011972571428571429,
      "loss": 2.4805,
      "step": 2620
    },
    {
      "epoch": 0.05410423293620895,
      "grad_norm": 0.31395336985588074,
      "learning_rate": 0.00012018285714285715,
      "loss": 2.5237,
      "step": 2630
    },
    {
      "epoch": 0.05430995245307666,
      "grad_norm": 0.3410395085811615,
      "learning_rate": 0.00012064,
      "loss": 2.5058,
      "step": 2640
    },
    {
      "epoch": 0.054515671969944376,
      "grad_norm": 0.2681521773338318,
      "learning_rate": 0.00012109714285714286,
      "loss": 2.5314,
      "step": 2650
    },
    {
      "epoch": 0.05472139148681209,
      "grad_norm": 0.3150806427001953,
      "learning_rate": 0.00012155428571428571,
      "loss": 2.5037,
      "step": 2660
    },
    {
      "epoch": 0.05492711100367981,
      "grad_norm": 0.3046301007270813,
      "learning_rate": 0.00012201142857142857,
      "loss": 2.5332,
      "step": 2670
    },
    {
      "epoch": 0.05513283052054752,
      "grad_norm": 0.28649646043777466,
      "learning_rate": 0.00012246857142857143,
      "loss": 2.5237,
      "step": 2680
    },
    {
      "epoch": 0.05533855003741524,
      "grad_norm": 0.2700512111186981,
      "learning_rate": 0.00012292571428571428,
      "loss": 2.5357,
      "step": 2690
    },
    {
      "epoch": 0.05554426955428295,
      "grad_norm": 0.27281415462493896,
      "learning_rate": 0.00012338285714285716,
      "loss": 2.5258,
      "step": 2700
    },
    {
      "epoch": 0.05574998907115067,
      "grad_norm": 0.298427939414978,
      "learning_rate": 0.00012384,
      "loss": 2.5441,
      "step": 2710
    },
    {
      "epoch": 0.055955708588018384,
      "grad_norm": 0.3253706991672516,
      "learning_rate": 0.00012429714285714286,
      "loss": 2.5184,
      "step": 2720
    },
    {
      "epoch": 0.05616142810488609,
      "grad_norm": 0.27573931217193604,
      "learning_rate": 0.0001247542857142857,
      "loss": 2.5311,
      "step": 2730
    },
    {
      "epoch": 0.05636714762175381,
      "grad_norm": 0.24916672706604004,
      "learning_rate": 0.00012521142857142858,
      "loss": 2.4912,
      "step": 2740
    },
    {
      "epoch": 0.05657286713862152,
      "grad_norm": 0.27328741550445557,
      "learning_rate": 0.00012566857142857143,
      "loss": 2.4799,
      "step": 2750
    },
    {
      "epoch": 0.05677858665548924,
      "grad_norm": 0.30516713857650757,
      "learning_rate": 0.00012612571428571428,
      "loss": 2.5165,
      "step": 2760
    },
    {
      "epoch": 0.056984306172356954,
      "grad_norm": 0.27830761671066284,
      "learning_rate": 0.00012658285714285715,
      "loss": 2.4735,
      "step": 2770
    },
    {
      "epoch": 0.05719002568922467,
      "grad_norm": 0.2891061305999756,
      "learning_rate": 0.00012704,
      "loss": 2.5527,
      "step": 2780
    },
    {
      "epoch": 0.057395745206092384,
      "grad_norm": 0.2706209421157837,
      "learning_rate": 0.00012749714285714288,
      "loss": 2.5489,
      "step": 2790
    },
    {
      "epoch": 0.0576014647229601,
      "grad_norm": 0.2629005014896393,
      "learning_rate": 0.00012795428571428573,
      "loss": 2.5336,
      "step": 2800
    },
    {
      "epoch": 0.057807184239827815,
      "grad_norm": 0.26995736360549927,
      "learning_rate": 0.00012841142857142858,
      "loss": 2.5002,
      "step": 2810
    },
    {
      "epoch": 0.05801290375669553,
      "grad_norm": 0.2635352611541748,
      "learning_rate": 0.00012886857142857143,
      "loss": 2.4918,
      "step": 2820
    },
    {
      "epoch": 0.05821862327356324,
      "grad_norm": 0.32839667797088623,
      "learning_rate": 0.0001293257142857143,
      "loss": 2.4881,
      "step": 2830
    },
    {
      "epoch": 0.058424342790430954,
      "grad_norm": 0.3059311509132385,
      "learning_rate": 0.00012978285714285715,
      "loss": 2.5146,
      "step": 2840
    },
    {
      "epoch": 0.05863006230729867,
      "grad_norm": 0.28370654582977295,
      "learning_rate": 0.00013024,
      "loss": 2.5125,
      "step": 2850
    },
    {
      "epoch": 0.058835781824166385,
      "grad_norm": 0.3125951290130615,
      "learning_rate": 0.00013069714285714288,
      "loss": 2.5286,
      "step": 2860
    },
    {
      "epoch": 0.0590415013410341,
      "grad_norm": 0.42022112011909485,
      "learning_rate": 0.00013115428571428572,
      "loss": 2.4997,
      "step": 2870
    },
    {
      "epoch": 0.059247220857901815,
      "grad_norm": 0.31037551164627075,
      "learning_rate": 0.00013161142857142857,
      "loss": 2.4854,
      "step": 2880
    },
    {
      "epoch": 0.05945294037476953,
      "grad_norm": 0.2622630000114441,
      "learning_rate": 0.00013206857142857142,
      "loss": 2.554,
      "step": 2890
    },
    {
      "epoch": 0.059658659891637246,
      "grad_norm": 0.2951025664806366,
      "learning_rate": 0.0001325257142857143,
      "loss": 2.5485,
      "step": 2900
    },
    {
      "epoch": 0.05986437940850496,
      "grad_norm": 0.27895528078079224,
      "learning_rate": 0.00013298285714285715,
      "loss": 2.4869,
      "step": 2910
    },
    {
      "epoch": 0.06007009892537268,
      "grad_norm": 0.2804126441478729,
      "learning_rate": 0.00013344,
      "loss": 2.4974,
      "step": 2920
    },
    {
      "epoch": 0.060275818442240385,
      "grad_norm": 0.3208950161933899,
      "learning_rate": 0.00013389714285714285,
      "loss": 2.4918,
      "step": 2930
    },
    {
      "epoch": 0.0604815379591081,
      "grad_norm": 0.270114004611969,
      "learning_rate": 0.00013435428571428572,
      "loss": 2.4752,
      "step": 2940
    },
    {
      "epoch": 0.060687257475975816,
      "grad_norm": 0.2903919219970703,
      "learning_rate": 0.0001348114285714286,
      "loss": 2.4953,
      "step": 2950
    },
    {
      "epoch": 0.06089297699284353,
      "grad_norm": 0.28217074275016785,
      "learning_rate": 0.00013526857142857145,
      "loss": 2.4839,
      "step": 2960
    },
    {
      "epoch": 0.061098696509711246,
      "grad_norm": 0.26636064052581787,
      "learning_rate": 0.0001357257142857143,
      "loss": 2.4943,
      "step": 2970
    },
    {
      "epoch": 0.06130441602657896,
      "grad_norm": 0.2728070914745331,
      "learning_rate": 0.00013618285714285714,
      "loss": 2.484,
      "step": 2980
    },
    {
      "epoch": 0.06151013554344668,
      "grad_norm": 0.31778162717819214,
      "learning_rate": 0.00013664000000000002,
      "loss": 2.5749,
      "step": 2990
    },
    {
      "epoch": 0.06171585506031439,
      "grad_norm": 0.29375672340393066,
      "learning_rate": 0.00013709714285714287,
      "loss": 2.511,
      "step": 3000
    },
    {
      "epoch": 0.06192157457718211,
      "grad_norm": 0.2797028124332428,
      "learning_rate": 0.00013755428571428572,
      "loss": 2.5331,
      "step": 3010
    },
    {
      "epoch": 0.06212729409404982,
      "grad_norm": 0.2944128215312958,
      "learning_rate": 0.00013801142857142857,
      "loss": 2.5165,
      "step": 3020
    },
    {
      "epoch": 0.06233301361091753,
      "grad_norm": 0.2729285955429077,
      "learning_rate": 0.00013846857142857144,
      "loss": 2.5199,
      "step": 3030
    },
    {
      "epoch": 0.06253873312778525,
      "grad_norm": 0.2982107698917389,
      "learning_rate": 0.0001389257142857143,
      "loss": 2.5595,
      "step": 3040
    },
    {
      "epoch": 0.06274445264465296,
      "grad_norm": 0.30891361832618713,
      "learning_rate": 0.00013938285714285714,
      "loss": 2.4878,
      "step": 3050
    },
    {
      "epoch": 0.06295017216152068,
      "grad_norm": 0.2868891656398773,
      "learning_rate": 0.00013984000000000002,
      "loss": 2.5201,
      "step": 3060
    },
    {
      "epoch": 0.06315589167838839,
      "grad_norm": 0.2674413323402405,
      "learning_rate": 0.00014029714285714287,
      "loss": 2.5043,
      "step": 3070
    },
    {
      "epoch": 0.0633616111952561,
      "grad_norm": 0.28668707609176636,
      "learning_rate": 0.00014075428571428571,
      "loss": 2.5179,
      "step": 3080
    },
    {
      "epoch": 0.06356733071212382,
      "grad_norm": 0.2675064206123352,
      "learning_rate": 0.00014121142857142856,
      "loss": 2.476,
      "step": 3090
    },
    {
      "epoch": 0.06377305022899153,
      "grad_norm": 0.2666856348514557,
      "learning_rate": 0.00014166857142857144,
      "loss": 2.5392,
      "step": 3100
    },
    {
      "epoch": 0.06397876974585925,
      "grad_norm": 0.28792932629585266,
      "learning_rate": 0.0001421257142857143,
      "loss": 2.4599,
      "step": 3110
    },
    {
      "epoch": 0.06418448926272696,
      "grad_norm": 0.27403172850608826,
      "learning_rate": 0.00014258285714285716,
      "loss": 2.4937,
      "step": 3120
    },
    {
      "epoch": 0.06439020877959468,
      "grad_norm": 0.27399057149887085,
      "learning_rate": 0.00014303999999999999,
      "loss": 2.5323,
      "step": 3130
    },
    {
      "epoch": 0.0645959282964624,
      "grad_norm": 0.31919732689857483,
      "learning_rate": 0.00014349714285714286,
      "loss": 2.5012,
      "step": 3140
    },
    {
      "epoch": 0.06480164781333012,
      "grad_norm": 0.2842711806297302,
      "learning_rate": 0.00014395428571428574,
      "loss": 2.5324,
      "step": 3150
    },
    {
      "epoch": 0.06500736733019782,
      "grad_norm": 0.28716400265693665,
      "learning_rate": 0.0001444114285714286,
      "loss": 2.5037,
      "step": 3160
    },
    {
      "epoch": 0.06521308684706555,
      "grad_norm": 0.34747496247291565,
      "learning_rate": 0.00014486857142857144,
      "loss": 2.5361,
      "step": 3170
    },
    {
      "epoch": 0.06541880636393325,
      "grad_norm": 0.2740767300128937,
      "learning_rate": 0.00014532571428571428,
      "loss": 2.5199,
      "step": 3180
    },
    {
      "epoch": 0.06562452588080096,
      "grad_norm": 0.2887432277202606,
      "learning_rate": 0.00014578285714285716,
      "loss": 2.4799,
      "step": 3190
    },
    {
      "epoch": 0.06583024539766869,
      "grad_norm": 0.278801828622818,
      "learning_rate": 0.00014624,
      "loss": 2.5061,
      "step": 3200
    },
    {
      "epoch": 0.0660359649145364,
      "grad_norm": 0.3513267934322357,
      "learning_rate": 0.00014669714285714286,
      "loss": 2.5359,
      "step": 3210
    },
    {
      "epoch": 0.06624168443140412,
      "grad_norm": 0.28975099325180054,
      "learning_rate": 0.0001471542857142857,
      "loss": 2.5287,
      "step": 3220
    },
    {
      "epoch": 0.06644740394827182,
      "grad_norm": 0.27459684014320374,
      "learning_rate": 0.00014761142857142858,
      "loss": 2.5214,
      "step": 3230
    },
    {
      "epoch": 0.06665312346513955,
      "grad_norm": 0.29580971598625183,
      "learning_rate": 0.00014806857142857143,
      "loss": 2.4836,
      "step": 3240
    },
    {
      "epoch": 0.06685884298200725,
      "grad_norm": 0.2905707359313965,
      "learning_rate": 0.00014852571428571428,
      "loss": 2.4947,
      "step": 3250
    },
    {
      "epoch": 0.06706456249887498,
      "grad_norm": 0.2642730176448822,
      "learning_rate": 0.00014898285714285716,
      "loss": 2.4721,
      "step": 3260
    },
    {
      "epoch": 0.06727028201574269,
      "grad_norm": 0.28350594639778137,
      "learning_rate": 0.00014944,
      "loss": 2.5245,
      "step": 3270
    },
    {
      "epoch": 0.0674760015326104,
      "grad_norm": 0.2738940119743347,
      "learning_rate": 0.00014989714285714288,
      "loss": 2.487,
      "step": 3280
    },
    {
      "epoch": 0.06768172104947812,
      "grad_norm": 0.27529141306877136,
      "learning_rate": 0.0001503542857142857,
      "loss": 2.5035,
      "step": 3290
    },
    {
      "epoch": 0.06788744056634582,
      "grad_norm": 0.2741159200668335,
      "learning_rate": 0.00015081142857142858,
      "loss": 2.5525,
      "step": 3300
    },
    {
      "epoch": 0.06809316008321355,
      "grad_norm": 0.28914928436279297,
      "learning_rate": 0.00015126857142857143,
      "loss": 2.5057,
      "step": 3310
    },
    {
      "epoch": 0.06829887960008126,
      "grad_norm": 0.29517802596092224,
      "learning_rate": 0.0001517257142857143,
      "loss": 2.5014,
      "step": 3320
    },
    {
      "epoch": 0.06850459911694898,
      "grad_norm": 0.28075212240219116,
      "learning_rate": 0.00015218285714285715,
      "loss": 2.5182,
      "step": 3330
    },
    {
      "epoch": 0.06871031863381669,
      "grad_norm": 0.29252389073371887,
      "learning_rate": 0.00015264,
      "loss": 2.5116,
      "step": 3340
    },
    {
      "epoch": 0.06891603815068441,
      "grad_norm": 0.2981751263141632,
      "learning_rate": 0.00015309714285714288,
      "loss": 2.5708,
      "step": 3350
    },
    {
      "epoch": 0.06912175766755212,
      "grad_norm": 0.26532453298568726,
      "learning_rate": 0.00015355428571428573,
      "loss": 2.4854,
      "step": 3360
    },
    {
      "epoch": 0.06932747718441984,
      "grad_norm": 0.3525300920009613,
      "learning_rate": 0.00015401142857142858,
      "loss": 2.5579,
      "step": 3370
    },
    {
      "epoch": 0.06953319670128755,
      "grad_norm": 0.27572157979011536,
      "learning_rate": 0.00015446857142857142,
      "loss": 2.5255,
      "step": 3380
    },
    {
      "epoch": 0.06973891621815526,
      "grad_norm": 0.2754066586494446,
      "learning_rate": 0.0001549257142857143,
      "loss": 2.5471,
      "step": 3390
    },
    {
      "epoch": 0.06994463573502298,
      "grad_norm": 0.2899334728717804,
      "learning_rate": 0.00015538285714285715,
      "loss": 2.5068,
      "step": 3400
    },
    {
      "epoch": 0.07015035525189069,
      "grad_norm": 0.29778730869293213,
      "learning_rate": 0.00015584,
      "loss": 2.5204,
      "step": 3410
    },
    {
      "epoch": 0.07035607476875841,
      "grad_norm": 0.29415029287338257,
      "learning_rate": 0.00015629714285714287,
      "loss": 2.4886,
      "step": 3420
    },
    {
      "epoch": 0.07056179428562612,
      "grad_norm": 0.2774258255958557,
      "learning_rate": 0.00015675428571428572,
      "loss": 2.5332,
      "step": 3430
    },
    {
      "epoch": 0.07076751380249384,
      "grad_norm": 0.3119576573371887,
      "learning_rate": 0.0001572114285714286,
      "loss": 2.5165,
      "step": 3440
    },
    {
      "epoch": 0.07097323331936155,
      "grad_norm": 0.27842196822166443,
      "learning_rate": 0.00015766857142857142,
      "loss": 2.4574,
      "step": 3450
    },
    {
      "epoch": 0.07117895283622927,
      "grad_norm": 0.32565414905548096,
      "learning_rate": 0.0001581257142857143,
      "loss": 2.4646,
      "step": 3460
    },
    {
      "epoch": 0.07138467235309698,
      "grad_norm": 0.2831774950027466,
      "learning_rate": 0.00015858285714285715,
      "loss": 2.4659,
      "step": 3470
    },
    {
      "epoch": 0.07159039186996469,
      "grad_norm": 0.2859323024749756,
      "learning_rate": 0.00015904000000000002,
      "loss": 2.4912,
      "step": 3480
    },
    {
      "epoch": 0.07179611138683241,
      "grad_norm": 0.3016364276409149,
      "learning_rate": 0.00015949714285714284,
      "loss": 2.486,
      "step": 3490
    },
    {
      "epoch": 0.07200183090370012,
      "grad_norm": 0.2644825279712677,
      "learning_rate": 0.00015995428571428572,
      "loss": 2.5123,
      "step": 3500
    },
    {
      "epoch": 0.07220755042056784,
      "grad_norm": 0.2828836739063263,
      "learning_rate": 0.00016041142857142857,
      "loss": 2.4828,
      "step": 3510
    },
    {
      "epoch": 0.07241326993743555,
      "grad_norm": 0.31679534912109375,
      "learning_rate": 0.00016086857142857144,
      "loss": 2.5428,
      "step": 3520
    },
    {
      "epoch": 0.07261898945430327,
      "grad_norm": 0.2722242474555969,
      "learning_rate": 0.0001613257142857143,
      "loss": 2.5568,
      "step": 3530
    },
    {
      "epoch": 0.07282470897117098,
      "grad_norm": 0.30324289202690125,
      "learning_rate": 0.00016178285714285714,
      "loss": 2.4813,
      "step": 3540
    },
    {
      "epoch": 0.0730304284880387,
      "grad_norm": 0.2816909849643707,
      "learning_rate": 0.00016224000000000002,
      "loss": 2.5389,
      "step": 3550
    },
    {
      "epoch": 0.07323614800490641,
      "grad_norm": 0.27559953927993774,
      "learning_rate": 0.00016269714285714287,
      "loss": 2.5013,
      "step": 3560
    },
    {
      "epoch": 0.07344186752177413,
      "grad_norm": 0.28984498977661133,
      "learning_rate": 0.00016315428571428572,
      "loss": 2.4867,
      "step": 3570
    },
    {
      "epoch": 0.07364758703864184,
      "grad_norm": 0.2888953983783722,
      "learning_rate": 0.00016361142857142857,
      "loss": 2.4812,
      "step": 3580
    },
    {
      "epoch": 0.07385330655550955,
      "grad_norm": 0.29542067646980286,
      "learning_rate": 0.00016406857142857144,
      "loss": 2.4721,
      "step": 3590
    },
    {
      "epoch": 0.07405902607237727,
      "grad_norm": 0.26283690333366394,
      "learning_rate": 0.0001645257142857143,
      "loss": 2.4719,
      "step": 3600
    },
    {
      "epoch": 0.07426474558924498,
      "grad_norm": 0.2807503342628479,
      "learning_rate": 0.00016498285714285714,
      "loss": 2.4718,
      "step": 3610
    },
    {
      "epoch": 0.0744704651061127,
      "grad_norm": 0.2650909721851349,
      "learning_rate": 0.00016544000000000002,
      "loss": 2.4578,
      "step": 3620
    },
    {
      "epoch": 0.07467618462298041,
      "grad_norm": 0.27427050471305847,
      "learning_rate": 0.00016589714285714286,
      "loss": 2.4764,
      "step": 3630
    },
    {
      "epoch": 0.07488190413984813,
      "grad_norm": 0.26343828439712524,
      "learning_rate": 0.00016635428571428574,
      "loss": 2.496,
      "step": 3640
    },
    {
      "epoch": 0.07508762365671584,
      "grad_norm": 0.3034794330596924,
      "learning_rate": 0.00016681142857142856,
      "loss": 2.5079,
      "step": 3650
    },
    {
      "epoch": 0.07529334317358356,
      "grad_norm": 0.30238136649131775,
      "learning_rate": 0.00016726857142857144,
      "loss": 2.5164,
      "step": 3660
    },
    {
      "epoch": 0.07549906269045127,
      "grad_norm": 0.2865150272846222,
      "learning_rate": 0.0001677257142857143,
      "loss": 2.5031,
      "step": 3670
    },
    {
      "epoch": 0.07570478220731898,
      "grad_norm": 0.2762262225151062,
      "learning_rate": 0.00016818285714285716,
      "loss": 2.527,
      "step": 3680
    },
    {
      "epoch": 0.0759105017241867,
      "grad_norm": 0.2808479368686676,
      "learning_rate": 0.00016863999999999998,
      "loss": 2.4746,
      "step": 3690
    },
    {
      "epoch": 0.07611622124105441,
      "grad_norm": 0.30186185240745544,
      "learning_rate": 0.00016909714285714286,
      "loss": 2.4718,
      "step": 3700
    },
    {
      "epoch": 0.07632194075792213,
      "grad_norm": 0.26748335361480713,
      "learning_rate": 0.00016955428571428574,
      "loss": 2.5042,
      "step": 3710
    },
    {
      "epoch": 0.07652766027478984,
      "grad_norm": 0.2890310287475586,
      "learning_rate": 0.00017001142857142859,
      "loss": 2.4614,
      "step": 3720
    },
    {
      "epoch": 0.07673337979165756,
      "grad_norm": 0.2664295434951782,
      "learning_rate": 0.00017046857142857143,
      "loss": 2.5255,
      "step": 3730
    },
    {
      "epoch": 0.07693909930852527,
      "grad_norm": 0.28579598665237427,
      "learning_rate": 0.00017092571428571428,
      "loss": 2.5181,
      "step": 3740
    },
    {
      "epoch": 0.077144818825393,
      "grad_norm": 0.2850196063518524,
      "learning_rate": 0.00017138285714285716,
      "loss": 2.4612,
      "step": 3750
    },
    {
      "epoch": 0.0773505383422607,
      "grad_norm": 0.2832065522670746,
      "learning_rate": 0.00017184,
      "loss": 2.5161,
      "step": 3760
    },
    {
      "epoch": 0.07755625785912842,
      "grad_norm": 0.30666056275367737,
      "learning_rate": 0.00017229714285714286,
      "loss": 2.4841,
      "step": 3770
    },
    {
      "epoch": 0.07776197737599613,
      "grad_norm": 0.2658347189426422,
      "learning_rate": 0.0001727542857142857,
      "loss": 2.4865,
      "step": 3780
    },
    {
      "epoch": 0.07796769689286384,
      "grad_norm": 0.2756913900375366,
      "learning_rate": 0.00017321142857142858,
      "loss": 2.5448,
      "step": 3790
    },
    {
      "epoch": 0.07817341640973156,
      "grad_norm": 0.29160574078559875,
      "learning_rate": 0.00017366857142857146,
      "loss": 2.467,
      "step": 3800
    },
    {
      "epoch": 0.07837913592659927,
      "grad_norm": 0.2942343056201935,
      "learning_rate": 0.00017412571428571428,
      "loss": 2.4891,
      "step": 3810
    },
    {
      "epoch": 0.078584855443467,
      "grad_norm": 0.29154112935066223,
      "learning_rate": 0.00017458285714285716,
      "loss": 2.5484,
      "step": 3820
    },
    {
      "epoch": 0.0787905749603347,
      "grad_norm": 0.2936139404773712,
      "learning_rate": 0.00017504,
      "loss": 2.5065,
      "step": 3830
    },
    {
      "epoch": 0.07899629447720243,
      "grad_norm": 0.27387937903404236,
      "learning_rate": 0.00017549714285714288,
      "loss": 2.4747,
      "step": 3840
    },
    {
      "epoch": 0.07920201399407013,
      "grad_norm": 0.2825835049152374,
      "learning_rate": 0.0001759542857142857,
      "loss": 2.4647,
      "step": 3850
    },
    {
      "epoch": 0.07940773351093786,
      "grad_norm": 0.29851970076560974,
      "learning_rate": 0.00017641142857142858,
      "loss": 2.4841,
      "step": 3860
    },
    {
      "epoch": 0.07961345302780556,
      "grad_norm": 0.29297009110450745,
      "learning_rate": 0.00017686857142857143,
      "loss": 2.5553,
      "step": 3870
    },
    {
      "epoch": 0.07981917254467327,
      "grad_norm": 0.2886919677257538,
      "learning_rate": 0.0001773257142857143,
      "loss": 2.4917,
      "step": 3880
    },
    {
      "epoch": 0.080024892061541,
      "grad_norm": 0.2851470410823822,
      "learning_rate": 0.00017778285714285715,
      "loss": 2.4854,
      "step": 3890
    },
    {
      "epoch": 0.0802306115784087,
      "grad_norm": 0.27367934584617615,
      "learning_rate": 0.00017824,
      "loss": 2.4751,
      "step": 3900
    },
    {
      "epoch": 0.08043633109527643,
      "grad_norm": 0.28036734461784363,
      "learning_rate": 0.00017869714285714288,
      "loss": 2.4732,
      "step": 3910
    },
    {
      "epoch": 0.08064205061214413,
      "grad_norm": 0.2776360809803009,
      "learning_rate": 0.00017915428571428573,
      "loss": 2.5235,
      "step": 3920
    },
    {
      "epoch": 0.08084777012901186,
      "grad_norm": 0.29457131028175354,
      "learning_rate": 0.00017961142857142857,
      "loss": 2.4671,
      "step": 3930
    },
    {
      "epoch": 0.08105348964587956,
      "grad_norm": 0.30159154534339905,
      "learning_rate": 0.00018006857142857142,
      "loss": 2.4944,
      "step": 3940
    },
    {
      "epoch": 0.08125920916274729,
      "grad_norm": 0.2919114828109741,
      "learning_rate": 0.0001805257142857143,
      "loss": 2.4794,
      "step": 3950
    },
    {
      "epoch": 0.081464928679615,
      "grad_norm": 0.28134360909461975,
      "learning_rate": 0.00018098285714285715,
      "loss": 2.4986,
      "step": 3960
    },
    {
      "epoch": 0.08167064819648272,
      "grad_norm": 0.3159438371658325,
      "learning_rate": 0.00018144,
      "loss": 2.4595,
      "step": 3970
    },
    {
      "epoch": 0.08187636771335043,
      "grad_norm": 0.27510902285575867,
      "learning_rate": 0.00018189714285714285,
      "loss": 2.5649,
      "step": 3980
    },
    {
      "epoch": 0.08208208723021813,
      "grad_norm": 0.27094557881355286,
      "learning_rate": 0.00018235428571428572,
      "loss": 2.5005,
      "step": 3990
    },
    {
      "epoch": 0.08228780674708586,
      "grad_norm": 0.25233298540115356,
      "learning_rate": 0.0001828114285714286,
      "loss": 2.5007,
      "step": 4000
    },
    {
      "epoch": 0.08249352626395356,
      "grad_norm": 0.2930702269077301,
      "learning_rate": 0.00018326857142857145,
      "loss": 2.4797,
      "step": 4010
    },
    {
      "epoch": 0.08269924578082129,
      "grad_norm": 0.28408506512641907,
      "learning_rate": 0.0001837257142857143,
      "loss": 2.5322,
      "step": 4020
    },
    {
      "epoch": 0.082904965297689,
      "grad_norm": 0.294292151927948,
      "learning_rate": 0.00018418285714285715,
      "loss": 2.4977,
      "step": 4030
    },
    {
      "epoch": 0.08311068481455672,
      "grad_norm": 0.2903966009616852,
      "learning_rate": 0.00018464000000000002,
      "loss": 2.5488,
      "step": 4040
    },
    {
      "epoch": 0.08331640433142443,
      "grad_norm": 0.2858489751815796,
      "learning_rate": 0.00018509714285714287,
      "loss": 2.4609,
      "step": 4050
    },
    {
      "epoch": 0.08352212384829215,
      "grad_norm": 0.2732684910297394,
      "learning_rate": 0.00018555428571428572,
      "loss": 2.5557,
      "step": 4060
    },
    {
      "epoch": 0.08372784336515986,
      "grad_norm": 0.2759654223918915,
      "learning_rate": 0.00018601142857142857,
      "loss": 2.4554,
      "step": 4070
    },
    {
      "epoch": 0.08393356288202757,
      "grad_norm": 0.2920929491519928,
      "learning_rate": 0.00018646857142857144,
      "loss": 2.5109,
      "step": 4080
    },
    {
      "epoch": 0.08413928239889529,
      "grad_norm": 0.3127349615097046,
      "learning_rate": 0.0001869257142857143,
      "loss": 2.5268,
      "step": 4090
    },
    {
      "epoch": 0.084345001915763,
      "grad_norm": 0.26617172360420227,
      "learning_rate": 0.00018738285714285714,
      "loss": 2.4931,
      "step": 4100
    },
    {
      "epoch": 0.08455072143263072,
      "grad_norm": 0.2801278829574585,
      "learning_rate": 0.00018784000000000002,
      "loss": 2.4443,
      "step": 4110
    },
    {
      "epoch": 0.08475644094949843,
      "grad_norm": 0.2908995449542999,
      "learning_rate": 0.00018829714285714287,
      "loss": 2.4775,
      "step": 4120
    },
    {
      "epoch": 0.08496216046636615,
      "grad_norm": 0.2796996533870697,
      "learning_rate": 0.00018875428571428572,
      "loss": 2.4936,
      "step": 4130
    },
    {
      "epoch": 0.08516787998323386,
      "grad_norm": 0.27731963992118835,
      "learning_rate": 0.00018921142857142856,
      "loss": 2.528,
      "step": 4140
    },
    {
      "epoch": 0.08537359950010158,
      "grad_norm": 0.25153815746307373,
      "learning_rate": 0.00018966857142857144,
      "loss": 2.4913,
      "step": 4150
    },
    {
      "epoch": 0.08557931901696929,
      "grad_norm": 0.3463711440563202,
      "learning_rate": 0.0001901257142857143,
      "loss": 2.5264,
      "step": 4160
    },
    {
      "epoch": 0.085785038533837,
      "grad_norm": 0.2833468019962311,
      "learning_rate": 0.00019058285714285717,
      "loss": 2.5034,
      "step": 4170
    },
    {
      "epoch": 0.08599075805070472,
      "grad_norm": 0.28383147716522217,
      "learning_rate": 0.00019104000000000001,
      "loss": 2.5269,
      "step": 4180
    },
    {
      "epoch": 0.08619647756757243,
      "grad_norm": 0.2798682749271393,
      "learning_rate": 0.00019149714285714286,
      "loss": 2.5157,
      "step": 4190
    },
    {
      "epoch": 0.08640219708444015,
      "grad_norm": 0.27863097190856934,
      "learning_rate": 0.00019195428571428574,
      "loss": 2.4902,
      "step": 4200
    },
    {
      "epoch": 0.08660791660130786,
      "grad_norm": 0.30466723442077637,
      "learning_rate": 0.0001924114285714286,
      "loss": 2.5027,
      "step": 4210
    },
    {
      "epoch": 0.08681363611817558,
      "grad_norm": 0.2892232835292816,
      "learning_rate": 0.00019286857142857144,
      "loss": 2.5218,
      "step": 4220
    },
    {
      "epoch": 0.08701935563504329,
      "grad_norm": 0.29690995812416077,
      "learning_rate": 0.00019332571428571429,
      "loss": 2.532,
      "step": 4230
    },
    {
      "epoch": 0.08722507515191101,
      "grad_norm": 0.2810819149017334,
      "learning_rate": 0.00019378285714285716,
      "loss": 2.4925,
      "step": 4240
    },
    {
      "epoch": 0.08743079466877872,
      "grad_norm": 0.2872879207134247,
      "learning_rate": 0.00019424,
      "loss": 2.4804,
      "step": 4250
    },
    {
      "epoch": 0.08763651418564644,
      "grad_norm": 0.27190321683883667,
      "learning_rate": 0.00019469714285714286,
      "loss": 2.504,
      "step": 4260
    },
    {
      "epoch": 0.08784223370251415,
      "grad_norm": 0.2877125144004822,
      "learning_rate": 0.00019515428571428574,
      "loss": 2.5,
      "step": 4270
    },
    {
      "epoch": 0.08804795321938186,
      "grad_norm": 0.31630563735961914,
      "learning_rate": 0.00019561142857142858,
      "loss": 2.5002,
      "step": 4280
    },
    {
      "epoch": 0.08825367273624958,
      "grad_norm": 0.29502689838409424,
      "learning_rate": 0.00019606857142857143,
      "loss": 2.4988,
      "step": 4290
    },
    {
      "epoch": 0.08845939225311729,
      "grad_norm": 0.2874017059803009,
      "learning_rate": 0.00019652571428571428,
      "loss": 2.5437,
      "step": 4300
    },
    {
      "epoch": 0.08866511176998501,
      "grad_norm": 0.277056485414505,
      "learning_rate": 0.00019698285714285716,
      "loss": 2.4968,
      "step": 4310
    },
    {
      "epoch": 0.08887083128685272,
      "grad_norm": 0.2601991593837738,
      "learning_rate": 0.00019744,
      "loss": 2.4819,
      "step": 4320
    },
    {
      "epoch": 0.08907655080372044,
      "grad_norm": 0.2708156406879425,
      "learning_rate": 0.00019789714285714288,
      "loss": 2.4798,
      "step": 4330
    },
    {
      "epoch": 0.08928227032058815,
      "grad_norm": 0.26316797733306885,
      "learning_rate": 0.0001983542857142857,
      "loss": 2.474,
      "step": 4340
    },
    {
      "epoch": 0.08948798983745587,
      "grad_norm": 0.2820214629173279,
      "learning_rate": 0.00019881142857142858,
      "loss": 2.4713,
      "step": 4350
    },
    {
      "epoch": 0.08969370935432358,
      "grad_norm": 0.2830589711666107,
      "learning_rate": 0.00019926857142857146,
      "loss": 2.4948,
      "step": 4360
    },
    {
      "epoch": 0.08989942887119129,
      "grad_norm": 0.3005788326263428,
      "learning_rate": 0.0001997257142857143,
      "loss": 2.4613,
      "step": 4370
    },
    {
      "epoch": 0.09010514838805901,
      "grad_norm": 0.3000292181968689,
      "learning_rate": 0.0001999999996054036,
      "loss": 2.4317,
      "step": 4380
    },
    {
      "epoch": 0.09031086790492672,
      "grad_norm": 0.2702089846134186,
      "learning_rate": 0.00019999999516619407,
      "loss": 2.482,
      "step": 4390
    },
    {
      "epoch": 0.09051658742179444,
      "grad_norm": 0.2741881012916565,
      "learning_rate": 0.00019999998579452969,
      "loss": 2.4742,
      "step": 4400
    },
    {
      "epoch": 0.09072230693866215,
      "grad_norm": 0.289101243019104,
      "learning_rate": 0.00019999997149041095,
      "loss": 2.5159,
      "step": 4410
    },
    {
      "epoch": 0.09092802645552987,
      "grad_norm": 0.26975709199905396,
      "learning_rate": 0.00019999995225383857,
      "loss": 2.5108,
      "step": 4420
    },
    {
      "epoch": 0.09113374597239758,
      "grad_norm": 0.27912136912345886,
      "learning_rate": 0.0001999999280848135,
      "loss": 2.5182,
      "step": 4430
    },
    {
      "epoch": 0.0913394654892653,
      "grad_norm": 0.3341665267944336,
      "learning_rate": 0.00019999989898333688,
      "loss": 2.4495,
      "step": 4440
    },
    {
      "epoch": 0.09154518500613301,
      "grad_norm": 0.28804633021354675,
      "learning_rate": 0.00019999986494941016,
      "loss": 2.4787,
      "step": 4450
    },
    {
      "epoch": 0.09175090452300073,
      "grad_norm": 0.3119073212146759,
      "learning_rate": 0.00019999982598303506,
      "loss": 2.5603,
      "step": 4460
    },
    {
      "epoch": 0.09195662403986844,
      "grad_norm": 0.33251240849494934,
      "learning_rate": 0.0001999997820842135,
      "loss": 2.4635,
      "step": 4470
    },
    {
      "epoch": 0.09216234355673615,
      "grad_norm": 0.284945547580719,
      "learning_rate": 0.00019999973325294756,
      "loss": 2.5288,
      "step": 4480
    },
    {
      "epoch": 0.09236806307360387,
      "grad_norm": 0.2693864405155182,
      "learning_rate": 0.00019999967948923977,
      "loss": 2.4503,
      "step": 4490
    },
    {
      "epoch": 0.09257378259047158,
      "grad_norm": 0.2929675579071045,
      "learning_rate": 0.00019999962079309268,
      "loss": 2.4672,
      "step": 4500
    },
    {
      "epoch": 0.0927795021073393,
      "grad_norm": 0.28846657276153564,
      "learning_rate": 0.00019999955716450927,
      "loss": 2.4888,
      "step": 4510
    },
    {
      "epoch": 0.09298522162420701,
      "grad_norm": 0.2941029667854309,
      "learning_rate": 0.0001999994886034926,
      "loss": 2.485,
      "step": 4520
    },
    {
      "epoch": 0.09319094114107473,
      "grad_norm": 0.2909509837627411,
      "learning_rate": 0.00019999941511004613,
      "loss": 2.5305,
      "step": 4530
    },
    {
      "epoch": 0.09339666065794244,
      "grad_norm": 0.28280314803123474,
      "learning_rate": 0.0001999993366841734,
      "loss": 2.5071,
      "step": 4540
    },
    {
      "epoch": 0.09360238017481017,
      "grad_norm": 0.28539004921913147,
      "learning_rate": 0.00019999925332587836,
      "loss": 2.5334,
      "step": 4550
    },
    {
      "epoch": 0.09380809969167787,
      "grad_norm": 0.2890188992023468,
      "learning_rate": 0.00019999916503516505,
      "loss": 2.4787,
      "step": 4560
    },
    {
      "epoch": 0.09401381920854558,
      "grad_norm": 0.2893696129322052,
      "learning_rate": 0.00019999907181203786,
      "loss": 2.4672,
      "step": 4570
    },
    {
      "epoch": 0.0942195387254133,
      "grad_norm": 0.28504014015197754,
      "learning_rate": 0.00019999897365650143,
      "loss": 2.5084,
      "step": 4580
    },
    {
      "epoch": 0.09442525824228101,
      "grad_norm": 0.2986901104450226,
      "learning_rate": 0.00019999887056856052,
      "loss": 2.5238,
      "step": 4590
    },
    {
      "epoch": 0.09463097775914873,
      "grad_norm": 0.3063555955886841,
      "learning_rate": 0.00019999876254822027,
      "loss": 2.495,
      "step": 4600
    },
    {
      "epoch": 0.09483669727601644,
      "grad_norm": 0.2780039608478546,
      "learning_rate": 0.000199998649595486,
      "loss": 2.4483,
      "step": 4610
    },
    {
      "epoch": 0.09504241679288417,
      "grad_norm": 0.3055119514465332,
      "learning_rate": 0.00019999853171036324,
      "loss": 2.5203,
      "step": 4620
    },
    {
      "epoch": 0.09524813630975187,
      "grad_norm": 0.2866416573524475,
      "learning_rate": 0.0001999984088928579,
      "loss": 2.518,
      "step": 4630
    },
    {
      "epoch": 0.0954538558266196,
      "grad_norm": 0.27735087275505066,
      "learning_rate": 0.00019999828114297593,
      "loss": 2.4702,
      "step": 4640
    },
    {
      "epoch": 0.0956595753434873,
      "grad_norm": 0.2915988862514496,
      "learning_rate": 0.00019999814846072367,
      "loss": 2.4833,
      "step": 4650
    },
    {
      "epoch": 0.09586529486035503,
      "grad_norm": 0.2598992884159088,
      "learning_rate": 0.00019999801084610765,
      "loss": 2.4785,
      "step": 4660
    },
    {
      "epoch": 0.09607101437722274,
      "grad_norm": 0.27690762281417847,
      "learning_rate": 0.0001999978682991347,
      "loss": 2.4882,
      "step": 4670
    },
    {
      "epoch": 0.09627673389409044,
      "grad_norm": 0.2688412666320801,
      "learning_rate": 0.00019999772081981187,
      "loss": 2.4803,
      "step": 4680
    },
    {
      "epoch": 0.09648245341095817,
      "grad_norm": 0.2937212288379669,
      "learning_rate": 0.00019999756840814636,
      "loss": 2.4961,
      "step": 4690
    },
    {
      "epoch": 0.09668817292782587,
      "grad_norm": 0.2911609411239624,
      "learning_rate": 0.00019999741106414574,
      "loss": 2.5076,
      "step": 4700
    },
    {
      "epoch": 0.0968938924446936,
      "grad_norm": 0.31240203976631165,
      "learning_rate": 0.0001999972487878177,
      "loss": 2.4319,
      "step": 4710
    },
    {
      "epoch": 0.0970996119615613,
      "grad_norm": 0.31680214405059814,
      "learning_rate": 0.00019999708157917033,
      "loss": 2.5131,
      "step": 4720
    },
    {
      "epoch": 0.09730533147842903,
      "grad_norm": 0.2828815281391144,
      "learning_rate": 0.00019999690943821189,
      "loss": 2.4892,
      "step": 4730
    },
    {
      "epoch": 0.09751105099529674,
      "grad_norm": 0.2775101661682129,
      "learning_rate": 0.00019999673236495075,
      "loss": 2.4661,
      "step": 4740
    },
    {
      "epoch": 0.09771677051216446,
      "grad_norm": 0.2804066836833954,
      "learning_rate": 0.00019999655035939578,
      "loss": 2.5013,
      "step": 4750
    },
    {
      "epoch": 0.09792249002903217,
      "grad_norm": 0.3046751618385315,
      "learning_rate": 0.00019999636342155584,
      "loss": 2.5606,
      "step": 4760
    },
    {
      "epoch": 0.09812820954589987,
      "grad_norm": 0.29198846220970154,
      "learning_rate": 0.00019999617155144026,
      "loss": 2.4885,
      "step": 4770
    },
    {
      "epoch": 0.0983339290627676,
      "grad_norm": 0.30743440985679626,
      "learning_rate": 0.00019999597474905842,
      "loss": 2.5046,
      "step": 4780
    },
    {
      "epoch": 0.0985396485796353,
      "grad_norm": 0.2696041166782379,
      "learning_rate": 0.00019999577301442006,
      "loss": 2.4988,
      "step": 4790
    },
    {
      "epoch": 0.09874536809650303,
      "grad_norm": 0.3048139810562134,
      "learning_rate": 0.00019999556634753515,
      "loss": 2.5202,
      "step": 4800
    },
    {
      "epoch": 0.09895108761337074,
      "grad_norm": 0.290265828371048,
      "learning_rate": 0.00019999535474841387,
      "loss": 2.466,
      "step": 4810
    },
    {
      "epoch": 0.09915680713023846,
      "grad_norm": 0.2748427093029022,
      "learning_rate": 0.0001999951382170666,
      "loss": 2.4911,
      "step": 4820
    },
    {
      "epoch": 0.09936252664710617,
      "grad_norm": 0.267113596200943,
      "learning_rate": 0.0001999949167535041,
      "loss": 2.4951,
      "step": 4830
    },
    {
      "epoch": 0.09956824616397389,
      "grad_norm": 0.26458287239074707,
      "learning_rate": 0.00019999469035773725,
      "loss": 2.5061,
      "step": 4840
    },
    {
      "epoch": 0.0997739656808416,
      "grad_norm": 0.28982317447662354,
      "learning_rate": 0.00019999445902977727,
      "loss": 2.4812,
      "step": 4850
    },
    {
      "epoch": 0.09997968519770932,
      "grad_norm": 0.2837985157966614,
      "learning_rate": 0.0001999942227696355,
      "loss": 2.4899,
      "step": 4860
    },
    {
      "epoch": 0.10018540471457703,
      "grad_norm": 0.2926451563835144,
      "learning_rate": 0.0001999939815773236,
      "loss": 2.5272,
      "step": 4870
    },
    {
      "epoch": 0.10039112423144474,
      "grad_norm": 0.27409160137176514,
      "learning_rate": 0.00019999373545285355,
      "loss": 2.4984,
      "step": 4880
    },
    {
      "epoch": 0.10059684374831246,
      "grad_norm": 0.27970609068870544,
      "learning_rate": 0.0001999934843962374,
      "loss": 2.5208,
      "step": 4890
    },
    {
      "epoch": 0.10080256326518017,
      "grad_norm": 0.2861156761646271,
      "learning_rate": 0.00019999322840748757,
      "loss": 2.5263,
      "step": 4900
    },
    {
      "epoch": 0.10100828278204789,
      "grad_norm": 0.3072018325328827,
      "learning_rate": 0.00019999296748661664,
      "loss": 2.473,
      "step": 4910
    },
    {
      "epoch": 0.1012140022989156,
      "grad_norm": 0.30679571628570557,
      "learning_rate": 0.00019999270163363755,
      "loss": 2.4663,
      "step": 4920
    },
    {
      "epoch": 0.10141972181578332,
      "grad_norm": 0.2993003726005554,
      "learning_rate": 0.00019999243084856342,
      "loss": 2.5234,
      "step": 4930
    },
    {
      "epoch": 0.10162544133265103,
      "grad_norm": 0.2710575461387634,
      "learning_rate": 0.0001999921551314075,
      "loss": 2.4759,
      "step": 4940
    },
    {
      "epoch": 0.10183116084951875,
      "grad_norm": 0.31503385305404663,
      "learning_rate": 0.0001999918744821835,
      "loss": 2.5044,
      "step": 4950
    },
    {
      "epoch": 0.10203688036638646,
      "grad_norm": 0.28193020820617676,
      "learning_rate": 0.0001999915889009052,
      "loss": 2.437,
      "step": 4960
    },
    {
      "epoch": 0.10224259988325417,
      "grad_norm": 0.2875705063343048,
      "learning_rate": 0.00019999129838758675,
      "loss": 2.4501,
      "step": 4970
    },
    {
      "epoch": 0.10244831940012189,
      "grad_norm": 0.2630259096622467,
      "learning_rate": 0.0001999910029422424,
      "loss": 2.499,
      "step": 4980
    },
    {
      "epoch": 0.1026540389169896,
      "grad_norm": 0.30563682317733765,
      "learning_rate": 0.0001999907025648868,
      "loss": 2.4882,
      "step": 4990
    },
    {
      "epoch": 0.10285975843385732,
      "grad_norm": 0.3111537992954254,
      "learning_rate": 0.0001999903972555347,
      "loss": 2.5102,
      "step": 5000
    },
    {
      "epoch": 0.10306547795072503,
      "grad_norm": 0.3067263662815094,
      "learning_rate": 0.00019999008701420117,
      "loss": 2.5011,
      "step": 5010
    },
    {
      "epoch": 0.10327119746759275,
      "grad_norm": 0.2832622826099396,
      "learning_rate": 0.00019998977184090152,
      "loss": 2.5292,
      "step": 5020
    },
    {
      "epoch": 0.10347691698446046,
      "grad_norm": 0.30312907695770264,
      "learning_rate": 0.00019998945173565135,
      "loss": 2.4994,
      "step": 5030
    },
    {
      "epoch": 0.10368263650132818,
      "grad_norm": 0.2742149233818054,
      "learning_rate": 0.00019998912669846636,
      "loss": 2.4641,
      "step": 5040
    },
    {
      "epoch": 0.10388835601819589,
      "grad_norm": 0.27944695949554443,
      "learning_rate": 0.00019998879672936267,
      "loss": 2.4927,
      "step": 5050
    },
    {
      "epoch": 0.10409407553506361,
      "grad_norm": 0.3261592388153076,
      "learning_rate": 0.00019998846182835646,
      "loss": 2.5031,
      "step": 5060
    },
    {
      "epoch": 0.10429979505193132,
      "grad_norm": 0.3439941108226776,
      "learning_rate": 0.00019998812199546433,
      "loss": 2.4793,
      "step": 5070
    },
    {
      "epoch": 0.10450551456879903,
      "grad_norm": 0.3141939043998718,
      "learning_rate": 0.00019998777723070302,
      "loss": 2.4899,
      "step": 5080
    },
    {
      "epoch": 0.10471123408566675,
      "grad_norm": 0.28504422307014465,
      "learning_rate": 0.00019998742753408953,
      "loss": 2.5041,
      "step": 5090
    },
    {
      "epoch": 0.10491695360253446,
      "grad_norm": 0.2762451171875,
      "learning_rate": 0.00019998707290564107,
      "loss": 2.4774,
      "step": 5100
    },
    {
      "epoch": 0.10512267311940218,
      "grad_norm": 0.275478333234787,
      "learning_rate": 0.0001999867133453752,
      "loss": 2.4426,
      "step": 5110
    },
    {
      "epoch": 0.10532839263626989,
      "grad_norm": 0.2743707001209259,
      "learning_rate": 0.0001999863488533096,
      "loss": 2.4918,
      "step": 5120
    },
    {
      "epoch": 0.10553411215313761,
      "grad_norm": 0.2939511239528656,
      "learning_rate": 0.0001999859794294623,
      "loss": 2.5436,
      "step": 5130
    },
    {
      "epoch": 0.10573983167000532,
      "grad_norm": 0.27073192596435547,
      "learning_rate": 0.00019998560507385147,
      "loss": 2.4996,
      "step": 5140
    },
    {
      "epoch": 0.10594555118687304,
      "grad_norm": 0.29401910305023193,
      "learning_rate": 0.00019998522578649558,
      "loss": 2.465,
      "step": 5150
    },
    {
      "epoch": 0.10615127070374075,
      "grad_norm": 0.2873281240463257,
      "learning_rate": 0.0001999848415674134,
      "loss": 2.574,
      "step": 5160
    },
    {
      "epoch": 0.10635699022060846,
      "grad_norm": 0.29377496242523193,
      "learning_rate": 0.0001999844524166238,
      "loss": 2.4952,
      "step": 5170
    },
    {
      "epoch": 0.10656270973747618,
      "grad_norm": 0.29201969504356384,
      "learning_rate": 0.00019998405833414603,
      "loss": 2.4822,
      "step": 5180
    },
    {
      "epoch": 0.10676842925434389,
      "grad_norm": 0.3198201060295105,
      "learning_rate": 0.00019998365931999952,
      "loss": 2.5248,
      "step": 5190
    },
    {
      "epoch": 0.10697414877121161,
      "grad_norm": 0.2867969572544098,
      "learning_rate": 0.00019998325537420395,
      "loss": 2.5248,
      "step": 5200
    },
    {
      "epoch": 0.10717986828807932,
      "grad_norm": 0.2764671742916107,
      "learning_rate": 0.00019998284649677922,
      "loss": 2.4782,
      "step": 5210
    },
    {
      "epoch": 0.10738558780494704,
      "grad_norm": 0.2789788842201233,
      "learning_rate": 0.0001999824326877455,
      "loss": 2.4891,
      "step": 5220
    },
    {
      "epoch": 0.10759130732181475,
      "grad_norm": 0.2795247435569763,
      "learning_rate": 0.0001999820139471232,
      "loss": 2.553,
      "step": 5230
    },
    {
      "epoch": 0.10779702683868247,
      "grad_norm": 0.27721837162971497,
      "learning_rate": 0.00019998159027493303,
      "loss": 2.4779,
      "step": 5240
    },
    {
      "epoch": 0.10800274635555018,
      "grad_norm": 0.28234630823135376,
      "learning_rate": 0.00019998116167119583,
      "loss": 2.4361,
      "step": 5250
    },
    {
      "epoch": 0.1082084658724179,
      "grad_norm": 0.26444920897483826,
      "learning_rate": 0.00019998072813593275,
      "loss": 2.5235,
      "step": 5260
    },
    {
      "epoch": 0.10841418538928561,
      "grad_norm": 0.29694098234176636,
      "learning_rate": 0.0001999802896691652,
      "loss": 2.4678,
      "step": 5270
    },
    {
      "epoch": 0.10861990490615332,
      "grad_norm": 0.29430505633354187,
      "learning_rate": 0.00019997984627091479,
      "loss": 2.5235,
      "step": 5280
    },
    {
      "epoch": 0.10882562442302104,
      "grad_norm": 0.27114707231521606,
      "learning_rate": 0.00019997939794120338,
      "loss": 2.4816,
      "step": 5290
    },
    {
      "epoch": 0.10903134393988875,
      "grad_norm": 0.25943246483802795,
      "learning_rate": 0.00019997894468005306,
      "loss": 2.4596,
      "step": 5300
    },
    {
      "epoch": 0.10923706345675648,
      "grad_norm": 0.2823009788990021,
      "learning_rate": 0.00019997848648748624,
      "loss": 2.5544,
      "step": 5310
    },
    {
      "epoch": 0.10944278297362418,
      "grad_norm": 0.29837632179260254,
      "learning_rate": 0.0001999780233635255,
      "loss": 2.4837,
      "step": 5320
    },
    {
      "epoch": 0.1096485024904919,
      "grad_norm": 0.300676554441452,
      "learning_rate": 0.0001999775553081937,
      "loss": 2.4278,
      "step": 5330
    },
    {
      "epoch": 0.10985422200735961,
      "grad_norm": 0.29591262340545654,
      "learning_rate": 0.00019997708232151392,
      "loss": 2.4953,
      "step": 5340
    },
    {
      "epoch": 0.11005994152422734,
      "grad_norm": 0.293526291847229,
      "learning_rate": 0.00019997660440350943,
      "loss": 2.4383,
      "step": 5350
    },
    {
      "epoch": 0.11026566104109504,
      "grad_norm": 0.2781655788421631,
      "learning_rate": 0.0001999761215542039,
      "loss": 2.5156,
      "step": 5360
    },
    {
      "epoch": 0.11047138055796275,
      "grad_norm": 0.27447113394737244,
      "learning_rate": 0.00019997563377362104,
      "loss": 2.4761,
      "step": 5370
    },
    {
      "epoch": 0.11067710007483048,
      "grad_norm": 0.3188239336013794,
      "learning_rate": 0.000199975141061785,
      "loss": 2.5085,
      "step": 5380
    },
    {
      "epoch": 0.11088281959169818,
      "grad_norm": 0.27870145440101624,
      "learning_rate": 0.00019997464341872006,
      "loss": 2.4602,
      "step": 5390
    },
    {
      "epoch": 0.1110885391085659,
      "grad_norm": 0.3097772002220154,
      "learning_rate": 0.00019997414084445075,
      "loss": 2.4397,
      "step": 5400
    },
    {
      "epoch": 0.11129425862543361,
      "grad_norm": 0.2843853831291199,
      "learning_rate": 0.00019997363333900187,
      "loss": 2.5795,
      "step": 5410
    },
    {
      "epoch": 0.11149997814230134,
      "grad_norm": 0.30155065655708313,
      "learning_rate": 0.00019997312090239844,
      "loss": 2.492,
      "step": 5420
    },
    {
      "epoch": 0.11170569765916905,
      "grad_norm": 0.30697762966156006,
      "learning_rate": 0.00019997260353466577,
      "loss": 2.4849,
      "step": 5430
    },
    {
      "epoch": 0.11191141717603677,
      "grad_norm": 0.34407520294189453,
      "learning_rate": 0.00019997208123582932,
      "loss": 2.4565,
      "step": 5440
    },
    {
      "epoch": 0.11211713669290448,
      "grad_norm": 0.3206419050693512,
      "learning_rate": 0.00019997155400591486,
      "loss": 2.4637,
      "step": 5450
    },
    {
      "epoch": 0.11232285620977218,
      "grad_norm": 0.27109575271606445,
      "learning_rate": 0.00019997102184494847,
      "loss": 2.5141,
      "step": 5460
    },
    {
      "epoch": 0.1125285757266399,
      "grad_norm": 0.29726138710975647,
      "learning_rate": 0.00019997048475295633,
      "loss": 2.5084,
      "step": 5470
    },
    {
      "epoch": 0.11273429524350762,
      "grad_norm": 0.27628740668296814,
      "learning_rate": 0.00019996994272996496,
      "loss": 2.4755,
      "step": 5480
    },
    {
      "epoch": 0.11294001476037534,
      "grad_norm": 0.28542080521583557,
      "learning_rate": 0.0001999693957760011,
      "loss": 2.4582,
      "step": 5490
    },
    {
      "epoch": 0.11314573427724305,
      "grad_norm": 0.2805195748806,
      "learning_rate": 0.00019996884389109168,
      "loss": 2.4406,
      "step": 5500
    },
    {
      "epoch": 0.11335145379411077,
      "grad_norm": 0.2929190695285797,
      "learning_rate": 0.00019996828707526397,
      "loss": 2.5197,
      "step": 5510
    },
    {
      "epoch": 0.11355717331097848,
      "grad_norm": 0.28968489170074463,
      "learning_rate": 0.0001999677253285454,
      "loss": 2.4711,
      "step": 5520
    },
    {
      "epoch": 0.1137628928278462,
      "grad_norm": 0.2867022454738617,
      "learning_rate": 0.00019996715865096376,
      "loss": 2.465,
      "step": 5530
    },
    {
      "epoch": 0.11396861234471391,
      "grad_norm": 0.2743290960788727,
      "learning_rate": 0.0001999665870425469,
      "loss": 2.4828,
      "step": 5540
    },
    {
      "epoch": 0.11417433186158163,
      "grad_norm": 0.29475846886634827,
      "learning_rate": 0.00019996601050332305,
      "loss": 2.4584,
      "step": 5550
    },
    {
      "epoch": 0.11438005137844934,
      "grad_norm": 0.3167267441749573,
      "learning_rate": 0.00019996542903332064,
      "loss": 2.501,
      "step": 5560
    },
    {
      "epoch": 0.11458577089531705,
      "grad_norm": 0.28600889444351196,
      "learning_rate": 0.0001999648426325684,
      "loss": 2.488,
      "step": 5570
    },
    {
      "epoch": 0.11479149041218477,
      "grad_norm": 0.29743456840515137,
      "learning_rate": 0.00019996425130109518,
      "loss": 2.4448,
      "step": 5580
    },
    {
      "epoch": 0.11499720992905248,
      "grad_norm": 0.2961181104183197,
      "learning_rate": 0.00019996365503893018,
      "loss": 2.4648,
      "step": 5590
    },
    {
      "epoch": 0.1152029294459202,
      "grad_norm": 0.33187878131866455,
      "learning_rate": 0.00019996305384610287,
      "loss": 2.4729,
      "step": 5600
    },
    {
      "epoch": 0.11540864896278791,
      "grad_norm": 0.2715202271938324,
      "learning_rate": 0.00019996244772264277,
      "loss": 2.4453,
      "step": 5610
    },
    {
      "epoch": 0.11561436847965563,
      "grad_norm": 0.31118297576904297,
      "learning_rate": 0.0001999618366685799,
      "loss": 2.5316,
      "step": 5620
    },
    {
      "epoch": 0.11582008799652334,
      "grad_norm": 0.29448410868644714,
      "learning_rate": 0.00019996122068394434,
      "loss": 2.4657,
      "step": 5630
    },
    {
      "epoch": 0.11602580751339106,
      "grad_norm": 0.2999609112739563,
      "learning_rate": 0.00019996059976876647,
      "loss": 2.5268,
      "step": 5640
    },
    {
      "epoch": 0.11623152703025877,
      "grad_norm": 0.2863084673881531,
      "learning_rate": 0.000199959973923077,
      "loss": 2.4857,
      "step": 5650
    },
    {
      "epoch": 0.11643724654712648,
      "grad_norm": 0.2877851724624634,
      "learning_rate": 0.00019995934314690668,
      "loss": 2.4762,
      "step": 5660
    },
    {
      "epoch": 0.1166429660639942,
      "grad_norm": 0.30989664793014526,
      "learning_rate": 0.00019995870744028666,
      "loss": 2.5285,
      "step": 5670
    },
    {
      "epoch": 0.11684868558086191,
      "grad_norm": 0.2645857036113739,
      "learning_rate": 0.00019995806680324834,
      "loss": 2.4627,
      "step": 5680
    },
    {
      "epoch": 0.11705440509772963,
      "grad_norm": 0.29828914999961853,
      "learning_rate": 0.00019995742123582328,
      "loss": 2.5148,
      "step": 5690
    },
    {
      "epoch": 0.11726012461459734,
      "grad_norm": 0.29475137591362,
      "learning_rate": 0.00019995677073804334,
      "loss": 2.502,
      "step": 5700
    },
    {
      "epoch": 0.11746584413146506,
      "grad_norm": 0.3400906026363373,
      "learning_rate": 0.0001999561153099406,
      "loss": 2.465,
      "step": 5710
    },
    {
      "epoch": 0.11767156364833277,
      "grad_norm": 0.30159610509872437,
      "learning_rate": 0.0001999554549515474,
      "loss": 2.4581,
      "step": 5720
    },
    {
      "epoch": 0.11787728316520049,
      "grad_norm": 0.29541608691215515,
      "learning_rate": 0.00019995478966289626,
      "loss": 2.5393,
      "step": 5730
    },
    {
      "epoch": 0.1180830026820682,
      "grad_norm": 0.29485848546028137,
      "learning_rate": 0.00019995411944402007,
      "loss": 2.4335,
      "step": 5740
    },
    {
      "epoch": 0.11828872219893592,
      "grad_norm": 0.3224409818649292,
      "learning_rate": 0.00019995344429495185,
      "loss": 2.5004,
      "step": 5750
    },
    {
      "epoch": 0.11849444171580363,
      "grad_norm": 0.28778716921806335,
      "learning_rate": 0.0001999527642157249,
      "loss": 2.4821,
      "step": 5760
    },
    {
      "epoch": 0.11870016123267134,
      "grad_norm": 0.30364808440208435,
      "learning_rate": 0.00019995207920637276,
      "loss": 2.5015,
      "step": 5770
    },
    {
      "epoch": 0.11890588074953906,
      "grad_norm": 0.2982844114303589,
      "learning_rate": 0.00019995138926692923,
      "loss": 2.5258,
      "step": 5780
    },
    {
      "epoch": 0.11911160026640677,
      "grad_norm": 0.30481472611427307,
      "learning_rate": 0.00019995069439742832,
      "loss": 2.4523,
      "step": 5790
    },
    {
      "epoch": 0.11931731978327449,
      "grad_norm": 0.30174118280410767,
      "learning_rate": 0.00019994999459790433,
      "loss": 2.4869,
      "step": 5800
    },
    {
      "epoch": 0.1195230393001422,
      "grad_norm": 0.30267995595932007,
      "learning_rate": 0.0001999492898683918,
      "loss": 2.4844,
      "step": 5810
    },
    {
      "epoch": 0.11972875881700992,
      "grad_norm": 0.31743836402893066,
      "learning_rate": 0.00019994858020892542,
      "loss": 2.5139,
      "step": 5820
    },
    {
      "epoch": 0.11993447833387763,
      "grad_norm": 0.31289437413215637,
      "learning_rate": 0.00019994786561954025,
      "loss": 2.4874,
      "step": 5830
    },
    {
      "epoch": 0.12014019785074535,
      "grad_norm": 0.2979384660720825,
      "learning_rate": 0.00019994714610027152,
      "loss": 2.4821,
      "step": 5840
    },
    {
      "epoch": 0.12034591736761306,
      "grad_norm": 0.29282811284065247,
      "learning_rate": 0.00019994642165115472,
      "loss": 2.5225,
      "step": 5850
    },
    {
      "epoch": 0.12055163688448077,
      "grad_norm": 0.3042233884334564,
      "learning_rate": 0.00019994569227222556,
      "loss": 2.5015,
      "step": 5860
    },
    {
      "epoch": 0.12075735640134849,
      "grad_norm": 0.293711394071579,
      "learning_rate": 0.00019994495796352006,
      "loss": 2.4892,
      "step": 5870
    },
    {
      "epoch": 0.1209630759182162,
      "grad_norm": 0.3397440016269684,
      "learning_rate": 0.00019994421872507442,
      "loss": 2.4307,
      "step": 5880
    },
    {
      "epoch": 0.12116879543508392,
      "grad_norm": 0.2998906970024109,
      "learning_rate": 0.0001999434745569251,
      "loss": 2.4942,
      "step": 5890
    },
    {
      "epoch": 0.12137451495195163,
      "grad_norm": 0.2917743921279907,
      "learning_rate": 0.00019994272545910882,
      "loss": 2.5015,
      "step": 5900
    },
    {
      "epoch": 0.12158023446881935,
      "grad_norm": 0.2894555330276489,
      "learning_rate": 0.0001999419714316625,
      "loss": 2.4782,
      "step": 5910
    },
    {
      "epoch": 0.12178595398568706,
      "grad_norm": 0.33137935400009155,
      "learning_rate": 0.00019994121247462335,
      "loss": 2.4924,
      "step": 5920
    },
    {
      "epoch": 0.12199167350255478,
      "grad_norm": 0.31057170033454895,
      "learning_rate": 0.0001999404485880288,
      "loss": 2.4744,
      "step": 5930
    },
    {
      "epoch": 0.12219739301942249,
      "grad_norm": 0.2957899570465088,
      "learning_rate": 0.00019993967977191652,
      "loss": 2.5424,
      "step": 5940
    },
    {
      "epoch": 0.12240311253629022,
      "grad_norm": 0.27041205763816833,
      "learning_rate": 0.00019993890602632445,
      "loss": 2.5424,
      "step": 5950
    },
    {
      "epoch": 0.12260883205315792,
      "grad_norm": 0.3272102475166321,
      "learning_rate": 0.00019993812735129076,
      "loss": 2.4834,
      "step": 5960
    },
    {
      "epoch": 0.12281455157002563,
      "grad_norm": 0.3025193214416504,
      "learning_rate": 0.00019993734374685384,
      "loss": 2.4965,
      "step": 5970
    },
    {
      "epoch": 0.12302027108689335,
      "grad_norm": 0.3073796331882477,
      "learning_rate": 0.00019993655521305235,
      "loss": 2.505,
      "step": 5980
    },
    {
      "epoch": 0.12322599060376106,
      "grad_norm": 0.30695533752441406,
      "learning_rate": 0.00019993576174992517,
      "loss": 2.4657,
      "step": 5990
    },
    {
      "epoch": 0.12343171012062878,
      "grad_norm": 0.3182266652584076,
      "learning_rate": 0.00019993496335751144,
      "loss": 2.4988,
      "step": 6000
    },
    {
      "epoch": 0.1236374296374965,
      "grad_norm": 0.32149094343185425,
      "learning_rate": 0.00019993416003585057,
      "loss": 2.4793,
      "step": 6010
    },
    {
      "epoch": 0.12384314915436422,
      "grad_norm": 0.27660056948661804,
      "learning_rate": 0.00019993335178498212,
      "loss": 2.4553,
      "step": 6020
    },
    {
      "epoch": 0.12404886867123192,
      "grad_norm": 0.2998429238796234,
      "learning_rate": 0.00019993253860494607,
      "loss": 2.4662,
      "step": 6030
    },
    {
      "epoch": 0.12425458818809965,
      "grad_norm": 0.28654491901397705,
      "learning_rate": 0.00019993172049578243,
      "loss": 2.4852,
      "step": 6040
    },
    {
      "epoch": 0.12446030770496735,
      "grad_norm": 0.29575103521347046,
      "learning_rate": 0.00019993089745753154,
      "loss": 2.5086,
      "step": 6050
    },
    {
      "epoch": 0.12466602722183506,
      "grad_norm": 0.30059096217155457,
      "learning_rate": 0.00019993006949023406,
      "loss": 2.5189,
      "step": 6060
    },
    {
      "epoch": 0.12487174673870279,
      "grad_norm": 0.302376925945282,
      "learning_rate": 0.0001999292365939308,
      "loss": 2.5187,
      "step": 6070
    },
    {
      "epoch": 0.1250774662555705,
      "grad_norm": 0.2912459373474121,
      "learning_rate": 0.00019992839876866288,
      "loss": 2.4908,
      "step": 6080
    },
    {
      "epoch": 0.12528318577243822,
      "grad_norm": 0.2944110631942749,
      "learning_rate": 0.00019992755601447155,
      "loss": 2.4817,
      "step": 6090
    },
    {
      "epoch": 0.12548890528930592,
      "grad_norm": 0.3088199496269226,
      "learning_rate": 0.00019992670833139843,
      "loss": 2.5526,
      "step": 6100
    },
    {
      "epoch": 0.12569462480617363,
      "grad_norm": 0.33321723341941833,
      "learning_rate": 0.00019992585571948536,
      "loss": 2.5065,
      "step": 6110
    },
    {
      "epoch": 0.12590034432304137,
      "grad_norm": 0.2965850234031677,
      "learning_rate": 0.00019992499817877432,
      "loss": 2.5096,
      "step": 6120
    },
    {
      "epoch": 0.12610606383990908,
      "grad_norm": 0.302975058555603,
      "learning_rate": 0.00019992413570930765,
      "loss": 2.4664,
      "step": 6130
    },
    {
      "epoch": 0.12631178335677679,
      "grad_norm": 0.33284443616867065,
      "learning_rate": 0.00019992326831112792,
      "loss": 2.5092,
      "step": 6140
    },
    {
      "epoch": 0.1265175028736445,
      "grad_norm": 0.3048298954963684,
      "learning_rate": 0.00019992239598427784,
      "loss": 2.5114,
      "step": 6150
    },
    {
      "epoch": 0.1267232223905122,
      "grad_norm": 0.2901105284690857,
      "learning_rate": 0.0001999215187288005,
      "loss": 2.4771,
      "step": 6160
    },
    {
      "epoch": 0.12692894190737994,
      "grad_norm": 0.30495569109916687,
      "learning_rate": 0.00019992063654473917,
      "loss": 2.5046,
      "step": 6170
    },
    {
      "epoch": 0.12713466142424765,
      "grad_norm": 0.2851726710796356,
      "learning_rate": 0.0001999197494321373,
      "loss": 2.5077,
      "step": 6180
    },
    {
      "epoch": 0.12734038094111536,
      "grad_norm": 0.303753137588501,
      "learning_rate": 0.0001999188573910387,
      "loss": 2.4754,
      "step": 6190
    },
    {
      "epoch": 0.12754610045798306,
      "grad_norm": 0.2960306406021118,
      "learning_rate": 0.00019991796042148735,
      "loss": 2.4602,
      "step": 6200
    },
    {
      "epoch": 0.1277518199748508,
      "grad_norm": 0.2997722029685974,
      "learning_rate": 0.0001999170585235275,
      "loss": 2.4856,
      "step": 6210
    },
    {
      "epoch": 0.1279575394917185,
      "grad_norm": 0.27895408868789673,
      "learning_rate": 0.00019991615169720364,
      "loss": 2.5205,
      "step": 6220
    },
    {
      "epoch": 0.12816325900858622,
      "grad_norm": 0.2786983549594879,
      "learning_rate": 0.00019991523994256053,
      "loss": 2.5224,
      "step": 6230
    },
    {
      "epoch": 0.12836897852545393,
      "grad_norm": 0.3162630796432495,
      "learning_rate": 0.00019991432325964305,
      "loss": 2.4845,
      "step": 6240
    },
    {
      "epoch": 0.12857469804232166,
      "grad_norm": 0.30686643719673157,
      "learning_rate": 0.0001999134016484965,
      "loss": 2.5193,
      "step": 6250
    },
    {
      "epoch": 0.12878041755918937,
      "grad_norm": 0.2819399833679199,
      "learning_rate": 0.00019991247510916632,
      "loss": 2.4768,
      "step": 6260
    },
    {
      "epoch": 0.12898613707605708,
      "grad_norm": 0.28951889276504517,
      "learning_rate": 0.00019991154364169816,
      "loss": 2.5108,
      "step": 6270
    },
    {
      "epoch": 0.1291918565929248,
      "grad_norm": 0.3151751756668091,
      "learning_rate": 0.00019991060724613803,
      "loss": 2.4851,
      "step": 6280
    },
    {
      "epoch": 0.1293975761097925,
      "grad_norm": 0.2930360436439514,
      "learning_rate": 0.0001999096659225321,
      "loss": 2.4675,
      "step": 6290
    },
    {
      "epoch": 0.12960329562666023,
      "grad_norm": 0.2904382646083832,
      "learning_rate": 0.0001999087196709268,
      "loss": 2.5196,
      "step": 6300
    },
    {
      "epoch": 0.12980901514352794,
      "grad_norm": 0.3456838130950928,
      "learning_rate": 0.00019990776849136878,
      "loss": 2.4837,
      "step": 6310
    },
    {
      "epoch": 0.13001473466039565,
      "grad_norm": 0.33685699105262756,
      "learning_rate": 0.00019990681238390496,
      "loss": 2.4556,
      "step": 6320
    },
    {
      "epoch": 0.13022045417726336,
      "grad_norm": 0.30361059308052063,
      "learning_rate": 0.00019990585134858254,
      "loss": 2.4723,
      "step": 6330
    },
    {
      "epoch": 0.1304261736941311,
      "grad_norm": 0.3076104521751404,
      "learning_rate": 0.00019990488538544886,
      "loss": 2.5389,
      "step": 6340
    },
    {
      "epoch": 0.1306318932109988,
      "grad_norm": 0.3153560757637024,
      "learning_rate": 0.00019990391449455165,
      "loss": 2.5243,
      "step": 6350
    },
    {
      "epoch": 0.1308376127278665,
      "grad_norm": 0.3156653046607971,
      "learning_rate": 0.0001999029386759387,
      "loss": 2.5042,
      "step": 6360
    },
    {
      "epoch": 0.13104333224473422,
      "grad_norm": 0.30411356687545776,
      "learning_rate": 0.0001999019579296582,
      "loss": 2.5014,
      "step": 6370
    },
    {
      "epoch": 0.13124905176160193,
      "grad_norm": 0.30324921011924744,
      "learning_rate": 0.00019990097225575855,
      "loss": 2.5144,
      "step": 6380
    },
    {
      "epoch": 0.13145477127846966,
      "grad_norm": 0.3208661675453186,
      "learning_rate": 0.0001998999816542883,
      "loss": 2.4647,
      "step": 6390
    },
    {
      "epoch": 0.13166049079533737,
      "grad_norm": 0.3124139904975891,
      "learning_rate": 0.00019989898612529636,
      "loss": 2.4889,
      "step": 6400
    },
    {
      "epoch": 0.13186621031220508,
      "grad_norm": 0.3104243874549866,
      "learning_rate": 0.0001998979856688318,
      "loss": 2.4391,
      "step": 6410
    },
    {
      "epoch": 0.1320719298290728,
      "grad_norm": 0.30660977959632874,
      "learning_rate": 0.000199896980284944,
      "loss": 2.4659,
      "step": 6420
    },
    {
      "epoch": 0.13227764934594052,
      "grad_norm": 0.3014138638973236,
      "learning_rate": 0.00019989596997368257,
      "loss": 2.5081,
      "step": 6430
    },
    {
      "epoch": 0.13248336886280823,
      "grad_norm": 0.29324933886528015,
      "learning_rate": 0.00019989495473509726,
      "loss": 2.452,
      "step": 6440
    },
    {
      "epoch": 0.13268908837967594,
      "grad_norm": 0.3047555387020111,
      "learning_rate": 0.0001998939345692382,
      "loss": 2.4703,
      "step": 6450
    },
    {
      "epoch": 0.13289480789654365,
      "grad_norm": 0.2981312572956085,
      "learning_rate": 0.0001998929094761557,
      "loss": 2.4162,
      "step": 6460
    },
    {
      "epoch": 0.13310052741341136,
      "grad_norm": 0.3105306625366211,
      "learning_rate": 0.00019989187945590036,
      "loss": 2.5347,
      "step": 6470
    },
    {
      "epoch": 0.1333062469302791,
      "grad_norm": 0.33337855339050293,
      "learning_rate": 0.00019989084450852294,
      "loss": 2.5072,
      "step": 6480
    },
    {
      "epoch": 0.1335119664471468,
      "grad_norm": 0.30149269104003906,
      "learning_rate": 0.00019988980463407448,
      "loss": 2.4845,
      "step": 6490
    },
    {
      "epoch": 0.1337176859640145,
      "grad_norm": 0.3005920350551605,
      "learning_rate": 0.0001998887598326063,
      "loss": 2.4425,
      "step": 6500
    },
    {
      "epoch": 0.13392340548088222,
      "grad_norm": 0.32799506187438965,
      "learning_rate": 0.00019988771010416993,
      "loss": 2.4718,
      "step": 6510
    },
    {
      "epoch": 0.13412912499774995,
      "grad_norm": 0.28535929322242737,
      "learning_rate": 0.00019988665544881716,
      "loss": 2.5121,
      "step": 6520
    },
    {
      "epoch": 0.13433484451461766,
      "grad_norm": 0.3068074882030487,
      "learning_rate": 0.00019988559586659997,
      "loss": 2.4948,
      "step": 6530
    },
    {
      "epoch": 0.13454056403148537,
      "grad_norm": 0.31668731570243835,
      "learning_rate": 0.00019988453135757063,
      "loss": 2.5299,
      "step": 6540
    },
    {
      "epoch": 0.13474628354835308,
      "grad_norm": 0.29373738169670105,
      "learning_rate": 0.0001998834619217817,
      "loss": 2.4877,
      "step": 6550
    },
    {
      "epoch": 0.1349520030652208,
      "grad_norm": 0.33740395307540894,
      "learning_rate": 0.00019988238755928587,
      "loss": 2.4912,
      "step": 6560
    },
    {
      "epoch": 0.13515772258208852,
      "grad_norm": 0.31762075424194336,
      "learning_rate": 0.00019988130827013615,
      "loss": 2.5065,
      "step": 6570
    },
    {
      "epoch": 0.13536344209895623,
      "grad_norm": 0.3062155246734619,
      "learning_rate": 0.00019988022405438582,
      "loss": 2.4926,
      "step": 6580
    },
    {
      "epoch": 0.13556916161582394,
      "grad_norm": 0.3113679587841034,
      "learning_rate": 0.00019987913491208828,
      "loss": 2.5278,
      "step": 6590
    },
    {
      "epoch": 0.13577488113269165,
      "grad_norm": 0.28259703516960144,
      "learning_rate": 0.00019987804084329734,
      "loss": 2.4935,
      "step": 6600
    },
    {
      "epoch": 0.13598060064955939,
      "grad_norm": 0.35274815559387207,
      "learning_rate": 0.00019987694184806686,
      "loss": 2.4907,
      "step": 6610
    },
    {
      "epoch": 0.1361863201664271,
      "grad_norm": 0.2858965992927551,
      "learning_rate": 0.00019987583792645112,
      "loss": 2.4411,
      "step": 6620
    },
    {
      "epoch": 0.1363920396832948,
      "grad_norm": 0.3097718060016632,
      "learning_rate": 0.00019987472907850458,
      "loss": 2.5107,
      "step": 6630
    },
    {
      "epoch": 0.1365977592001625,
      "grad_norm": 0.29680532217025757,
      "learning_rate": 0.00019987361530428185,
      "loss": 2.4917,
      "step": 6640
    },
    {
      "epoch": 0.13680347871703025,
      "grad_norm": 0.2896421253681183,
      "learning_rate": 0.00019987249660383797,
      "loss": 2.5283,
      "step": 6650
    },
    {
      "epoch": 0.13700919823389796,
      "grad_norm": 0.5954927802085876,
      "learning_rate": 0.00019987137297722808,
      "loss": 2.5393,
      "step": 6660
    },
    {
      "epoch": 0.13721491775076566,
      "grad_norm": 0.29563719034194946,
      "learning_rate": 0.00019987024442450757,
      "loss": 2.5612,
      "step": 6670
    },
    {
      "epoch": 0.13742063726763337,
      "grad_norm": 0.2796277403831482,
      "learning_rate": 0.00019986911094573213,
      "loss": 2.4596,
      "step": 6680
    },
    {
      "epoch": 0.13762635678450108,
      "grad_norm": 0.30342742800712585,
      "learning_rate": 0.00019986797254095765,
      "loss": 2.4404,
      "step": 6690
    },
    {
      "epoch": 0.13783207630136882,
      "grad_norm": 0.3173634111881256,
      "learning_rate": 0.00019986682921024033,
      "loss": 2.4517,
      "step": 6700
    },
    {
      "epoch": 0.13803779581823653,
      "grad_norm": 0.2945812940597534,
      "learning_rate": 0.00019986568095363655,
      "loss": 2.4563,
      "step": 6710
    },
    {
      "epoch": 0.13824351533510423,
      "grad_norm": 0.3141669034957886,
      "learning_rate": 0.0001998645277712029,
      "loss": 2.5055,
      "step": 6720
    },
    {
      "epoch": 0.13844923485197194,
      "grad_norm": 0.2777768075466156,
      "learning_rate": 0.0001998633696629963,
      "loss": 2.4079,
      "step": 6730
    },
    {
      "epoch": 0.13865495436883968,
      "grad_norm": 0.31368428468704224,
      "learning_rate": 0.00019986220662907386,
      "loss": 2.5196,
      "step": 6740
    },
    {
      "epoch": 0.1388606738857074,
      "grad_norm": 0.3183036744594574,
      "learning_rate": 0.00019986103866949294,
      "loss": 2.4657,
      "step": 6750
    },
    {
      "epoch": 0.1390663934025751,
      "grad_norm": 0.30846768617630005,
      "learning_rate": 0.0001998598657843112,
      "loss": 2.4545,
      "step": 6760
    },
    {
      "epoch": 0.1392721129194428,
      "grad_norm": 0.3052728772163391,
      "learning_rate": 0.00019985868797358643,
      "loss": 2.5266,
      "step": 6770
    },
    {
      "epoch": 0.1394778324363105,
      "grad_norm": 0.28437793254852295,
      "learning_rate": 0.00019985750523737674,
      "loss": 2.4921,
      "step": 6780
    },
    {
      "epoch": 0.13968355195317825,
      "grad_norm": 0.28517523407936096,
      "learning_rate": 0.00019985631757574046,
      "loss": 2.4896,
      "step": 6790
    },
    {
      "epoch": 0.13988927147004596,
      "grad_norm": 0.30273887515068054,
      "learning_rate": 0.0001998551249887362,
      "loss": 2.5223,
      "step": 6800
    },
    {
      "epoch": 0.14009499098691366,
      "grad_norm": 0.30449777841567993,
      "learning_rate": 0.0001998539274764228,
      "loss": 2.5345,
      "step": 6810
    },
    {
      "epoch": 0.14030071050378137,
      "grad_norm": 0.3016957640647888,
      "learning_rate": 0.00019985272503885926,
      "loss": 2.4629,
      "step": 6820
    },
    {
      "epoch": 0.1405064300206491,
      "grad_norm": 0.30464136600494385,
      "learning_rate": 0.00019985151767610494,
      "loss": 2.5243,
      "step": 6830
    },
    {
      "epoch": 0.14071214953751682,
      "grad_norm": 0.31694841384887695,
      "learning_rate": 0.00019985030538821936,
      "loss": 2.4595,
      "step": 6840
    },
    {
      "epoch": 0.14091786905438453,
      "grad_norm": 0.2891586422920227,
      "learning_rate": 0.00019984908817526233,
      "loss": 2.4438,
      "step": 6850
    },
    {
      "epoch": 0.14112358857125223,
      "grad_norm": 0.29904377460479736,
      "learning_rate": 0.00019984786603729392,
      "loss": 2.5024,
      "step": 6860
    },
    {
      "epoch": 0.14132930808811994,
      "grad_norm": 0.29940134286880493,
      "learning_rate": 0.00019984663897437439,
      "loss": 2.4791,
      "step": 6870
    },
    {
      "epoch": 0.14153502760498768,
      "grad_norm": 0.30437901616096497,
      "learning_rate": 0.00019984540698656425,
      "loss": 2.5409,
      "step": 6880
    },
    {
      "epoch": 0.1417407471218554,
      "grad_norm": 0.31833696365356445,
      "learning_rate": 0.00019984417007392423,
      "loss": 2.4781,
      "step": 6890
    },
    {
      "epoch": 0.1419464666387231,
      "grad_norm": 0.31236374378204346,
      "learning_rate": 0.00019984292823651545,
      "loss": 2.5455,
      "step": 6900
    },
    {
      "epoch": 0.1421521861555908,
      "grad_norm": 0.2820510268211365,
      "learning_rate": 0.00019984168147439907,
      "loss": 2.4632,
      "step": 6910
    },
    {
      "epoch": 0.14235790567245854,
      "grad_norm": 0.3111339211463928,
      "learning_rate": 0.00019984042978763662,
      "loss": 2.4889,
      "step": 6920
    },
    {
      "epoch": 0.14256362518932625,
      "grad_norm": 0.29388564825057983,
      "learning_rate": 0.00019983917317628982,
      "loss": 2.5251,
      "step": 6930
    },
    {
      "epoch": 0.14276934470619396,
      "grad_norm": 0.3337874114513397,
      "learning_rate": 0.0001998379116404207,
      "loss": 2.5427,
      "step": 6940
    },
    {
      "epoch": 0.14297506422306167,
      "grad_norm": 0.31289583444595337,
      "learning_rate": 0.00019983664518009143,
      "loss": 2.5479,
      "step": 6950
    },
    {
      "epoch": 0.14318078373992937,
      "grad_norm": 0.288287490606308,
      "learning_rate": 0.00019983537379536447,
      "loss": 2.5477,
      "step": 6960
    },
    {
      "epoch": 0.1433865032567971,
      "grad_norm": 0.30935025215148926,
      "learning_rate": 0.00019983409748630258,
      "loss": 2.4801,
      "step": 6970
    },
    {
      "epoch": 0.14359222277366482,
      "grad_norm": 0.3057328462600708,
      "learning_rate": 0.0001998328162529687,
      "loss": 2.5143,
      "step": 6980
    },
    {
      "epoch": 0.14379794229053253,
      "grad_norm": 0.29885178804397583,
      "learning_rate": 0.000199831530095426,
      "loss": 2.4801,
      "step": 6990
    },
    {
      "epoch": 0.14400366180740024,
      "grad_norm": 0.29236671328544617,
      "learning_rate": 0.00019983023901373796,
      "loss": 2.4658,
      "step": 7000
    },
    {
      "epoch": 0.14420938132426797,
      "grad_norm": 0.35659706592559814,
      "learning_rate": 0.00019982894300796825,
      "loss": 2.4915,
      "step": 7010
    },
    {
      "epoch": 0.14441510084113568,
      "grad_norm": 0.3046221137046814,
      "learning_rate": 0.00019982764207818074,
      "loss": 2.4376,
      "step": 7020
    },
    {
      "epoch": 0.1446208203580034,
      "grad_norm": 0.303613543510437,
      "learning_rate": 0.00019982633622443968,
      "loss": 2.4846,
      "step": 7030
    },
    {
      "epoch": 0.1448265398748711,
      "grad_norm": 0.3272626996040344,
      "learning_rate": 0.00019982502544680945,
      "loss": 2.4947,
      "step": 7040
    },
    {
      "epoch": 0.1450322593917388,
      "grad_norm": 0.3014932870864868,
      "learning_rate": 0.00019982370974535465,
      "loss": 2.4581,
      "step": 7050
    },
    {
      "epoch": 0.14523797890860654,
      "grad_norm": 0.39194995164871216,
      "learning_rate": 0.00019982238912014024,
      "loss": 2.5038,
      "step": 7060
    },
    {
      "epoch": 0.14544369842547425,
      "grad_norm": 0.32676056027412415,
      "learning_rate": 0.00019982106357123132,
      "loss": 2.5289,
      "step": 7070
    },
    {
      "epoch": 0.14564941794234196,
      "grad_norm": 0.28496530652046204,
      "learning_rate": 0.00019981973309869333,
      "loss": 2.4413,
      "step": 7080
    },
    {
      "epoch": 0.14585513745920967,
      "grad_norm": 0.3058985769748688,
      "learning_rate": 0.00019981839770259185,
      "loss": 2.4921,
      "step": 7090
    },
    {
      "epoch": 0.1460608569760774,
      "grad_norm": 0.2984072268009186,
      "learning_rate": 0.00019981705738299275,
      "loss": 2.4953,
      "step": 7100
    },
    {
      "epoch": 0.1462665764929451,
      "grad_norm": 0.31063312292099,
      "learning_rate": 0.00019981571213996215,
      "loss": 2.4417,
      "step": 7110
    },
    {
      "epoch": 0.14647229600981282,
      "grad_norm": 0.32386189699172974,
      "learning_rate": 0.00019981436197356642,
      "loss": 2.522,
      "step": 7120
    },
    {
      "epoch": 0.14667801552668053,
      "grad_norm": 0.31986165046691895,
      "learning_rate": 0.00019981300688387213,
      "loss": 2.5086,
      "step": 7130
    },
    {
      "epoch": 0.14688373504354826,
      "grad_norm": 0.33035123348236084,
      "learning_rate": 0.00019981164687094612,
      "loss": 2.4919,
      "step": 7140
    },
    {
      "epoch": 0.14708945456041597,
      "grad_norm": 0.31358930468559265,
      "learning_rate": 0.00019981028193485548,
      "loss": 2.4319,
      "step": 7150
    },
    {
      "epoch": 0.14729517407728368,
      "grad_norm": 0.3311873972415924,
      "learning_rate": 0.0001998089120756675,
      "loss": 2.4723,
      "step": 7160
    },
    {
      "epoch": 0.1475008935941514,
      "grad_norm": 0.315912663936615,
      "learning_rate": 0.0001998075372934498,
      "loss": 2.4752,
      "step": 7170
    },
    {
      "epoch": 0.1477066131110191,
      "grad_norm": 0.3041195869445801,
      "learning_rate": 0.0001998061575882702,
      "loss": 2.465,
      "step": 7180
    },
    {
      "epoch": 0.14791233262788683,
      "grad_norm": 0.2913614809513092,
      "learning_rate": 0.0001998047729601967,
      "loss": 2.4485,
      "step": 7190
    },
    {
      "epoch": 0.14811805214475454,
      "grad_norm": 0.3035030663013458,
      "learning_rate": 0.0001998033834092976,
      "loss": 2.5253,
      "step": 7200
    },
    {
      "epoch": 0.14832377166162225,
      "grad_norm": 0.29237252473831177,
      "learning_rate": 0.00019980198893564151,
      "loss": 2.464,
      "step": 7210
    },
    {
      "epoch": 0.14852949117848996,
      "grad_norm": 0.29318952560424805,
      "learning_rate": 0.00019980058953929713,
      "loss": 2.4646,
      "step": 7220
    },
    {
      "epoch": 0.1487352106953577,
      "grad_norm": 0.33761289715766907,
      "learning_rate": 0.0001997991852203335,
      "loss": 2.4363,
      "step": 7230
    },
    {
      "epoch": 0.1489409302122254,
      "grad_norm": 0.29868391156196594,
      "learning_rate": 0.00019979777597881992,
      "loss": 2.4884,
      "step": 7240
    },
    {
      "epoch": 0.1491466497290931,
      "grad_norm": 0.32159218192100525,
      "learning_rate": 0.0001997963618148259,
      "loss": 2.496,
      "step": 7250
    },
    {
      "epoch": 0.14935236924596082,
      "grad_norm": 0.300709992647171,
      "learning_rate": 0.00019979494272842117,
      "loss": 2.4899,
      "step": 7260
    },
    {
      "epoch": 0.14955808876282853,
      "grad_norm": 0.32351556420326233,
      "learning_rate": 0.00019979351871967572,
      "loss": 2.4266,
      "step": 7270
    },
    {
      "epoch": 0.14976380827969626,
      "grad_norm": 0.332604318857193,
      "learning_rate": 0.0001997920897886598,
      "loss": 2.4181,
      "step": 7280
    },
    {
      "epoch": 0.14996952779656397,
      "grad_norm": 0.2927042543888092,
      "learning_rate": 0.0001997906559354439,
      "loss": 2.5367,
      "step": 7290
    },
    {
      "epoch": 0.15017524731343168,
      "grad_norm": 0.319656103849411,
      "learning_rate": 0.00019978921716009876,
      "loss": 2.5099,
      "step": 7300
    },
    {
      "epoch": 0.1503809668302994,
      "grad_norm": 0.3182654082775116,
      "learning_rate": 0.00019978777346269528,
      "loss": 2.5179,
      "step": 7310
    },
    {
      "epoch": 0.15058668634716713,
      "grad_norm": 0.29445791244506836,
      "learning_rate": 0.00019978632484330474,
      "loss": 2.5595,
      "step": 7320
    },
    {
      "epoch": 0.15079240586403483,
      "grad_norm": 0.3172418475151062,
      "learning_rate": 0.00019978487130199856,
      "loss": 2.5376,
      "step": 7330
    },
    {
      "epoch": 0.15099812538090254,
      "grad_norm": 0.3118588626384735,
      "learning_rate": 0.00019978341283884846,
      "loss": 2.4847,
      "step": 7340
    },
    {
      "epoch": 0.15120384489777025,
      "grad_norm": 0.32794255018234253,
      "learning_rate": 0.00019978194945392632,
      "loss": 2.4516,
      "step": 7350
    },
    {
      "epoch": 0.15140956441463796,
      "grad_norm": 0.31231650710105896,
      "learning_rate": 0.0001997804811473044,
      "loss": 2.5011,
      "step": 7360
    },
    {
      "epoch": 0.1516152839315057,
      "grad_norm": 0.276390016078949,
      "learning_rate": 0.00019977900791905503,
      "loss": 2.4809,
      "step": 7370
    },
    {
      "epoch": 0.1518210034483734,
      "grad_norm": 0.3242877721786499,
      "learning_rate": 0.00019977752976925098,
      "loss": 2.5085,
      "step": 7380
    },
    {
      "epoch": 0.1520267229652411,
      "grad_norm": 0.31247252225875854,
      "learning_rate": 0.00019977604669796508,
      "loss": 2.4483,
      "step": 7390
    },
    {
      "epoch": 0.15223244248210882,
      "grad_norm": 0.2946126461029053,
      "learning_rate": 0.00019977455870527053,
      "loss": 2.5104,
      "step": 7400
    },
    {
      "epoch": 0.15243816199897656,
      "grad_norm": 0.3000539243221283,
      "learning_rate": 0.0001997730657912407,
      "loss": 2.5284,
      "step": 7410
    },
    {
      "epoch": 0.15264388151584427,
      "grad_norm": 0.33906111121177673,
      "learning_rate": 0.00019977156795594923,
      "loss": 2.4766,
      "step": 7420
    },
    {
      "epoch": 0.15284960103271197,
      "grad_norm": 0.3224266469478607,
      "learning_rate": 0.00019977006519946998,
      "loss": 2.5388,
      "step": 7430
    },
    {
      "epoch": 0.15305532054957968,
      "grad_norm": 0.29093044996261597,
      "learning_rate": 0.0001997685575218771,
      "loss": 2.5248,
      "step": 7440
    },
    {
      "epoch": 0.1532610400664474,
      "grad_norm": 0.3099437355995178,
      "learning_rate": 0.00019976704492324497,
      "loss": 2.4755,
      "step": 7450
    },
    {
      "epoch": 0.15346675958331513,
      "grad_norm": 0.279391884803772,
      "learning_rate": 0.0001997655274036482,
      "loss": 2.4636,
      "step": 7460
    },
    {
      "epoch": 0.15367247910018283,
      "grad_norm": 0.33109742403030396,
      "learning_rate": 0.0001997640049631616,
      "loss": 2.5201,
      "step": 7470
    },
    {
      "epoch": 0.15387819861705054,
      "grad_norm": 0.3208921551704407,
      "learning_rate": 0.0001997624776018603,
      "loss": 2.482,
      "step": 7480
    },
    {
      "epoch": 0.15408391813391825,
      "grad_norm": 0.3147161304950714,
      "learning_rate": 0.00019976094531981957,
      "loss": 2.5216,
      "step": 7490
    },
    {
      "epoch": 0.154289637650786,
      "grad_norm": 0.31205666065216064,
      "learning_rate": 0.00019975940811711506,
      "loss": 2.4045,
      "step": 7500
    },
    {
      "epoch": 0.1544953571676537,
      "grad_norm": 0.3018381893634796,
      "learning_rate": 0.0001997578659938226,
      "loss": 2.4841,
      "step": 7510
    },
    {
      "epoch": 0.1547010766845214,
      "grad_norm": 0.29379943013191223,
      "learning_rate": 0.00019975631895001821,
      "loss": 2.4898,
      "step": 7520
    },
    {
      "epoch": 0.1549067962013891,
      "grad_norm": 0.3168879449367523,
      "learning_rate": 0.00019975476698577823,
      "loss": 2.4553,
      "step": 7530
    },
    {
      "epoch": 0.15511251571825685,
      "grad_norm": 0.3026787340641022,
      "learning_rate": 0.00019975321010117915,
      "loss": 2.4444,
      "step": 7540
    },
    {
      "epoch": 0.15531823523512456,
      "grad_norm": 0.29276880621910095,
      "learning_rate": 0.00019975164829629782,
      "loss": 2.435,
      "step": 7550
    },
    {
      "epoch": 0.15552395475199227,
      "grad_norm": 0.3477056920528412,
      "learning_rate": 0.00019975008157121127,
      "loss": 2.4777,
      "step": 7560
    },
    {
      "epoch": 0.15572967426885997,
      "grad_norm": 0.2977745532989502,
      "learning_rate": 0.00019974850992599677,
      "loss": 2.4695,
      "step": 7570
    },
    {
      "epoch": 0.15593539378572768,
      "grad_norm": 0.29833388328552246,
      "learning_rate": 0.00019974693336073185,
      "loss": 2.4807,
      "step": 7580
    },
    {
      "epoch": 0.15614111330259542,
      "grad_norm": 0.3133496940135956,
      "learning_rate": 0.00019974535187549424,
      "loss": 2.4365,
      "step": 7590
    },
    {
      "epoch": 0.15634683281946313,
      "grad_norm": 0.34972211718559265,
      "learning_rate": 0.000199743765470362,
      "loss": 2.4834,
      "step": 7600
    },
    {
      "epoch": 0.15655255233633084,
      "grad_norm": 0.3174692690372467,
      "learning_rate": 0.0001997421741454133,
      "loss": 2.5082,
      "step": 7610
    },
    {
      "epoch": 0.15675827185319854,
      "grad_norm": 0.30609530210494995,
      "learning_rate": 0.0001997405779007267,
      "loss": 2.4447,
      "step": 7620
    },
    {
      "epoch": 0.15696399137006628,
      "grad_norm": 0.301453173160553,
      "learning_rate": 0.00019973897673638093,
      "loss": 2.4895,
      "step": 7630
    },
    {
      "epoch": 0.157169710886934,
      "grad_norm": 0.3037416934967041,
      "learning_rate": 0.00019973737065245495,
      "loss": 2.4547,
      "step": 7640
    },
    {
      "epoch": 0.1573754304038017,
      "grad_norm": 0.32348382472991943,
      "learning_rate": 0.00019973575964902796,
      "loss": 2.5144,
      "step": 7650
    },
    {
      "epoch": 0.1575811499206694,
      "grad_norm": 0.30057740211486816,
      "learning_rate": 0.00019973414372617945,
      "loss": 2.4652,
      "step": 7660
    },
    {
      "epoch": 0.15778686943753711,
      "grad_norm": 0.31768012046813965,
      "learning_rate": 0.0001997325228839891,
      "loss": 2.5015,
      "step": 7670
    },
    {
      "epoch": 0.15799258895440485,
      "grad_norm": 0.30978164076805115,
      "learning_rate": 0.0001997308971225369,
      "loss": 2.4198,
      "step": 7680
    },
    {
      "epoch": 0.15819830847127256,
      "grad_norm": 0.30913710594177246,
      "learning_rate": 0.000199729266441903,
      "loss": 2.5265,
      "step": 7690
    },
    {
      "epoch": 0.15840402798814027,
      "grad_norm": 0.2880324423313141,
      "learning_rate": 0.00019972763084216786,
      "loss": 2.4623,
      "step": 7700
    },
    {
      "epoch": 0.15860974750500798,
      "grad_norm": 0.3057457208633423,
      "learning_rate": 0.00019972599032341213,
      "loss": 2.5039,
      "step": 7710
    },
    {
      "epoch": 0.1588154670218757,
      "grad_norm": 0.3186277449131012,
      "learning_rate": 0.00019972434488571672,
      "loss": 2.4633,
      "step": 7720
    },
    {
      "epoch": 0.15902118653874342,
      "grad_norm": 0.30486559867858887,
      "learning_rate": 0.00019972269452916286,
      "loss": 2.4775,
      "step": 7730
    },
    {
      "epoch": 0.15922690605561113,
      "grad_norm": 0.30859503149986267,
      "learning_rate": 0.00019972103925383186,
      "loss": 2.506,
      "step": 7740
    },
    {
      "epoch": 0.15943262557247884,
      "grad_norm": 0.3125554919242859,
      "learning_rate": 0.0001997193790598054,
      "loss": 2.5039,
      "step": 7750
    },
    {
      "epoch": 0.15963834508934654,
      "grad_norm": 0.31131377816200256,
      "learning_rate": 0.0001997177139471654,
      "loss": 2.4623,
      "step": 7760
    },
    {
      "epoch": 0.15984406460621428,
      "grad_norm": 0.31456446647644043,
      "learning_rate": 0.00019971604391599398,
      "loss": 2.4493,
      "step": 7770
    },
    {
      "epoch": 0.160049784123082,
      "grad_norm": 0.33199378848075867,
      "learning_rate": 0.00019971436896637346,
      "loss": 2.4858,
      "step": 7780
    },
    {
      "epoch": 0.1602555036399497,
      "grad_norm": 0.2962685823440552,
      "learning_rate": 0.0001997126890983865,
      "loss": 2.4462,
      "step": 7790
    },
    {
      "epoch": 0.1604612231568174,
      "grad_norm": 0.32081523537635803,
      "learning_rate": 0.000199711004312116,
      "loss": 2.4567,
      "step": 7800
    },
    {
      "epoch": 0.16066694267368514,
      "grad_norm": 0.3161945939064026,
      "learning_rate": 0.00019970931460764498,
      "loss": 2.4471,
      "step": 7810
    },
    {
      "epoch": 0.16087266219055285,
      "grad_norm": 0.30032238364219666,
      "learning_rate": 0.00019970761998505684,
      "loss": 2.5396,
      "step": 7820
    },
    {
      "epoch": 0.16107838170742056,
      "grad_norm": 0.29356032609939575,
      "learning_rate": 0.00019970592044443513,
      "loss": 2.4792,
      "step": 7830
    },
    {
      "epoch": 0.16128410122428827,
      "grad_norm": 0.30941176414489746,
      "learning_rate": 0.0001997042159858637,
      "loss": 2.4886,
      "step": 7840
    },
    {
      "epoch": 0.16148982074115598,
      "grad_norm": 0.3101857602596283,
      "learning_rate": 0.00019970250660942664,
      "loss": 2.4574,
      "step": 7850
    },
    {
      "epoch": 0.1616955402580237,
      "grad_norm": 0.3318101167678833,
      "learning_rate": 0.00019970079231520824,
      "loss": 2.4921,
      "step": 7860
    },
    {
      "epoch": 0.16190125977489142,
      "grad_norm": 0.2882389426231384,
      "learning_rate": 0.00019969907310329305,
      "loss": 2.451,
      "step": 7870
    },
    {
      "epoch": 0.16210697929175913,
      "grad_norm": 0.32434767484664917,
      "learning_rate": 0.00019969734897376586,
      "loss": 2.5296,
      "step": 7880
    },
    {
      "epoch": 0.16231269880862684,
      "grad_norm": 0.31312820315361023,
      "learning_rate": 0.00019969561992671175,
      "loss": 2.4674,
      "step": 7890
    },
    {
      "epoch": 0.16251841832549457,
      "grad_norm": 0.31449800729751587,
      "learning_rate": 0.000199693885962216,
      "loss": 2.4558,
      "step": 7900
    },
    {
      "epoch": 0.16272413784236228,
      "grad_norm": 0.2989133298397064,
      "learning_rate": 0.0001996921470803641,
      "loss": 2.4492,
      "step": 7910
    },
    {
      "epoch": 0.16292985735923,
      "grad_norm": 0.35624876618385315,
      "learning_rate": 0.00019969040328124185,
      "loss": 2.4947,
      "step": 7920
    },
    {
      "epoch": 0.1631355768760977,
      "grad_norm": 0.3008812963962555,
      "learning_rate": 0.00019968865456493524,
      "loss": 2.4797,
      "step": 7930
    },
    {
      "epoch": 0.16334129639296543,
      "grad_norm": 0.3221741318702698,
      "learning_rate": 0.00019968690093153052,
      "loss": 2.4574,
      "step": 7940
    },
    {
      "epoch": 0.16354701590983314,
      "grad_norm": 0.35466814041137695,
      "learning_rate": 0.00019968514238111426,
      "loss": 2.5223,
      "step": 7950
    },
    {
      "epoch": 0.16375273542670085,
      "grad_norm": 0.31091341376304626,
      "learning_rate": 0.0001996833789137731,
      "loss": 2.4787,
      "step": 7960
    },
    {
      "epoch": 0.16395845494356856,
      "grad_norm": 0.30201244354248047,
      "learning_rate": 0.00019968161052959409,
      "loss": 2.5002,
      "step": 7970
    },
    {
      "epoch": 0.16416417446043627,
      "grad_norm": 0.30815330147743225,
      "learning_rate": 0.00019967983722866443,
      "loss": 2.4408,
      "step": 7980
    },
    {
      "epoch": 0.164369893977304,
      "grad_norm": 0.2999531626701355,
      "learning_rate": 0.00019967805901107156,
      "loss": 2.5559,
      "step": 7990
    },
    {
      "epoch": 0.1645756134941717,
      "grad_norm": 0.30609115958213806,
      "learning_rate": 0.00019967627587690328,
      "loss": 2.4821,
      "step": 8000
    },
    {
      "epoch": 0.16478133301103942,
      "grad_norm": 0.2964194416999817,
      "learning_rate": 0.00019967448782624744,
      "loss": 2.44,
      "step": 8010
    },
    {
      "epoch": 0.16498705252790713,
      "grad_norm": 0.3091222643852234,
      "learning_rate": 0.00019967269485919226,
      "loss": 2.4805,
      "step": 8020
    },
    {
      "epoch": 0.16519277204477487,
      "grad_norm": 0.34489256143569946,
      "learning_rate": 0.00019967089697582623,
      "loss": 2.5288,
      "step": 8030
    },
    {
      "epoch": 0.16539849156164257,
      "grad_norm": 0.3173450231552124,
      "learning_rate": 0.000199669094176238,
      "loss": 2.4235,
      "step": 8040
    },
    {
      "epoch": 0.16560421107851028,
      "grad_norm": 0.32812896370887756,
      "learning_rate": 0.00019966728646051647,
      "loss": 2.4256,
      "step": 8050
    },
    {
      "epoch": 0.165809930595378,
      "grad_norm": 0.32122308015823364,
      "learning_rate": 0.0001996654738287508,
      "loss": 2.4385,
      "step": 8060
    },
    {
      "epoch": 0.1660156501122457,
      "grad_norm": 0.3433575928211212,
      "learning_rate": 0.00019966365628103043,
      "loss": 2.4935,
      "step": 8070
    },
    {
      "epoch": 0.16622136962911344,
      "grad_norm": 0.3679540455341339,
      "learning_rate": 0.000199661833817445,
      "loss": 2.515,
      "step": 8080
    },
    {
      "epoch": 0.16642708914598114,
      "grad_norm": 0.33326372504234314,
      "learning_rate": 0.00019966000643808444,
      "loss": 2.4548,
      "step": 8090
    },
    {
      "epoch": 0.16663280866284885,
      "grad_norm": 0.30943289399147034,
      "learning_rate": 0.00019965817414303877,
      "loss": 2.5037,
      "step": 8100
    },
    {
      "epoch": 0.16683852817971656,
      "grad_norm": 0.3002689480781555,
      "learning_rate": 0.0001996563369323985,
      "loss": 2.5029,
      "step": 8110
    },
    {
      "epoch": 0.1670442476965843,
      "grad_norm": 0.312099426984787,
      "learning_rate": 0.00019965449480625417,
      "loss": 2.5505,
      "step": 8120
    },
    {
      "epoch": 0.167249967213452,
      "grad_norm": 0.313140869140625,
      "learning_rate": 0.00019965264776469662,
      "loss": 2.4456,
      "step": 8130
    },
    {
      "epoch": 0.1674556867303197,
      "grad_norm": 0.31470876932144165,
      "learning_rate": 0.00019965079580781708,
      "loss": 2.4343,
      "step": 8140
    },
    {
      "epoch": 0.16766140624718742,
      "grad_norm": 0.2997951805591583,
      "learning_rate": 0.00019964893893570674,
      "loss": 2.3929,
      "step": 8150
    },
    {
      "epoch": 0.16786712576405513,
      "grad_norm": 0.328090637922287,
      "learning_rate": 0.0001996470771484573,
      "loss": 2.5058,
      "step": 8160
    },
    {
      "epoch": 0.16807284528092287,
      "grad_norm": 0.33549466729164124,
      "learning_rate": 0.00019964521044616058,
      "loss": 2.4815,
      "step": 8170
    },
    {
      "epoch": 0.16827856479779058,
      "grad_norm": 0.4474456310272217,
      "learning_rate": 0.0001996433388289086,
      "loss": 2.4352,
      "step": 8180
    },
    {
      "epoch": 0.16848428431465828,
      "grad_norm": 0.30400151014328003,
      "learning_rate": 0.0001996414622967937,
      "loss": 2.5004,
      "step": 8190
    },
    {
      "epoch": 0.168690003831526,
      "grad_norm": 0.2950890064239502,
      "learning_rate": 0.00019963958084990845,
      "loss": 2.4944,
      "step": 8200
    },
    {
      "epoch": 0.16889572334839373,
      "grad_norm": 0.29661375284194946,
      "learning_rate": 0.00019963769448834567,
      "loss": 2.4407,
      "step": 8210
    },
    {
      "epoch": 0.16910144286526144,
      "grad_norm": 0.32008692622184753,
      "learning_rate": 0.00019963580321219837,
      "loss": 2.5011,
      "step": 8220
    },
    {
      "epoch": 0.16930716238212914,
      "grad_norm": 0.314508318901062,
      "learning_rate": 0.0001996339070215598,
      "loss": 2.5062,
      "step": 8230
    },
    {
      "epoch": 0.16951288189899685,
      "grad_norm": 0.318726509809494,
      "learning_rate": 0.00019963200591652363,
      "loss": 2.4886,
      "step": 8240
    },
    {
      "epoch": 0.16971860141586456,
      "grad_norm": 0.30095186829566956,
      "learning_rate": 0.00019963009989718347,
      "loss": 2.4707,
      "step": 8250
    },
    {
      "epoch": 0.1699243209327323,
      "grad_norm": 0.36315393447875977,
      "learning_rate": 0.00019962818896363343,
      "loss": 2.4834,
      "step": 8260
    },
    {
      "epoch": 0.1701300404496,
      "grad_norm": 0.3096279203891754,
      "learning_rate": 0.0001996262731159677,
      "loss": 2.4361,
      "step": 8270
    },
    {
      "epoch": 0.17033575996646771,
      "grad_norm": 0.3188806474208832,
      "learning_rate": 0.00019962435235428084,
      "loss": 2.4319,
      "step": 8280
    },
    {
      "epoch": 0.17054147948333542,
      "grad_norm": 0.3356764316558838,
      "learning_rate": 0.00019962242667866757,
      "loss": 2.5008,
      "step": 8290
    },
    {
      "epoch": 0.17074719900020316,
      "grad_norm": 0.32055795192718506,
      "learning_rate": 0.00019962049608922286,
      "loss": 2.4448,
      "step": 8300
    },
    {
      "epoch": 0.17095291851707087,
      "grad_norm": 0.3128540515899658,
      "learning_rate": 0.00019961856058604193,
      "loss": 2.4434,
      "step": 8310
    },
    {
      "epoch": 0.17115863803393858,
      "grad_norm": 0.33519312739372253,
      "learning_rate": 0.00019961662016922032,
      "loss": 2.4708,
      "step": 8320
    },
    {
      "epoch": 0.17136435755080628,
      "grad_norm": 0.30880674719810486,
      "learning_rate": 0.0001996146748388536,
      "loss": 2.4499,
      "step": 8330
    },
    {
      "epoch": 0.171570077067674,
      "grad_norm": 0.33901500701904297,
      "learning_rate": 0.0001996127245950379,
      "loss": 2.4475,
      "step": 8340
    },
    {
      "epoch": 0.17177579658454173,
      "grad_norm": 0.2870865762233734,
      "learning_rate": 0.00019961076943786924,
      "loss": 2.4327,
      "step": 8350
    },
    {
      "epoch": 0.17198151610140944,
      "grad_norm": 0.3266279399394989,
      "learning_rate": 0.00019960880936744415,
      "loss": 2.4824,
      "step": 8360
    },
    {
      "epoch": 0.17218723561827715,
      "grad_norm": 0.31132039427757263,
      "learning_rate": 0.00019960684438385931,
      "loss": 2.4853,
      "step": 8370
    },
    {
      "epoch": 0.17239295513514485,
      "grad_norm": 0.3069552481174469,
      "learning_rate": 0.00019960487448721166,
      "loss": 2.4729,
      "step": 8380
    },
    {
      "epoch": 0.1725986746520126,
      "grad_norm": 0.3111433982849121,
      "learning_rate": 0.0001996028996775983,
      "loss": 2.521,
      "step": 8390
    },
    {
      "epoch": 0.1728043941688803,
      "grad_norm": 0.32707253098487854,
      "learning_rate": 0.00019960091995511667,
      "loss": 2.5015,
      "step": 8400
    },
    {
      "epoch": 0.173010113685748,
      "grad_norm": 0.31440696120262146,
      "learning_rate": 0.00019959893531986441,
      "loss": 2.4332,
      "step": 8410
    },
    {
      "epoch": 0.17321583320261572,
      "grad_norm": 0.2934231758117676,
      "learning_rate": 0.00019959694577193943,
      "loss": 2.4921,
      "step": 8420
    },
    {
      "epoch": 0.17342155271948345,
      "grad_norm": 0.28948405385017395,
      "learning_rate": 0.00019959495131143985,
      "loss": 2.4785,
      "step": 8430
    },
    {
      "epoch": 0.17362727223635116,
      "grad_norm": 0.3159981966018677,
      "learning_rate": 0.00019959295193846405,
      "loss": 2.4612,
      "step": 8440
    },
    {
      "epoch": 0.17383299175321887,
      "grad_norm": 0.3110564053058624,
      "learning_rate": 0.0001995909476531107,
      "loss": 2.4337,
      "step": 8450
    },
    {
      "epoch": 0.17403871127008658,
      "grad_norm": 0.36317580938339233,
      "learning_rate": 0.00019958893845547856,
      "loss": 2.4821,
      "step": 8460
    },
    {
      "epoch": 0.17424443078695429,
      "grad_norm": 0.3068574368953705,
      "learning_rate": 0.00019958692434566674,
      "loss": 2.4101,
      "step": 8470
    },
    {
      "epoch": 0.17445015030382202,
      "grad_norm": 0.2995978593826294,
      "learning_rate": 0.00019958490532377468,
      "loss": 2.3778,
      "step": 8480
    },
    {
      "epoch": 0.17465586982068973,
      "grad_norm": 0.3638255298137665,
      "learning_rate": 0.0001995828813899019,
      "loss": 2.5156,
      "step": 8490
    },
    {
      "epoch": 0.17486158933755744,
      "grad_norm": 0.33397814631462097,
      "learning_rate": 0.00019958085254414824,
      "loss": 2.4862,
      "step": 8500
    },
    {
      "epoch": 0.17506730885442515,
      "grad_norm": 0.3093731701374054,
      "learning_rate": 0.00019957881878661377,
      "loss": 2.4271,
      "step": 8510
    },
    {
      "epoch": 0.17527302837129288,
      "grad_norm": 0.2838977873325348,
      "learning_rate": 0.00019957678011739882,
      "loss": 2.5024,
      "step": 8520
    },
    {
      "epoch": 0.1754787478881606,
      "grad_norm": 0.33672472834587097,
      "learning_rate": 0.00019957473653660395,
      "loss": 2.5379,
      "step": 8530
    },
    {
      "epoch": 0.1756844674050283,
      "grad_norm": 0.3189390301704407,
      "learning_rate": 0.00019957268804432992,
      "loss": 2.4844,
      "step": 8540
    },
    {
      "epoch": 0.175890186921896,
      "grad_norm": 0.30490079522132874,
      "learning_rate": 0.00019957063464067778,
      "loss": 2.4537,
      "step": 8550
    },
    {
      "epoch": 0.17609590643876372,
      "grad_norm": 0.31048691272735596,
      "learning_rate": 0.00019956857632574883,
      "loss": 2.4853,
      "step": 8560
    },
    {
      "epoch": 0.17630162595563145,
      "grad_norm": 0.3772953152656555,
      "learning_rate": 0.00019956651309964465,
      "loss": 2.4841,
      "step": 8570
    },
    {
      "epoch": 0.17650734547249916,
      "grad_norm": 0.3044201731681824,
      "learning_rate": 0.00019956444496246689,
      "loss": 2.4887,
      "step": 8580
    },
    {
      "epoch": 0.17671306498936687,
      "grad_norm": 0.37320470809936523,
      "learning_rate": 0.00019956237191431765,
      "loss": 2.4712,
      "step": 8590
    },
    {
      "epoch": 0.17691878450623458,
      "grad_norm": 0.29024481773376465,
      "learning_rate": 0.00019956029395529914,
      "loss": 2.466,
      "step": 8600
    },
    {
      "epoch": 0.1771245040231023,
      "grad_norm": 0.3220975399017334,
      "learning_rate": 0.00019955821108551386,
      "loss": 2.4731,
      "step": 8610
    },
    {
      "epoch": 0.17733022353997002,
      "grad_norm": 0.4825293719768524,
      "learning_rate": 0.0001995561233050646,
      "loss": 2.4648,
      "step": 8620
    },
    {
      "epoch": 0.17753594305683773,
      "grad_norm": 0.32193776965141296,
      "learning_rate": 0.00019955403061405424,
      "loss": 2.4865,
      "step": 8630
    },
    {
      "epoch": 0.17774166257370544,
      "grad_norm": 0.31662532687187195,
      "learning_rate": 0.00019955193301258607,
      "loss": 2.4714,
      "step": 8640
    },
    {
      "epoch": 0.17794738209057315,
      "grad_norm": 0.29774075746536255,
      "learning_rate": 0.00019954983050076357,
      "loss": 2.5597,
      "step": 8650
    },
    {
      "epoch": 0.17815310160744088,
      "grad_norm": 0.33714422583580017,
      "learning_rate": 0.00019954772307869035,
      "loss": 2.4368,
      "step": 8660
    },
    {
      "epoch": 0.1783588211243086,
      "grad_norm": 0.32647374272346497,
      "learning_rate": 0.00019954561074647046,
      "loss": 2.4643,
      "step": 8670
    },
    {
      "epoch": 0.1785645406411763,
      "grad_norm": 0.3376840054988861,
      "learning_rate": 0.00019954349350420806,
      "loss": 2.4336,
      "step": 8680
    },
    {
      "epoch": 0.178770260158044,
      "grad_norm": 0.31406456232070923,
      "learning_rate": 0.00019954137135200757,
      "loss": 2.3985,
      "step": 8690
    },
    {
      "epoch": 0.17897597967491174,
      "grad_norm": 0.29924681782722473,
      "learning_rate": 0.00019953924428997363,
      "loss": 2.4828,
      "step": 8700
    },
    {
      "epoch": 0.17918169919177945,
      "grad_norm": 0.3163608908653259,
      "learning_rate": 0.00019953711231821125,
      "loss": 2.4736,
      "step": 8710
    },
    {
      "epoch": 0.17938741870864716,
      "grad_norm": 0.2919032871723175,
      "learning_rate": 0.0001995349754368255,
      "loss": 2.5053,
      "step": 8720
    },
    {
      "epoch": 0.17959313822551487,
      "grad_norm": 0.324136346578598,
      "learning_rate": 0.0001995328336459218,
      "loss": 2.4355,
      "step": 8730
    },
    {
      "epoch": 0.17979885774238258,
      "grad_norm": 0.328779399394989,
      "learning_rate": 0.00019953068694560582,
      "loss": 2.484,
      "step": 8740
    },
    {
      "epoch": 0.18000457725925031,
      "grad_norm": 0.32428091764450073,
      "learning_rate": 0.00019952853533598345,
      "loss": 2.4991,
      "step": 8750
    },
    {
      "epoch": 0.18021029677611802,
      "grad_norm": 0.3151005804538727,
      "learning_rate": 0.00019952637881716077,
      "loss": 2.5339,
      "step": 8760
    },
    {
      "epoch": 0.18041601629298573,
      "grad_norm": 0.3121671676635742,
      "learning_rate": 0.00019952421738924416,
      "loss": 2.4423,
      "step": 8770
    },
    {
      "epoch": 0.18062173580985344,
      "grad_norm": 0.3331892788410187,
      "learning_rate": 0.00019952205105234027,
      "loss": 2.4582,
      "step": 8780
    },
    {
      "epoch": 0.18082745532672118,
      "grad_norm": 0.3264451026916504,
      "learning_rate": 0.00019951987980655592,
      "loss": 2.5021,
      "step": 8790
    },
    {
      "epoch": 0.18103317484358888,
      "grad_norm": 0.3217250406742096,
      "learning_rate": 0.00019951770365199826,
      "loss": 2.5404,
      "step": 8800
    },
    {
      "epoch": 0.1812388943604566,
      "grad_norm": 0.32602179050445557,
      "learning_rate": 0.00019951552258877454,
      "loss": 2.4999,
      "step": 8810
    },
    {
      "epoch": 0.1814446138773243,
      "grad_norm": 0.3238198757171631,
      "learning_rate": 0.0001995133366169924,
      "loss": 2.4911,
      "step": 8820
    },
    {
      "epoch": 0.18165033339419204,
      "grad_norm": 0.3333653211593628,
      "learning_rate": 0.00019951114573675964,
      "loss": 2.5534,
      "step": 8830
    },
    {
      "epoch": 0.18185605291105975,
      "grad_norm": 0.3261537551879883,
      "learning_rate": 0.00019950894994818437,
      "loss": 2.5114,
      "step": 8840
    },
    {
      "epoch": 0.18206177242792745,
      "grad_norm": 0.3265519440174103,
      "learning_rate": 0.00019950674925137482,
      "loss": 2.4264,
      "step": 8850
    },
    {
      "epoch": 0.18226749194479516,
      "grad_norm": 0.3917374908924103,
      "learning_rate": 0.00019950454364643956,
      "loss": 2.5434,
      "step": 8860
    },
    {
      "epoch": 0.18247321146166287,
      "grad_norm": 0.29984116554260254,
      "learning_rate": 0.00019950233313348743,
      "loss": 2.4298,
      "step": 8870
    },
    {
      "epoch": 0.1826789309785306,
      "grad_norm": 0.31229838728904724,
      "learning_rate": 0.00019950011771262743,
      "loss": 2.4522,
      "step": 8880
    },
    {
      "epoch": 0.18288465049539832,
      "grad_norm": 0.2851927876472473,
      "learning_rate": 0.00019949789738396882,
      "loss": 2.4632,
      "step": 8890
    },
    {
      "epoch": 0.18309037001226602,
      "grad_norm": 0.3157372772693634,
      "learning_rate": 0.00019949567214762112,
      "loss": 2.4546,
      "step": 8900
    },
    {
      "epoch": 0.18329608952913373,
      "grad_norm": 0.3204069137573242,
      "learning_rate": 0.0001994934420036941,
      "loss": 2.5146,
      "step": 8910
    },
    {
      "epoch": 0.18350180904600147,
      "grad_norm": 0.3257351219654083,
      "learning_rate": 0.0001994912069522978,
      "loss": 2.4562,
      "step": 8920
    },
    {
      "epoch": 0.18370752856286918,
      "grad_norm": 0.3340483605861664,
      "learning_rate": 0.0001994889669935424,
      "loss": 2.5263,
      "step": 8930
    },
    {
      "epoch": 0.18391324807973689,
      "grad_norm": 0.2975577116012573,
      "learning_rate": 0.0001994867221275384,
      "loss": 2.4937,
      "step": 8940
    },
    {
      "epoch": 0.1841189675966046,
      "grad_norm": 0.3325275778770447,
      "learning_rate": 0.00019948447235439653,
      "loss": 2.5157,
      "step": 8950
    },
    {
      "epoch": 0.1843246871134723,
      "grad_norm": 0.3312109708786011,
      "learning_rate": 0.00019948221767422778,
      "loss": 2.479,
      "step": 8960
    },
    {
      "epoch": 0.18453040663034004,
      "grad_norm": 0.3303583562374115,
      "learning_rate": 0.00019947995808714333,
      "loss": 2.4378,
      "step": 8970
    },
    {
      "epoch": 0.18473612614720775,
      "grad_norm": 0.30671414732933044,
      "learning_rate": 0.00019947769359325465,
      "loss": 2.4553,
      "step": 8980
    },
    {
      "epoch": 0.18494184566407545,
      "grad_norm": 0.3478755056858063,
      "learning_rate": 0.00019947542419267346,
      "loss": 2.5564,
      "step": 8990
    },
    {
      "epoch": 0.18514756518094316,
      "grad_norm": 0.3282041549682617,
      "learning_rate": 0.00019947314988551164,
      "loss": 2.4753,
      "step": 9000
    },
    {
      "epoch": 0.1853532846978109,
      "grad_norm": 0.3090152144432068,
      "learning_rate": 0.00019947087067188142,
      "loss": 2.4834,
      "step": 9010
    },
    {
      "epoch": 0.1855590042146786,
      "grad_norm": 0.3082723021507263,
      "learning_rate": 0.00019946858655189517,
      "loss": 2.4645,
      "step": 9020
    },
    {
      "epoch": 0.18576472373154632,
      "grad_norm": 0.31323039531707764,
      "learning_rate": 0.00019946629752566565,
      "loss": 2.5305,
      "step": 9030
    },
    {
      "epoch": 0.18597044324841402,
      "grad_norm": 0.3637779951095581,
      "learning_rate": 0.0001994640035933056,
      "loss": 2.5028,
      "step": 9040
    },
    {
      "epoch": 0.18617616276528173,
      "grad_norm": 0.3089400827884674,
      "learning_rate": 0.00019946170475492836,
      "loss": 2.4283,
      "step": 9050
    },
    {
      "epoch": 0.18638188228214947,
      "grad_norm": 0.3322465717792511,
      "learning_rate": 0.00019945940101064716,
      "loss": 2.5152,
      "step": 9060
    },
    {
      "epoch": 0.18658760179901718,
      "grad_norm": 0.3698974847793579,
      "learning_rate": 0.00019945709236057573,
      "loss": 2.4832,
      "step": 9070
    },
    {
      "epoch": 0.18679332131588489,
      "grad_norm": 0.33086082339286804,
      "learning_rate": 0.0001994547788048279,
      "loss": 2.4937,
      "step": 9080
    },
    {
      "epoch": 0.1869990408327526,
      "grad_norm": 0.3131813406944275,
      "learning_rate": 0.0001994524603435178,
      "loss": 2.4267,
      "step": 9090
    },
    {
      "epoch": 0.18720476034962033,
      "grad_norm": 0.3474844992160797,
      "learning_rate": 0.0001994501369767598,
      "loss": 2.499,
      "step": 9100
    },
    {
      "epoch": 0.18741047986648804,
      "grad_norm": 0.32142066955566406,
      "learning_rate": 0.00019944780870466845,
      "loss": 2.4758,
      "step": 9110
    },
    {
      "epoch": 0.18761619938335575,
      "grad_norm": 0.30913981795310974,
      "learning_rate": 0.00019944547552735865,
      "loss": 2.5113,
      "step": 9120
    },
    {
      "epoch": 0.18782191890022346,
      "grad_norm": 0.2951039671897888,
      "learning_rate": 0.00019944313744494542,
      "loss": 2.5187,
      "step": 9130
    },
    {
      "epoch": 0.18802763841709116,
      "grad_norm": 0.3211038410663605,
      "learning_rate": 0.00019944079445754416,
      "loss": 2.5284,
      "step": 9140
    },
    {
      "epoch": 0.1882333579339589,
      "grad_norm": 0.3536459803581238,
      "learning_rate": 0.00019943844656527038,
      "loss": 2.4835,
      "step": 9150
    },
    {
      "epoch": 0.1884390774508266,
      "grad_norm": 0.33433955907821655,
      "learning_rate": 0.00019943609376823988,
      "loss": 2.5367,
      "step": 9160
    },
    {
      "epoch": 0.18864479696769432,
      "grad_norm": 0.3252914249897003,
      "learning_rate": 0.00019943373606656875,
      "loss": 2.4221,
      "step": 9170
    },
    {
      "epoch": 0.18885051648456203,
      "grad_norm": 0.3219344913959503,
      "learning_rate": 0.0001994313734603733,
      "loss": 2.4581,
      "step": 9180
    },
    {
      "epoch": 0.18905623600142976,
      "grad_norm": 0.31944766640663147,
      "learning_rate": 0.00019942900594977002,
      "loss": 2.4794,
      "step": 9190
    },
    {
      "epoch": 0.18926195551829747,
      "grad_norm": 0.3012363612651825,
      "learning_rate": 0.00019942663353487568,
      "loss": 2.4244,
      "step": 9200
    },
    {
      "epoch": 0.18946767503516518,
      "grad_norm": 0.3646935522556305,
      "learning_rate": 0.00019942425621580733,
      "loss": 2.5096,
      "step": 9210
    },
    {
      "epoch": 0.1896733945520329,
      "grad_norm": 0.3089256286621094,
      "learning_rate": 0.0001994218739926822,
      "loss": 2.4893,
      "step": 9220
    },
    {
      "epoch": 0.18987911406890062,
      "grad_norm": 0.3289298117160797,
      "learning_rate": 0.00019941948686561783,
      "loss": 2.4441,
      "step": 9230
    },
    {
      "epoch": 0.19008483358576833,
      "grad_norm": 0.30099472403526306,
      "learning_rate": 0.00019941709483473194,
      "loss": 2.4666,
      "step": 9240
    },
    {
      "epoch": 0.19029055310263604,
      "grad_norm": 0.3344591557979584,
      "learning_rate": 0.00019941469790014251,
      "loss": 2.4667,
      "step": 9250
    },
    {
      "epoch": 0.19049627261950375,
      "grad_norm": 0.33307206630706787,
      "learning_rate": 0.0001994122960619678,
      "loss": 2.4771,
      "step": 9260
    },
    {
      "epoch": 0.19070199213637146,
      "grad_norm": 0.34925466775894165,
      "learning_rate": 0.00019940988932032626,
      "loss": 2.4442,
      "step": 9270
    },
    {
      "epoch": 0.1909077116532392,
      "grad_norm": 0.330102801322937,
      "learning_rate": 0.00019940747767533657,
      "loss": 2.4315,
      "step": 9280
    },
    {
      "epoch": 0.1911134311701069,
      "grad_norm": 0.34781157970428467,
      "learning_rate": 0.00019940506112711774,
      "loss": 2.4516,
      "step": 9290
    },
    {
      "epoch": 0.1913191506869746,
      "grad_norm": 0.3082883954048157,
      "learning_rate": 0.00019940263967578893,
      "loss": 2.4414,
      "step": 9300
    },
    {
      "epoch": 0.19152487020384232,
      "grad_norm": 0.32498225569725037,
      "learning_rate": 0.00019940021332146956,
      "loss": 2.5019,
      "step": 9310
    },
    {
      "epoch": 0.19173058972071005,
      "grad_norm": 0.311790406703949,
      "learning_rate": 0.00019939778206427936,
      "loss": 2.4895,
      "step": 9320
    },
    {
      "epoch": 0.19193630923757776,
      "grad_norm": 0.32955440878868103,
      "learning_rate": 0.00019939534590433818,
      "loss": 2.4734,
      "step": 9330
    },
    {
      "epoch": 0.19214202875444547,
      "grad_norm": 0.3365033268928528,
      "learning_rate": 0.0001993929048417663,
      "loss": 2.5273,
      "step": 9340
    },
    {
      "epoch": 0.19234774827131318,
      "grad_norm": 0.35204142332077026,
      "learning_rate": 0.000199390458876684,
      "loss": 2.4798,
      "step": 9350
    },
    {
      "epoch": 0.1925534677881809,
      "grad_norm": 0.3729337751865387,
      "learning_rate": 0.00019938800800921197,
      "loss": 2.4535,
      "step": 9360
    },
    {
      "epoch": 0.19275918730504862,
      "grad_norm": 0.33231833577156067,
      "learning_rate": 0.00019938555223947114,
      "loss": 2.4222,
      "step": 9370
    },
    {
      "epoch": 0.19296490682191633,
      "grad_norm": 0.34077951312065125,
      "learning_rate": 0.0001993830915675826,
      "loss": 2.4269,
      "step": 9380
    },
    {
      "epoch": 0.19317062633878404,
      "grad_norm": 0.33239978551864624,
      "learning_rate": 0.0001993806259936677,
      "loss": 2.4117,
      "step": 9390
    },
    {
      "epoch": 0.19337634585565175,
      "grad_norm": 0.31791025400161743,
      "learning_rate": 0.00019937815551784808,
      "loss": 2.4905,
      "step": 9400
    },
    {
      "epoch": 0.19358206537251949,
      "grad_norm": 0.30783212184906006,
      "learning_rate": 0.00019937568014024562,
      "loss": 2.4565,
      "step": 9410
    },
    {
      "epoch": 0.1937877848893872,
      "grad_norm": 0.33385396003723145,
      "learning_rate": 0.00019937319986098237,
      "loss": 2.4617,
      "step": 9420
    },
    {
      "epoch": 0.1939935044062549,
      "grad_norm": 0.4454164206981659,
      "learning_rate": 0.0001993707146801807,
      "loss": 2.4963,
      "step": 9430
    },
    {
      "epoch": 0.1941992239231226,
      "grad_norm": 0.31239283084869385,
      "learning_rate": 0.00019936822459796317,
      "loss": 2.4959,
      "step": 9440
    },
    {
      "epoch": 0.19440494343999032,
      "grad_norm": 0.3219435214996338,
      "learning_rate": 0.0001993657296144526,
      "loss": 2.4781,
      "step": 9450
    },
    {
      "epoch": 0.19461066295685805,
      "grad_norm": 0.30166780948638916,
      "learning_rate": 0.0001993632297297721,
      "loss": 2.4289,
      "step": 9460
    },
    {
      "epoch": 0.19481638247372576,
      "grad_norm": 0.31523576378822327,
      "learning_rate": 0.00019936072494404494,
      "loss": 2.4558,
      "step": 9470
    },
    {
      "epoch": 0.19502210199059347,
      "grad_norm": 0.29959428310394287,
      "learning_rate": 0.00019935821525739463,
      "loss": 2.4357,
      "step": 9480
    },
    {
      "epoch": 0.19522782150746118,
      "grad_norm": 0.3218022584915161,
      "learning_rate": 0.00019935570066994504,
      "loss": 2.4723,
      "step": 9490
    },
    {
      "epoch": 0.19543354102432892,
      "grad_norm": 0.32261839509010315,
      "learning_rate": 0.00019935318118182016,
      "loss": 2.4826,
      "step": 9500
    },
    {
      "epoch": 0.19563926054119662,
      "grad_norm": 0.42650967836380005,
      "learning_rate": 0.00019935065679314425,
      "loss": 2.431,
      "step": 9510
    },
    {
      "epoch": 0.19584498005806433,
      "grad_norm": 0.36069804430007935,
      "learning_rate": 0.00019934812750404185,
      "loss": 2.5221,
      "step": 9520
    },
    {
      "epoch": 0.19605069957493204,
      "grad_norm": 0.34167248010635376,
      "learning_rate": 0.00019934559331463768,
      "loss": 2.4577,
      "step": 9530
    },
    {
      "epoch": 0.19625641909179975,
      "grad_norm": 0.33677181601524353,
      "learning_rate": 0.00019934305422505676,
      "loss": 2.4746,
      "step": 9540
    },
    {
      "epoch": 0.19646213860866749,
      "grad_norm": 0.36354860663414,
      "learning_rate": 0.00019934051023542436,
      "loss": 2.494,
      "step": 9550
    },
    {
      "epoch": 0.1966678581255352,
      "grad_norm": 0.3403088450431824,
      "learning_rate": 0.00019933796134586587,
      "loss": 2.4705,
      "step": 9560
    },
    {
      "epoch": 0.1968735776424029,
      "grad_norm": 0.32672354578971863,
      "learning_rate": 0.00019933540755650713,
      "loss": 2.4424,
      "step": 9570
    },
    {
      "epoch": 0.1970792971592706,
      "grad_norm": 0.3230912387371063,
      "learning_rate": 0.00019933284886747404,
      "loss": 2.4911,
      "step": 9580
    },
    {
      "epoch": 0.19728501667613835,
      "grad_norm": 0.3312944769859314,
      "learning_rate": 0.0001993302852788928,
      "loss": 2.4852,
      "step": 9590
    },
    {
      "epoch": 0.19749073619300606,
      "grad_norm": 0.32318422198295593,
      "learning_rate": 0.0001993277167908899,
      "loss": 2.4631,
      "step": 9600
    },
    {
      "epoch": 0.19769645570987376,
      "grad_norm": 0.34191539883613586,
      "learning_rate": 0.00019932514340359196,
      "loss": 2.4655,
      "step": 9610
    },
    {
      "epoch": 0.19790217522674147,
      "grad_norm": 0.331247478723526,
      "learning_rate": 0.00019932256511712596,
      "loss": 2.491,
      "step": 9620
    },
    {
      "epoch": 0.19810789474360918,
      "grad_norm": 0.30937227606773376,
      "learning_rate": 0.00019931998193161907,
      "loss": 2.4658,
      "step": 9630
    },
    {
      "epoch": 0.19831361426047692,
      "grad_norm": 0.33300554752349854,
      "learning_rate": 0.0001993173938471987,
      "loss": 2.4931,
      "step": 9640
    },
    {
      "epoch": 0.19851933377734463,
      "grad_norm": 0.3078831434249878,
      "learning_rate": 0.00019931480086399255,
      "loss": 2.4988,
      "step": 9650
    },
    {
      "epoch": 0.19872505329421233,
      "grad_norm": 0.32460835576057434,
      "learning_rate": 0.00019931220298212844,
      "loss": 2.5046,
      "step": 9660
    },
    {
      "epoch": 0.19893077281108004,
      "grad_norm": 0.32413142919540405,
      "learning_rate": 0.00019930960020173453,
      "loss": 2.4212,
      "step": 9670
    },
    {
      "epoch": 0.19913649232794778,
      "grad_norm": 0.34731796383857727,
      "learning_rate": 0.0001993069925229392,
      "loss": 2.4771,
      "step": 9680
    },
    {
      "epoch": 0.1993422118448155,
      "grad_norm": 0.3262815475463867,
      "learning_rate": 0.00019930437994587115,
      "loss": 2.4746,
      "step": 9690
    },
    {
      "epoch": 0.1995479313616832,
      "grad_norm": 0.3335166573524475,
      "learning_rate": 0.00019930176247065913,
      "loss": 2.5222,
      "step": 9700
    },
    {
      "epoch": 0.1997536508785509,
      "grad_norm": 0.3380264639854431,
      "learning_rate": 0.0001992991400974323,
      "loss": 2.5118,
      "step": 9710
    },
    {
      "epoch": 0.19995937039541864,
      "grad_norm": 0.3319474756717682,
      "learning_rate": 0.00019929651282632002,
      "loss": 2.5025,
      "step": 9720
    },
    {
      "epoch": 0.20016508991228635,
      "grad_norm": 0.3277375400066376,
      "learning_rate": 0.00019929388065745187,
      "loss": 2.4652,
      "step": 9730
    },
    {
      "epoch": 0.20037080942915406,
      "grad_norm": 0.3078034520149231,
      "learning_rate": 0.00019929124359095767,
      "loss": 2.4882,
      "step": 9740
    },
    {
      "epoch": 0.20057652894602176,
      "grad_norm": 0.3406197130680084,
      "learning_rate": 0.0001992886016269675,
      "loss": 2.5209,
      "step": 9750
    },
    {
      "epoch": 0.20078224846288947,
      "grad_norm": 0.30417290329933167,
      "learning_rate": 0.00019928595476561162,
      "loss": 2.4681,
      "step": 9760
    },
    {
      "epoch": 0.2009879679797572,
      "grad_norm": 0.34987980127334595,
      "learning_rate": 0.0001992833030070207,
      "loss": 2.481,
      "step": 9770
    },
    {
      "epoch": 0.20119368749662492,
      "grad_norm": 0.2950115203857422,
      "learning_rate": 0.00019928064635132544,
      "loss": 2.445,
      "step": 9780
    },
    {
      "epoch": 0.20139940701349263,
      "grad_norm": 0.3200487792491913,
      "learning_rate": 0.0001992779847986569,
      "loss": 2.5059,
      "step": 9790
    },
    {
      "epoch": 0.20160512653036033,
      "grad_norm": 0.34216466546058655,
      "learning_rate": 0.00019927531834914642,
      "loss": 2.5073,
      "step": 9800
    },
    {
      "epoch": 0.20181084604722807,
      "grad_norm": 0.34079116582870483,
      "learning_rate": 0.00019927264700292542,
      "loss": 2.4103,
      "step": 9810
    },
    {
      "epoch": 0.20201656556409578,
      "grad_norm": 0.31766268610954285,
      "learning_rate": 0.00019926997076012573,
      "loss": 2.3916,
      "step": 9820
    },
    {
      "epoch": 0.2022222850809635,
      "grad_norm": 0.3113534152507782,
      "learning_rate": 0.00019926728962087935,
      "loss": 2.4856,
      "step": 9830
    },
    {
      "epoch": 0.2024280045978312,
      "grad_norm": 0.3510902225971222,
      "learning_rate": 0.00019926460358531852,
      "loss": 2.4547,
      "step": 9840
    },
    {
      "epoch": 0.2026337241146989,
      "grad_norm": 0.3303961455821991,
      "learning_rate": 0.0001992619126535757,
      "loss": 2.4787,
      "step": 9850
    },
    {
      "epoch": 0.20283944363156664,
      "grad_norm": 0.3263988196849823,
      "learning_rate": 0.00019925921682578364,
      "loss": 2.494,
      "step": 9860
    },
    {
      "epoch": 0.20304516314843435,
      "grad_norm": 0.32719165086746216,
      "learning_rate": 0.0001992565161020753,
      "loss": 2.5098,
      "step": 9870
    },
    {
      "epoch": 0.20325088266530206,
      "grad_norm": 0.3199273347854614,
      "learning_rate": 0.00019925381048258394,
      "loss": 2.4752,
      "step": 9880
    },
    {
      "epoch": 0.20345660218216977,
      "grad_norm": 0.3161574602127075,
      "learning_rate": 0.00019925109996744297,
      "loss": 2.4569,
      "step": 9890
    },
    {
      "epoch": 0.2036623216990375,
      "grad_norm": 0.34324607253074646,
      "learning_rate": 0.0001992483845567861,
      "loss": 2.4754,
      "step": 9900
    },
    {
      "epoch": 0.2038680412159052,
      "grad_norm": 0.3405337333679199,
      "learning_rate": 0.00019924566425074726,
      "loss": 2.4294,
      "step": 9910
    },
    {
      "epoch": 0.20407376073277292,
      "grad_norm": 0.3184560239315033,
      "learning_rate": 0.0001992429390494606,
      "loss": 2.5018,
      "step": 9920
    },
    {
      "epoch": 0.20427948024964063,
      "grad_norm": 0.3052673041820526,
      "learning_rate": 0.00019924020895306055,
      "loss": 2.4767,
      "step": 9930
    },
    {
      "epoch": 0.20448519976650834,
      "grad_norm": 0.3098764717578888,
      "learning_rate": 0.0001992374739616818,
      "loss": 2.4247,
      "step": 9940
    },
    {
      "epoch": 0.20469091928337607,
      "grad_norm": 0.33231332898139954,
      "learning_rate": 0.00019923473407545928,
      "loss": 2.4377,
      "step": 9950
    },
    {
      "epoch": 0.20489663880024378,
      "grad_norm": 0.3238327205181122,
      "learning_rate": 0.00019923198929452807,
      "loss": 2.4763,
      "step": 9960
    },
    {
      "epoch": 0.2051023583171115,
      "grad_norm": 0.32535800337791443,
      "learning_rate": 0.00019922923961902358,
      "loss": 2.3981,
      "step": 9970
    },
    {
      "epoch": 0.2053080778339792,
      "grad_norm": 0.31477978825569153,
      "learning_rate": 0.00019922648504908138,
      "loss": 2.4606,
      "step": 9980
    },
    {
      "epoch": 0.20551379735084693,
      "grad_norm": 0.41518455743789673,
      "learning_rate": 0.00019922372558483747,
      "loss": 2.4943,
      "step": 9990
    },
    {
      "epoch": 0.20571951686771464,
      "grad_norm": 0.3416023254394531,
      "learning_rate": 0.00019922096122642783,
      "loss": 2.505,
      "step": 10000
    },
    {
      "epoch": 0.20592523638458235,
      "grad_norm": 0.3033093214035034,
      "learning_rate": 0.0001992181919739889,
      "loss": 2.4825,
      "step": 10010
    },
    {
      "epoch": 0.20613095590145006,
      "grad_norm": 0.30910080671310425,
      "learning_rate": 0.0001992154178276572,
      "loss": 2.4473,
      "step": 10020
    },
    {
      "epoch": 0.20633667541831777,
      "grad_norm": 0.3518248498439789,
      "learning_rate": 0.00019921263878756965,
      "loss": 2.4743,
      "step": 10030
    },
    {
      "epoch": 0.2065423949351855,
      "grad_norm": 0.32416704297065735,
      "learning_rate": 0.0001992098548538632,
      "loss": 2.4861,
      "step": 10040
    },
    {
      "epoch": 0.2067481144520532,
      "grad_norm": 0.3369573652744293,
      "learning_rate": 0.0001992070660266753,
      "loss": 2.4473,
      "step": 10050
    },
    {
      "epoch": 0.20695383396892092,
      "grad_norm": 0.33426979184150696,
      "learning_rate": 0.0001992042723061434,
      "loss": 2.4852,
      "step": 10060
    },
    {
      "epoch": 0.20715955348578863,
      "grad_norm": 0.33557868003845215,
      "learning_rate": 0.00019920147369240535,
      "loss": 2.5138,
      "step": 10070
    },
    {
      "epoch": 0.20736527300265636,
      "grad_norm": 0.2945903539657593,
      "learning_rate": 0.00019919867018559922,
      "loss": 2.5111,
      "step": 10080
    },
    {
      "epoch": 0.20757099251952407,
      "grad_norm": 0.34050363302230835,
      "learning_rate": 0.00019919586178586326,
      "loss": 2.4294,
      "step": 10090
    },
    {
      "epoch": 0.20777671203639178,
      "grad_norm": 0.32982322573661804,
      "learning_rate": 0.00019919304849333596,
      "loss": 2.4147,
      "step": 10100
    },
    {
      "epoch": 0.2079824315532595,
      "grad_norm": 0.3060300946235657,
      "learning_rate": 0.00019919023030815614,
      "loss": 2.5152,
      "step": 10110
    },
    {
      "epoch": 0.20818815107012723,
      "grad_norm": 0.33133020997047424,
      "learning_rate": 0.00019918740723046273,
      "loss": 2.5107,
      "step": 10120
    },
    {
      "epoch": 0.20839387058699493,
      "grad_norm": 0.3233058750629425,
      "learning_rate": 0.00019918457926039509,
      "loss": 2.4907,
      "step": 10130
    },
    {
      "epoch": 0.20859959010386264,
      "grad_norm": 0.32973262667655945,
      "learning_rate": 0.00019918174639809263,
      "loss": 2.4311,
      "step": 10140
    },
    {
      "epoch": 0.20880530962073035,
      "grad_norm": 0.3160794675350189,
      "learning_rate": 0.00019917890864369509,
      "loss": 2.4674,
      "step": 10150
    },
    {
      "epoch": 0.20901102913759806,
      "grad_norm": 0.3261222839355469,
      "learning_rate": 0.00019917606599734243,
      "loss": 2.5027,
      "step": 10160
    },
    {
      "epoch": 0.2092167486544658,
      "grad_norm": 0.3398030698299408,
      "learning_rate": 0.0001991732184591749,
      "loss": 2.4544,
      "step": 10170
    },
    {
      "epoch": 0.2094224681713335,
      "grad_norm": 0.34958982467651367,
      "learning_rate": 0.00019917036602933293,
      "loss": 2.4309,
      "step": 10180
    },
    {
      "epoch": 0.2096281876882012,
      "grad_norm": 0.30432239174842834,
      "learning_rate": 0.0001991675087079572,
      "loss": 2.4863,
      "step": 10190
    },
    {
      "epoch": 0.20983390720506892,
      "grad_norm": 0.3641868829727173,
      "learning_rate": 0.00019916464649518867,
      "loss": 2.5042,
      "step": 10200
    },
    {
      "epoch": 0.21003962672193666,
      "grad_norm": 0.33906325697898865,
      "learning_rate": 0.00019916177939116852,
      "loss": 2.5168,
      "step": 10210
    },
    {
      "epoch": 0.21024534623880436,
      "grad_norm": 0.3332676887512207,
      "learning_rate": 0.00019915890739603817,
      "loss": 2.4324,
      "step": 10220
    },
    {
      "epoch": 0.21045106575567207,
      "grad_norm": 0.34702157974243164,
      "learning_rate": 0.00019915603050993925,
      "loss": 2.4557,
      "step": 10230
    },
    {
      "epoch": 0.21065678527253978,
      "grad_norm": 0.3155815303325653,
      "learning_rate": 0.0001991531487330137,
      "loss": 2.4442,
      "step": 10240
    },
    {
      "epoch": 0.2108625047894075,
      "grad_norm": 0.3418843448162079,
      "learning_rate": 0.00019915026206540363,
      "loss": 2.483,
      "step": 10250
    },
    {
      "epoch": 0.21106822430627523,
      "grad_norm": 0.3096221387386322,
      "learning_rate": 0.00019914737050725142,
      "loss": 2.4643,
      "step": 10260
    },
    {
      "epoch": 0.21127394382314293,
      "grad_norm": 0.31138381361961365,
      "learning_rate": 0.00019914447405869974,
      "loss": 2.4725,
      "step": 10270
    },
    {
      "epoch": 0.21147966334001064,
      "grad_norm": 0.32826581597328186,
      "learning_rate": 0.0001991415727198914,
      "loss": 2.4282,
      "step": 10280
    },
    {
      "epoch": 0.21168538285687835,
      "grad_norm": 0.31600984930992126,
      "learning_rate": 0.00019913866649096958,
      "loss": 2.4349,
      "step": 10290
    },
    {
      "epoch": 0.2118911023737461,
      "grad_norm": 0.3784540891647339,
      "learning_rate": 0.00019913575537207755,
      "loss": 2.4755,
      "step": 10300
    },
    {
      "epoch": 0.2120968218906138,
      "grad_norm": 0.5096489191055298,
      "learning_rate": 0.0001991328393633589,
      "loss": 2.4913,
      "step": 10310
    },
    {
      "epoch": 0.2123025414074815,
      "grad_norm": 0.32391273975372314,
      "learning_rate": 0.00019912991846495756,
      "loss": 2.4722,
      "step": 10320
    },
    {
      "epoch": 0.2125082609243492,
      "grad_norm": 0.3949863016605377,
      "learning_rate": 0.00019912699267701748,
      "loss": 2.4505,
      "step": 10330
    },
    {
      "epoch": 0.21271398044121692,
      "grad_norm": 0.3182070851325989,
      "learning_rate": 0.00019912406199968307,
      "loss": 2.4835,
      "step": 10340
    },
    {
      "epoch": 0.21291969995808466,
      "grad_norm": 0.32976338267326355,
      "learning_rate": 0.00019912112643309882,
      "loss": 2.4768,
      "step": 10350
    },
    {
      "epoch": 0.21312541947495237,
      "grad_norm": 0.3472321629524231,
      "learning_rate": 0.00019911818597740958,
      "loss": 2.4299,
      "step": 10360
    },
    {
      "epoch": 0.21333113899182007,
      "grad_norm": 0.3347379267215729,
      "learning_rate": 0.0001991152406327603,
      "loss": 2.4764,
      "step": 10370
    },
    {
      "epoch": 0.21353685850868778,
      "grad_norm": 0.3118552565574646,
      "learning_rate": 0.00019911229039929634,
      "loss": 2.4787,
      "step": 10380
    },
    {
      "epoch": 0.21374257802555552,
      "grad_norm": 0.32356932759284973,
      "learning_rate": 0.0001991093352771632,
      "loss": 2.472,
      "step": 10390
    },
    {
      "epoch": 0.21394829754242323,
      "grad_norm": 0.3490203619003296,
      "learning_rate": 0.00019910637526650662,
      "loss": 2.4856,
      "step": 10400
    },
    {
      "epoch": 0.21415401705929094,
      "grad_norm": 0.3298364579677582,
      "learning_rate": 0.00019910341036747262,
      "loss": 2.4596,
      "step": 10410
    },
    {
      "epoch": 0.21435973657615864,
      "grad_norm": 0.34440264105796814,
      "learning_rate": 0.00019910044058020745,
      "loss": 2.4537,
      "step": 10420
    },
    {
      "epoch": 0.21456545609302635,
      "grad_norm": 0.3402847647666931,
      "learning_rate": 0.00019909746590485755,
      "loss": 2.4337,
      "step": 10430
    },
    {
      "epoch": 0.2147711756098941,
      "grad_norm": 0.37444740533828735,
      "learning_rate": 0.0001990944863415697,
      "loss": 2.4239,
      "step": 10440
    },
    {
      "epoch": 0.2149768951267618,
      "grad_norm": 0.33268219232559204,
      "learning_rate": 0.00019909150189049082,
      "loss": 2.4657,
      "step": 10450
    },
    {
      "epoch": 0.2151826146436295,
      "grad_norm": 0.3365817070007324,
      "learning_rate": 0.00019908851255176813,
      "loss": 2.5266,
      "step": 10460
    },
    {
      "epoch": 0.2153883341604972,
      "grad_norm": 0.32435521483421326,
      "learning_rate": 0.0001990855183255491,
      "loss": 2.4399,
      "step": 10470
    },
    {
      "epoch": 0.21559405367736495,
      "grad_norm": 0.32941070199012756,
      "learning_rate": 0.0001990825192119814,
      "loss": 2.4432,
      "step": 10480
    },
    {
      "epoch": 0.21579977319423266,
      "grad_norm": 0.3580717146396637,
      "learning_rate": 0.00019907951521121295,
      "loss": 2.4652,
      "step": 10490
    },
    {
      "epoch": 0.21600549271110037,
      "grad_norm": 0.3388438820838928,
      "learning_rate": 0.00019907650632339195,
      "loss": 2.4987,
      "step": 10500
    },
    {
      "epoch": 0.21621121222796807,
      "grad_norm": 0.3108331263065338,
      "learning_rate": 0.00019907349254866675,
      "loss": 2.4957,
      "step": 10510
    },
    {
      "epoch": 0.2164169317448358,
      "grad_norm": 0.33109092712402344,
      "learning_rate": 0.00019907047388718611,
      "loss": 2.4679,
      "step": 10520
    },
    {
      "epoch": 0.21662265126170352,
      "grad_norm": 0.323932409286499,
      "learning_rate": 0.00019906745033909884,
      "loss": 2.4276,
      "step": 10530
    },
    {
      "epoch": 0.21682837077857123,
      "grad_norm": 0.3183653652667999,
      "learning_rate": 0.0001990644219045541,
      "loss": 2.4635,
      "step": 10540
    },
    {
      "epoch": 0.21703409029543894,
      "grad_norm": 0.39943927526474,
      "learning_rate": 0.00019906138858370124,
      "loss": 2.4365,
      "step": 10550
    },
    {
      "epoch": 0.21723980981230664,
      "grad_norm": 0.35097748041152954,
      "learning_rate": 0.00019905835037668993,
      "loss": 2.4084,
      "step": 10560
    },
    {
      "epoch": 0.21744552932917438,
      "grad_norm": 0.3687262535095215,
      "learning_rate": 0.00019905530728367,
      "loss": 2.4301,
      "step": 10570
    },
    {
      "epoch": 0.2176512488460421,
      "grad_norm": 0.34339097142219543,
      "learning_rate": 0.00019905225930479152,
      "loss": 2.4602,
      "step": 10580
    },
    {
      "epoch": 0.2178569683629098,
      "grad_norm": 0.3659166693687439,
      "learning_rate": 0.00019904920644020488,
      "loss": 2.4556,
      "step": 10590
    },
    {
      "epoch": 0.2180626878797775,
      "grad_norm": 0.35384541749954224,
      "learning_rate": 0.00019904614869006065,
      "loss": 2.5229,
      "step": 10600
    },
    {
      "epoch": 0.21826840739664524,
      "grad_norm": 0.3396588861942291,
      "learning_rate": 0.00019904308605450964,
      "loss": 2.4929,
      "step": 10610
    },
    {
      "epoch": 0.21847412691351295,
      "grad_norm": 0.34973835945129395,
      "learning_rate": 0.00019904001853370288,
      "loss": 2.4797,
      "step": 10620
    },
    {
      "epoch": 0.21867984643038066,
      "grad_norm": 0.3385082185268402,
      "learning_rate": 0.00019903694612779176,
      "loss": 2.4881,
      "step": 10630
    },
    {
      "epoch": 0.21888556594724837,
      "grad_norm": 0.3456483483314514,
      "learning_rate": 0.0001990338688369277,
      "loss": 2.4835,
      "step": 10640
    },
    {
      "epoch": 0.21909128546411608,
      "grad_norm": 0.33397212624549866,
      "learning_rate": 0.00019903078666126263,
      "loss": 2.4541,
      "step": 10650
    },
    {
      "epoch": 0.2192970049809838,
      "grad_norm": 0.32345902919769287,
      "learning_rate": 0.0001990276996009485,
      "loss": 2.5246,
      "step": 10660
    },
    {
      "epoch": 0.21950272449785152,
      "grad_norm": 0.3734922409057617,
      "learning_rate": 0.00019902460765613755,
      "loss": 2.4942,
      "step": 10670
    },
    {
      "epoch": 0.21970844401471923,
      "grad_norm": 0.3472301959991455,
      "learning_rate": 0.00019902151082698235,
      "loss": 2.4244,
      "step": 10680
    },
    {
      "epoch": 0.21991416353158694,
      "grad_norm": 0.35985469818115234,
      "learning_rate": 0.0001990184091136356,
      "loss": 2.4424,
      "step": 10690
    },
    {
      "epoch": 0.22011988304845467,
      "grad_norm": 0.315579891204834,
      "learning_rate": 0.00019901530251625036,
      "loss": 2.4194,
      "step": 10700
    },
    {
      "epoch": 0.22032560256532238,
      "grad_norm": 0.3499460816383362,
      "learning_rate": 0.00019901219103497976,
      "loss": 2.4977,
      "step": 10710
    },
    {
      "epoch": 0.2205313220821901,
      "grad_norm": 0.3114253282546997,
      "learning_rate": 0.00019900907466997736,
      "loss": 2.4315,
      "step": 10720
    },
    {
      "epoch": 0.2207370415990578,
      "grad_norm": 0.3385411202907562,
      "learning_rate": 0.00019900595342139683,
      "loss": 2.4926,
      "step": 10730
    },
    {
      "epoch": 0.2209427611159255,
      "grad_norm": 0.3646399676799774,
      "learning_rate": 0.00019900282728939215,
      "loss": 2.4947,
      "step": 10740
    },
    {
      "epoch": 0.22114848063279324,
      "grad_norm": 0.3267088532447815,
      "learning_rate": 0.0001989996962741175,
      "loss": 2.4672,
      "step": 10750
    },
    {
      "epoch": 0.22135420014966095,
      "grad_norm": 0.35613101720809937,
      "learning_rate": 0.0001989965603757273,
      "loss": 2.4546,
      "step": 10760
    },
    {
      "epoch": 0.22155991966652866,
      "grad_norm": 0.3355243504047394,
      "learning_rate": 0.00019899341959437626,
      "loss": 2.4505,
      "step": 10770
    },
    {
      "epoch": 0.22176563918339637,
      "grad_norm": 0.3101673424243927,
      "learning_rate": 0.00019899027393021926,
      "loss": 2.4609,
      "step": 10780
    },
    {
      "epoch": 0.2219713587002641,
      "grad_norm": 0.34186017513275146,
      "learning_rate": 0.00019898712338341148,
      "loss": 2.4501,
      "step": 10790
    },
    {
      "epoch": 0.2221770782171318,
      "grad_norm": 0.32084882259368896,
      "learning_rate": 0.00019898396795410835,
      "loss": 2.5681,
      "step": 10800
    },
    {
      "epoch": 0.22238279773399952,
      "grad_norm": 0.3255872130393982,
      "learning_rate": 0.00019898080764246546,
      "loss": 2.4785,
      "step": 10810
    },
    {
      "epoch": 0.22258851725086723,
      "grad_norm": 0.31095534563064575,
      "learning_rate": 0.0001989776424486387,
      "loss": 2.4754,
      "step": 10820
    },
    {
      "epoch": 0.22279423676773494,
      "grad_norm": 0.3258175849914551,
      "learning_rate": 0.0001989744723727842,
      "loss": 2.4806,
      "step": 10830
    },
    {
      "epoch": 0.22299995628460267,
      "grad_norm": 0.3330577611923218,
      "learning_rate": 0.00019897129741505834,
      "loss": 2.5122,
      "step": 10840
    },
    {
      "epoch": 0.22320567580147038,
      "grad_norm": 0.3320689797401428,
      "learning_rate": 0.0001989681175756177,
      "loss": 2.46,
      "step": 10850
    },
    {
      "epoch": 0.2234113953183381,
      "grad_norm": 0.34255144000053406,
      "learning_rate": 0.00019896493285461911,
      "loss": 2.5247,
      "step": 10860
    },
    {
      "epoch": 0.2236171148352058,
      "grad_norm": 0.32026588916778564,
      "learning_rate": 0.00019896174325221972,
      "loss": 2.4951,
      "step": 10870
    },
    {
      "epoch": 0.22382283435207354,
      "grad_norm": 0.3178042769432068,
      "learning_rate": 0.00019895854876857678,
      "loss": 2.4409,
      "step": 10880
    },
    {
      "epoch": 0.22402855386894124,
      "grad_norm": 0.33465421199798584,
      "learning_rate": 0.0001989553494038479,
      "loss": 2.4812,
      "step": 10890
    },
    {
      "epoch": 0.22423427338580895,
      "grad_norm": 0.34681880474090576,
      "learning_rate": 0.00019895214515819088,
      "loss": 2.4819,
      "step": 10900
    },
    {
      "epoch": 0.22443999290267666,
      "grad_norm": 0.33637234568595886,
      "learning_rate": 0.00019894893603176377,
      "loss": 2.4744,
      "step": 10910
    },
    {
      "epoch": 0.22464571241954437,
      "grad_norm": 0.3236424922943115,
      "learning_rate": 0.00019894572202472483,
      "loss": 2.4516,
      "step": 10920
    },
    {
      "epoch": 0.2248514319364121,
      "grad_norm": 0.3273901641368866,
      "learning_rate": 0.00019894250313723264,
      "loss": 2.4924,
      "step": 10930
    },
    {
      "epoch": 0.2250571514532798,
      "grad_norm": 0.3488750457763672,
      "learning_rate": 0.00019893927936944593,
      "loss": 2.4423,
      "step": 10940
    },
    {
      "epoch": 0.22526287097014752,
      "grad_norm": 0.3428117632865906,
      "learning_rate": 0.0001989360507215237,
      "loss": 2.4679,
      "step": 10950
    },
    {
      "epoch": 0.22546859048701523,
      "grad_norm": 0.3351612091064453,
      "learning_rate": 0.00019893281719362524,
      "loss": 2.4241,
      "step": 10960
    },
    {
      "epoch": 0.22567431000388297,
      "grad_norm": 0.3396591246128082,
      "learning_rate": 0.00019892957878591005,
      "loss": 2.5185,
      "step": 10970
    },
    {
      "epoch": 0.22588002952075067,
      "grad_norm": 0.30014950037002563,
      "learning_rate": 0.00019892633549853782,
      "loss": 2.5047,
      "step": 10980
    },
    {
      "epoch": 0.22608574903761838,
      "grad_norm": 0.31767910718917847,
      "learning_rate": 0.00019892308733166853,
      "loss": 2.477,
      "step": 10990
    },
    {
      "epoch": 0.2262914685544861,
      "grad_norm": 0.34937575459480286,
      "learning_rate": 0.00019891983428546245,
      "loss": 2.4718,
      "step": 11000
    },
    {
      "epoch": 0.22649718807135383,
      "grad_norm": 0.32293111085891724,
      "learning_rate": 0.00019891657636007993,
      "loss": 2.492,
      "step": 11010
    },
    {
      "epoch": 0.22670290758822154,
      "grad_norm": 0.34383004903793335,
      "learning_rate": 0.00019891331355568177,
      "loss": 2.4753,
      "step": 11020
    },
    {
      "epoch": 0.22690862710508924,
      "grad_norm": 0.32009556889533997,
      "learning_rate": 0.00019891004587242888,
      "loss": 2.4885,
      "step": 11030
    },
    {
      "epoch": 0.22711434662195695,
      "grad_norm": 0.3166736364364624,
      "learning_rate": 0.0001989067733104824,
      "loss": 2.5252,
      "step": 11040
    },
    {
      "epoch": 0.22732006613882466,
      "grad_norm": 0.32017526030540466,
      "learning_rate": 0.00019890349587000375,
      "loss": 2.4836,
      "step": 11050
    },
    {
      "epoch": 0.2275257856556924,
      "grad_norm": 0.3853808641433716,
      "learning_rate": 0.00019890021355115463,
      "loss": 2.4992,
      "step": 11060
    },
    {
      "epoch": 0.2277315051725601,
      "grad_norm": 0.308885395526886,
      "learning_rate": 0.00019889692635409695,
      "loss": 2.4321,
      "step": 11070
    },
    {
      "epoch": 0.22793722468942781,
      "grad_norm": 0.3175845742225647,
      "learning_rate": 0.00019889363427899278,
      "loss": 2.4911,
      "step": 11080
    },
    {
      "epoch": 0.22814294420629552,
      "grad_norm": 0.3344056308269501,
      "learning_rate": 0.00019889033732600452,
      "loss": 2.4568,
      "step": 11090
    },
    {
      "epoch": 0.22834866372316326,
      "grad_norm": 0.35439449548721313,
      "learning_rate": 0.00019888703549529485,
      "loss": 2.474,
      "step": 11100
    },
    {
      "epoch": 0.22855438324003097,
      "grad_norm": 0.3545062243938446,
      "learning_rate": 0.00019888372878702656,
      "loss": 2.5082,
      "step": 11110
    },
    {
      "epoch": 0.22876010275689868,
      "grad_norm": 0.31506502628326416,
      "learning_rate": 0.00019888041720136278,
      "loss": 2.4741,
      "step": 11120
    },
    {
      "epoch": 0.22896582227376638,
      "grad_norm": 0.3689611852169037,
      "learning_rate": 0.00019887710073846686,
      "loss": 2.3935,
      "step": 11130
    },
    {
      "epoch": 0.2291715417906341,
      "grad_norm": 0.3508700728416443,
      "learning_rate": 0.00019887377939850238,
      "loss": 2.4049,
      "step": 11140
    },
    {
      "epoch": 0.22937726130750183,
      "grad_norm": 0.3261789083480835,
      "learning_rate": 0.00019887045318163314,
      "loss": 2.4557,
      "step": 11150
    },
    {
      "epoch": 0.22958298082436954,
      "grad_norm": 0.31698814034461975,
      "learning_rate": 0.00019886712208802326,
      "loss": 2.501,
      "step": 11160
    },
    {
      "epoch": 0.22978870034123725,
      "grad_norm": 0.32426050305366516,
      "learning_rate": 0.00019886378611783697,
      "loss": 2.4833,
      "step": 11170
    },
    {
      "epoch": 0.22999441985810495,
      "grad_norm": 0.33519041538238525,
      "learning_rate": 0.00019886044527123888,
      "loss": 2.4776,
      "step": 11180
    },
    {
      "epoch": 0.2302001393749727,
      "grad_norm": 0.33634328842163086,
      "learning_rate": 0.00019885709954839376,
      "loss": 2.5115,
      "step": 11190
    },
    {
      "epoch": 0.2304058588918404,
      "grad_norm": 0.3342229127883911,
      "learning_rate": 0.0001988537489494666,
      "loss": 2.5062,
      "step": 11200
    },
    {
      "epoch": 0.2306115784087081,
      "grad_norm": 0.33388444781303406,
      "learning_rate": 0.00019885039347462268,
      "loss": 2.4106,
      "step": 11210
    },
    {
      "epoch": 0.23081729792557582,
      "grad_norm": 0.3554550111293793,
      "learning_rate": 0.00019884703312402757,
      "loss": 2.4666,
      "step": 11220
    },
    {
      "epoch": 0.23102301744244352,
      "grad_norm": 0.3321956992149353,
      "learning_rate": 0.00019884366789784693,
      "loss": 2.4602,
      "step": 11230
    },
    {
      "epoch": 0.23122873695931126,
      "grad_norm": 0.3310193419456482,
      "learning_rate": 0.00019884029779624677,
      "loss": 2.5153,
      "step": 11240
    },
    {
      "epoch": 0.23143445647617897,
      "grad_norm": 0.32258522510528564,
      "learning_rate": 0.00019883692281939336,
      "loss": 2.4151,
      "step": 11250
    },
    {
      "epoch": 0.23164017599304668,
      "grad_norm": 0.32336655259132385,
      "learning_rate": 0.00019883354296745318,
      "loss": 2.4742,
      "step": 11260
    },
    {
      "epoch": 0.23184589550991438,
      "grad_norm": 0.35968017578125,
      "learning_rate": 0.00019883015824059285,
      "loss": 2.4771,
      "step": 11270
    },
    {
      "epoch": 0.23205161502678212,
      "grad_norm": 0.317533016204834,
      "learning_rate": 0.0001988267686389794,
      "loss": 2.4404,
      "step": 11280
    },
    {
      "epoch": 0.23225733454364983,
      "grad_norm": 0.3742232918739319,
      "learning_rate": 0.00019882337416278,
      "loss": 2.4994,
      "step": 11290
    },
    {
      "epoch": 0.23246305406051754,
      "grad_norm": 0.3398011326789856,
      "learning_rate": 0.00019881997481216204,
      "loss": 2.5331,
      "step": 11300
    },
    {
      "epoch": 0.23266877357738525,
      "grad_norm": 0.35071632266044617,
      "learning_rate": 0.00019881657058729325,
      "loss": 2.3966,
      "step": 11310
    },
    {
      "epoch": 0.23287449309425295,
      "grad_norm": 0.34318074584007263,
      "learning_rate": 0.0001988131614883415,
      "loss": 2.4469,
      "step": 11320
    },
    {
      "epoch": 0.2330802126111207,
      "grad_norm": 0.34122973680496216,
      "learning_rate": 0.000198809747515475,
      "loss": 2.4812,
      "step": 11330
    },
    {
      "epoch": 0.2332859321279884,
      "grad_norm": 0.34448152780532837,
      "learning_rate": 0.00019880632866886209,
      "loss": 2.4326,
      "step": 11340
    },
    {
      "epoch": 0.2334916516448561,
      "grad_norm": 0.3406423032283783,
      "learning_rate": 0.0001988029049486714,
      "loss": 2.465,
      "step": 11350
    },
    {
      "epoch": 0.23369737116172382,
      "grad_norm": 0.335211843252182,
      "learning_rate": 0.0001987994763550718,
      "loss": 2.4899,
      "step": 11360
    },
    {
      "epoch": 0.23390309067859155,
      "grad_norm": 0.36572209000587463,
      "learning_rate": 0.00019879604288823246,
      "loss": 2.4911,
      "step": 11370
    },
    {
      "epoch": 0.23410881019545926,
      "grad_norm": 0.35713568329811096,
      "learning_rate": 0.0001987926045483227,
      "loss": 2.4735,
      "step": 11380
    },
    {
      "epoch": 0.23431452971232697,
      "grad_norm": 0.3326855003833771,
      "learning_rate": 0.00019878916133551209,
      "loss": 2.4848,
      "step": 11390
    },
    {
      "epoch": 0.23452024922919468,
      "grad_norm": 0.3400776982307434,
      "learning_rate": 0.0001987857132499705,
      "loss": 2.4968,
      "step": 11400
    },
    {
      "epoch": 0.2347259687460624,
      "grad_norm": 0.3291510343551636,
      "learning_rate": 0.000198782260291868,
      "loss": 2.3871,
      "step": 11410
    },
    {
      "epoch": 0.23493168826293012,
      "grad_norm": 0.34478285908699036,
      "learning_rate": 0.0001987788024613749,
      "loss": 2.5202,
      "step": 11420
    },
    {
      "epoch": 0.23513740777979783,
      "grad_norm": 0.33721500635147095,
      "learning_rate": 0.00019877533975866174,
      "loss": 2.4077,
      "step": 11430
    },
    {
      "epoch": 0.23534312729666554,
      "grad_norm": 0.34144127368927,
      "learning_rate": 0.00019877187218389933,
      "loss": 2.5213,
      "step": 11440
    },
    {
      "epoch": 0.23554884681353325,
      "grad_norm": 0.39732369780540466,
      "learning_rate": 0.00019876839973725872,
      "loss": 2.4386,
      "step": 11450
    },
    {
      "epoch": 0.23575456633040098,
      "grad_norm": 0.3341405391693115,
      "learning_rate": 0.00019876492241891115,
      "loss": 2.4568,
      "step": 11460
    },
    {
      "epoch": 0.2359602858472687,
      "grad_norm": 0.3564382791519165,
      "learning_rate": 0.0001987614402290282,
      "loss": 2.5115,
      "step": 11470
    },
    {
      "epoch": 0.2361660053641364,
      "grad_norm": 0.3306933343410492,
      "learning_rate": 0.00019875795316778155,
      "loss": 2.4406,
      "step": 11480
    },
    {
      "epoch": 0.2363717248810041,
      "grad_norm": 0.3407732844352722,
      "learning_rate": 0.00019875446123534328,
      "loss": 2.4695,
      "step": 11490
    },
    {
      "epoch": 0.23657744439787184,
      "grad_norm": 0.34326669573783875,
      "learning_rate": 0.00019875096443188554,
      "loss": 2.5098,
      "step": 11500
    },
    {
      "epoch": 0.23678316391473955,
      "grad_norm": 0.3916619122028351,
      "learning_rate": 0.00019874746275758088,
      "loss": 2.4946,
      "step": 11510
    },
    {
      "epoch": 0.23698888343160726,
      "grad_norm": 0.3376692235469818,
      "learning_rate": 0.00019874395621260197,
      "loss": 2.4991,
      "step": 11520
    },
    {
      "epoch": 0.23719460294847497,
      "grad_norm": 0.34312155842781067,
      "learning_rate": 0.0001987404447971218,
      "loss": 2.4693,
      "step": 11530
    },
    {
      "epoch": 0.23740032246534268,
      "grad_norm": 0.33836111426353455,
      "learning_rate": 0.00019873692851131357,
      "loss": 2.395,
      "step": 11540
    },
    {
      "epoch": 0.23760604198221041,
      "grad_norm": 0.3329942226409912,
      "learning_rate": 0.00019873340735535071,
      "loss": 2.5136,
      "step": 11550
    },
    {
      "epoch": 0.23781176149907812,
      "grad_norm": 0.34922903776168823,
      "learning_rate": 0.0001987298813294069,
      "loss": 2.4724,
      "step": 11560
    },
    {
      "epoch": 0.23801748101594583,
      "grad_norm": 0.3355723023414612,
      "learning_rate": 0.00019872635043365603,
      "loss": 2.446,
      "step": 11570
    },
    {
      "epoch": 0.23822320053281354,
      "grad_norm": 0.3508017063140869,
      "learning_rate": 0.0001987228146682723,
      "loss": 2.4653,
      "step": 11580
    },
    {
      "epoch": 0.23842892004968128,
      "grad_norm": 0.3026282489299774,
      "learning_rate": 0.0001987192740334301,
      "loss": 2.4857,
      "step": 11590
    },
    {
      "epoch": 0.23863463956654898,
      "grad_norm": 0.38187238574028015,
      "learning_rate": 0.00019871572852930407,
      "loss": 2.4785,
      "step": 11600
    },
    {
      "epoch": 0.2388403590834167,
      "grad_norm": 0.31400489807128906,
      "learning_rate": 0.0001987121781560691,
      "loss": 2.4432,
      "step": 11610
    },
    {
      "epoch": 0.2390460786002844,
      "grad_norm": 0.3200959265232086,
      "learning_rate": 0.00019870862291390028,
      "loss": 2.4505,
      "step": 11620
    },
    {
      "epoch": 0.2392517981171521,
      "grad_norm": 0.32918667793273926,
      "learning_rate": 0.00019870506280297298,
      "loss": 2.4921,
      "step": 11630
    },
    {
      "epoch": 0.23945751763401985,
      "grad_norm": 0.332572340965271,
      "learning_rate": 0.00019870149782346284,
      "loss": 2.4572,
      "step": 11640
    },
    {
      "epoch": 0.23966323715088755,
      "grad_norm": 0.40002676844596863,
      "learning_rate": 0.00019869792797554565,
      "loss": 2.5406,
      "step": 11650
    },
    {
      "epoch": 0.23986895666775526,
      "grad_norm": 0.33599206805229187,
      "learning_rate": 0.0001986943532593975,
      "loss": 2.5007,
      "step": 11660
    },
    {
      "epoch": 0.24007467618462297,
      "grad_norm": 0.33946317434310913,
      "learning_rate": 0.00019869077367519475,
      "loss": 2.4376,
      "step": 11670
    },
    {
      "epoch": 0.2402803957014907,
      "grad_norm": 0.3272766172885895,
      "learning_rate": 0.00019868718922311392,
      "loss": 2.444,
      "step": 11680
    },
    {
      "epoch": 0.24048611521835841,
      "grad_norm": 0.40208688378334045,
      "learning_rate": 0.00019868359990333185,
      "loss": 2.4208,
      "step": 11690
    },
    {
      "epoch": 0.24069183473522612,
      "grad_norm": 0.3519081771373749,
      "learning_rate": 0.00019868000571602554,
      "loss": 2.4566,
      "step": 11700
    },
    {
      "epoch": 0.24089755425209383,
      "grad_norm": 0.34106123447418213,
      "learning_rate": 0.00019867640666137225,
      "loss": 2.4694,
      "step": 11710
    },
    {
      "epoch": 0.24110327376896154,
      "grad_norm": 0.3271540403366089,
      "learning_rate": 0.0001986728027395496,
      "loss": 2.502,
      "step": 11720
    },
    {
      "epoch": 0.24130899328582928,
      "grad_norm": 0.3351426422595978,
      "learning_rate": 0.00019866919395073528,
      "loss": 2.4689,
      "step": 11730
    },
    {
      "epoch": 0.24151471280269698,
      "grad_norm": 0.3293943405151367,
      "learning_rate": 0.00019866558029510732,
      "loss": 2.449,
      "step": 11740
    },
    {
      "epoch": 0.2417204323195647,
      "grad_norm": 0.3375256359577179,
      "learning_rate": 0.00019866196177284392,
      "loss": 2.4861,
      "step": 11750
    },
    {
      "epoch": 0.2419261518364324,
      "grad_norm": 0.3760293126106262,
      "learning_rate": 0.00019865833838412359,
      "loss": 2.4532,
      "step": 11760
    },
    {
      "epoch": 0.24213187135330014,
      "grad_norm": 0.32598623633384705,
      "learning_rate": 0.00019865471012912503,
      "loss": 2.4543,
      "step": 11770
    },
    {
      "epoch": 0.24233759087016785,
      "grad_norm": 0.32312265038490295,
      "learning_rate": 0.00019865107700802725,
      "loss": 2.4273,
      "step": 11780
    },
    {
      "epoch": 0.24254331038703555,
      "grad_norm": 0.3073538839817047,
      "learning_rate": 0.00019864743902100944,
      "loss": 2.52,
      "step": 11790
    },
    {
      "epoch": 0.24274902990390326,
      "grad_norm": 0.34686949849128723,
      "learning_rate": 0.000198643796168251,
      "loss": 2.4793,
      "step": 11800
    },
    {
      "epoch": 0.242954749420771,
      "grad_norm": 0.3247312605381012,
      "learning_rate": 0.00019864014844993166,
      "loss": 2.4828,
      "step": 11810
    },
    {
      "epoch": 0.2431604689376387,
      "grad_norm": 0.32961973547935486,
      "learning_rate": 0.0001986364958662313,
      "loss": 2.52,
      "step": 11820
    },
    {
      "epoch": 0.24336618845450642,
      "grad_norm": 0.3312273621559143,
      "learning_rate": 0.0001986328384173301,
      "loss": 2.4673,
      "step": 11830
    },
    {
      "epoch": 0.24357190797137412,
      "grad_norm": 0.35148003697395325,
      "learning_rate": 0.0001986291761034085,
      "loss": 2.4359,
      "step": 11840
    },
    {
      "epoch": 0.24377762748824183,
      "grad_norm": 0.34527456760406494,
      "learning_rate": 0.0001986255089246471,
      "loss": 2.4805,
      "step": 11850
    },
    {
      "epoch": 0.24398334700510957,
      "grad_norm": 0.33253511786460876,
      "learning_rate": 0.00019862183688122676,
      "loss": 2.4764,
      "step": 11860
    },
    {
      "epoch": 0.24418906652197728,
      "grad_norm": 0.3309907913208008,
      "learning_rate": 0.00019861815997332865,
      "loss": 2.4974,
      "step": 11870
    },
    {
      "epoch": 0.24439478603884499,
      "grad_norm": 0.3403717279434204,
      "learning_rate": 0.0001986144782011341,
      "loss": 2.4759,
      "step": 11880
    },
    {
      "epoch": 0.2446005055557127,
      "grad_norm": 0.3626827001571655,
      "learning_rate": 0.00019861079156482473,
      "loss": 2.495,
      "step": 11890
    },
    {
      "epoch": 0.24480622507258043,
      "grad_norm": 0.3310409188270569,
      "learning_rate": 0.00019860710006458238,
      "loss": 2.4753,
      "step": 11900
    },
    {
      "epoch": 0.24501194458944814,
      "grad_norm": 0.333132803440094,
      "learning_rate": 0.00019860340370058915,
      "loss": 2.4743,
      "step": 11910
    },
    {
      "epoch": 0.24521766410631585,
      "grad_norm": 0.3502195477485657,
      "learning_rate": 0.00019859970247302733,
      "loss": 2.4395,
      "step": 11920
    },
    {
      "epoch": 0.24542338362318356,
      "grad_norm": 0.3487304449081421,
      "learning_rate": 0.00019859599638207947,
      "loss": 2.4271,
      "step": 11930
    },
    {
      "epoch": 0.24562910314005126,
      "grad_norm": 0.3242722451686859,
      "learning_rate": 0.00019859228542792838,
      "loss": 2.5088,
      "step": 11940
    },
    {
      "epoch": 0.245834822656919,
      "grad_norm": 0.33465999364852905,
      "learning_rate": 0.00019858856961075714,
      "loss": 2.4314,
      "step": 11950
    },
    {
      "epoch": 0.2460405421737867,
      "grad_norm": 0.3330681324005127,
      "learning_rate": 0.000198584848930749,
      "loss": 2.5136,
      "step": 11960
    },
    {
      "epoch": 0.24624626169065442,
      "grad_norm": 0.3545211851596832,
      "learning_rate": 0.0001985811233880875,
      "loss": 2.4944,
      "step": 11970
    },
    {
      "epoch": 0.24645198120752213,
      "grad_norm": 0.3459272086620331,
      "learning_rate": 0.00019857739298295636,
      "loss": 2.43,
      "step": 11980
    },
    {
      "epoch": 0.24665770072438986,
      "grad_norm": 0.34491705894470215,
      "learning_rate": 0.00019857365771553962,
      "loss": 2.4019,
      "step": 11990
    },
    {
      "epoch": 0.24686342024125757,
      "grad_norm": 0.4018625020980835,
      "learning_rate": 0.00019856991758602152,
      "loss": 2.481,
      "step": 12000
    },
    {
      "epoch": 0.24706913975812528,
      "grad_norm": 0.34115323424339294,
      "learning_rate": 0.00019856617259458653,
      "loss": 2.4711,
      "step": 12010
    },
    {
      "epoch": 0.247274859274993,
      "grad_norm": 0.34006357192993164,
      "learning_rate": 0.00019856242274141932,
      "loss": 2.4468,
      "step": 12020
    },
    {
      "epoch": 0.2474805787918607,
      "grad_norm": 0.36389389634132385,
      "learning_rate": 0.00019855866802670494,
      "loss": 2.4244,
      "step": 12030
    },
    {
      "epoch": 0.24768629830872843,
      "grad_norm": 0.3343614935874939,
      "learning_rate": 0.0001985549084506285,
      "loss": 2.4523,
      "step": 12040
    },
    {
      "epoch": 0.24789201782559614,
      "grad_norm": 0.31766659021377563,
      "learning_rate": 0.00019855114401337557,
      "loss": 2.443,
      "step": 12050
    },
    {
      "epoch": 0.24809773734246385,
      "grad_norm": 0.3284468948841095,
      "learning_rate": 0.00019854737471513168,
      "loss": 2.516,
      "step": 12060
    },
    {
      "epoch": 0.24830345685933156,
      "grad_norm": 0.36736515164375305,
      "learning_rate": 0.0001985436005560828,
      "loss": 2.4148,
      "step": 12070
    },
    {
      "epoch": 0.2485091763761993,
      "grad_norm": 0.31749314069747925,
      "learning_rate": 0.00019853982153641514,
      "loss": 2.4068,
      "step": 12080
    },
    {
      "epoch": 0.248714895893067,
      "grad_norm": 0.321085661649704,
      "learning_rate": 0.00019853603765631505,
      "loss": 2.4649,
      "step": 12090
    },
    {
      "epoch": 0.2489206154099347,
      "grad_norm": 0.35309621691703796,
      "learning_rate": 0.0001985322489159692,
      "loss": 2.499,
      "step": 12100
    },
    {
      "epoch": 0.24912633492680242,
      "grad_norm": 0.32039597630500793,
      "learning_rate": 0.00019852845531556442,
      "loss": 2.4801,
      "step": 12110
    },
    {
      "epoch": 0.24933205444367013,
      "grad_norm": 0.34082067012786865,
      "learning_rate": 0.00019852465685528788,
      "loss": 2.4367,
      "step": 12120
    },
    {
      "epoch": 0.24953777396053786,
      "grad_norm": 0.3380609452724457,
      "learning_rate": 0.00019852085353532688,
      "loss": 2.4754,
      "step": 12130
    },
    {
      "epoch": 0.24974349347740557,
      "grad_norm": 0.32564765214920044,
      "learning_rate": 0.00019851704535586907,
      "loss": 2.4842,
      "step": 12140
    },
    {
      "epoch": 0.24994921299427328,
      "grad_norm": 0.3338511884212494,
      "learning_rate": 0.00019851323231710226,
      "loss": 2.4125,
      "step": 12150
    },
    {
      "epoch": 0.250154932511141,
      "grad_norm": 0.33462268114089966,
      "learning_rate": 0.0001985094144192145,
      "loss": 2.4517,
      "step": 12160
    },
    {
      "epoch": 0.2503606520280087,
      "grad_norm": 1.2803459167480469,
      "learning_rate": 0.0001985055916623942,
      "loss": 2.5021,
      "step": 12170
    },
    {
      "epoch": 0.25056637154487643,
      "grad_norm": 0.3286500573158264,
      "learning_rate": 0.00019850176404682983,
      "loss": 2.4743,
      "step": 12180
    },
    {
      "epoch": 0.25077209106174414,
      "grad_norm": 0.33238330483436584,
      "learning_rate": 0.0001984979315727102,
      "loss": 2.5007,
      "step": 12190
    },
    {
      "epoch": 0.25097781057861185,
      "grad_norm": 0.3505330979824066,
      "learning_rate": 0.00019849409424022436,
      "loss": 2.5036,
      "step": 12200
    },
    {
      "epoch": 0.25118353009547956,
      "grad_norm": 0.3863309323787689,
      "learning_rate": 0.0001984902520495616,
      "loss": 2.4549,
      "step": 12210
    },
    {
      "epoch": 0.25138924961234727,
      "grad_norm": 0.33347806334495544,
      "learning_rate": 0.00019848640500091138,
      "loss": 2.4718,
      "step": 12220
    },
    {
      "epoch": 0.251594969129215,
      "grad_norm": 0.32291096448898315,
      "learning_rate": 0.0001984825530944635,
      "loss": 2.4965,
      "step": 12230
    },
    {
      "epoch": 0.25180068864608274,
      "grad_norm": 0.376544713973999,
      "learning_rate": 0.00019847869633040795,
      "loss": 2.436,
      "step": 12240
    },
    {
      "epoch": 0.25200640816295045,
      "grad_norm": 0.37242311239242554,
      "learning_rate": 0.00019847483470893494,
      "loss": 2.4524,
      "step": 12250
    },
    {
      "epoch": 0.25221212767981815,
      "grad_norm": 0.342772901058197,
      "learning_rate": 0.000198470968230235,
      "loss": 2.4532,
      "step": 12260
    },
    {
      "epoch": 0.25241784719668586,
      "grad_norm": 0.3818259537220001,
      "learning_rate": 0.00019846709689449874,
      "loss": 2.4258,
      "step": 12270
    },
    {
      "epoch": 0.25262356671355357,
      "grad_norm": 0.35331639647483826,
      "learning_rate": 0.0001984632207019172,
      "loss": 2.4562,
      "step": 12280
    },
    {
      "epoch": 0.2528292862304213,
      "grad_norm": 0.3431641757488251,
      "learning_rate": 0.00019845933965268153,
      "loss": 2.4282,
      "step": 12290
    },
    {
      "epoch": 0.253035005747289,
      "grad_norm": 0.32234200835227966,
      "learning_rate": 0.0001984554537469832,
      "loss": 2.4523,
      "step": 12300
    },
    {
      "epoch": 0.2532407252641567,
      "grad_norm": 0.35752490162849426,
      "learning_rate": 0.00019845156298501385,
      "loss": 2.4063,
      "step": 12310
    },
    {
      "epoch": 0.2534464447810244,
      "grad_norm": 0.3423653841018677,
      "learning_rate": 0.00019844766736696535,
      "loss": 2.446,
      "step": 12320
    },
    {
      "epoch": 0.25365216429789217,
      "grad_norm": 0.33986300230026245,
      "learning_rate": 0.00019844376689302995,
      "loss": 2.4429,
      "step": 12330
    },
    {
      "epoch": 0.2538578838147599,
      "grad_norm": 0.3366287052631378,
      "learning_rate": 0.00019843986156339996,
      "loss": 2.4414,
      "step": 12340
    },
    {
      "epoch": 0.2540636033316276,
      "grad_norm": 0.33169734477996826,
      "learning_rate": 0.00019843595137826803,
      "loss": 2.5128,
      "step": 12350
    },
    {
      "epoch": 0.2542693228484953,
      "grad_norm": 0.352388858795166,
      "learning_rate": 0.00019843203633782702,
      "loss": 2.4877,
      "step": 12360
    },
    {
      "epoch": 0.254475042365363,
      "grad_norm": 0.3403949737548828,
      "learning_rate": 0.00019842811644227006,
      "loss": 2.4429,
      "step": 12370
    },
    {
      "epoch": 0.2546807618822307,
      "grad_norm": 0.34571370482444763,
      "learning_rate": 0.0001984241916917905,
      "loss": 2.5074,
      "step": 12380
    },
    {
      "epoch": 0.2548864813990984,
      "grad_norm": 0.3465155065059662,
      "learning_rate": 0.00019842026208658192,
      "loss": 2.5097,
      "step": 12390
    },
    {
      "epoch": 0.2550922009159661,
      "grad_norm": 0.34126201272010803,
      "learning_rate": 0.00019841632762683807,
      "loss": 2.5214,
      "step": 12400
    },
    {
      "epoch": 0.25529792043283384,
      "grad_norm": 0.3653898239135742,
      "learning_rate": 0.00019841238831275315,
      "loss": 2.4567,
      "step": 12410
    },
    {
      "epoch": 0.2555036399497016,
      "grad_norm": 0.3271891176700592,
      "learning_rate": 0.00019840844414452136,
      "loss": 2.5296,
      "step": 12420
    },
    {
      "epoch": 0.2557093594665693,
      "grad_norm": 0.31165751814842224,
      "learning_rate": 0.0001984044951223373,
      "loss": 2.5108,
      "step": 12430
    },
    {
      "epoch": 0.255915078983437,
      "grad_norm": 0.3469754755496979,
      "learning_rate": 0.00019840054124639576,
      "loss": 2.5426,
      "step": 12440
    },
    {
      "epoch": 0.2561207985003047,
      "grad_norm": 0.3176420032978058,
      "learning_rate": 0.00019839658251689167,
      "loss": 2.4812,
      "step": 12450
    },
    {
      "epoch": 0.25632651801717243,
      "grad_norm": 0.32384857535362244,
      "learning_rate": 0.00019839261893402044,
      "loss": 2.4309,
      "step": 12460
    },
    {
      "epoch": 0.25653223753404014,
      "grad_norm": 0.3147852122783661,
      "learning_rate": 0.00019838865049797745,
      "loss": 2.4911,
      "step": 12470
    },
    {
      "epoch": 0.25673795705090785,
      "grad_norm": 0.3517884910106659,
      "learning_rate": 0.00019838467720895847,
      "loss": 2.4484,
      "step": 12480
    },
    {
      "epoch": 0.25694367656777556,
      "grad_norm": 0.4375491738319397,
      "learning_rate": 0.00019838069906715956,
      "loss": 2.48,
      "step": 12490
    },
    {
      "epoch": 0.2571493960846433,
      "grad_norm": 0.3513950705528259,
      "learning_rate": 0.00019837671607277678,
      "loss": 2.4069,
      "step": 12500
    },
    {
      "epoch": 0.25735511560151103,
      "grad_norm": 0.35054337978363037,
      "learning_rate": 0.00019837272822600678,
      "loss": 2.4223,
      "step": 12510
    },
    {
      "epoch": 0.25756083511837874,
      "grad_norm": 0.330720454454422,
      "learning_rate": 0.0001983687355270461,
      "loss": 2.4769,
      "step": 12520
    },
    {
      "epoch": 0.25776655463524645,
      "grad_norm": 0.33108022809028625,
      "learning_rate": 0.00019836473797609178,
      "loss": 2.5084,
      "step": 12530
    },
    {
      "epoch": 0.25797227415211416,
      "grad_norm": 0.37939006090164185,
      "learning_rate": 0.00019836073557334092,
      "loss": 2.4891,
      "step": 12540
    },
    {
      "epoch": 0.25817799366898186,
      "grad_norm": 0.3475536108016968,
      "learning_rate": 0.000198356728318991,
      "loss": 2.4626,
      "step": 12550
    },
    {
      "epoch": 0.2583837131858496,
      "grad_norm": 0.3185167610645294,
      "learning_rate": 0.00019835271621323965,
      "loss": 2.5009,
      "step": 12560
    },
    {
      "epoch": 0.2585894327027173,
      "grad_norm": 0.3472559154033661,
      "learning_rate": 0.00019834869925628476,
      "loss": 2.4394,
      "step": 12570
    },
    {
      "epoch": 0.258795152219585,
      "grad_norm": 0.3394505977630615,
      "learning_rate": 0.0001983446774483245,
      "loss": 2.4461,
      "step": 12580
    },
    {
      "epoch": 0.25900087173645275,
      "grad_norm": 0.35148733854293823,
      "learning_rate": 0.0001983406507895572,
      "loss": 2.4801,
      "step": 12590
    },
    {
      "epoch": 0.25920659125332046,
      "grad_norm": 0.3366451859474182,
      "learning_rate": 0.0001983366192801815,
      "loss": 2.4865,
      "step": 12600
    },
    {
      "epoch": 0.25941231077018817,
      "grad_norm": 0.3524177074432373,
      "learning_rate": 0.00019833258292039618,
      "loss": 2.4935,
      "step": 12610
    },
    {
      "epoch": 0.2596180302870559,
      "grad_norm": 0.357912540435791,
      "learning_rate": 0.00019832854171040044,
      "loss": 2.4236,
      "step": 12620
    },
    {
      "epoch": 0.2598237498039236,
      "grad_norm": 0.3396613895893097,
      "learning_rate": 0.00019832449565039358,
      "loss": 2.4989,
      "step": 12630
    },
    {
      "epoch": 0.2600294693207913,
      "grad_norm": 0.33318009972572327,
      "learning_rate": 0.00019832044474057514,
      "loss": 2.4611,
      "step": 12640
    },
    {
      "epoch": 0.260235188837659,
      "grad_norm": 0.366176038980484,
      "learning_rate": 0.00019831638898114493,
      "loss": 2.4802,
      "step": 12650
    },
    {
      "epoch": 0.2604409083545267,
      "grad_norm": 0.33015188574790955,
      "learning_rate": 0.000198312328372303,
      "loss": 2.4289,
      "step": 12660
    },
    {
      "epoch": 0.2606466278713944,
      "grad_norm": 0.3219914436340332,
      "learning_rate": 0.00019830826291424968,
      "loss": 2.4472,
      "step": 12670
    },
    {
      "epoch": 0.2608523473882622,
      "grad_norm": 0.3465077877044678,
      "learning_rate": 0.00019830419260718547,
      "loss": 2.4247,
      "step": 12680
    },
    {
      "epoch": 0.2610580669051299,
      "grad_norm": 0.38943395018577576,
      "learning_rate": 0.0001983001174513111,
      "loss": 2.4985,
      "step": 12690
    },
    {
      "epoch": 0.2612637864219976,
      "grad_norm": 0.6224178671836853,
      "learning_rate": 0.00019829603744682763,
      "loss": 2.5009,
      "step": 12700
    },
    {
      "epoch": 0.2614695059388653,
      "grad_norm": 0.33437857031822205,
      "learning_rate": 0.00019829195259393626,
      "loss": 2.4794,
      "step": 12710
    },
    {
      "epoch": 0.261675225455733,
      "grad_norm": 0.3324921131134033,
      "learning_rate": 0.00019828786289283854,
      "loss": 2.4011,
      "step": 12720
    },
    {
      "epoch": 0.2618809449726007,
      "grad_norm": 0.357937753200531,
      "learning_rate": 0.00019828376834373613,
      "loss": 2.5099,
      "step": 12730
    },
    {
      "epoch": 0.26208666448946843,
      "grad_norm": 0.35201141238212585,
      "learning_rate": 0.000198279668946831,
      "loss": 2.5138,
      "step": 12740
    },
    {
      "epoch": 0.26229238400633614,
      "grad_norm": 0.36717572808265686,
      "learning_rate": 0.00019827556470232536,
      "loss": 2.5062,
      "step": 12750
    },
    {
      "epoch": 0.26249810352320385,
      "grad_norm": 0.34658002853393555,
      "learning_rate": 0.00019827145561042165,
      "loss": 2.4189,
      "step": 12760
    },
    {
      "epoch": 0.2627038230400716,
      "grad_norm": 0.36333978176116943,
      "learning_rate": 0.00019826734167132255,
      "loss": 2.4521,
      "step": 12770
    },
    {
      "epoch": 0.2629095425569393,
      "grad_norm": 0.3388671576976776,
      "learning_rate": 0.00019826322288523098,
      "loss": 2.5198,
      "step": 12780
    },
    {
      "epoch": 0.26311526207380703,
      "grad_norm": 0.31361475586891174,
      "learning_rate": 0.0001982590992523501,
      "loss": 2.4774,
      "step": 12790
    },
    {
      "epoch": 0.26332098159067474,
      "grad_norm": 0.3558061420917511,
      "learning_rate": 0.00019825497077288328,
      "loss": 2.4531,
      "step": 12800
    },
    {
      "epoch": 0.26352670110754245,
      "grad_norm": 0.3454948365688324,
      "learning_rate": 0.0001982508374470342,
      "loss": 2.4447,
      "step": 12810
    },
    {
      "epoch": 0.26373242062441016,
      "grad_norm": 0.3557266592979431,
      "learning_rate": 0.00019824669927500673,
      "loss": 2.512,
      "step": 12820
    },
    {
      "epoch": 0.26393814014127787,
      "grad_norm": 0.34819215536117554,
      "learning_rate": 0.00019824255625700493,
      "loss": 2.3774,
      "step": 12830
    },
    {
      "epoch": 0.2641438596581456,
      "grad_norm": 0.35308268666267395,
      "learning_rate": 0.0001982384083932332,
      "loss": 2.4812,
      "step": 12840
    },
    {
      "epoch": 0.2643495791750133,
      "grad_norm": 0.3186645805835724,
      "learning_rate": 0.0001982342556838961,
      "loss": 2.4424,
      "step": 12850
    },
    {
      "epoch": 0.26455529869188105,
      "grad_norm": 0.3576783835887909,
      "learning_rate": 0.0001982300981291985,
      "loss": 2.4721,
      "step": 12860
    },
    {
      "epoch": 0.26476101820874876,
      "grad_norm": 0.36422058939933777,
      "learning_rate": 0.00019822593572934544,
      "loss": 2.4917,
      "step": 12870
    },
    {
      "epoch": 0.26496673772561646,
      "grad_norm": 0.32346397638320923,
      "learning_rate": 0.00019822176848454225,
      "loss": 2.4602,
      "step": 12880
    },
    {
      "epoch": 0.26517245724248417,
      "grad_norm": 0.427842378616333,
      "learning_rate": 0.00019821759639499447,
      "loss": 2.4464,
      "step": 12890
    },
    {
      "epoch": 0.2653781767593519,
      "grad_norm": 0.35719960927963257,
      "learning_rate": 0.00019821341946090786,
      "loss": 2.4993,
      "step": 12900
    },
    {
      "epoch": 0.2655838962762196,
      "grad_norm": 0.3544873893260956,
      "learning_rate": 0.0001982092376824885,
      "loss": 2.4114,
      "step": 12910
    },
    {
      "epoch": 0.2657896157930873,
      "grad_norm": 0.346081018447876,
      "learning_rate": 0.00019820505105994261,
      "loss": 2.5223,
      "step": 12920
    },
    {
      "epoch": 0.265995335309955,
      "grad_norm": 0.3723428547382355,
      "learning_rate": 0.0001982008595934767,
      "loss": 2.5003,
      "step": 12930
    },
    {
      "epoch": 0.2662010548268227,
      "grad_norm": 0.34469643235206604,
      "learning_rate": 0.0001981966632832975,
      "loss": 2.455,
      "step": 12940
    },
    {
      "epoch": 0.2664067743436905,
      "grad_norm": 0.35636594891548157,
      "learning_rate": 0.000198192462129612,
      "loss": 2.5069,
      "step": 12950
    },
    {
      "epoch": 0.2666124938605582,
      "grad_norm": 0.37492799758911133,
      "learning_rate": 0.00019818825613262747,
      "loss": 2.4868,
      "step": 12960
    },
    {
      "epoch": 0.2668182133774259,
      "grad_norm": 0.340966135263443,
      "learning_rate": 0.00019818404529255127,
      "loss": 2.4225,
      "step": 12970
    },
    {
      "epoch": 0.2670239328942936,
      "grad_norm": 0.35834449529647827,
      "learning_rate": 0.0001981798296095912,
      "loss": 2.4406,
      "step": 12980
    },
    {
      "epoch": 0.2672296524111613,
      "grad_norm": 0.36890605092048645,
      "learning_rate": 0.00019817560908395513,
      "loss": 2.5027,
      "step": 12990
    },
    {
      "epoch": 0.267435371928029,
      "grad_norm": 0.35187801718711853,
      "learning_rate": 0.0001981713837158513,
      "loss": 2.5097,
      "step": 13000
    },
    {
      "epoch": 0.26764109144489673,
      "grad_norm": 0.34927046298980713,
      "learning_rate": 0.000198167153505488,
      "loss": 2.4526,
      "step": 13010
    },
    {
      "epoch": 0.26784681096176444,
      "grad_norm": 0.33342015743255615,
      "learning_rate": 0.00019816291845307403,
      "loss": 2.4669,
      "step": 13020
    },
    {
      "epoch": 0.26805253047863214,
      "grad_norm": 0.32447707653045654,
      "learning_rate": 0.0001981586785588182,
      "loss": 2.4928,
      "step": 13030
    },
    {
      "epoch": 0.2682582499954999,
      "grad_norm": 0.3749179244041443,
      "learning_rate": 0.00019815443382292967,
      "loss": 2.4093,
      "step": 13040
    },
    {
      "epoch": 0.2684639695123676,
      "grad_norm": 0.3483120799064636,
      "learning_rate": 0.00019815018424561775,
      "loss": 2.4806,
      "step": 13050
    },
    {
      "epoch": 0.2686696890292353,
      "grad_norm": 0.3251934051513672,
      "learning_rate": 0.00019814592982709213,
      "loss": 2.44,
      "step": 13060
    },
    {
      "epoch": 0.26887540854610303,
      "grad_norm": 0.3271900415420532,
      "learning_rate": 0.00019814167056756264,
      "loss": 2.4158,
      "step": 13070
    },
    {
      "epoch": 0.26908112806297074,
      "grad_norm": 0.3687092065811157,
      "learning_rate": 0.00019813740646723932,
      "loss": 2.4493,
      "step": 13080
    },
    {
      "epoch": 0.26928684757983845,
      "grad_norm": 0.3329165279865265,
      "learning_rate": 0.00019813313752633255,
      "loss": 2.4527,
      "step": 13090
    },
    {
      "epoch": 0.26949256709670616,
      "grad_norm": 0.3571287989616394,
      "learning_rate": 0.00019812886374505283,
      "loss": 2.4605,
      "step": 13100
    },
    {
      "epoch": 0.26969828661357387,
      "grad_norm": 0.38538700342178345,
      "learning_rate": 0.00019812458512361104,
      "loss": 2.5353,
      "step": 13110
    },
    {
      "epoch": 0.2699040061304416,
      "grad_norm": 0.32954859733581543,
      "learning_rate": 0.00019812030166221816,
      "loss": 2.4024,
      "step": 13120
    },
    {
      "epoch": 0.27010972564730934,
      "grad_norm": 0.41412150859832764,
      "learning_rate": 0.00019811601336108548,
      "loss": 2.4084,
      "step": 13130
    },
    {
      "epoch": 0.27031544516417705,
      "grad_norm": 0.3865831792354584,
      "learning_rate": 0.00019811172022042453,
      "loss": 2.4845,
      "step": 13140
    },
    {
      "epoch": 0.27052116468104476,
      "grad_norm": 0.32924512028694153,
      "learning_rate": 0.00019810742224044708,
      "loss": 2.4766,
      "step": 13150
    },
    {
      "epoch": 0.27072688419791247,
      "grad_norm": 0.37310656905174255,
      "learning_rate": 0.00019810311942136511,
      "loss": 2.4508,
      "step": 13160
    },
    {
      "epoch": 0.2709326037147802,
      "grad_norm": 0.3551391363143921,
      "learning_rate": 0.00019809881176339087,
      "loss": 2.4669,
      "step": 13170
    },
    {
      "epoch": 0.2711383232316479,
      "grad_norm": 0.32984575629234314,
      "learning_rate": 0.0001980944992667368,
      "loss": 2.4281,
      "step": 13180
    },
    {
      "epoch": 0.2713440427485156,
      "grad_norm": 0.3323022127151489,
      "learning_rate": 0.00019809018193161563,
      "loss": 2.459,
      "step": 13190
    },
    {
      "epoch": 0.2715497622653833,
      "grad_norm": 0.35757094621658325,
      "learning_rate": 0.00019808585975824032,
      "loss": 2.4444,
      "step": 13200
    },
    {
      "epoch": 0.271755481782251,
      "grad_norm": 0.331807404756546,
      "learning_rate": 0.00019808153274682405,
      "loss": 2.3814,
      "step": 13210
    },
    {
      "epoch": 0.27196120129911877,
      "grad_norm": 0.3415902853012085,
      "learning_rate": 0.00019807720089758026,
      "loss": 2.4298,
      "step": 13220
    },
    {
      "epoch": 0.2721669208159865,
      "grad_norm": 0.36034685373306274,
      "learning_rate": 0.0001980728642107226,
      "loss": 2.4856,
      "step": 13230
    },
    {
      "epoch": 0.2723726403328542,
      "grad_norm": 0.334492027759552,
      "learning_rate": 0.000198068522686465,
      "loss": 2.4592,
      "step": 13240
    },
    {
      "epoch": 0.2725783598497219,
      "grad_norm": 0.3451021611690521,
      "learning_rate": 0.00019806417632502155,
      "loss": 2.4812,
      "step": 13250
    },
    {
      "epoch": 0.2727840793665896,
      "grad_norm": 0.34191566705703735,
      "learning_rate": 0.00019805982512660668,
      "loss": 2.523,
      "step": 13260
    },
    {
      "epoch": 0.2729897988834573,
      "grad_norm": 0.3736017644405365,
      "learning_rate": 0.000198055469091435,
      "loss": 2.4433,
      "step": 13270
    },
    {
      "epoch": 0.273195518400325,
      "grad_norm": 0.3476080894470215,
      "learning_rate": 0.00019805110821972138,
      "loss": 2.4334,
      "step": 13280
    },
    {
      "epoch": 0.27340123791719273,
      "grad_norm": 0.35981473326683044,
      "learning_rate": 0.0001980467425116809,
      "loss": 2.4682,
      "step": 13290
    },
    {
      "epoch": 0.2736069574340605,
      "grad_norm": 0.3628665506839752,
      "learning_rate": 0.00019804237196752888,
      "loss": 2.4449,
      "step": 13300
    },
    {
      "epoch": 0.2738126769509282,
      "grad_norm": 0.3409368097782135,
      "learning_rate": 0.00019803799658748094,
      "loss": 2.4605,
      "step": 13310
    },
    {
      "epoch": 0.2740183964677959,
      "grad_norm": 0.34197983145713806,
      "learning_rate": 0.0001980336163717529,
      "loss": 2.4444,
      "step": 13320
    },
    {
      "epoch": 0.2742241159846636,
      "grad_norm": 0.3567298650741577,
      "learning_rate": 0.00019802923132056073,
      "loss": 2.4685,
      "step": 13330
    },
    {
      "epoch": 0.2744298355015313,
      "grad_norm": 0.334044873714447,
      "learning_rate": 0.0001980248414341208,
      "loss": 2.4651,
      "step": 13340
    },
    {
      "epoch": 0.27463555501839904,
      "grad_norm": 0.38042503595352173,
      "learning_rate": 0.0001980204467126496,
      "loss": 2.4364,
      "step": 13350
    },
    {
      "epoch": 0.27484127453526674,
      "grad_norm": 0.3593912422657013,
      "learning_rate": 0.0001980160471563639,
      "loss": 2.4635,
      "step": 13360
    },
    {
      "epoch": 0.27504699405213445,
      "grad_norm": 0.3661609888076782,
      "learning_rate": 0.00019801164276548073,
      "loss": 2.5474,
      "step": 13370
    },
    {
      "epoch": 0.27525271356900216,
      "grad_norm": 0.3415403962135315,
      "learning_rate": 0.00019800723354021735,
      "loss": 2.4479,
      "step": 13380
    },
    {
      "epoch": 0.2754584330858699,
      "grad_norm": 0.34639889001846313,
      "learning_rate": 0.00019800281948079121,
      "loss": 2.424,
      "step": 13390
    },
    {
      "epoch": 0.27566415260273763,
      "grad_norm": 0.34471413493156433,
      "learning_rate": 0.00019799840058742002,
      "loss": 2.4434,
      "step": 13400
    },
    {
      "epoch": 0.27586987211960534,
      "grad_norm": 0.3444064259529114,
      "learning_rate": 0.00019799397686032176,
      "loss": 2.4221,
      "step": 13410
    },
    {
      "epoch": 0.27607559163647305,
      "grad_norm": 0.3640916347503662,
      "learning_rate": 0.00019798954829971458,
      "loss": 2.4335,
      "step": 13420
    },
    {
      "epoch": 0.27628131115334076,
      "grad_norm": 0.32824641466140747,
      "learning_rate": 0.00019798511490581703,
      "loss": 2.4414,
      "step": 13430
    },
    {
      "epoch": 0.27648703067020847,
      "grad_norm": 0.3532430827617645,
      "learning_rate": 0.00019798067667884767,
      "loss": 2.4717,
      "step": 13440
    },
    {
      "epoch": 0.2766927501870762,
      "grad_norm": 0.34894487261772156,
      "learning_rate": 0.00019797623361902548,
      "loss": 2.3658,
      "step": 13450
    },
    {
      "epoch": 0.2768984697039439,
      "grad_norm": 0.3397105932235718,
      "learning_rate": 0.0001979717857265696,
      "loss": 2.5228,
      "step": 13460
    },
    {
      "epoch": 0.2771041892208116,
      "grad_norm": 0.33550867438316345,
      "learning_rate": 0.00019796733300169941,
      "loss": 2.461,
      "step": 13470
    },
    {
      "epoch": 0.27730990873767936,
      "grad_norm": 0.32426080107688904,
      "learning_rate": 0.00019796287544463452,
      "loss": 2.4029,
      "step": 13480
    },
    {
      "epoch": 0.27751562825454706,
      "grad_norm": 0.3596261143684387,
      "learning_rate": 0.00019795841305559483,
      "loss": 2.4671,
      "step": 13490
    },
    {
      "epoch": 0.2777213477714148,
      "grad_norm": 0.3550777733325958,
      "learning_rate": 0.00019795394583480043,
      "loss": 2.4933,
      "step": 13500
    },
    {
      "epoch": 0.2779270672882825,
      "grad_norm": 0.3238138258457184,
      "learning_rate": 0.0001979494737824717,
      "loss": 2.5034,
      "step": 13510
    },
    {
      "epoch": 0.2781327868051502,
      "grad_norm": 0.3105212152004242,
      "learning_rate": 0.00019794499689882914,
      "loss": 2.4806,
      "step": 13520
    },
    {
      "epoch": 0.2783385063220179,
      "grad_norm": 0.3389400541782379,
      "learning_rate": 0.00019794051518409365,
      "loss": 2.4168,
      "step": 13530
    },
    {
      "epoch": 0.2785442258388856,
      "grad_norm": 0.3779008090496063,
      "learning_rate": 0.00019793602863848623,
      "loss": 2.4747,
      "step": 13540
    },
    {
      "epoch": 0.2787499453557533,
      "grad_norm": 0.3615622818470001,
      "learning_rate": 0.00019793153726222824,
      "loss": 2.478,
      "step": 13550
    },
    {
      "epoch": 0.278955664872621,
      "grad_norm": 0.3886925280094147,
      "learning_rate": 0.00019792704105554116,
      "loss": 2.4424,
      "step": 13560
    },
    {
      "epoch": 0.2791613843894888,
      "grad_norm": 0.3555203378200531,
      "learning_rate": 0.00019792254001864678,
      "loss": 2.4174,
      "step": 13570
    },
    {
      "epoch": 0.2793671039063565,
      "grad_norm": 0.3423709571361542,
      "learning_rate": 0.0001979180341517671,
      "loss": 2.4551,
      "step": 13580
    },
    {
      "epoch": 0.2795728234232242,
      "grad_norm": 0.3206384479999542,
      "learning_rate": 0.0001979135234551244,
      "loss": 2.4869,
      "step": 13590
    },
    {
      "epoch": 0.2797785429400919,
      "grad_norm": 0.33800795674324036,
      "learning_rate": 0.0001979090079289412,
      "loss": 2.4873,
      "step": 13600
    },
    {
      "epoch": 0.2799842624569596,
      "grad_norm": 0.3769948184490204,
      "learning_rate": 0.0001979044875734401,
      "loss": 2.4094,
      "step": 13610
    },
    {
      "epoch": 0.28018998197382733,
      "grad_norm": 0.3600119352340698,
      "learning_rate": 0.00019789996238884418,
      "loss": 2.4702,
      "step": 13620
    },
    {
      "epoch": 0.28039570149069504,
      "grad_norm": 0.3878192603588104,
      "learning_rate": 0.00019789543237537662,
      "loss": 2.4736,
      "step": 13630
    },
    {
      "epoch": 0.28060142100756275,
      "grad_norm": 0.3512905240058899,
      "learning_rate": 0.00019789089753326084,
      "loss": 2.4188,
      "step": 13640
    },
    {
      "epoch": 0.28080714052443045,
      "grad_norm": 0.35475194454193115,
      "learning_rate": 0.0001978863578627205,
      "loss": 2.4664,
      "step": 13650
    },
    {
      "epoch": 0.2810128600412982,
      "grad_norm": 0.3818192183971405,
      "learning_rate": 0.00019788181336397956,
      "loss": 2.4596,
      "step": 13660
    },
    {
      "epoch": 0.2812185795581659,
      "grad_norm": 0.3155059814453125,
      "learning_rate": 0.0001978772640372622,
      "loss": 2.436,
      "step": 13670
    },
    {
      "epoch": 0.28142429907503363,
      "grad_norm": 0.3786330819129944,
      "learning_rate": 0.00019787270988279272,
      "loss": 2.4895,
      "step": 13680
    },
    {
      "epoch": 0.28163001859190134,
      "grad_norm": 0.315814733505249,
      "learning_rate": 0.0001978681509007958,
      "loss": 2.4951,
      "step": 13690
    },
    {
      "epoch": 0.28183573810876905,
      "grad_norm": 0.3646946847438812,
      "learning_rate": 0.00019786358709149635,
      "loss": 2.4405,
      "step": 13700
    },
    {
      "epoch": 0.28204145762563676,
      "grad_norm": 0.37894096970558167,
      "learning_rate": 0.0001978590184551194,
      "loss": 2.4497,
      "step": 13710
    },
    {
      "epoch": 0.28224717714250447,
      "grad_norm": 0.3427011966705322,
      "learning_rate": 0.0001978544449918904,
      "loss": 2.4759,
      "step": 13720
    },
    {
      "epoch": 0.2824528966593722,
      "grad_norm": 0.35565102100372314,
      "learning_rate": 0.00019784986670203482,
      "loss": 2.5059,
      "step": 13730
    },
    {
      "epoch": 0.2826586161762399,
      "grad_norm": 0.373543381690979,
      "learning_rate": 0.00019784528358577852,
      "loss": 2.4832,
      "step": 13740
    },
    {
      "epoch": 0.28286433569310765,
      "grad_norm": 0.32996976375579834,
      "learning_rate": 0.00019784069564334763,
      "loss": 2.4593,
      "step": 13750
    },
    {
      "epoch": 0.28307005520997536,
      "grad_norm": 0.3358573913574219,
      "learning_rate": 0.00019783610287496832,
      "loss": 2.4553,
      "step": 13760
    },
    {
      "epoch": 0.28327577472684307,
      "grad_norm": 0.36506009101867676,
      "learning_rate": 0.00019783150528086723,
      "loss": 2.4986,
      "step": 13770
    },
    {
      "epoch": 0.2834814942437108,
      "grad_norm": 0.3731844127178192,
      "learning_rate": 0.00019782690286127114,
      "loss": 2.4404,
      "step": 13780
    },
    {
      "epoch": 0.2836872137605785,
      "grad_norm": 0.3346928358078003,
      "learning_rate": 0.00019782229561640698,
      "loss": 2.4311,
      "step": 13790
    },
    {
      "epoch": 0.2838929332774462,
      "grad_norm": 0.3305301070213318,
      "learning_rate": 0.00019781768354650207,
      "loss": 2.436,
      "step": 13800
    },
    {
      "epoch": 0.2840986527943139,
      "grad_norm": 0.33782485127449036,
      "learning_rate": 0.00019781306665178386,
      "loss": 2.4638,
      "step": 13810
    },
    {
      "epoch": 0.2843043723111816,
      "grad_norm": 0.373256117105484,
      "learning_rate": 0.0001978084449324801,
      "loss": 2.4113,
      "step": 13820
    },
    {
      "epoch": 0.2845100918280493,
      "grad_norm": 0.3854256868362427,
      "learning_rate": 0.0001978038183888187,
      "loss": 2.4241,
      "step": 13830
    },
    {
      "epoch": 0.2847158113449171,
      "grad_norm": 0.3295060992240906,
      "learning_rate": 0.00019779918702102797,
      "loss": 2.4268,
      "step": 13840
    },
    {
      "epoch": 0.2849215308617848,
      "grad_norm": 0.3369465470314026,
      "learning_rate": 0.00019779455082933627,
      "loss": 2.4644,
      "step": 13850
    },
    {
      "epoch": 0.2851272503786525,
      "grad_norm": 0.3417814075946808,
      "learning_rate": 0.00019778990981397226,
      "loss": 2.4768,
      "step": 13860
    },
    {
      "epoch": 0.2853329698955202,
      "grad_norm": 0.3385421633720398,
      "learning_rate": 0.0001977852639751649,
      "loss": 2.4775,
      "step": 13870
    },
    {
      "epoch": 0.2855386894123879,
      "grad_norm": 0.3801940381526947,
      "learning_rate": 0.00019778061331314338,
      "loss": 2.4775,
      "step": 13880
    },
    {
      "epoch": 0.2857444089292556,
      "grad_norm": 0.34952834248542786,
      "learning_rate": 0.00019777595782813698,
      "loss": 2.4199,
      "step": 13890
    },
    {
      "epoch": 0.28595012844612333,
      "grad_norm": 0.3293331563472748,
      "learning_rate": 0.00019777129752037543,
      "loss": 2.4627,
      "step": 13900
    },
    {
      "epoch": 0.28615584796299104,
      "grad_norm": 0.35771992802619934,
      "learning_rate": 0.00019776663239008855,
      "loss": 2.4386,
      "step": 13910
    },
    {
      "epoch": 0.28636156747985875,
      "grad_norm": 0.361398309469223,
      "learning_rate": 0.00019776196243750647,
      "loss": 2.4522,
      "step": 13920
    },
    {
      "epoch": 0.2865672869967265,
      "grad_norm": 0.3813258111476898,
      "learning_rate": 0.00019775728766285953,
      "loss": 2.4681,
      "step": 13930
    },
    {
      "epoch": 0.2867730065135942,
      "grad_norm": 0.35064390301704407,
      "learning_rate": 0.0001977526080663783,
      "loss": 2.4954,
      "step": 13940
    },
    {
      "epoch": 0.28697872603046193,
      "grad_norm": 0.3477584421634674,
      "learning_rate": 0.00019774792364829358,
      "loss": 2.4435,
      "step": 13950
    },
    {
      "epoch": 0.28718444554732964,
      "grad_norm": 0.3567909300327301,
      "learning_rate": 0.00019774323440883645,
      "loss": 2.4689,
      "step": 13960
    },
    {
      "epoch": 0.28739016506419734,
      "grad_norm": 0.3553900718688965,
      "learning_rate": 0.00019773854034823822,
      "loss": 2.4547,
      "step": 13970
    },
    {
      "epoch": 0.28759588458106505,
      "grad_norm": 0.34930214285850525,
      "learning_rate": 0.00019773384146673043,
      "loss": 2.5136,
      "step": 13980
    },
    {
      "epoch": 0.28780160409793276,
      "grad_norm": 0.37246567010879517,
      "learning_rate": 0.00019772913776454478,
      "loss": 2.4662,
      "step": 13990
    },
    {
      "epoch": 0.28800732361480047,
      "grad_norm": 0.3873332440853119,
      "learning_rate": 0.00019772442924191334,
      "loss": 2.4483,
      "step": 14000
    },
    {
      "epoch": 0.2882130431316682,
      "grad_norm": 0.3507571816444397,
      "learning_rate": 0.00019771971589906833,
      "loss": 2.4375,
      "step": 14010
    },
    {
      "epoch": 0.28841876264853594,
      "grad_norm": 0.34448009729385376,
      "learning_rate": 0.00019771499773624227,
      "loss": 2.4759,
      "step": 14020
    },
    {
      "epoch": 0.28862448216540365,
      "grad_norm": 0.36863258481025696,
      "learning_rate": 0.00019771027475366785,
      "loss": 2.4822,
      "step": 14030
    },
    {
      "epoch": 0.28883020168227136,
      "grad_norm": 0.3387008309364319,
      "learning_rate": 0.000197705546951578,
      "loss": 2.4698,
      "step": 14040
    },
    {
      "epoch": 0.28903592119913907,
      "grad_norm": 0.34426912665367126,
      "learning_rate": 0.00019770081433020598,
      "loss": 2.4795,
      "step": 14050
    },
    {
      "epoch": 0.2892416407160068,
      "grad_norm": 0.3373279869556427,
      "learning_rate": 0.00019769607688978517,
      "loss": 2.4249,
      "step": 14060
    },
    {
      "epoch": 0.2894473602328745,
      "grad_norm": 0.352308064699173,
      "learning_rate": 0.0001976913346305493,
      "loss": 2.4716,
      "step": 14070
    },
    {
      "epoch": 0.2896530797497422,
      "grad_norm": 0.32711878418922424,
      "learning_rate": 0.00019768658755273225,
      "loss": 2.4142,
      "step": 14080
    },
    {
      "epoch": 0.2898587992666099,
      "grad_norm": 0.3362842798233032,
      "learning_rate": 0.00019768183565656812,
      "loss": 2.448,
      "step": 14090
    },
    {
      "epoch": 0.2900645187834776,
      "grad_norm": 0.3521013557910919,
      "learning_rate": 0.00019767707894229137,
      "loss": 2.4547,
      "step": 14100
    },
    {
      "epoch": 0.2902702383003454,
      "grad_norm": 0.33890822529792786,
      "learning_rate": 0.0001976723174101366,
      "loss": 2.4909,
      "step": 14110
    },
    {
      "epoch": 0.2904759578172131,
      "grad_norm": 0.3415135145187378,
      "learning_rate": 0.00019766755106033864,
      "loss": 2.4746,
      "step": 14120
    },
    {
      "epoch": 0.2906816773340808,
      "grad_norm": 0.3474385142326355,
      "learning_rate": 0.0001976627798931326,
      "loss": 2.4853,
      "step": 14130
    },
    {
      "epoch": 0.2908873968509485,
      "grad_norm": 0.3570965528488159,
      "learning_rate": 0.00019765800390875385,
      "loss": 2.4732,
      "step": 14140
    },
    {
      "epoch": 0.2910931163678162,
      "grad_norm": 0.34026265144348145,
      "learning_rate": 0.00019765322310743793,
      "loss": 2.4268,
      "step": 14150
    },
    {
      "epoch": 0.2912988358846839,
      "grad_norm": 0.3261706233024597,
      "learning_rate": 0.00019764843748942067,
      "loss": 2.4606,
      "step": 14160
    },
    {
      "epoch": 0.2915045554015516,
      "grad_norm": 0.34777212142944336,
      "learning_rate": 0.00019764364705493812,
      "loss": 2.4862,
      "step": 14170
    },
    {
      "epoch": 0.29171027491841933,
      "grad_norm": 0.33548226952552795,
      "learning_rate": 0.00019763885180422653,
      "loss": 2.4523,
      "step": 14180
    },
    {
      "epoch": 0.2919159944352871,
      "grad_norm": 0.3771194815635681,
      "learning_rate": 0.00019763405173752244,
      "loss": 2.442,
      "step": 14190
    },
    {
      "epoch": 0.2921217139521548,
      "grad_norm": 0.3307367265224457,
      "learning_rate": 0.00019762924685506265,
      "loss": 2.4895,
      "step": 14200
    },
    {
      "epoch": 0.2923274334690225,
      "grad_norm": 0.3307071626186371,
      "learning_rate": 0.0001976244371570841,
      "loss": 2.4663,
      "step": 14210
    },
    {
      "epoch": 0.2925331529858902,
      "grad_norm": 0.3407650589942932,
      "learning_rate": 0.0001976196226438241,
      "loss": 2.4703,
      "step": 14220
    },
    {
      "epoch": 0.29273887250275793,
      "grad_norm": 0.35906538367271423,
      "learning_rate": 0.00019761480331552004,
      "loss": 2.4116,
      "step": 14230
    },
    {
      "epoch": 0.29294459201962564,
      "grad_norm": 0.3730180263519287,
      "learning_rate": 0.00019760997917240967,
      "loss": 2.4999,
      "step": 14240
    },
    {
      "epoch": 0.29315031153649335,
      "grad_norm": 0.3485845923423767,
      "learning_rate": 0.00019760515021473096,
      "loss": 2.4257,
      "step": 14250
    },
    {
      "epoch": 0.29335603105336105,
      "grad_norm": 0.3643578886985779,
      "learning_rate": 0.00019760031644272205,
      "loss": 2.4926,
      "step": 14260
    },
    {
      "epoch": 0.29356175057022876,
      "grad_norm": 0.3652748465538025,
      "learning_rate": 0.00019759547785662141,
      "loss": 2.4545,
      "step": 14270
    },
    {
      "epoch": 0.2937674700870965,
      "grad_norm": 0.3470701277256012,
      "learning_rate": 0.00019759063445666766,
      "loss": 2.484,
      "step": 14280
    },
    {
      "epoch": 0.29397318960396424,
      "grad_norm": 0.4126950204372406,
      "learning_rate": 0.00019758578624309975,
      "loss": 2.4559,
      "step": 14290
    },
    {
      "epoch": 0.29417890912083194,
      "grad_norm": 0.3619217872619629,
      "learning_rate": 0.0001975809332161567,
      "loss": 2.5353,
      "step": 14300
    },
    {
      "epoch": 0.29438462863769965,
      "grad_norm": 0.33021220564842224,
      "learning_rate": 0.00019757607537607805,
      "loss": 2.4421,
      "step": 14310
    },
    {
      "epoch": 0.29459034815456736,
      "grad_norm": 0.36900198459625244,
      "learning_rate": 0.0001975712127231033,
      "loss": 2.4499,
      "step": 14320
    },
    {
      "epoch": 0.29479606767143507,
      "grad_norm": 0.3417375683784485,
      "learning_rate": 0.00019756634525747234,
      "loss": 2.4439,
      "step": 14330
    },
    {
      "epoch": 0.2950017871883028,
      "grad_norm": 0.35495153069496155,
      "learning_rate": 0.00019756147297942523,
      "loss": 2.4414,
      "step": 14340
    },
    {
      "epoch": 0.2952075067051705,
      "grad_norm": 0.34816715121269226,
      "learning_rate": 0.00019755659588920228,
      "loss": 2.4954,
      "step": 14350
    },
    {
      "epoch": 0.2954132262220382,
      "grad_norm": 0.33239102363586426,
      "learning_rate": 0.0001975517139870441,
      "loss": 2.4616,
      "step": 14360
    },
    {
      "epoch": 0.29561894573890596,
      "grad_norm": 0.32947784662246704,
      "learning_rate": 0.00019754682727319146,
      "loss": 2.4817,
      "step": 14370
    },
    {
      "epoch": 0.29582466525577367,
      "grad_norm": 0.38368168473243713,
      "learning_rate": 0.0001975419357478854,
      "loss": 2.424,
      "step": 14380
    },
    {
      "epoch": 0.2960303847726414,
      "grad_norm": 0.36252060532569885,
      "learning_rate": 0.0001975370394113672,
      "loss": 2.4983,
      "step": 14390
    },
    {
      "epoch": 0.2962361042895091,
      "grad_norm": 0.3552895486354828,
      "learning_rate": 0.00019753213826387834,
      "loss": 2.4706,
      "step": 14400
    },
    {
      "epoch": 0.2964418238063768,
      "grad_norm": 0.36135151982307434,
      "learning_rate": 0.0001975272323056606,
      "loss": 2.4707,
      "step": 14410
    },
    {
      "epoch": 0.2966475433232445,
      "grad_norm": 0.32866930961608887,
      "learning_rate": 0.00019752232153695594,
      "loss": 2.4528,
      "step": 14420
    },
    {
      "epoch": 0.2968532628401122,
      "grad_norm": 0.3537212312221527,
      "learning_rate": 0.00019751740595800658,
      "loss": 2.4437,
      "step": 14430
    },
    {
      "epoch": 0.2970589823569799,
      "grad_norm": 0.30892544984817505,
      "learning_rate": 0.000197512485569055,
      "loss": 2.4259,
      "step": 14440
    },
    {
      "epoch": 0.2972647018738476,
      "grad_norm": 0.38106298446655273,
      "learning_rate": 0.00019750756037034388,
      "loss": 2.4889,
      "step": 14450
    },
    {
      "epoch": 0.2974704213907154,
      "grad_norm": 0.3608478903770447,
      "learning_rate": 0.0001975026303621162,
      "loss": 2.4764,
      "step": 14460
    },
    {
      "epoch": 0.2976761409075831,
      "grad_norm": 0.38572046160697937,
      "learning_rate": 0.00019749769554461502,
      "loss": 2.4545,
      "step": 14470
    },
    {
      "epoch": 0.2978818604244508,
      "grad_norm": 0.31860554218292236,
      "learning_rate": 0.00019749275591808386,
      "loss": 2.4211,
      "step": 14480
    },
    {
      "epoch": 0.2980875799413185,
      "grad_norm": 0.33379244804382324,
      "learning_rate": 0.00019748781148276633,
      "loss": 2.5286,
      "step": 14490
    },
    {
      "epoch": 0.2982932994581862,
      "grad_norm": 0.3450962007045746,
      "learning_rate": 0.0001974828622389063,
      "loss": 2.4573,
      "step": 14500
    },
    {
      "epoch": 0.29849901897505393,
      "grad_norm": 0.3828440010547638,
      "learning_rate": 0.00019747790818674789,
      "loss": 2.395,
      "step": 14510
    },
    {
      "epoch": 0.29870473849192164,
      "grad_norm": 0.3413909673690796,
      "learning_rate": 0.00019747294932653543,
      "loss": 2.4233,
      "step": 14520
    },
    {
      "epoch": 0.29891045800878935,
      "grad_norm": 0.3215341866016388,
      "learning_rate": 0.0001974679856585136,
      "loss": 2.4448,
      "step": 14530
    },
    {
      "epoch": 0.29911617752565706,
      "grad_norm": 0.35675275325775146,
      "learning_rate": 0.00019746301718292714,
      "loss": 2.5484,
      "step": 14540
    },
    {
      "epoch": 0.2993218970425248,
      "grad_norm": 0.33219265937805176,
      "learning_rate": 0.00019745804390002114,
      "loss": 2.4795,
      "step": 14550
    },
    {
      "epoch": 0.29952761655939253,
      "grad_norm": 0.38480401039123535,
      "learning_rate": 0.00019745306581004092,
      "loss": 2.395,
      "step": 14560
    },
    {
      "epoch": 0.29973333607626024,
      "grad_norm": 0.3440357446670532,
      "learning_rate": 0.00019744808291323205,
      "loss": 2.4349,
      "step": 14570
    },
    {
      "epoch": 0.29993905559312795,
      "grad_norm": 0.379141241312027,
      "learning_rate": 0.00019744309520984023,
      "loss": 2.4215,
      "step": 14580
    },
    {
      "epoch": 0.30014477510999565,
      "grad_norm": 0.3348241150379181,
      "learning_rate": 0.00019743810270011157,
      "loss": 2.4823,
      "step": 14590
    },
    {
      "epoch": 0.30035049462686336,
      "grad_norm": 0.36909669637680054,
      "learning_rate": 0.00019743310538429225,
      "loss": 2.4848,
      "step": 14600
    },
    {
      "epoch": 0.30055621414373107,
      "grad_norm": 0.3357868194580078,
      "learning_rate": 0.00019742810326262882,
      "loss": 2.4796,
      "step": 14610
    },
    {
      "epoch": 0.3007619336605988,
      "grad_norm": 0.32233506441116333,
      "learning_rate": 0.00019742309633536794,
      "loss": 2.4276,
      "step": 14620
    },
    {
      "epoch": 0.3009676531774665,
      "grad_norm": 0.39570730924606323,
      "learning_rate": 0.00019741808460275664,
      "loss": 2.4148,
      "step": 14630
    },
    {
      "epoch": 0.30117337269433425,
      "grad_norm": 0.3653952479362488,
      "learning_rate": 0.00019741306806504206,
      "loss": 2.4707,
      "step": 14640
    },
    {
      "epoch": 0.30137909221120196,
      "grad_norm": 0.3427884876728058,
      "learning_rate": 0.0001974080467224717,
      "loss": 2.4857,
      "step": 14650
    },
    {
      "epoch": 0.30158481172806967,
      "grad_norm": 0.32583916187286377,
      "learning_rate": 0.0001974030205752932,
      "loss": 2.3842,
      "step": 14660
    },
    {
      "epoch": 0.3017905312449374,
      "grad_norm": 0.37407469749450684,
      "learning_rate": 0.00019739798962375445,
      "loss": 2.4862,
      "step": 14670
    },
    {
      "epoch": 0.3019962507618051,
      "grad_norm": 0.3293852210044861,
      "learning_rate": 0.00019739295386810365,
      "loss": 2.4125,
      "step": 14680
    },
    {
      "epoch": 0.3022019702786728,
      "grad_norm": 0.35855618119239807,
      "learning_rate": 0.00019738791330858917,
      "loss": 2.4847,
      "step": 14690
    },
    {
      "epoch": 0.3024076897955405,
      "grad_norm": 0.316982239484787,
      "learning_rate": 0.0001973828679454596,
      "loss": 2.4519,
      "step": 14700
    },
    {
      "epoch": 0.3026134093124082,
      "grad_norm": 0.4252597391605377,
      "learning_rate": 0.00019737781777896387,
      "loss": 2.4931,
      "step": 14710
    },
    {
      "epoch": 0.3028191288292759,
      "grad_norm": 0.33699384331703186,
      "learning_rate": 0.000197372762809351,
      "loss": 2.455,
      "step": 14720
    },
    {
      "epoch": 0.3030248483461437,
      "grad_norm": 0.35760724544525146,
      "learning_rate": 0.0001973677030368704,
      "loss": 2.471,
      "step": 14730
    },
    {
      "epoch": 0.3032305678630114,
      "grad_norm": 0.3831222951412201,
      "learning_rate": 0.00019736263846177154,
      "loss": 2.4992,
      "step": 14740
    },
    {
      "epoch": 0.3034362873798791,
      "grad_norm": 0.32746222615242004,
      "learning_rate": 0.0001973575690843043,
      "loss": 2.4388,
      "step": 14750
    },
    {
      "epoch": 0.3036420068967468,
      "grad_norm": 0.3384803533554077,
      "learning_rate": 0.00019735249490471875,
      "loss": 2.4951,
      "step": 14760
    },
    {
      "epoch": 0.3038477264136145,
      "grad_norm": 0.3629736304283142,
      "learning_rate": 0.0001973474159232651,
      "loss": 2.4216,
      "step": 14770
    },
    {
      "epoch": 0.3040534459304822,
      "grad_norm": 0.33723434805870056,
      "learning_rate": 0.00019734233214019394,
      "loss": 2.4442,
      "step": 14780
    },
    {
      "epoch": 0.30425916544734993,
      "grad_norm": 0.3402160704135895,
      "learning_rate": 0.00019733724355575592,
      "loss": 2.4361,
      "step": 14790
    },
    {
      "epoch": 0.30446488496421764,
      "grad_norm": 0.35175836086273193,
      "learning_rate": 0.00019733215017020216,
      "loss": 2.4605,
      "step": 14800
    },
    {
      "epoch": 0.30467060448108535,
      "grad_norm": 0.3368481993675232,
      "learning_rate": 0.0001973270519837838,
      "loss": 2.4661,
      "step": 14810
    },
    {
      "epoch": 0.3048763239979531,
      "grad_norm": 0.3258921802043915,
      "learning_rate": 0.00019732194899675236,
      "loss": 2.4462,
      "step": 14820
    },
    {
      "epoch": 0.3050820435148208,
      "grad_norm": 0.3571975529193878,
      "learning_rate": 0.0001973168412093595,
      "loss": 2.3985,
      "step": 14830
    },
    {
      "epoch": 0.30528776303168853,
      "grad_norm": 0.418039470911026,
      "learning_rate": 0.0001973117286218572,
      "loss": 2.4086,
      "step": 14840
    },
    {
      "epoch": 0.30549348254855624,
      "grad_norm": 0.3445793092250824,
      "learning_rate": 0.00019730661123449757,
      "loss": 2.4557,
      "step": 14850
    },
    {
      "epoch": 0.30569920206542395,
      "grad_norm": 0.35829493403434753,
      "learning_rate": 0.0001973014890475331,
      "loss": 2.5502,
      "step": 14860
    },
    {
      "epoch": 0.30590492158229166,
      "grad_norm": 0.372261106967926,
      "learning_rate": 0.00019729636206121637,
      "loss": 2.4719,
      "step": 14870
    },
    {
      "epoch": 0.30611064109915936,
      "grad_norm": 0.3582857847213745,
      "learning_rate": 0.00019729123027580032,
      "loss": 2.4718,
      "step": 14880
    },
    {
      "epoch": 0.3063163606160271,
      "grad_norm": 0.31200775504112244,
      "learning_rate": 0.00019728609369153806,
      "loss": 2.4499,
      "step": 14890
    },
    {
      "epoch": 0.3065220801328948,
      "grad_norm": 0.32468435168266296,
      "learning_rate": 0.00019728095230868293,
      "loss": 2.4128,
      "step": 14900
    },
    {
      "epoch": 0.30672779964976254,
      "grad_norm": 0.3625171482563019,
      "learning_rate": 0.00019727580612748856,
      "loss": 2.4522,
      "step": 14910
    },
    {
      "epoch": 0.30693351916663025,
      "grad_norm": 0.35834965109825134,
      "learning_rate": 0.00019727065514820875,
      "loss": 2.4746,
      "step": 14920
    },
    {
      "epoch": 0.30713923868349796,
      "grad_norm": 0.3746784031391144,
      "learning_rate": 0.00019726549937109758,
      "loss": 2.4474,
      "step": 14930
    },
    {
      "epoch": 0.30734495820036567,
      "grad_norm": 0.37097808718681335,
      "learning_rate": 0.00019726033879640938,
      "loss": 2.4278,
      "step": 14940
    },
    {
      "epoch": 0.3075506777172334,
      "grad_norm": 0.3598169982433319,
      "learning_rate": 0.00019725517342439864,
      "loss": 2.4856,
      "step": 14950
    },
    {
      "epoch": 0.3077563972341011,
      "grad_norm": 0.3665542006492615,
      "learning_rate": 0.0001972500032553202,
      "loss": 2.4641,
      "step": 14960
    },
    {
      "epoch": 0.3079621167509688,
      "grad_norm": 0.3570284843444824,
      "learning_rate": 0.00019724482828942903,
      "loss": 2.4735,
      "step": 14970
    },
    {
      "epoch": 0.3081678362678365,
      "grad_norm": 0.33594027161598206,
      "learning_rate": 0.00019723964852698042,
      "loss": 2.4577,
      "step": 14980
    },
    {
      "epoch": 0.3083735557847042,
      "grad_norm": 0.37365448474884033,
      "learning_rate": 0.00019723446396822982,
      "loss": 2.5157,
      "step": 14990
    },
    {
      "epoch": 0.308579275301572,
      "grad_norm": 0.3574356436729431,
      "learning_rate": 0.00019722927461343298,
      "loss": 2.4328,
      "step": 15000
    },
    {
      "epoch": 0.3087849948184397,
      "grad_norm": 0.3773297071456909,
      "learning_rate": 0.00019722408046284588,
      "loss": 2.4901,
      "step": 15010
    },
    {
      "epoch": 0.3089907143353074,
      "grad_norm": 0.35926568508148193,
      "learning_rate": 0.00019721888151672467,
      "loss": 2.4491,
      "step": 15020
    },
    {
      "epoch": 0.3091964338521751,
      "grad_norm": 0.34952083230018616,
      "learning_rate": 0.00019721367777532584,
      "loss": 2.451,
      "step": 15030
    },
    {
      "epoch": 0.3094021533690428,
      "grad_norm": 0.34592998027801514,
      "learning_rate": 0.00019720846923890602,
      "loss": 2.4736,
      "step": 15040
    },
    {
      "epoch": 0.3096078728859105,
      "grad_norm": 0.35279837250709534,
      "learning_rate": 0.00019720325590772214,
      "loss": 2.4241,
      "step": 15050
    },
    {
      "epoch": 0.3098135924027782,
      "grad_norm": 0.3278006911277771,
      "learning_rate": 0.00019719803778203135,
      "loss": 2.3794,
      "step": 15060
    },
    {
      "epoch": 0.31001931191964593,
      "grad_norm": 0.3408955931663513,
      "learning_rate": 0.000197192814862091,
      "loss": 2.4485,
      "step": 15070
    },
    {
      "epoch": 0.3102250314365137,
      "grad_norm": 0.34059616923332214,
      "learning_rate": 0.00019718758714815874,
      "loss": 2.4842,
      "step": 15080
    },
    {
      "epoch": 0.3104307509533814,
      "grad_norm": 0.3316989541053772,
      "learning_rate": 0.0001971823546404924,
      "loss": 2.4407,
      "step": 15090
    },
    {
      "epoch": 0.3106364704702491,
      "grad_norm": 0.35182055830955505,
      "learning_rate": 0.00019717711733935014,
      "loss": 2.4777,
      "step": 15100
    },
    {
      "epoch": 0.3108421899871168,
      "grad_norm": 0.3282371759414673,
      "learning_rate": 0.0001971718752449902,
      "loss": 2.4276,
      "step": 15110
    },
    {
      "epoch": 0.31104790950398453,
      "grad_norm": 0.34721994400024414,
      "learning_rate": 0.00019716662835767115,
      "loss": 2.4227,
      "step": 15120
    },
    {
      "epoch": 0.31125362902085224,
      "grad_norm": 0.3579603433609009,
      "learning_rate": 0.00019716137667765183,
      "loss": 2.515,
      "step": 15130
    },
    {
      "epoch": 0.31145934853771995,
      "grad_norm": 0.35993289947509766,
      "learning_rate": 0.0001971561202051913,
      "loss": 2.4581,
      "step": 15140
    },
    {
      "epoch": 0.31166506805458766,
      "grad_norm": 0.37371814250946045,
      "learning_rate": 0.00019715085894054875,
      "loss": 2.4642,
      "step": 15150
    },
    {
      "epoch": 0.31187078757145537,
      "grad_norm": 0.3503867983818054,
      "learning_rate": 0.00019714559288398377,
      "loss": 2.4559,
      "step": 15160
    },
    {
      "epoch": 0.31207650708832313,
      "grad_norm": 0.3432513475418091,
      "learning_rate": 0.00019714032203575608,
      "loss": 2.4212,
      "step": 15170
    },
    {
      "epoch": 0.31228222660519084,
      "grad_norm": 0.4694240093231201,
      "learning_rate": 0.00019713504639612563,
      "loss": 2.4271,
      "step": 15180
    },
    {
      "epoch": 0.31248794612205855,
      "grad_norm": 0.35578444600105286,
      "learning_rate": 0.00019712976596535268,
      "loss": 2.4039,
      "step": 15190
    },
    {
      "epoch": 0.31269366563892625,
      "grad_norm": 0.3454340100288391,
      "learning_rate": 0.00019712448074369764,
      "loss": 2.4605,
      "step": 15200
    },
    {
      "epoch": 0.31289938515579396,
      "grad_norm": 0.3465436100959778,
      "learning_rate": 0.0001971191907314213,
      "loss": 2.4364,
      "step": 15210
    },
    {
      "epoch": 0.31310510467266167,
      "grad_norm": 0.372271865606308,
      "learning_rate": 0.00019711389592878443,
      "loss": 2.5089,
      "step": 15220
    },
    {
      "epoch": 0.3133108241895294,
      "grad_norm": 0.33155563473701477,
      "learning_rate": 0.00019710859633604834,
      "loss": 2.4569,
      "step": 15230
    },
    {
      "epoch": 0.3135165437063971,
      "grad_norm": 0.3436894118785858,
      "learning_rate": 0.00019710329195347435,
      "loss": 2.4723,
      "step": 15240
    },
    {
      "epoch": 0.3137222632232648,
      "grad_norm": 0.363290935754776,
      "learning_rate": 0.00019709798278132412,
      "loss": 2.4531,
      "step": 15250
    },
    {
      "epoch": 0.31392798274013256,
      "grad_norm": 0.36781007051467896,
      "learning_rate": 0.0001970926688198595,
      "loss": 2.5085,
      "step": 15260
    },
    {
      "epoch": 0.31413370225700027,
      "grad_norm": 0.35428759455680847,
      "learning_rate": 0.00019708735006934266,
      "loss": 2.4889,
      "step": 15270
    },
    {
      "epoch": 0.314339421773868,
      "grad_norm": 0.38009586930274963,
      "learning_rate": 0.00019708202653003588,
      "loss": 2.4314,
      "step": 15280
    },
    {
      "epoch": 0.3145451412907357,
      "grad_norm": 0.3718028962612152,
      "learning_rate": 0.00019707669820220175,
      "loss": 2.4628,
      "step": 15290
    },
    {
      "epoch": 0.3147508608076034,
      "grad_norm": 0.3564557731151581,
      "learning_rate": 0.00019707136508610314,
      "loss": 2.4292,
      "step": 15300
    },
    {
      "epoch": 0.3149565803244711,
      "grad_norm": 0.37975451350212097,
      "learning_rate": 0.000197066027182003,
      "loss": 2.4591,
      "step": 15310
    },
    {
      "epoch": 0.3151622998413388,
      "grad_norm": 0.34129956364631653,
      "learning_rate": 0.00019706068449016476,
      "loss": 2.414,
      "step": 15320
    },
    {
      "epoch": 0.3153680193582065,
      "grad_norm": 0.37172022461891174,
      "learning_rate": 0.00019705533701085183,
      "loss": 2.4631,
      "step": 15330
    },
    {
      "epoch": 0.31557373887507423,
      "grad_norm": 0.38176941871643066,
      "learning_rate": 0.00019704998474432802,
      "loss": 2.4591,
      "step": 15340
    },
    {
      "epoch": 0.315779458391942,
      "grad_norm": 0.3516595661640167,
      "learning_rate": 0.00019704462769085733,
      "loss": 2.4509,
      "step": 15350
    },
    {
      "epoch": 0.3159851779088097,
      "grad_norm": 0.36536142230033875,
      "learning_rate": 0.000197039265850704,
      "loss": 2.4487,
      "step": 15360
    },
    {
      "epoch": 0.3161908974256774,
      "grad_norm": 0.33942827582359314,
      "learning_rate": 0.00019703389922413246,
      "loss": 2.5074,
      "step": 15370
    },
    {
      "epoch": 0.3163966169425451,
      "grad_norm": 0.3271961808204651,
      "learning_rate": 0.00019702852781140745,
      "loss": 2.4356,
      "step": 15380
    },
    {
      "epoch": 0.3166023364594128,
      "grad_norm": 0.35204267501831055,
      "learning_rate": 0.00019702315161279393,
      "loss": 2.3861,
      "step": 15390
    },
    {
      "epoch": 0.31680805597628053,
      "grad_norm": 0.35196810960769653,
      "learning_rate": 0.00019701777062855702,
      "loss": 2.4667,
      "step": 15400
    },
    {
      "epoch": 0.31701377549314824,
      "grad_norm": 0.3316003382205963,
      "learning_rate": 0.00019701238485896215,
      "loss": 2.5042,
      "step": 15410
    },
    {
      "epoch": 0.31721949501001595,
      "grad_norm": 0.36778712272644043,
      "learning_rate": 0.00019700699430427504,
      "loss": 2.4519,
      "step": 15420
    },
    {
      "epoch": 0.31742521452688366,
      "grad_norm": 0.3478104770183563,
      "learning_rate": 0.0001970015989647615,
      "loss": 2.4905,
      "step": 15430
    },
    {
      "epoch": 0.3176309340437514,
      "grad_norm": 0.33092185854911804,
      "learning_rate": 0.00019699619884068769,
      "loss": 2.4841,
      "step": 15440
    },
    {
      "epoch": 0.31783665356061913,
      "grad_norm": 0.4148654341697693,
      "learning_rate": 0.00019699079393231995,
      "loss": 2.4389,
      "step": 15450
    },
    {
      "epoch": 0.31804237307748684,
      "grad_norm": 0.36892637610435486,
      "learning_rate": 0.00019698538423992487,
      "loss": 2.4345,
      "step": 15460
    },
    {
      "epoch": 0.31824809259435455,
      "grad_norm": 0.4653764069080353,
      "learning_rate": 0.0001969799697637693,
      "loss": 2.4037,
      "step": 15470
    },
    {
      "epoch": 0.31845381211122226,
      "grad_norm": 0.3579219579696655,
      "learning_rate": 0.00019697455050412028,
      "loss": 2.4615,
      "step": 15480
    },
    {
      "epoch": 0.31865953162808996,
      "grad_norm": 0.4207220673561096,
      "learning_rate": 0.00019696912646124517,
      "loss": 2.4568,
      "step": 15490
    },
    {
      "epoch": 0.3188652511449577,
      "grad_norm": 0.3517290949821472,
      "learning_rate": 0.00019696369763541144,
      "loss": 2.4105,
      "step": 15500
    },
    {
      "epoch": 0.3190709706618254,
      "grad_norm": 0.3500545918941498,
      "learning_rate": 0.0001969582640268869,
      "loss": 2.4619,
      "step": 15510
    },
    {
      "epoch": 0.3192766901786931,
      "grad_norm": 0.37190327048301697,
      "learning_rate": 0.00019695282563593957,
      "loss": 2.4999,
      "step": 15520
    },
    {
      "epoch": 0.31948240969556085,
      "grad_norm": 0.34781593084335327,
      "learning_rate": 0.00019694738246283768,
      "loss": 2.4311,
      "step": 15530
    },
    {
      "epoch": 0.31968812921242856,
      "grad_norm": 0.3498952090740204,
      "learning_rate": 0.0001969419345078497,
      "loss": 2.4419,
      "step": 15540
    },
    {
      "epoch": 0.31989384872929627,
      "grad_norm": 0.3566450774669647,
      "learning_rate": 0.00019693648177124439,
      "loss": 2.4633,
      "step": 15550
    },
    {
      "epoch": 0.320099568246164,
      "grad_norm": 0.35908782482147217,
      "learning_rate": 0.00019693102425329064,
      "loss": 2.4298,
      "step": 15560
    },
    {
      "epoch": 0.3203052877630317,
      "grad_norm": 0.36130160093307495,
      "learning_rate": 0.00019692556195425768,
      "loss": 2.4869,
      "step": 15570
    },
    {
      "epoch": 0.3205110072798994,
      "grad_norm": 0.3264145851135254,
      "learning_rate": 0.00019692009487441494,
      "loss": 2.4652,
      "step": 15580
    },
    {
      "epoch": 0.3207167267967671,
      "grad_norm": 0.34961140155792236,
      "learning_rate": 0.00019691462301403204,
      "loss": 2.4032,
      "step": 15590
    },
    {
      "epoch": 0.3209224463136348,
      "grad_norm": 0.3465854227542877,
      "learning_rate": 0.00019690914637337893,
      "loss": 2.4818,
      "step": 15600
    },
    {
      "epoch": 0.3211281658305025,
      "grad_norm": 0.3844793736934662,
      "learning_rate": 0.00019690366495272573,
      "loss": 2.452,
      "step": 15610
    },
    {
      "epoch": 0.3213338853473703,
      "grad_norm": 0.3638714849948883,
      "learning_rate": 0.00019689817875234283,
      "loss": 2.4322,
      "step": 15620
    },
    {
      "epoch": 0.321539604864238,
      "grad_norm": 0.3441510796546936,
      "learning_rate": 0.00019689268777250076,
      "loss": 2.4656,
      "step": 15630
    },
    {
      "epoch": 0.3217453243811057,
      "grad_norm": 0.3367244005203247,
      "learning_rate": 0.00019688719201347042,
      "loss": 2.4792,
      "step": 15640
    },
    {
      "epoch": 0.3219510438979734,
      "grad_norm": 0.357190877199173,
      "learning_rate": 0.00019688169147552285,
      "loss": 2.4446,
      "step": 15650
    },
    {
      "epoch": 0.3221567634148411,
      "grad_norm": 0.3428981304168701,
      "learning_rate": 0.0001968761861589294,
      "loss": 2.4741,
      "step": 15660
    },
    {
      "epoch": 0.3223624829317088,
      "grad_norm": 0.3191334903240204,
      "learning_rate": 0.0001968706760639616,
      "loss": 2.4445,
      "step": 15670
    },
    {
      "epoch": 0.32256820244857654,
      "grad_norm": 0.3559427559375763,
      "learning_rate": 0.0001968651611908912,
      "loss": 2.4882,
      "step": 15680
    },
    {
      "epoch": 0.32277392196544424,
      "grad_norm": 0.36135897040367126,
      "learning_rate": 0.0001968596415399903,
      "loss": 2.4228,
      "step": 15690
    },
    {
      "epoch": 0.32297964148231195,
      "grad_norm": 0.3408377170562744,
      "learning_rate": 0.00019685411711153105,
      "loss": 2.4478,
      "step": 15700
    },
    {
      "epoch": 0.3231853609991797,
      "grad_norm": 0.33714088797569275,
      "learning_rate": 0.00019684858790578605,
      "loss": 2.5009,
      "step": 15710
    },
    {
      "epoch": 0.3233910805160474,
      "grad_norm": 0.3482567071914673,
      "learning_rate": 0.00019684305392302792,
      "loss": 2.4669,
      "step": 15720
    },
    {
      "epoch": 0.32359680003291513,
      "grad_norm": 0.35202404856681824,
      "learning_rate": 0.0001968375151635297,
      "loss": 2.476,
      "step": 15730
    },
    {
      "epoch": 0.32380251954978284,
      "grad_norm": 0.3735470473766327,
      "learning_rate": 0.00019683197162756458,
      "loss": 2.4544,
      "step": 15740
    },
    {
      "epoch": 0.32400823906665055,
      "grad_norm": 0.3587222695350647,
      "learning_rate": 0.00019682642331540594,
      "loss": 2.4012,
      "step": 15750
    },
    {
      "epoch": 0.32421395858351826,
      "grad_norm": 0.33978718519210815,
      "learning_rate": 0.00019682087022732746,
      "loss": 2.4691,
      "step": 15760
    },
    {
      "epoch": 0.32441967810038597,
      "grad_norm": 0.4224723279476166,
      "learning_rate": 0.0001968153123636031,
      "loss": 2.4707,
      "step": 15770
    },
    {
      "epoch": 0.3246253976172537,
      "grad_norm": 0.32157424092292786,
      "learning_rate": 0.00019680974972450694,
      "loss": 2.4222,
      "step": 15780
    },
    {
      "epoch": 0.3248311171341214,
      "grad_norm": 0.3662106394767761,
      "learning_rate": 0.00019680418231031338,
      "loss": 2.4688,
      "step": 15790
    },
    {
      "epoch": 0.32503683665098915,
      "grad_norm": 0.34607040882110596,
      "learning_rate": 0.00019679861012129703,
      "loss": 2.4457,
      "step": 15800
    },
    {
      "epoch": 0.32524255616785686,
      "grad_norm": 0.35018548369407654,
      "learning_rate": 0.00019679303315773276,
      "loss": 2.4348,
      "step": 15810
    },
    {
      "epoch": 0.32544827568472456,
      "grad_norm": 0.3855641782283783,
      "learning_rate": 0.00019678745141989558,
      "loss": 2.4275,
      "step": 15820
    },
    {
      "epoch": 0.3256539952015923,
      "grad_norm": 0.3752077519893646,
      "learning_rate": 0.00019678186490806082,
      "loss": 2.4172,
      "step": 15830
    },
    {
      "epoch": 0.32585971471846,
      "grad_norm": 0.38206180930137634,
      "learning_rate": 0.00019677627362250412,
      "loss": 2.4604,
      "step": 15840
    },
    {
      "epoch": 0.3260654342353277,
      "grad_norm": 0.4187292754650116,
      "learning_rate": 0.0001967706775635012,
      "loss": 2.478,
      "step": 15850
    },
    {
      "epoch": 0.3262711537521954,
      "grad_norm": 0.32384318113327026,
      "learning_rate": 0.00019676507673132808,
      "loss": 2.4346,
      "step": 15860
    },
    {
      "epoch": 0.3264768732690631,
      "grad_norm": 0.3722093105316162,
      "learning_rate": 0.00019675947112626104,
      "loss": 2.4679,
      "step": 15870
    },
    {
      "epoch": 0.32668259278593087,
      "grad_norm": 0.34430834650993347,
      "learning_rate": 0.00019675386074857656,
      "loss": 2.4356,
      "step": 15880
    },
    {
      "epoch": 0.3268883123027986,
      "grad_norm": 0.35217058658599854,
      "learning_rate": 0.00019674824559855138,
      "loss": 2.4379,
      "step": 15890
    },
    {
      "epoch": 0.3270940318196663,
      "grad_norm": 0.38102757930755615,
      "learning_rate": 0.00019674262567646246,
      "loss": 2.3896,
      "step": 15900
    },
    {
      "epoch": 0.327299751336534,
      "grad_norm": 0.36934903264045715,
      "learning_rate": 0.000196737000982587,
      "loss": 2.4918,
      "step": 15910
    },
    {
      "epoch": 0.3275054708534017,
      "grad_norm": 0.4055452346801758,
      "learning_rate": 0.00019673137151720243,
      "loss": 2.4833,
      "step": 15920
    },
    {
      "epoch": 0.3277111903702694,
      "grad_norm": 0.326262503862381,
      "learning_rate": 0.00019672573728058646,
      "loss": 2.4438,
      "step": 15930
    },
    {
      "epoch": 0.3279169098871371,
      "grad_norm": 0.339085191488266,
      "learning_rate": 0.0001967200982730169,
      "loss": 2.4918,
      "step": 15940
    },
    {
      "epoch": 0.32812262940400483,
      "grad_norm": 0.335777223110199,
      "learning_rate": 0.000196714454494772,
      "loss": 2.5,
      "step": 15950
    },
    {
      "epoch": 0.32832834892087254,
      "grad_norm": 0.35484758019447327,
      "learning_rate": 0.00019670880594613007,
      "loss": 2.4748,
      "step": 15960
    },
    {
      "epoch": 0.3285340684377403,
      "grad_norm": 0.3548508286476135,
      "learning_rate": 0.00019670315262736976,
      "loss": 2.4294,
      "step": 15970
    },
    {
      "epoch": 0.328739787954608,
      "grad_norm": 0.351125031709671,
      "learning_rate": 0.00019669749453876987,
      "loss": 2.4233,
      "step": 15980
    },
    {
      "epoch": 0.3289455074714757,
      "grad_norm": 0.3874521255493164,
      "learning_rate": 0.00019669183168060953,
      "loss": 2.4069,
      "step": 15990
    },
    {
      "epoch": 0.3291512269883434,
      "grad_norm": 0.4403078258037567,
      "learning_rate": 0.00019668616405316803,
      "loss": 2.5557,
      "step": 16000
    },
    {
      "epoch": 0.32935694650521113,
      "grad_norm": 0.3713671863079071,
      "learning_rate": 0.00019668049165672496,
      "loss": 2.437,
      "step": 16010
    },
    {
      "epoch": 0.32956266602207884,
      "grad_norm": 0.3380211889743805,
      "learning_rate": 0.00019667481449156003,
      "loss": 2.4612,
      "step": 16020
    },
    {
      "epoch": 0.32976838553894655,
      "grad_norm": 0.36405250430107117,
      "learning_rate": 0.00019666913255795337,
      "loss": 2.4331,
      "step": 16030
    },
    {
      "epoch": 0.32997410505581426,
      "grad_norm": 0.38203397393226624,
      "learning_rate": 0.00019666344585618517,
      "loss": 2.4387,
      "step": 16040
    },
    {
      "epoch": 0.33017982457268197,
      "grad_norm": 0.3932330310344696,
      "learning_rate": 0.00019665775438653594,
      "loss": 2.4052,
      "step": 16050
    },
    {
      "epoch": 0.33038554408954973,
      "grad_norm": 0.3474453389644623,
      "learning_rate": 0.00019665205814928637,
      "loss": 2.4306,
      "step": 16060
    },
    {
      "epoch": 0.33059126360641744,
      "grad_norm": 0.3518466651439667,
      "learning_rate": 0.0001966463571447175,
      "loss": 2.4827,
      "step": 16070
    },
    {
      "epoch": 0.33079698312328515,
      "grad_norm": 0.3655492663383484,
      "learning_rate": 0.0001966406513731105,
      "loss": 2.5152,
      "step": 16080
    },
    {
      "epoch": 0.33100270264015286,
      "grad_norm": 0.392190158367157,
      "learning_rate": 0.00019663494083474678,
      "loss": 2.4823,
      "step": 16090
    },
    {
      "epoch": 0.33120842215702057,
      "grad_norm": 0.32626819610595703,
      "learning_rate": 0.00019662922552990804,
      "loss": 2.4861,
      "step": 16100
    },
    {
      "epoch": 0.3314141416738883,
      "grad_norm": 0.326641321182251,
      "learning_rate": 0.00019662350545887613,
      "loss": 2.4489,
      "step": 16110
    },
    {
      "epoch": 0.331619861190756,
      "grad_norm": 0.35042285919189453,
      "learning_rate": 0.00019661778062193328,
      "loss": 2.4978,
      "step": 16120
    },
    {
      "epoch": 0.3318255807076237,
      "grad_norm": 0.37506428360939026,
      "learning_rate": 0.00019661205101936178,
      "loss": 2.46,
      "step": 16130
    },
    {
      "epoch": 0.3320313002244914,
      "grad_norm": 0.45969969034194946,
      "learning_rate": 0.0001966063166514443,
      "loss": 2.4649,
      "step": 16140
    },
    {
      "epoch": 0.33223701974135916,
      "grad_norm": 0.39714840054512024,
      "learning_rate": 0.00019660057751846367,
      "loss": 2.4559,
      "step": 16150
    },
    {
      "epoch": 0.33244273925822687,
      "grad_norm": 0.36717939376831055,
      "learning_rate": 0.00019659483362070293,
      "loss": 2.4479,
      "step": 16160
    },
    {
      "epoch": 0.3326484587750946,
      "grad_norm": 0.3699367642402649,
      "learning_rate": 0.00019658908495844544,
      "loss": 2.4762,
      "step": 16170
    },
    {
      "epoch": 0.3328541782919623,
      "grad_norm": 0.3644274175167084,
      "learning_rate": 0.00019658333153197474,
      "loss": 2.4539,
      "step": 16180
    },
    {
      "epoch": 0.33305989780883,
      "grad_norm": 0.352400004863739,
      "learning_rate": 0.00019657757334157457,
      "loss": 2.4748,
      "step": 16190
    },
    {
      "epoch": 0.3332656173256977,
      "grad_norm": 0.3994518518447876,
      "learning_rate": 0.00019657181038752907,
      "loss": 2.4809,
      "step": 16200
    },
    {
      "epoch": 0.3334713368425654,
      "grad_norm": 0.37678560614585876,
      "learning_rate": 0.00019656604267012236,
      "loss": 2.435,
      "step": 16210
    },
    {
      "epoch": 0.3336770563594331,
      "grad_norm": 0.34672850370407104,
      "learning_rate": 0.000196560270189639,
      "loss": 2.5114,
      "step": 16220
    },
    {
      "epoch": 0.33388277587630083,
      "grad_norm": 0.334947407245636,
      "learning_rate": 0.0001965544929463637,
      "loss": 2.4565,
      "step": 16230
    },
    {
      "epoch": 0.3340884953931686,
      "grad_norm": 0.37798312306404114,
      "learning_rate": 0.00019654871094058142,
      "loss": 2.4953,
      "step": 16240
    },
    {
      "epoch": 0.3342942149100363,
      "grad_norm": 0.3494730293750763,
      "learning_rate": 0.00019654292417257735,
      "loss": 2.4592,
      "step": 16250
    },
    {
      "epoch": 0.334499934426904,
      "grad_norm": 0.33361634612083435,
      "learning_rate": 0.00019653713264263696,
      "loss": 2.4217,
      "step": 16260
    },
    {
      "epoch": 0.3347056539437717,
      "grad_norm": 0.37212154269218445,
      "learning_rate": 0.00019653133635104587,
      "loss": 2.4777,
      "step": 16270
    },
    {
      "epoch": 0.3349113734606394,
      "grad_norm": 0.3621521592140198,
      "learning_rate": 0.00019652553529808997,
      "loss": 2.4254,
      "step": 16280
    },
    {
      "epoch": 0.33511709297750714,
      "grad_norm": 0.3577134609222412,
      "learning_rate": 0.00019651972948405542,
      "loss": 2.4718,
      "step": 16290
    },
    {
      "epoch": 0.33532281249437484,
      "grad_norm": 0.3361198902130127,
      "learning_rate": 0.0001965139189092286,
      "loss": 2.4591,
      "step": 16300
    },
    {
      "epoch": 0.33552853201124255,
      "grad_norm": 0.35241907835006714,
      "learning_rate": 0.0001965081035738961,
      "loss": 2.4839,
      "step": 16310
    },
    {
      "epoch": 0.33573425152811026,
      "grad_norm": 0.3449227213859558,
      "learning_rate": 0.00019650228347834473,
      "loss": 2.3921,
      "step": 16320
    },
    {
      "epoch": 0.335939971044978,
      "grad_norm": 0.34443625807762146,
      "learning_rate": 0.00019649645862286158,
      "loss": 2.4658,
      "step": 16330
    },
    {
      "epoch": 0.33614569056184573,
      "grad_norm": 0.33583375811576843,
      "learning_rate": 0.000196490629007734,
      "loss": 2.4846,
      "step": 16340
    },
    {
      "epoch": 0.33635141007871344,
      "grad_norm": 0.35660895705223083,
      "learning_rate": 0.0001964847946332495,
      "loss": 2.5217,
      "step": 16350
    },
    {
      "epoch": 0.33655712959558115,
      "grad_norm": 0.321585088968277,
      "learning_rate": 0.00019647895549969584,
      "loss": 2.4072,
      "step": 16360
    },
    {
      "epoch": 0.33676284911244886,
      "grad_norm": 0.35872882604599,
      "learning_rate": 0.0001964731116073611,
      "loss": 2.499,
      "step": 16370
    },
    {
      "epoch": 0.33696856862931657,
      "grad_norm": 0.3526359498500824,
      "learning_rate": 0.00019646726295653345,
      "loss": 2.4536,
      "step": 16380
    },
    {
      "epoch": 0.3371742881461843,
      "grad_norm": 0.3227928876876831,
      "learning_rate": 0.0001964614095475014,
      "loss": 2.4527,
      "step": 16390
    },
    {
      "epoch": 0.337380007663052,
      "grad_norm": 0.3515554368495941,
      "learning_rate": 0.00019645555138055363,
      "loss": 2.4885,
      "step": 16400
    },
    {
      "epoch": 0.3375857271799197,
      "grad_norm": 0.3747572600841522,
      "learning_rate": 0.00019644968845597917,
      "loss": 2.4691,
      "step": 16410
    },
    {
      "epoch": 0.33779144669678746,
      "grad_norm": 0.3663490414619446,
      "learning_rate": 0.00019644382077406718,
      "loss": 2.4601,
      "step": 16420
    },
    {
      "epoch": 0.33799716621365516,
      "grad_norm": 0.3476848900318146,
      "learning_rate": 0.00019643794833510705,
      "loss": 2.4488,
      "step": 16430
    },
    {
      "epoch": 0.3382028857305229,
      "grad_norm": 0.3577786087989807,
      "learning_rate": 0.00019643207113938845,
      "loss": 2.4264,
      "step": 16440
    },
    {
      "epoch": 0.3384086052473906,
      "grad_norm": 0.3313087224960327,
      "learning_rate": 0.00019642618918720128,
      "loss": 2.4347,
      "step": 16450
    },
    {
      "epoch": 0.3386143247642583,
      "grad_norm": 0.33714625239372253,
      "learning_rate": 0.00019642030247883562,
      "loss": 2.5529,
      "step": 16460
    },
    {
      "epoch": 0.338820044281126,
      "grad_norm": 0.36162057518959045,
      "learning_rate": 0.00019641441101458194,
      "loss": 2.5273,
      "step": 16470
    },
    {
      "epoch": 0.3390257637979937,
      "grad_norm": 0.3194255530834198,
      "learning_rate": 0.0001964085147947307,
      "loss": 2.4792,
      "step": 16480
    },
    {
      "epoch": 0.3392314833148614,
      "grad_norm": 0.3479728400707245,
      "learning_rate": 0.00019640261381957281,
      "loss": 2.4028,
      "step": 16490
    },
    {
      "epoch": 0.3394372028317291,
      "grad_norm": 0.3726096749305725,
      "learning_rate": 0.0001963967080893993,
      "loss": 2.4228,
      "step": 16500
    },
    {
      "epoch": 0.3396429223485969,
      "grad_norm": 0.3362901508808136,
      "learning_rate": 0.0001963907976045015,
      "loss": 2.4216,
      "step": 16510
    },
    {
      "epoch": 0.3398486418654646,
      "grad_norm": 0.3902552127838135,
      "learning_rate": 0.00019638488236517091,
      "loss": 2.4136,
      "step": 16520
    },
    {
      "epoch": 0.3400543613823323,
      "grad_norm": 0.36165180802345276,
      "learning_rate": 0.0001963789623716993,
      "loss": 2.4634,
      "step": 16530
    },
    {
      "epoch": 0.3402600808992,
      "grad_norm": 0.36791184544563293,
      "learning_rate": 0.0001963730376243787,
      "loss": 2.4259,
      "step": 16540
    },
    {
      "epoch": 0.3404658004160677,
      "grad_norm": 0.37024378776550293,
      "learning_rate": 0.0001963671081235013,
      "loss": 2.4479,
      "step": 16550
    },
    {
      "epoch": 0.34067151993293543,
      "grad_norm": 0.3511365056037903,
      "learning_rate": 0.0001963611738693596,
      "loss": 2.4621,
      "step": 16560
    },
    {
      "epoch": 0.34087723944980314,
      "grad_norm": 0.37033477425575256,
      "learning_rate": 0.0001963552348622463,
      "loss": 2.4367,
      "step": 16570
    },
    {
      "epoch": 0.34108295896667085,
      "grad_norm": 0.3195410966873169,
      "learning_rate": 0.00019634929110245436,
      "loss": 2.4715,
      "step": 16580
    },
    {
      "epoch": 0.34128867848353855,
      "grad_norm": 0.4181515872478485,
      "learning_rate": 0.0001963433425902769,
      "loss": 2.4991,
      "step": 16590
    },
    {
      "epoch": 0.3414943980004063,
      "grad_norm": 0.37145090103149414,
      "learning_rate": 0.00019633738932600735,
      "loss": 2.4061,
      "step": 16600
    },
    {
      "epoch": 0.341700117517274,
      "grad_norm": 0.36325061321258545,
      "learning_rate": 0.00019633143130993937,
      "loss": 2.5137,
      "step": 16610
    },
    {
      "epoch": 0.34190583703414174,
      "grad_norm": 0.356076180934906,
      "learning_rate": 0.00019632546854236687,
      "loss": 2.518,
      "step": 16620
    },
    {
      "epoch": 0.34211155655100944,
      "grad_norm": 0.3470366597175598,
      "learning_rate": 0.00019631950102358387,
      "loss": 2.3847,
      "step": 16630
    },
    {
      "epoch": 0.34231727606787715,
      "grad_norm": 0.3684405982494354,
      "learning_rate": 0.00019631352875388477,
      "loss": 2.4879,
      "step": 16640
    },
    {
      "epoch": 0.34252299558474486,
      "grad_norm": 0.36360910534858704,
      "learning_rate": 0.00019630755173356414,
      "loss": 2.4128,
      "step": 16650
    },
    {
      "epoch": 0.34272871510161257,
      "grad_norm": 0.35346484184265137,
      "learning_rate": 0.0001963015699629168,
      "loss": 2.3818,
      "step": 16660
    },
    {
      "epoch": 0.3429344346184803,
      "grad_norm": 0.34655600786209106,
      "learning_rate": 0.0001962955834422378,
      "loss": 2.5216,
      "step": 16670
    },
    {
      "epoch": 0.343140154135348,
      "grad_norm": 0.340861439704895,
      "learning_rate": 0.00019628959217182242,
      "loss": 2.4102,
      "step": 16680
    },
    {
      "epoch": 0.34334587365221575,
      "grad_norm": 0.34264642000198364,
      "learning_rate": 0.00019628359615196618,
      "loss": 2.4218,
      "step": 16690
    },
    {
      "epoch": 0.34355159316908346,
      "grad_norm": 0.3690847158432007,
      "learning_rate": 0.0001962775953829648,
      "loss": 2.4747,
      "step": 16700
    },
    {
      "epoch": 0.34375731268595117,
      "grad_norm": 0.3461332619190216,
      "learning_rate": 0.0001962715898651143,
      "loss": 2.4475,
      "step": 16710
    },
    {
      "epoch": 0.3439630322028189,
      "grad_norm": 0.4234471917152405,
      "learning_rate": 0.0001962655795987109,
      "loss": 2.469,
      "step": 16720
    },
    {
      "epoch": 0.3441687517196866,
      "grad_norm": 0.39071664214134216,
      "learning_rate": 0.00019625956458405104,
      "loss": 2.4602,
      "step": 16730
    },
    {
      "epoch": 0.3443744712365543,
      "grad_norm": 0.3622083365917206,
      "learning_rate": 0.00019625354482143142,
      "loss": 2.4484,
      "step": 16740
    },
    {
      "epoch": 0.344580190753422,
      "grad_norm": 0.355909526348114,
      "learning_rate": 0.00019624752031114894,
      "loss": 2.456,
      "step": 16750
    },
    {
      "epoch": 0.3447859102702897,
      "grad_norm": 0.3439774215221405,
      "learning_rate": 0.00019624149105350073,
      "loss": 2.4793,
      "step": 16760
    },
    {
      "epoch": 0.34499162978715747,
      "grad_norm": 0.35941997170448303,
      "learning_rate": 0.00019623545704878427,
      "loss": 2.4814,
      "step": 16770
    },
    {
      "epoch": 0.3451973493040252,
      "grad_norm": 0.3619665205478668,
      "learning_rate": 0.00019622941829729713,
      "loss": 2.446,
      "step": 16780
    },
    {
      "epoch": 0.3454030688208929,
      "grad_norm": 0.3765788674354553,
      "learning_rate": 0.00019622337479933716,
      "loss": 2.4497,
      "step": 16790
    },
    {
      "epoch": 0.3456087883377606,
      "grad_norm": 0.3231230080127716,
      "learning_rate": 0.0001962173265552025,
      "loss": 2.5032,
      "step": 16800
    },
    {
      "epoch": 0.3458145078546283,
      "grad_norm": 0.3314642608165741,
      "learning_rate": 0.0001962112735651914,
      "loss": 2.4464,
      "step": 16810
    },
    {
      "epoch": 0.346020227371496,
      "grad_norm": 0.3924795985221863,
      "learning_rate": 0.00019620521582960248,
      "loss": 2.466,
      "step": 16820
    },
    {
      "epoch": 0.3462259468883637,
      "grad_norm": 0.3920884430408478,
      "learning_rate": 0.0001961991533487345,
      "loss": 2.4805,
      "step": 16830
    },
    {
      "epoch": 0.34643166640523143,
      "grad_norm": 0.3880730867385864,
      "learning_rate": 0.00019619308612288654,
      "loss": 2.4093,
      "step": 16840
    },
    {
      "epoch": 0.34663738592209914,
      "grad_norm": 0.3210188150405884,
      "learning_rate": 0.00019618701415235782,
      "loss": 2.4144,
      "step": 16850
    },
    {
      "epoch": 0.3468431054389669,
      "grad_norm": 0.37267640233039856,
      "learning_rate": 0.00019618093743744783,
      "loss": 2.4839,
      "step": 16860
    },
    {
      "epoch": 0.3470488249558346,
      "grad_norm": 0.3401337265968323,
      "learning_rate": 0.00019617485597845632,
      "loss": 2.3772,
      "step": 16870
    },
    {
      "epoch": 0.3472545444727023,
      "grad_norm": 0.4067986011505127,
      "learning_rate": 0.00019616876977568328,
      "loss": 2.4471,
      "step": 16880
    },
    {
      "epoch": 0.34746026398957003,
      "grad_norm": 0.3667071759700775,
      "learning_rate": 0.00019616267882942884,
      "loss": 2.4516,
      "step": 16890
    },
    {
      "epoch": 0.34766598350643774,
      "grad_norm": 0.3176417648792267,
      "learning_rate": 0.00019615658313999352,
      "loss": 2.5104,
      "step": 16900
    },
    {
      "epoch": 0.34787170302330545,
      "grad_norm": 0.3443341553211212,
      "learning_rate": 0.00019615048270767792,
      "loss": 2.4422,
      "step": 16910
    },
    {
      "epoch": 0.34807742254017315,
      "grad_norm": 0.3447472155094147,
      "learning_rate": 0.00019614437753278301,
      "loss": 2.4683,
      "step": 16920
    },
    {
      "epoch": 0.34828314205704086,
      "grad_norm": 0.36837178468704224,
      "learning_rate": 0.00019613826761560982,
      "loss": 2.4748,
      "step": 16930
    },
    {
      "epoch": 0.34848886157390857,
      "grad_norm": 0.36610108613967896,
      "learning_rate": 0.00019613215295645982,
      "loss": 2.4614,
      "step": 16940
    },
    {
      "epoch": 0.34869458109077633,
      "grad_norm": 0.36510327458381653,
      "learning_rate": 0.00019612603355563456,
      "loss": 2.4873,
      "step": 16950
    },
    {
      "epoch": 0.34890030060764404,
      "grad_norm": 0.36987313628196716,
      "learning_rate": 0.00019611990941343588,
      "loss": 2.4232,
      "step": 16960
    },
    {
      "epoch": 0.34910602012451175,
      "grad_norm": 0.3664250373840332,
      "learning_rate": 0.00019611378053016587,
      "loss": 2.4969,
      "step": 16970
    },
    {
      "epoch": 0.34931173964137946,
      "grad_norm": 0.3517944812774658,
      "learning_rate": 0.0001961076469061268,
      "loss": 2.4862,
      "step": 16980
    },
    {
      "epoch": 0.34951745915824717,
      "grad_norm": 0.3410840928554535,
      "learning_rate": 0.00019610150854162126,
      "loss": 2.4702,
      "step": 16990
    },
    {
      "epoch": 0.3497231786751149,
      "grad_norm": 0.3586931526660919,
      "learning_rate": 0.000196095365436952,
      "loss": 2.4627,
      "step": 17000
    },
    {
      "epoch": 0.3499288981919826,
      "grad_norm": 0.3473363518714905,
      "learning_rate": 0.000196089217592422,
      "loss": 2.4169,
      "step": 17010
    },
    {
      "epoch": 0.3501346177088503,
      "grad_norm": 0.3994719088077545,
      "learning_rate": 0.00019608306500833454,
      "loss": 2.474,
      "step": 17020
    },
    {
      "epoch": 0.350340337225718,
      "grad_norm": 0.3674686551094055,
      "learning_rate": 0.00019607690768499306,
      "loss": 2.4217,
      "step": 17030
    },
    {
      "epoch": 0.35054605674258577,
      "grad_norm": 0.37210655212402344,
      "learning_rate": 0.00019607074562270126,
      "loss": 2.4564,
      "step": 17040
    },
    {
      "epoch": 0.3507517762594535,
      "grad_norm": 0.37009793519973755,
      "learning_rate": 0.00019606457882176311,
      "loss": 2.3885,
      "step": 17050
    },
    {
      "epoch": 0.3509574957763212,
      "grad_norm": 0.3726683259010315,
      "learning_rate": 0.0001960584072824828,
      "loss": 2.5268,
      "step": 17060
    },
    {
      "epoch": 0.3511632152931889,
      "grad_norm": 0.3643658459186554,
      "learning_rate": 0.00019605223100516473,
      "loss": 2.433,
      "step": 17070
    },
    {
      "epoch": 0.3513689348100566,
      "grad_norm": 0.36460208892822266,
      "learning_rate": 0.00019604604999011346,
      "loss": 2.4388,
      "step": 17080
    },
    {
      "epoch": 0.3515746543269243,
      "grad_norm": 0.3389417827129364,
      "learning_rate": 0.000196039864237634,
      "loss": 2.458,
      "step": 17090
    },
    {
      "epoch": 0.351780373843792,
      "grad_norm": 0.37887337803840637,
      "learning_rate": 0.00019603367374803135,
      "loss": 2.4487,
      "step": 17100
    },
    {
      "epoch": 0.3519860933606597,
      "grad_norm": 0.35415881872177124,
      "learning_rate": 0.00019602747852161088,
      "loss": 2.4666,
      "step": 17110
    },
    {
      "epoch": 0.35219181287752743,
      "grad_norm": 0.3589562177658081,
      "learning_rate": 0.0001960212785586782,
      "loss": 2.436,
      "step": 17120
    },
    {
      "epoch": 0.3523975323943952,
      "grad_norm": 0.34496384859085083,
      "learning_rate": 0.00019601507385953913,
      "loss": 2.4305,
      "step": 17130
    },
    {
      "epoch": 0.3526032519112629,
      "grad_norm": 0.3552485704421997,
      "learning_rate": 0.00019600886442449968,
      "loss": 2.4601,
      "step": 17140
    },
    {
      "epoch": 0.3528089714281306,
      "grad_norm": 0.36018073558807373,
      "learning_rate": 0.0001960026502538661,
      "loss": 2.4909,
      "step": 17150
    },
    {
      "epoch": 0.3530146909449983,
      "grad_norm": 0.34298187494277954,
      "learning_rate": 0.00019599643134794498,
      "loss": 2.461,
      "step": 17160
    },
    {
      "epoch": 0.35322041046186603,
      "grad_norm": 0.35662227869033813,
      "learning_rate": 0.00019599020770704297,
      "loss": 2.4099,
      "step": 17170
    },
    {
      "epoch": 0.35342612997873374,
      "grad_norm": 0.3511582911014557,
      "learning_rate": 0.00019598397933146713,
      "loss": 2.5102,
      "step": 17180
    },
    {
      "epoch": 0.35363184949560145,
      "grad_norm": 0.3531174957752228,
      "learning_rate": 0.00019597774622152463,
      "loss": 2.5273,
      "step": 17190
    },
    {
      "epoch": 0.35383756901246916,
      "grad_norm": 0.3598797023296356,
      "learning_rate": 0.0001959715083775229,
      "loss": 2.4955,
      "step": 17200
    },
    {
      "epoch": 0.35404328852933686,
      "grad_norm": 0.40482354164123535,
      "learning_rate": 0.0001959652657997697,
      "loss": 2.4419,
      "step": 17210
    },
    {
      "epoch": 0.3542490080462046,
      "grad_norm": 0.34706249833106995,
      "learning_rate": 0.00019595901848857287,
      "loss": 2.4507,
      "step": 17220
    },
    {
      "epoch": 0.35445472756307234,
      "grad_norm": 0.34281232953071594,
      "learning_rate": 0.0001959527664442405,
      "loss": 2.4916,
      "step": 17230
    },
    {
      "epoch": 0.35466044707994004,
      "grad_norm": 0.3733677566051483,
      "learning_rate": 0.00019594650966708112,
      "loss": 2.501,
      "step": 17240
    },
    {
      "epoch": 0.35486616659680775,
      "grad_norm": 0.350512832403183,
      "learning_rate": 0.00019594024815740326,
      "loss": 2.5125,
      "step": 17250
    },
    {
      "epoch": 0.35507188611367546,
      "grad_norm": 0.3781852424144745,
      "learning_rate": 0.00019593398191551574,
      "loss": 2.4608,
      "step": 17260
    },
    {
      "epoch": 0.35527760563054317,
      "grad_norm": 0.3431013226509094,
      "learning_rate": 0.00019592771094172767,
      "loss": 2.479,
      "step": 17270
    },
    {
      "epoch": 0.3554833251474109,
      "grad_norm": 0.3524942994117737,
      "learning_rate": 0.0001959214352363484,
      "loss": 2.4228,
      "step": 17280
    },
    {
      "epoch": 0.3556890446642786,
      "grad_norm": 0.3582597076892853,
      "learning_rate": 0.0001959151547996874,
      "loss": 2.4814,
      "step": 17290
    },
    {
      "epoch": 0.3558947641811463,
      "grad_norm": 0.35901525616645813,
      "learning_rate": 0.0001959088696320545,
      "loss": 2.4781,
      "step": 17300
    },
    {
      "epoch": 0.35610048369801406,
      "grad_norm": 0.3494977653026581,
      "learning_rate": 0.0001959025797337597,
      "loss": 2.4023,
      "step": 17310
    },
    {
      "epoch": 0.35630620321488177,
      "grad_norm": 0.36979979276657104,
      "learning_rate": 0.00019589628510511325,
      "loss": 2.434,
      "step": 17320
    },
    {
      "epoch": 0.3565119227317495,
      "grad_norm": 0.3477361500263214,
      "learning_rate": 0.00019588998574642563,
      "loss": 2.4228,
      "step": 17330
    },
    {
      "epoch": 0.3567176422486172,
      "grad_norm": 0.3659219443798065,
      "learning_rate": 0.00019588368165800754,
      "loss": 2.4395,
      "step": 17340
    },
    {
      "epoch": 0.3569233617654849,
      "grad_norm": 0.3744538128376007,
      "learning_rate": 0.00019587737284016996,
      "loss": 2.4639,
      "step": 17350
    },
    {
      "epoch": 0.3571290812823526,
      "grad_norm": 0.3832266628742218,
      "learning_rate": 0.00019587105929322403,
      "loss": 2.4739,
      "step": 17360
    },
    {
      "epoch": 0.3573348007992203,
      "grad_norm": 0.38501983880996704,
      "learning_rate": 0.0001958647410174812,
      "loss": 2.4957,
      "step": 17370
    },
    {
      "epoch": 0.357540520316088,
      "grad_norm": 0.3676680028438568,
      "learning_rate": 0.00019585841801325306,
      "loss": 2.4935,
      "step": 17380
    },
    {
      "epoch": 0.3577462398329557,
      "grad_norm": 0.3629882335662842,
      "learning_rate": 0.00019585209028085156,
      "loss": 2.4444,
      "step": 17390
    },
    {
      "epoch": 0.3579519593498235,
      "grad_norm": 0.35868966579437256,
      "learning_rate": 0.00019584575782058874,
      "loss": 2.4567,
      "step": 17400
    },
    {
      "epoch": 0.3581576788666912,
      "grad_norm": 0.362700492143631,
      "learning_rate": 0.00019583942063277702,
      "loss": 2.4696,
      "step": 17410
    },
    {
      "epoch": 0.3583633983835589,
      "grad_norm": 0.34274426102638245,
      "learning_rate": 0.00019583307871772894,
      "loss": 2.44,
      "step": 17420
    },
    {
      "epoch": 0.3585691179004266,
      "grad_norm": 0.3838105797767639,
      "learning_rate": 0.0001958267320757573,
      "loss": 2.4647,
      "step": 17430
    },
    {
      "epoch": 0.3587748374172943,
      "grad_norm": 0.37546372413635254,
      "learning_rate": 0.00019582038070717515,
      "loss": 2.5084,
      "step": 17440
    },
    {
      "epoch": 0.35898055693416203,
      "grad_norm": 0.3749295473098755,
      "learning_rate": 0.00019581402461229578,
      "loss": 2.4688,
      "step": 17450
    },
    {
      "epoch": 0.35918627645102974,
      "grad_norm": 0.33665597438812256,
      "learning_rate": 0.00019580766379143273,
      "loss": 2.3956,
      "step": 17460
    },
    {
      "epoch": 0.35939199596789745,
      "grad_norm": 0.3469108045101166,
      "learning_rate": 0.0001958012982448997,
      "loss": 2.4849,
      "step": 17470
    },
    {
      "epoch": 0.35959771548476516,
      "grad_norm": 0.3613719642162323,
      "learning_rate": 0.00019579492797301067,
      "loss": 2.442,
      "step": 17480
    },
    {
      "epoch": 0.3598034350016329,
      "grad_norm": 0.3670737147331238,
      "learning_rate": 0.00019578855297607985,
      "loss": 2.4214,
      "step": 17490
    },
    {
      "epoch": 0.36000915451850063,
      "grad_norm": 0.37056803703308105,
      "learning_rate": 0.00019578217325442174,
      "loss": 2.4243,
      "step": 17500
    },
    {
      "epoch": 0.36021487403536834,
      "grad_norm": 0.3719739317893982,
      "learning_rate": 0.00019577578880835094,
      "loss": 2.4532,
      "step": 17510
    },
    {
      "epoch": 0.36042059355223605,
      "grad_norm": 0.38489335775375366,
      "learning_rate": 0.00019576939963818243,
      "loss": 2.4805,
      "step": 17520
    },
    {
      "epoch": 0.36062631306910375,
      "grad_norm": 0.35594701766967773,
      "learning_rate": 0.00019576300574423127,
      "loss": 2.432,
      "step": 17530
    },
    {
      "epoch": 0.36083203258597146,
      "grad_norm": 0.3506256341934204,
      "learning_rate": 0.0001957566071268129,
      "loss": 2.4391,
      "step": 17540
    },
    {
      "epoch": 0.36103775210283917,
      "grad_norm": 0.34662991762161255,
      "learning_rate": 0.00019575020378624291,
      "loss": 2.4827,
      "step": 17550
    },
    {
      "epoch": 0.3612434716197069,
      "grad_norm": 0.37234005331993103,
      "learning_rate": 0.00019574379572283714,
      "loss": 2.4485,
      "step": 17560
    },
    {
      "epoch": 0.36144919113657464,
      "grad_norm": 0.3755777180194855,
      "learning_rate": 0.00019573738293691167,
      "loss": 2.4717,
      "step": 17570
    },
    {
      "epoch": 0.36165491065344235,
      "grad_norm": 0.3792058229446411,
      "learning_rate": 0.00019573096542878281,
      "loss": 2.5284,
      "step": 17580
    },
    {
      "epoch": 0.36186063017031006,
      "grad_norm": 0.35278579592704773,
      "learning_rate": 0.0001957245431987671,
      "loss": 2.4798,
      "step": 17590
    },
    {
      "epoch": 0.36206634968717777,
      "grad_norm": 0.35296669602394104,
      "learning_rate": 0.0001957181162471813,
      "loss": 2.4779,
      "step": 17600
    },
    {
      "epoch": 0.3622720692040455,
      "grad_norm": 0.35406121611595154,
      "learning_rate": 0.00019571168457434244,
      "loss": 2.4274,
      "step": 17610
    },
    {
      "epoch": 0.3624777887209132,
      "grad_norm": 0.34852227568626404,
      "learning_rate": 0.00019570524818056774,
      "loss": 2.4786,
      "step": 17620
    },
    {
      "epoch": 0.3626835082377809,
      "grad_norm": 0.35567665100097656,
      "learning_rate": 0.0001956988070661747,
      "loss": 2.4413,
      "step": 17630
    },
    {
      "epoch": 0.3628892277546486,
      "grad_norm": 0.34672391414642334,
      "learning_rate": 0.00019569236123148098,
      "loss": 2.4226,
      "step": 17640
    },
    {
      "epoch": 0.3630949472715163,
      "grad_norm": 0.35419854521751404,
      "learning_rate": 0.00019568591067680452,
      "loss": 2.4597,
      "step": 17650
    },
    {
      "epoch": 0.3633006667883841,
      "grad_norm": 0.37163788080215454,
      "learning_rate": 0.00019567945540246355,
      "loss": 2.4886,
      "step": 17660
    },
    {
      "epoch": 0.3635063863052518,
      "grad_norm": 0.4186282455921173,
      "learning_rate": 0.0001956729954087764,
      "loss": 2.4163,
      "step": 17670
    },
    {
      "epoch": 0.3637121058221195,
      "grad_norm": 0.3735387325286865,
      "learning_rate": 0.00019566653069606177,
      "loss": 2.411,
      "step": 17680
    },
    {
      "epoch": 0.3639178253389872,
      "grad_norm": 0.34080612659454346,
      "learning_rate": 0.00019566006126463852,
      "loss": 2.3768,
      "step": 17690
    },
    {
      "epoch": 0.3641235448558549,
      "grad_norm": 0.35265040397644043,
      "learning_rate": 0.00019565358711482567,
      "loss": 2.4429,
      "step": 17700
    },
    {
      "epoch": 0.3643292643727226,
      "grad_norm": 0.3403213918209076,
      "learning_rate": 0.00019564710824694268,
      "loss": 2.4089,
      "step": 17710
    },
    {
      "epoch": 0.3645349838895903,
      "grad_norm": 0.41498854756355286,
      "learning_rate": 0.00019564062466130902,
      "loss": 2.4349,
      "step": 17720
    },
    {
      "epoch": 0.36474070340645803,
      "grad_norm": 0.33879560232162476,
      "learning_rate": 0.00019563413635824452,
      "loss": 2.4383,
      "step": 17730
    },
    {
      "epoch": 0.36494642292332574,
      "grad_norm": 0.39521604776382446,
      "learning_rate": 0.00019562764333806923,
      "loss": 2.4297,
      "step": 17740
    },
    {
      "epoch": 0.3651521424401935,
      "grad_norm": 0.35286813974380493,
      "learning_rate": 0.0001956211456011034,
      "loss": 2.4518,
      "step": 17750
    },
    {
      "epoch": 0.3653578619570612,
      "grad_norm": 0.4116096496582031,
      "learning_rate": 0.00019561464314766754,
      "loss": 2.444,
      "step": 17760
    },
    {
      "epoch": 0.3655635814739289,
      "grad_norm": 0.3889339566230774,
      "learning_rate": 0.00019560813597808237,
      "loss": 2.4517,
      "step": 17770
    },
    {
      "epoch": 0.36576930099079663,
      "grad_norm": 0.37995392084121704,
      "learning_rate": 0.00019560162409266885,
      "loss": 2.4915,
      "step": 17780
    },
    {
      "epoch": 0.36597502050766434,
      "grad_norm": 0.37326115369796753,
      "learning_rate": 0.00019559510749174818,
      "loss": 2.5424,
      "step": 17790
    },
    {
      "epoch": 0.36618074002453205,
      "grad_norm": 0.38686248660087585,
      "learning_rate": 0.00019558858617564178,
      "loss": 2.3965,
      "step": 17800
    },
    {
      "epoch": 0.36638645954139976,
      "grad_norm": 0.3373009264469147,
      "learning_rate": 0.0001955820601446713,
      "loss": 2.3993,
      "step": 17810
    },
    {
      "epoch": 0.36659217905826746,
      "grad_norm": 0.3764932453632355,
      "learning_rate": 0.0001955755293991587,
      "loss": 2.4967,
      "step": 17820
    },
    {
      "epoch": 0.3667978985751352,
      "grad_norm": 0.3452965021133423,
      "learning_rate": 0.00019556899393942604,
      "loss": 2.4544,
      "step": 17830
    },
    {
      "epoch": 0.36700361809200294,
      "grad_norm": 0.4042501449584961,
      "learning_rate": 0.00019556245376579567,
      "loss": 2.5116,
      "step": 17840
    },
    {
      "epoch": 0.36720933760887065,
      "grad_norm": 0.37897735834121704,
      "learning_rate": 0.00019555590887859026,
      "loss": 2.4579,
      "step": 17850
    },
    {
      "epoch": 0.36741505712573835,
      "grad_norm": 0.35594844818115234,
      "learning_rate": 0.00019554935927813255,
      "loss": 2.4715,
      "step": 17860
    },
    {
      "epoch": 0.36762077664260606,
      "grad_norm": 0.3672553598880768,
      "learning_rate": 0.00019554280496474565,
      "loss": 2.3856,
      "step": 17870
    },
    {
      "epoch": 0.36782649615947377,
      "grad_norm": 0.41554728150367737,
      "learning_rate": 0.00019553624593875277,
      "loss": 2.4696,
      "step": 17880
    },
    {
      "epoch": 0.3680322156763415,
      "grad_norm": 0.3522597849369049,
      "learning_rate": 0.00019552968220047756,
      "loss": 2.4433,
      "step": 17890
    },
    {
      "epoch": 0.3682379351932092,
      "grad_norm": 0.3682694137096405,
      "learning_rate": 0.00019552311375024364,
      "loss": 2.4433,
      "step": 17900
    },
    {
      "epoch": 0.3684436547100769,
      "grad_norm": 0.4091147184371948,
      "learning_rate": 0.0001955165405883751,
      "loss": 2.4824,
      "step": 17910
    },
    {
      "epoch": 0.3686493742269446,
      "grad_norm": 0.38204699754714966,
      "learning_rate": 0.0001955099627151961,
      "loss": 2.4514,
      "step": 17920
    },
    {
      "epoch": 0.36885509374381237,
      "grad_norm": 0.3421104848384857,
      "learning_rate": 0.00019550338013103112,
      "loss": 2.473,
      "step": 17930
    },
    {
      "epoch": 0.3690608132606801,
      "grad_norm": 0.34799924492836,
      "learning_rate": 0.0001954967928362048,
      "loss": 2.4714,
      "step": 17940
    },
    {
      "epoch": 0.3692665327775478,
      "grad_norm": 0.34528279304504395,
      "learning_rate": 0.0001954902008310421,
      "loss": 2.4621,
      "step": 17950
    },
    {
      "epoch": 0.3694722522944155,
      "grad_norm": 0.3472975194454193,
      "learning_rate": 0.00019548360411586818,
      "loss": 2.4427,
      "step": 17960
    },
    {
      "epoch": 0.3696779718112832,
      "grad_norm": 0.3351616859436035,
      "learning_rate": 0.00019547700269100836,
      "loss": 2.3914,
      "step": 17970
    },
    {
      "epoch": 0.3698836913281509,
      "grad_norm": 0.36412888765335083,
      "learning_rate": 0.0001954703965567883,
      "loss": 2.4268,
      "step": 17980
    },
    {
      "epoch": 0.3700894108450186,
      "grad_norm": 0.348041296005249,
      "learning_rate": 0.00019546378571353378,
      "loss": 2.4682,
      "step": 17990
    },
    {
      "epoch": 0.3702951303618863,
      "grad_norm": 0.37488844990730286,
      "learning_rate": 0.00019545717016157096,
      "loss": 2.3989,
      "step": 18000
    },
    {
      "epoch": 0.37050084987875403,
      "grad_norm": 0.36255747079849243,
      "learning_rate": 0.00019545054990122615,
      "loss": 2.4633,
      "step": 18010
    },
    {
      "epoch": 0.3707065693956218,
      "grad_norm": 0.3859076499938965,
      "learning_rate": 0.0001954439249328258,
      "loss": 2.4966,
      "step": 18020
    },
    {
      "epoch": 0.3709122889124895,
      "grad_norm": 0.3519482910633087,
      "learning_rate": 0.0001954372952566968,
      "loss": 2.4843,
      "step": 18030
    },
    {
      "epoch": 0.3711180084293572,
      "grad_norm": 0.3936574459075928,
      "learning_rate": 0.00019543066087316605,
      "loss": 2.3798,
      "step": 18040
    },
    {
      "epoch": 0.3713237279462249,
      "grad_norm": 0.400555819272995,
      "learning_rate": 0.00019542402178256084,
      "loss": 2.4826,
      "step": 18050
    },
    {
      "epoch": 0.37152944746309263,
      "grad_norm": 0.34705615043640137,
      "learning_rate": 0.00019541737798520865,
      "loss": 2.4736,
      "step": 18060
    },
    {
      "epoch": 0.37173516697996034,
      "grad_norm": 0.35671931505203247,
      "learning_rate": 0.00019541072948143714,
      "loss": 2.4807,
      "step": 18070
    },
    {
      "epoch": 0.37194088649682805,
      "grad_norm": 0.3789048194885254,
      "learning_rate": 0.0001954040762715743,
      "loss": 2.4121,
      "step": 18080
    },
    {
      "epoch": 0.37214660601369576,
      "grad_norm": 0.3560580611228943,
      "learning_rate": 0.00019539741835594826,
      "loss": 2.4764,
      "step": 18090
    },
    {
      "epoch": 0.37235232553056347,
      "grad_norm": 0.36566367745399475,
      "learning_rate": 0.00019539075573488744,
      "loss": 2.5024,
      "step": 18100
    },
    {
      "epoch": 0.37255804504743123,
      "grad_norm": 0.35540997982025146,
      "learning_rate": 0.0001953840884087204,
      "loss": 2.4411,
      "step": 18110
    },
    {
      "epoch": 0.37276376456429894,
      "grad_norm": 0.37191271781921387,
      "learning_rate": 0.0001953774163777761,
      "loss": 2.4453,
      "step": 18120
    },
    {
      "epoch": 0.37296948408116665,
      "grad_norm": 0.40614622831344604,
      "learning_rate": 0.0001953707396423836,
      "loss": 2.4591,
      "step": 18130
    },
    {
      "epoch": 0.37317520359803436,
      "grad_norm": 0.36182817816734314,
      "learning_rate": 0.00019536405820287222,
      "loss": 2.4438,
      "step": 18140
    },
    {
      "epoch": 0.37338092311490206,
      "grad_norm": 0.39544129371643066,
      "learning_rate": 0.00019535737205957153,
      "loss": 2.3994,
      "step": 18150
    },
    {
      "epoch": 0.37358664263176977,
      "grad_norm": 0.33449679613113403,
      "learning_rate": 0.0001953506812128113,
      "loss": 2.4538,
      "step": 18160
    },
    {
      "epoch": 0.3737923621486375,
      "grad_norm": 0.49898865818977356,
      "learning_rate": 0.00019534398566292157,
      "loss": 2.4303,
      "step": 18170
    },
    {
      "epoch": 0.3739980816655052,
      "grad_norm": 0.3562653362751007,
      "learning_rate": 0.00019533728541023257,
      "loss": 2.4853,
      "step": 18180
    },
    {
      "epoch": 0.3742038011823729,
      "grad_norm": 0.40411412715911865,
      "learning_rate": 0.0001953305804550748,
      "loss": 2.4116,
      "step": 18190
    },
    {
      "epoch": 0.37440952069924066,
      "grad_norm": 0.3550198972225189,
      "learning_rate": 0.000195323870797779,
      "loss": 2.4483,
      "step": 18200
    },
    {
      "epoch": 0.37461524021610837,
      "grad_norm": 0.35186976194381714,
      "learning_rate": 0.00019531715643867611,
      "loss": 2.4055,
      "step": 18210
    },
    {
      "epoch": 0.3748209597329761,
      "grad_norm": 0.4022753834724426,
      "learning_rate": 0.0001953104373780973,
      "loss": 2.4505,
      "step": 18220
    },
    {
      "epoch": 0.3750266792498438,
      "grad_norm": 0.35391613841056824,
      "learning_rate": 0.000195303713616374,
      "loss": 2.4539,
      "step": 18230
    },
    {
      "epoch": 0.3752323987667115,
      "grad_norm": 0.3576520085334778,
      "learning_rate": 0.00019529698515383787,
      "loss": 2.4016,
      "step": 18240
    },
    {
      "epoch": 0.3754381182835792,
      "grad_norm": 0.3489946126937866,
      "learning_rate": 0.00019529025199082077,
      "loss": 2.4588,
      "step": 18250
    },
    {
      "epoch": 0.3756438378004469,
      "grad_norm": 0.3591071367263794,
      "learning_rate": 0.00019528351412765476,
      "loss": 2.4832,
      "step": 18260
    },
    {
      "epoch": 0.3758495573173146,
      "grad_norm": 0.3537827730178833,
      "learning_rate": 0.00019527677156467227,
      "loss": 2.403,
      "step": 18270
    },
    {
      "epoch": 0.37605527683418233,
      "grad_norm": 0.3671211302280426,
      "learning_rate": 0.00019527002430220583,
      "loss": 2.4342,
      "step": 18280
    },
    {
      "epoch": 0.3762609963510501,
      "grad_norm": 0.3525741398334503,
      "learning_rate": 0.00019526327234058822,
      "loss": 2.4415,
      "step": 18290
    },
    {
      "epoch": 0.3764667158679178,
      "grad_norm": 0.35091954469680786,
      "learning_rate": 0.00019525651568015254,
      "loss": 2.4655,
      "step": 18300
    },
    {
      "epoch": 0.3766724353847855,
      "grad_norm": 0.35855889320373535,
      "learning_rate": 0.00019524975432123204,
      "loss": 2.4726,
      "step": 18310
    },
    {
      "epoch": 0.3768781549016532,
      "grad_norm": 0.4628433287143707,
      "learning_rate": 0.0001952429882641602,
      "loss": 2.4459,
      "step": 18320
    },
    {
      "epoch": 0.3770838744185209,
      "grad_norm": 0.35938695073127747,
      "learning_rate": 0.00019523621750927074,
      "loss": 2.4342,
      "step": 18330
    },
    {
      "epoch": 0.37728959393538863,
      "grad_norm": 0.3553842604160309,
      "learning_rate": 0.00019522944205689765,
      "loss": 2.4437,
      "step": 18340
    },
    {
      "epoch": 0.37749531345225634,
      "grad_norm": 0.3793298006057739,
      "learning_rate": 0.00019522266190737515,
      "loss": 2.4504,
      "step": 18350
    },
    {
      "epoch": 0.37770103296912405,
      "grad_norm": 0.3579571843147278,
      "learning_rate": 0.0001952158770610376,
      "loss": 2.4074,
      "step": 18360
    },
    {
      "epoch": 0.37790675248599176,
      "grad_norm": 0.36786478757858276,
      "learning_rate": 0.0001952090875182197,
      "loss": 2.447,
      "step": 18370
    },
    {
      "epoch": 0.3781124720028595,
      "grad_norm": 0.34768879413604736,
      "learning_rate": 0.00019520229327925635,
      "loss": 2.4586,
      "step": 18380
    },
    {
      "epoch": 0.37831819151972723,
      "grad_norm": 0.38216063380241394,
      "learning_rate": 0.0001951954943444827,
      "loss": 2.4727,
      "step": 18390
    },
    {
      "epoch": 0.37852391103659494,
      "grad_norm": 0.3602868914604187,
      "learning_rate": 0.00019518869071423405,
      "loss": 2.4687,
      "step": 18400
    },
    {
      "epoch": 0.37872963055346265,
      "grad_norm": 0.3846316337585449,
      "learning_rate": 0.00019518188238884596,
      "loss": 2.4493,
      "step": 18410
    },
    {
      "epoch": 0.37893535007033036,
      "grad_norm": 0.35522377490997314,
      "learning_rate": 0.00019517506936865434,
      "loss": 2.4131,
      "step": 18420
    },
    {
      "epoch": 0.37914106958719807,
      "grad_norm": 0.3864860534667969,
      "learning_rate": 0.00019516825165399518,
      "loss": 2.4781,
      "step": 18430
    },
    {
      "epoch": 0.3793467891040658,
      "grad_norm": 0.3873072564601898,
      "learning_rate": 0.00019516142924520474,
      "loss": 2.4413,
      "step": 18440
    },
    {
      "epoch": 0.3795525086209335,
      "grad_norm": 0.36701202392578125,
      "learning_rate": 0.0001951546021426196,
      "loss": 2.4096,
      "step": 18450
    },
    {
      "epoch": 0.37975822813780125,
      "grad_norm": 0.40587013959884644,
      "learning_rate": 0.00019514777034657647,
      "loss": 2.5287,
      "step": 18460
    },
    {
      "epoch": 0.37996394765466895,
      "grad_norm": 0.392682820558548,
      "learning_rate": 0.00019514093385741225,
      "loss": 2.4172,
      "step": 18470
    },
    {
      "epoch": 0.38016966717153666,
      "grad_norm": 0.5134025812149048,
      "learning_rate": 0.0001951340926754643,
      "loss": 2.4145,
      "step": 18480
    },
    {
      "epoch": 0.38037538668840437,
      "grad_norm": 0.3721019923686981,
      "learning_rate": 0.00019512724680106994,
      "loss": 2.4575,
      "step": 18490
    },
    {
      "epoch": 0.3805811062052721,
      "grad_norm": 0.3569692373275757,
      "learning_rate": 0.00019512039623456687,
      "loss": 2.4321,
      "step": 18500
    },
    {
      "epoch": 0.3807868257221398,
      "grad_norm": 0.35237008333206177,
      "learning_rate": 0.00019511354097629302,
      "loss": 2.4455,
      "step": 18510
    },
    {
      "epoch": 0.3809925452390075,
      "grad_norm": 0.3580493628978729,
      "learning_rate": 0.0001951066810265865,
      "loss": 2.4654,
      "step": 18520
    },
    {
      "epoch": 0.3811982647558752,
      "grad_norm": 0.417975515127182,
      "learning_rate": 0.00019509981638578567,
      "loss": 2.4436,
      "step": 18530
    },
    {
      "epoch": 0.3814039842727429,
      "grad_norm": 0.3597569167613983,
      "learning_rate": 0.00019509294705422913,
      "loss": 2.3956,
      "step": 18540
    },
    {
      "epoch": 0.3816097037896107,
      "grad_norm": 0.4670741856098175,
      "learning_rate": 0.0001950860730322557,
      "loss": 2.4304,
      "step": 18550
    },
    {
      "epoch": 0.3818154233064784,
      "grad_norm": 0.36450377106666565,
      "learning_rate": 0.00019507919432020445,
      "loss": 2.4161,
      "step": 18560
    },
    {
      "epoch": 0.3820211428233461,
      "grad_norm": 0.36217692494392395,
      "learning_rate": 0.00019507231091841468,
      "loss": 2.4837,
      "step": 18570
    },
    {
      "epoch": 0.3822268623402138,
      "grad_norm": 0.3539550006389618,
      "learning_rate": 0.00019506542282722586,
      "loss": 2.4091,
      "step": 18580
    },
    {
      "epoch": 0.3824325818570815,
      "grad_norm": 0.35454505681991577,
      "learning_rate": 0.0001950585300469778,
      "loss": 2.4838,
      "step": 18590
    },
    {
      "epoch": 0.3826383013739492,
      "grad_norm": 0.36254987120628357,
      "learning_rate": 0.0001950516325780104,
      "loss": 2.4443,
      "step": 18600
    },
    {
      "epoch": 0.3828440208908169,
      "grad_norm": 0.34262973070144653,
      "learning_rate": 0.00019504473042066401,
      "loss": 2.406,
      "step": 18610
    },
    {
      "epoch": 0.38304974040768464,
      "grad_norm": 0.39902204275131226,
      "learning_rate": 0.000195037823575279,
      "loss": 2.4535,
      "step": 18620
    },
    {
      "epoch": 0.38325545992455234,
      "grad_norm": 0.3371909558773041,
      "learning_rate": 0.000195030912042196,
      "loss": 2.3762,
      "step": 18630
    },
    {
      "epoch": 0.3834611794414201,
      "grad_norm": 0.3708229959011078,
      "learning_rate": 0.00019502399582175598,
      "loss": 2.496,
      "step": 18640
    },
    {
      "epoch": 0.3836668989582878,
      "grad_norm": 0.37230902910232544,
      "learning_rate": 0.0001950170749143001,
      "loss": 2.4799,
      "step": 18650
    },
    {
      "epoch": 0.3838726184751555,
      "grad_norm": 0.37906911969184875,
      "learning_rate": 0.00019501014932016966,
      "loss": 2.4894,
      "step": 18660
    },
    {
      "epoch": 0.38407833799202323,
      "grad_norm": 0.3611088991165161,
      "learning_rate": 0.00019500321903970628,
      "loss": 2.4181,
      "step": 18670
    },
    {
      "epoch": 0.38428405750889094,
      "grad_norm": 0.3538464307785034,
      "learning_rate": 0.00019499628407325186,
      "loss": 2.4765,
      "step": 18680
    },
    {
      "epoch": 0.38448977702575865,
      "grad_norm": 0.375939279794693,
      "learning_rate": 0.0001949893444211484,
      "loss": 2.4655,
      "step": 18690
    },
    {
      "epoch": 0.38469549654262636,
      "grad_norm": 0.3443872630596161,
      "learning_rate": 0.00019498240008373818,
      "loss": 2.4687,
      "step": 18700
    },
    {
      "epoch": 0.38490121605949407,
      "grad_norm": 0.3627678453922272,
      "learning_rate": 0.00019497545106136378,
      "loss": 2.4147,
      "step": 18710
    },
    {
      "epoch": 0.3851069355763618,
      "grad_norm": 0.35914483666419983,
      "learning_rate": 0.00019496849735436793,
      "loss": 2.4605,
      "step": 18720
    },
    {
      "epoch": 0.38531265509322954,
      "grad_norm": 0.3370596170425415,
      "learning_rate": 0.00019496153896309362,
      "loss": 2.471,
      "step": 18730
    },
    {
      "epoch": 0.38551837461009725,
      "grad_norm": 0.40325143933296204,
      "learning_rate": 0.0001949545758878841,
      "loss": 2.4417,
      "step": 18740
    },
    {
      "epoch": 0.38572409412696496,
      "grad_norm": 0.4005095660686493,
      "learning_rate": 0.00019494760812908274,
      "loss": 2.4897,
      "step": 18750
    },
    {
      "epoch": 0.38592981364383266,
      "grad_norm": 0.4404039978981018,
      "learning_rate": 0.00019494063568703329,
      "loss": 2.4468,
      "step": 18760
    },
    {
      "epoch": 0.3861355331607004,
      "grad_norm": 0.3686574101448059,
      "learning_rate": 0.00019493365856207965,
      "loss": 2.4304,
      "step": 18770
    },
    {
      "epoch": 0.3863412526775681,
      "grad_norm": 0.3540560007095337,
      "learning_rate": 0.00019492667675456599,
      "loss": 2.4726,
      "step": 18780
    },
    {
      "epoch": 0.3865469721944358,
      "grad_norm": 0.36435145139694214,
      "learning_rate": 0.0001949196902648366,
      "loss": 2.4281,
      "step": 18790
    },
    {
      "epoch": 0.3867526917113035,
      "grad_norm": 0.42655283212661743,
      "learning_rate": 0.00019491269909323617,
      "loss": 2.4589,
      "step": 18800
    },
    {
      "epoch": 0.3869584112281712,
      "grad_norm": 0.365951269865036,
      "learning_rate": 0.0001949057032401095,
      "loss": 2.487,
      "step": 18810
    },
    {
      "epoch": 0.38716413074503897,
      "grad_norm": 0.3981263041496277,
      "learning_rate": 0.00019489870270580166,
      "loss": 2.3943,
      "step": 18820
    },
    {
      "epoch": 0.3873698502619067,
      "grad_norm": 0.35577261447906494,
      "learning_rate": 0.00019489169749065796,
      "loss": 2.4606,
      "step": 18830
    },
    {
      "epoch": 0.3875755697787744,
      "grad_norm": 0.35103839635849,
      "learning_rate": 0.00019488468759502393,
      "loss": 2.437,
      "step": 18840
    },
    {
      "epoch": 0.3877812892956421,
      "grad_norm": 0.3557088375091553,
      "learning_rate": 0.0001948776730192453,
      "loss": 2.3901,
      "step": 18850
    },
    {
      "epoch": 0.3879870088125098,
      "grad_norm": 0.36715710163116455,
      "learning_rate": 0.00019487065376366808,
      "loss": 2.4947,
      "step": 18860
    },
    {
      "epoch": 0.3881927283293775,
      "grad_norm": 0.3929601311683655,
      "learning_rate": 0.0001948636298286385,
      "loss": 2.4615,
      "step": 18870
    },
    {
      "epoch": 0.3883984478462452,
      "grad_norm": 0.35871678590774536,
      "learning_rate": 0.00019485660121450302,
      "loss": 2.4625,
      "step": 18880
    },
    {
      "epoch": 0.38860416736311293,
      "grad_norm": 0.3719671964645386,
      "learning_rate": 0.0001948495679216083,
      "loss": 2.4656,
      "step": 18890
    },
    {
      "epoch": 0.38880988687998064,
      "grad_norm": 0.38954421877861023,
      "learning_rate": 0.00019484252995030126,
      "loss": 2.4329,
      "step": 18900
    },
    {
      "epoch": 0.3890156063968484,
      "grad_norm": 0.36311742663383484,
      "learning_rate": 0.00019483548730092905,
      "loss": 2.5242,
      "step": 18910
    },
    {
      "epoch": 0.3892213259137161,
      "grad_norm": 0.3693294823169708,
      "learning_rate": 0.00019482843997383902,
      "loss": 2.4371,
      "step": 18920
    },
    {
      "epoch": 0.3894270454305838,
      "grad_norm": 0.3599436581134796,
      "learning_rate": 0.00019482138796937882,
      "loss": 2.457,
      "step": 18930
    },
    {
      "epoch": 0.3896327649474515,
      "grad_norm": 0.3932023048400879,
      "learning_rate": 0.00019481433128789628,
      "loss": 2.4585,
      "step": 18940
    },
    {
      "epoch": 0.38983848446431923,
      "grad_norm": 0.46726179122924805,
      "learning_rate": 0.00019480726992973942,
      "loss": 2.4515,
      "step": 18950
    },
    {
      "epoch": 0.39004420398118694,
      "grad_norm": 0.367302805185318,
      "learning_rate": 0.0001948002038952566,
      "loss": 2.4599,
      "step": 18960
    },
    {
      "epoch": 0.39024992349805465,
      "grad_norm": 0.34846651554107666,
      "learning_rate": 0.0001947931331847963,
      "loss": 2.451,
      "step": 18970
    },
    {
      "epoch": 0.39045564301492236,
      "grad_norm": 0.33315616846084595,
      "learning_rate": 0.00019478605779870732,
      "loss": 2.3977,
      "step": 18980
    },
    {
      "epoch": 0.39066136253179007,
      "grad_norm": 0.41012462973594666,
      "learning_rate": 0.00019477897773733863,
      "loss": 2.4651,
      "step": 18990
    },
    {
      "epoch": 0.39086708204865783,
      "grad_norm": 0.36166951060295105,
      "learning_rate": 0.00019477189300103943,
      "loss": 2.4734,
      "step": 19000
    },
    {
      "epoch": 0.39107280156552554,
      "grad_norm": 0.37772318720817566,
      "learning_rate": 0.0001947648035901592,
      "loss": 2.5169,
      "step": 19010
    },
    {
      "epoch": 0.39127852108239325,
      "grad_norm": 0.3937853276729584,
      "learning_rate": 0.00019475770950504764,
      "loss": 2.4324,
      "step": 19020
    },
    {
      "epoch": 0.39148424059926096,
      "grad_norm": 0.3427097201347351,
      "learning_rate": 0.0001947506107460546,
      "loss": 2.4098,
      "step": 19030
    },
    {
      "epoch": 0.39168996011612867,
      "grad_norm": 0.43921270966529846,
      "learning_rate": 0.00019474350731353028,
      "loss": 2.4847,
      "step": 19040
    },
    {
      "epoch": 0.3918956796329964,
      "grad_norm": 0.40278759598731995,
      "learning_rate": 0.00019473639920782504,
      "loss": 2.4769,
      "step": 19050
    },
    {
      "epoch": 0.3921013991498641,
      "grad_norm": 0.3698771893978119,
      "learning_rate": 0.00019472928642928948,
      "loss": 2.4294,
      "step": 19060
    },
    {
      "epoch": 0.3923071186667318,
      "grad_norm": 0.38227760791778564,
      "learning_rate": 0.00019472216897827442,
      "loss": 2.4267,
      "step": 19070
    },
    {
      "epoch": 0.3925128381835995,
      "grad_norm": 0.37158119678497314,
      "learning_rate": 0.00019471504685513095,
      "loss": 2.3675,
      "step": 19080
    },
    {
      "epoch": 0.39271855770046726,
      "grad_norm": 0.34601879119873047,
      "learning_rate": 0.00019470792006021036,
      "loss": 2.4633,
      "step": 19090
    },
    {
      "epoch": 0.39292427721733497,
      "grad_norm": 0.3764301538467407,
      "learning_rate": 0.00019470078859386414,
      "loss": 2.4477,
      "step": 19100
    },
    {
      "epoch": 0.3931299967342027,
      "grad_norm": 0.4228726327419281,
      "learning_rate": 0.00019469365245644408,
      "loss": 2.4408,
      "step": 19110
    },
    {
      "epoch": 0.3933357162510704,
      "grad_norm": 0.3710949122905731,
      "learning_rate": 0.0001946865116483022,
      "loss": 2.41,
      "step": 19120
    },
    {
      "epoch": 0.3935414357679381,
      "grad_norm": 0.372601181268692,
      "learning_rate": 0.00019467936616979063,
      "loss": 2.4323,
      "step": 19130
    },
    {
      "epoch": 0.3937471552848058,
      "grad_norm": 0.35541027784347534,
      "learning_rate": 0.00019467221602126192,
      "loss": 2.4047,
      "step": 19140
    },
    {
      "epoch": 0.3939528748016735,
      "grad_norm": 0.35801416635513306,
      "learning_rate": 0.00019466506120306866,
      "loss": 2.4467,
      "step": 19150
    },
    {
      "epoch": 0.3941585943185412,
      "grad_norm": 0.3654569387435913,
      "learning_rate": 0.0001946579017155638,
      "loss": 2.5117,
      "step": 19160
    },
    {
      "epoch": 0.39436431383540893,
      "grad_norm": 0.388908326625824,
      "learning_rate": 0.00019465073755910048,
      "loss": 2.4559,
      "step": 19170
    },
    {
      "epoch": 0.3945700333522767,
      "grad_norm": 0.40161851048469543,
      "learning_rate": 0.00019464356873403208,
      "loss": 2.4211,
      "step": 19180
    },
    {
      "epoch": 0.3947757528691444,
      "grad_norm": 0.373899906873703,
      "learning_rate": 0.00019463639524071213,
      "loss": 2.5144,
      "step": 19190
    },
    {
      "epoch": 0.3949814723860121,
      "grad_norm": 0.38340064883232117,
      "learning_rate": 0.00019462921707949455,
      "loss": 2.4927,
      "step": 19200
    },
    {
      "epoch": 0.3951871919028798,
      "grad_norm": 0.3545483350753784,
      "learning_rate": 0.00019462203425073333,
      "loss": 2.4205,
      "step": 19210
    },
    {
      "epoch": 0.39539291141974753,
      "grad_norm": 0.36018234491348267,
      "learning_rate": 0.00019461484675478282,
      "loss": 2.4644,
      "step": 19220
    },
    {
      "epoch": 0.39559863093661524,
      "grad_norm": 0.3879425525665283,
      "learning_rate": 0.00019460765459199748,
      "loss": 2.4795,
      "step": 19230
    },
    {
      "epoch": 0.39580435045348294,
      "grad_norm": 0.37010595202445984,
      "learning_rate": 0.0001946004577627321,
      "loss": 2.447,
      "step": 19240
    },
    {
      "epoch": 0.39601006997035065,
      "grad_norm": 0.3901787996292114,
      "learning_rate": 0.00019459325626734164,
      "loss": 2.4642,
      "step": 19250
    },
    {
      "epoch": 0.39621578948721836,
      "grad_norm": 0.4000190198421478,
      "learning_rate": 0.00019458605010618134,
      "loss": 2.436,
      "step": 19260
    },
    {
      "epoch": 0.3964215090040861,
      "grad_norm": 0.36284130811691284,
      "learning_rate": 0.00019457883927960663,
      "loss": 2.4582,
      "step": 19270
    },
    {
      "epoch": 0.39662722852095383,
      "grad_norm": 0.37019866704940796,
      "learning_rate": 0.00019457162378797314,
      "loss": 2.4308,
      "step": 19280
    },
    {
      "epoch": 0.39683294803782154,
      "grad_norm": 0.3629526197910309,
      "learning_rate": 0.00019456440363163678,
      "loss": 2.4913,
      "step": 19290
    },
    {
      "epoch": 0.39703866755468925,
      "grad_norm": 0.377131849527359,
      "learning_rate": 0.00019455717881095377,
      "loss": 2.5252,
      "step": 19300
    },
    {
      "epoch": 0.39724438707155696,
      "grad_norm": 0.3841972351074219,
      "learning_rate": 0.00019454994932628037,
      "loss": 2.4343,
      "step": 19310
    },
    {
      "epoch": 0.39745010658842467,
      "grad_norm": 0.3522060811519623,
      "learning_rate": 0.0001945427151779732,
      "loss": 2.465,
      "step": 19320
    },
    {
      "epoch": 0.3976558261052924,
      "grad_norm": 0.6335909962654114,
      "learning_rate": 0.0001945354763663891,
      "loss": 2.5078,
      "step": 19330
    },
    {
      "epoch": 0.3978615456221601,
      "grad_norm": 0.37387746572494507,
      "learning_rate": 0.00019452823289188508,
      "loss": 2.5082,
      "step": 19340
    },
    {
      "epoch": 0.39806726513902785,
      "grad_norm": 0.39080217480659485,
      "learning_rate": 0.0001945209847548185,
      "loss": 2.4618,
      "step": 19350
    },
    {
      "epoch": 0.39827298465589556,
      "grad_norm": 0.34073400497436523,
      "learning_rate": 0.00019451373195554676,
      "loss": 2.4606,
      "step": 19360
    },
    {
      "epoch": 0.39847870417276327,
      "grad_norm": 0.3626621961593628,
      "learning_rate": 0.0001945064744944277,
      "loss": 2.4244,
      "step": 19370
    },
    {
      "epoch": 0.398684423689631,
      "grad_norm": 0.34329336881637573,
      "learning_rate": 0.00019449921237181924,
      "loss": 2.421,
      "step": 19380
    },
    {
      "epoch": 0.3988901432064987,
      "grad_norm": 0.387297123670578,
      "learning_rate": 0.0001944919455880796,
      "loss": 2.4297,
      "step": 19390
    },
    {
      "epoch": 0.3990958627233664,
      "grad_norm": 0.394162118434906,
      "learning_rate": 0.00019448467414356717,
      "loss": 2.42,
      "step": 19400
    },
    {
      "epoch": 0.3993015822402341,
      "grad_norm": 0.36454880237579346,
      "learning_rate": 0.00019447739803864068,
      "loss": 2.4091,
      "step": 19410
    },
    {
      "epoch": 0.3995073017571018,
      "grad_norm": 0.3800557851791382,
      "learning_rate": 0.000194470117273659,
      "loss": 2.4607,
      "step": 19420
    },
    {
      "epoch": 0.3997130212739695,
      "grad_norm": 0.36387279629707336,
      "learning_rate": 0.00019446283184898117,
      "loss": 2.4883,
      "step": 19430
    },
    {
      "epoch": 0.3999187407908373,
      "grad_norm": 0.3678983449935913,
      "learning_rate": 0.00019445554176496666,
      "loss": 2.4254,
      "step": 19440
    },
    {
      "epoch": 0.400124460307705,
      "grad_norm": 0.34935662150382996,
      "learning_rate": 0.00019444824702197498,
      "loss": 2.4345,
      "step": 19450
    },
    {
      "epoch": 0.4003301798245727,
      "grad_norm": 0.4083402156829834,
      "learning_rate": 0.00019444094762036595,
      "loss": 2.4545,
      "step": 19460
    },
    {
      "epoch": 0.4005358993414404,
      "grad_norm": 0.3552243113517761,
      "learning_rate": 0.00019443364356049964,
      "loss": 2.4841,
      "step": 19470
    },
    {
      "epoch": 0.4007416188583081,
      "grad_norm": 0.37678155303001404,
      "learning_rate": 0.0001944263348427363,
      "loss": 2.456,
      "step": 19480
    },
    {
      "epoch": 0.4009473383751758,
      "grad_norm": 0.4447004795074463,
      "learning_rate": 0.00019441902146743637,
      "loss": 2.4581,
      "step": 19490
    },
    {
      "epoch": 0.40115305789204353,
      "grad_norm": 0.35741767287254333,
      "learning_rate": 0.00019441170343496066,
      "loss": 2.4423,
      "step": 19500
    },
    {
      "epoch": 0.40135877740891124,
      "grad_norm": 0.38040149211883545,
      "learning_rate": 0.00019440438074567014,
      "loss": 2.488,
      "step": 19510
    },
    {
      "epoch": 0.40156449692577895,
      "grad_norm": 0.3884707987308502,
      "learning_rate": 0.00019439705339992588,
      "loss": 2.5145,
      "step": 19520
    },
    {
      "epoch": 0.4017702164426467,
      "grad_norm": 0.37862929701805115,
      "learning_rate": 0.00019438972139808946,
      "loss": 2.4644,
      "step": 19530
    },
    {
      "epoch": 0.4019759359595144,
      "grad_norm": 0.3626866638660431,
      "learning_rate": 0.0001943823847405224,
      "loss": 2.4221,
      "step": 19540
    },
    {
      "epoch": 0.4021816554763821,
      "grad_norm": 0.33704259991645813,
      "learning_rate": 0.00019437504342758664,
      "loss": 2.462,
      "step": 19550
    },
    {
      "epoch": 0.40238737499324984,
      "grad_norm": 0.3689080774784088,
      "learning_rate": 0.00019436769745964424,
      "loss": 2.4164,
      "step": 19560
    },
    {
      "epoch": 0.40259309451011754,
      "grad_norm": 0.4028424620628357,
      "learning_rate": 0.0001943603468370576,
      "loss": 2.4484,
      "step": 19570
    },
    {
      "epoch": 0.40279881402698525,
      "grad_norm": 0.39449766278266907,
      "learning_rate": 0.00019435299156018925,
      "loss": 2.4136,
      "step": 19580
    },
    {
      "epoch": 0.40300453354385296,
      "grad_norm": 0.3399105668067932,
      "learning_rate": 0.00019434563162940198,
      "loss": 2.4663,
      "step": 19590
    },
    {
      "epoch": 0.40321025306072067,
      "grad_norm": 0.3456963002681732,
      "learning_rate": 0.00019433826704505882,
      "loss": 2.4367,
      "step": 19600
    },
    {
      "epoch": 0.4034159725775884,
      "grad_norm": 0.34498271346092224,
      "learning_rate": 0.00019433089780752303,
      "loss": 2.4569,
      "step": 19610
    },
    {
      "epoch": 0.40362169209445614,
      "grad_norm": 0.36295121908187866,
      "learning_rate": 0.0001943235239171581,
      "loss": 2.4259,
      "step": 19620
    },
    {
      "epoch": 0.40382741161132385,
      "grad_norm": 0.38666829466819763,
      "learning_rate": 0.00019431614537432772,
      "loss": 2.4456,
      "step": 19630
    },
    {
      "epoch": 0.40403313112819156,
      "grad_norm": 0.36559733748435974,
      "learning_rate": 0.00019430876217939587,
      "loss": 2.4594,
      "step": 19640
    },
    {
      "epoch": 0.40423885064505927,
      "grad_norm": 0.364868700504303,
      "learning_rate": 0.0001943013743327267,
      "loss": 2.4726,
      "step": 19650
    },
    {
      "epoch": 0.404444570161927,
      "grad_norm": 0.3668701946735382,
      "learning_rate": 0.0001942939818346846,
      "loss": 2.4548,
      "step": 19660
    },
    {
      "epoch": 0.4046502896787947,
      "grad_norm": 0.37344682216644287,
      "learning_rate": 0.00019428658468563422,
      "loss": 2.4305,
      "step": 19670
    },
    {
      "epoch": 0.4048560091956624,
      "grad_norm": 0.3526633381843567,
      "learning_rate": 0.00019427918288594044,
      "loss": 2.4557,
      "step": 19680
    },
    {
      "epoch": 0.4050617287125301,
      "grad_norm": 0.34954342246055603,
      "learning_rate": 0.00019427177643596832,
      "loss": 2.4785,
      "step": 19690
    },
    {
      "epoch": 0.4052674482293978,
      "grad_norm": 0.34735554456710815,
      "learning_rate": 0.0001942643653360832,
      "loss": 2.4229,
      "step": 19700
    },
    {
      "epoch": 0.4054731677462656,
      "grad_norm": 0.36344051361083984,
      "learning_rate": 0.0001942569495866506,
      "loss": 2.4698,
      "step": 19710
    },
    {
      "epoch": 0.4056788872631333,
      "grad_norm": 0.3611033856868744,
      "learning_rate": 0.0001942495291880363,
      "loss": 2.4699,
      "step": 19720
    },
    {
      "epoch": 0.405884606780001,
      "grad_norm": 0.3665675222873688,
      "learning_rate": 0.00019424210414060634,
      "loss": 2.441,
      "step": 19730
    },
    {
      "epoch": 0.4060903262968687,
      "grad_norm": 0.37138625979423523,
      "learning_rate": 0.00019423467444472693,
      "loss": 2.4361,
      "step": 19740
    },
    {
      "epoch": 0.4062960458137364,
      "grad_norm": 0.3997369706630707,
      "learning_rate": 0.00019422724010076454,
      "loss": 2.4798,
      "step": 19750
    },
    {
      "epoch": 0.4065017653306041,
      "grad_norm": 0.36361247301101685,
      "learning_rate": 0.00019421980110908592,
      "loss": 2.3886,
      "step": 19760
    },
    {
      "epoch": 0.4067074848474718,
      "grad_norm": 0.31579872965812683,
      "learning_rate": 0.00019421235747005787,
      "loss": 2.4122,
      "step": 19770
    },
    {
      "epoch": 0.40691320436433953,
      "grad_norm": 0.3664533197879791,
      "learning_rate": 0.0001942049091840477,
      "loss": 2.4715,
      "step": 19780
    },
    {
      "epoch": 0.40711892388120724,
      "grad_norm": 0.3826402425765991,
      "learning_rate": 0.0001941974562514227,
      "loss": 2.3751,
      "step": 19790
    },
    {
      "epoch": 0.407324643398075,
      "grad_norm": 0.4084574282169342,
      "learning_rate": 0.00019418999867255047,
      "loss": 2.4419,
      "step": 19800
    },
    {
      "epoch": 0.4075303629149427,
      "grad_norm": 0.3969990313053131,
      "learning_rate": 0.00019418253644779888,
      "loss": 2.4417,
      "step": 19810
    },
    {
      "epoch": 0.4077360824318104,
      "grad_norm": 0.3675908148288727,
      "learning_rate": 0.000194175069577536,
      "loss": 2.5051,
      "step": 19820
    },
    {
      "epoch": 0.40794180194867813,
      "grad_norm": 0.37467822432518005,
      "learning_rate": 0.00019416759806213017,
      "loss": 2.4374,
      "step": 19830
    },
    {
      "epoch": 0.40814752146554584,
      "grad_norm": 0.36463114619255066,
      "learning_rate": 0.00019416012190194982,
      "loss": 2.3964,
      "step": 19840
    },
    {
      "epoch": 0.40835324098241355,
      "grad_norm": 0.3625369966030121,
      "learning_rate": 0.0001941526410973638,
      "loss": 2.45,
      "step": 19850
    },
    {
      "epoch": 0.40855896049928125,
      "grad_norm": 0.4169713258743286,
      "learning_rate": 0.00019414515564874108,
      "loss": 2.4891,
      "step": 19860
    },
    {
      "epoch": 0.40876468001614896,
      "grad_norm": 0.350847452878952,
      "learning_rate": 0.00019413766555645084,
      "loss": 2.4747,
      "step": 19870
    },
    {
      "epoch": 0.40897039953301667,
      "grad_norm": 0.36562076210975647,
      "learning_rate": 0.00019413017082086254,
      "loss": 2.4193,
      "step": 19880
    },
    {
      "epoch": 0.40917611904988443,
      "grad_norm": 0.3973733186721802,
      "learning_rate": 0.00019412267144234586,
      "loss": 2.4033,
      "step": 19890
    },
    {
      "epoch": 0.40938183856675214,
      "grad_norm": 0.3741910755634308,
      "learning_rate": 0.00019411516742127072,
      "loss": 2.4412,
      "step": 19900
    },
    {
      "epoch": 0.40958755808361985,
      "grad_norm": 0.33846762776374817,
      "learning_rate": 0.00019410765875800724,
      "loss": 2.3843,
      "step": 19910
    },
    {
      "epoch": 0.40979327760048756,
      "grad_norm": 0.35663458704948425,
      "learning_rate": 0.00019410014545292577,
      "loss": 2.4493,
      "step": 19920
    },
    {
      "epoch": 0.40999899711735527,
      "grad_norm": 0.35777902603149414,
      "learning_rate": 0.00019409262750639693,
      "loss": 2.4212,
      "step": 19930
    },
    {
      "epoch": 0.410204716634223,
      "grad_norm": 0.3567296266555786,
      "learning_rate": 0.0001940851049187915,
      "loss": 2.4571,
      "step": 19940
    },
    {
      "epoch": 0.4104104361510907,
      "grad_norm": 0.38123103976249695,
      "learning_rate": 0.00019407757769048054,
      "loss": 2.4603,
      "step": 19950
    },
    {
      "epoch": 0.4106161556679584,
      "grad_norm": 0.3767983019351959,
      "learning_rate": 0.00019407004582183533,
      "loss": 2.4856,
      "step": 19960
    },
    {
      "epoch": 0.4108218751848261,
      "grad_norm": 0.38537055253982544,
      "learning_rate": 0.00019406250931322742,
      "loss": 2.4385,
      "step": 19970
    },
    {
      "epoch": 0.41102759470169387,
      "grad_norm": 0.3534615933895111,
      "learning_rate": 0.00019405496816502846,
      "loss": 2.4371,
      "step": 19980
    },
    {
      "epoch": 0.4112333142185616,
      "grad_norm": 0.37801066040992737,
      "learning_rate": 0.00019404742237761046,
      "loss": 2.4762,
      "step": 19990
    },
    {
      "epoch": 0.4114390337354293,
      "grad_norm": 0.38376083970069885,
      "learning_rate": 0.00019403987195134564,
      "loss": 2.4288,
      "step": 20000
    },
    {
      "epoch": 0.411644753252297,
      "grad_norm": 0.40705838799476624,
      "learning_rate": 0.00019403231688660638,
      "loss": 2.3942,
      "step": 20010
    },
    {
      "epoch": 0.4118504727691647,
      "grad_norm": 0.3468928933143616,
      "learning_rate": 0.0001940247571837653,
      "loss": 2.4645,
      "step": 20020
    },
    {
      "epoch": 0.4120561922860324,
      "grad_norm": 0.36626365780830383,
      "learning_rate": 0.0001940171928431954,
      "loss": 2.4152,
      "step": 20030
    },
    {
      "epoch": 0.4122619118029001,
      "grad_norm": 0.3670152425765991,
      "learning_rate": 0.00019400962386526967,
      "loss": 2.4589,
      "step": 20040
    },
    {
      "epoch": 0.4124676313197678,
      "grad_norm": 0.38112831115722656,
      "learning_rate": 0.0001940020502503615,
      "loss": 2.4206,
      "step": 20050
    },
    {
      "epoch": 0.41267335083663553,
      "grad_norm": 0.34350845217704773,
      "learning_rate": 0.00019399447199884443,
      "loss": 2.4031,
      "step": 20060
    },
    {
      "epoch": 0.4128790703535033,
      "grad_norm": 0.37629005312919617,
      "learning_rate": 0.00019398688911109223,
      "loss": 2.4133,
      "step": 20070
    },
    {
      "epoch": 0.413084789870371,
      "grad_norm": 0.381253719329834,
      "learning_rate": 0.000193979301587479,
      "loss": 2.4456,
      "step": 20080
    },
    {
      "epoch": 0.4132905093872387,
      "grad_norm": 0.42358115315437317,
      "learning_rate": 0.00019397170942837895,
      "loss": 2.4195,
      "step": 20090
    },
    {
      "epoch": 0.4134962289041064,
      "grad_norm": 0.46781933307647705,
      "learning_rate": 0.00019396411263416654,
      "loss": 2.5203,
      "step": 20100
    },
    {
      "epoch": 0.41370194842097413,
      "grad_norm": 0.3694808781147003,
      "learning_rate": 0.0001939565112052165,
      "loss": 2.4731,
      "step": 20110
    },
    {
      "epoch": 0.41390766793784184,
      "grad_norm": 0.36122390627861023,
      "learning_rate": 0.00019394890514190375,
      "loss": 2.4242,
      "step": 20120
    },
    {
      "epoch": 0.41411338745470955,
      "grad_norm": 0.623526394367218,
      "learning_rate": 0.0001939412944446035,
      "loss": 2.4383,
      "step": 20130
    },
    {
      "epoch": 0.41431910697157726,
      "grad_norm": 0.35247740149497986,
      "learning_rate": 0.00019393367911369112,
      "loss": 2.461,
      "step": 20140
    },
    {
      "epoch": 0.414524826488445,
      "grad_norm": 0.35764044523239136,
      "learning_rate": 0.0001939260591495422,
      "loss": 2.4422,
      "step": 20150
    },
    {
      "epoch": 0.41473054600531273,
      "grad_norm": 0.355183869600296,
      "learning_rate": 0.0001939184345525326,
      "loss": 2.4546,
      "step": 20160
    },
    {
      "epoch": 0.41493626552218044,
      "grad_norm": 0.34143903851509094,
      "learning_rate": 0.00019391080532303847,
      "loss": 2.4199,
      "step": 20170
    },
    {
      "epoch": 0.41514198503904814,
      "grad_norm": 0.4224339723587036,
      "learning_rate": 0.00019390317146143602,
      "loss": 2.4467,
      "step": 20180
    },
    {
      "epoch": 0.41534770455591585,
      "grad_norm": 0.36201053857803345,
      "learning_rate": 0.00019389553296810186,
      "loss": 2.3847,
      "step": 20190
    },
    {
      "epoch": 0.41555342407278356,
      "grad_norm": 0.3883681893348694,
      "learning_rate": 0.0001938878898434127,
      "loss": 2.5355,
      "step": 20200
    },
    {
      "epoch": 0.41575914358965127,
      "grad_norm": 0.3705250918865204,
      "learning_rate": 0.0001938802420877456,
      "loss": 2.4632,
      "step": 20210
    },
    {
      "epoch": 0.415964863106519,
      "grad_norm": 0.38312989473342896,
      "learning_rate": 0.00019387258970147772,
      "loss": 2.4533,
      "step": 20220
    },
    {
      "epoch": 0.4161705826233867,
      "grad_norm": 0.35826924443244934,
      "learning_rate": 0.00019386493268498653,
      "loss": 2.4081,
      "step": 20230
    },
    {
      "epoch": 0.41637630214025445,
      "grad_norm": 0.36019137501716614,
      "learning_rate": 0.00019385727103864975,
      "loss": 2.4821,
      "step": 20240
    },
    {
      "epoch": 0.41658202165712216,
      "grad_norm": 0.3466084897518158,
      "learning_rate": 0.00019384960476284522,
      "loss": 2.4127,
      "step": 20250
    },
    {
      "epoch": 0.41678774117398987,
      "grad_norm": 0.3776668310165405,
      "learning_rate": 0.00019384193385795108,
      "loss": 2.4139,
      "step": 20260
    },
    {
      "epoch": 0.4169934606908576,
      "grad_norm": 0.362819105386734,
      "learning_rate": 0.00019383425832434576,
      "loss": 2.417,
      "step": 20270
    },
    {
      "epoch": 0.4171991802077253,
      "grad_norm": 0.38767167925834656,
      "learning_rate": 0.00019382657816240778,
      "loss": 2.4882,
      "step": 20280
    },
    {
      "epoch": 0.417404899724593,
      "grad_norm": 0.36639147996902466,
      "learning_rate": 0.00019381889337251603,
      "loss": 2.4013,
      "step": 20290
    },
    {
      "epoch": 0.4176106192414607,
      "grad_norm": 0.37698107957839966,
      "learning_rate": 0.00019381120395504952,
      "loss": 2.4616,
      "step": 20300
    },
    {
      "epoch": 0.4178163387583284,
      "grad_norm": 0.3458140194416046,
      "learning_rate": 0.0001938035099103875,
      "loss": 2.4131,
      "step": 20310
    },
    {
      "epoch": 0.4180220582751961,
      "grad_norm": 0.3700259029865265,
      "learning_rate": 0.00019379581123890951,
      "loss": 2.494,
      "step": 20320
    },
    {
      "epoch": 0.4182277777920639,
      "grad_norm": 0.3641001582145691,
      "learning_rate": 0.0001937881079409953,
      "loss": 2.443,
      "step": 20330
    },
    {
      "epoch": 0.4184334973089316,
      "grad_norm": 0.3706926703453064,
      "learning_rate": 0.00019378040001702477,
      "loss": 2.4519,
      "step": 20340
    },
    {
      "epoch": 0.4186392168257993,
      "grad_norm": 0.38349416851997375,
      "learning_rate": 0.0001937726874673782,
      "loss": 2.4596,
      "step": 20350
    },
    {
      "epoch": 0.418844936342667,
      "grad_norm": 0.35461026430130005,
      "learning_rate": 0.0001937649702924359,
      "loss": 2.4647,
      "step": 20360
    },
    {
      "epoch": 0.4190506558595347,
      "grad_norm": 0.3749164342880249,
      "learning_rate": 0.00019375724849257861,
      "loss": 2.4697,
      "step": 20370
    },
    {
      "epoch": 0.4192563753764024,
      "grad_norm": 0.37011829018592834,
      "learning_rate": 0.00019374952206818716,
      "loss": 2.4609,
      "step": 20380
    },
    {
      "epoch": 0.41946209489327013,
      "grad_norm": 0.34123823046684265,
      "learning_rate": 0.00019374179101964268,
      "loss": 2.4184,
      "step": 20390
    },
    {
      "epoch": 0.41966781441013784,
      "grad_norm": 0.4021851718425751,
      "learning_rate": 0.00019373405534732645,
      "loss": 2.4038,
      "step": 20400
    },
    {
      "epoch": 0.41987353392700555,
      "grad_norm": 0.3523194193840027,
      "learning_rate": 0.00019372631505162007,
      "loss": 2.4559,
      "step": 20410
    },
    {
      "epoch": 0.4200792534438733,
      "grad_norm": 0.3969064950942993,
      "learning_rate": 0.00019371857013290533,
      "loss": 2.4345,
      "step": 20420
    },
    {
      "epoch": 0.420284972960741,
      "grad_norm": 0.3643832504749298,
      "learning_rate": 0.00019371082059156422,
      "loss": 2.4185,
      "step": 20430
    },
    {
      "epoch": 0.42049069247760873,
      "grad_norm": 0.4223002791404724,
      "learning_rate": 0.000193703066427979,
      "loss": 2.423,
      "step": 20440
    },
    {
      "epoch": 0.42069641199447644,
      "grad_norm": 0.41938015818595886,
      "learning_rate": 0.00019369530764253213,
      "loss": 2.4244,
      "step": 20450
    },
    {
      "epoch": 0.42090213151134415,
      "grad_norm": 0.45138829946517944,
      "learning_rate": 0.0001936875442356063,
      "loss": 2.4546,
      "step": 20460
    },
    {
      "epoch": 0.42110785102821185,
      "grad_norm": 0.3312098979949951,
      "learning_rate": 0.00019367977620758444,
      "loss": 2.4178,
      "step": 20470
    },
    {
      "epoch": 0.42131357054507956,
      "grad_norm": 0.3889661431312561,
      "learning_rate": 0.00019367200355884973,
      "loss": 2.4393,
      "step": 20480
    },
    {
      "epoch": 0.42151929006194727,
      "grad_norm": 0.39429259300231934,
      "learning_rate": 0.00019366422628978554,
      "loss": 2.4448,
      "step": 20490
    },
    {
      "epoch": 0.421725009578815,
      "grad_norm": 0.3503669798374176,
      "learning_rate": 0.00019365644440077546,
      "loss": 2.4788,
      "step": 20500
    },
    {
      "epoch": 0.42193072909568274,
      "grad_norm": 0.4113217890262604,
      "learning_rate": 0.00019364865789220335,
      "loss": 2.438,
      "step": 20510
    },
    {
      "epoch": 0.42213644861255045,
      "grad_norm": 0.46433815360069275,
      "learning_rate": 0.00019364086676445328,
      "loss": 2.4905,
      "step": 20520
    },
    {
      "epoch": 0.42234216812941816,
      "grad_norm": 0.3581361472606659,
      "learning_rate": 0.00019363307101790954,
      "loss": 2.3986,
      "step": 20530
    },
    {
      "epoch": 0.42254788764628587,
      "grad_norm": 0.3789346516132355,
      "learning_rate": 0.00019362527065295662,
      "loss": 2.4654,
      "step": 20540
    },
    {
      "epoch": 0.4227536071631536,
      "grad_norm": 0.4150618612766266,
      "learning_rate": 0.00019361746566997932,
      "loss": 2.4349,
      "step": 20550
    },
    {
      "epoch": 0.4229593266800213,
      "grad_norm": 0.3683769106864929,
      "learning_rate": 0.00019360965606936257,
      "loss": 2.3985,
      "step": 20560
    },
    {
      "epoch": 0.423165046196889,
      "grad_norm": 0.40252718329429626,
      "learning_rate": 0.0001936018418514916,
      "loss": 2.459,
      "step": 20570
    },
    {
      "epoch": 0.4233707657137567,
      "grad_norm": 0.3699021339416504,
      "learning_rate": 0.00019359402301675184,
      "loss": 2.5173,
      "step": 20580
    },
    {
      "epoch": 0.4235764852306244,
      "grad_norm": 0.38139379024505615,
      "learning_rate": 0.00019358619956552896,
      "loss": 2.4638,
      "step": 20590
    },
    {
      "epoch": 0.4237822047474922,
      "grad_norm": 0.3512270450592041,
      "learning_rate": 0.00019357837149820885,
      "loss": 2.4607,
      "step": 20600
    },
    {
      "epoch": 0.4239879242643599,
      "grad_norm": 0.3748112618923187,
      "learning_rate": 0.00019357053881517762,
      "loss": 2.4803,
      "step": 20610
    },
    {
      "epoch": 0.4241936437812276,
      "grad_norm": 0.40893304347991943,
      "learning_rate": 0.0001935627015168216,
      "loss": 2.4232,
      "step": 20620
    },
    {
      "epoch": 0.4243993632980953,
      "grad_norm": 0.3797997534275055,
      "learning_rate": 0.00019355485960352735,
      "loss": 2.4609,
      "step": 20630
    },
    {
      "epoch": 0.424605082814963,
      "grad_norm": 0.37237733602523804,
      "learning_rate": 0.00019354701307568167,
      "loss": 2.4402,
      "step": 20640
    },
    {
      "epoch": 0.4248108023318307,
      "grad_norm": 0.3498428165912628,
      "learning_rate": 0.00019353916193367166,
      "loss": 2.4701,
      "step": 20650
    },
    {
      "epoch": 0.4250165218486984,
      "grad_norm": 0.3849548399448395,
      "learning_rate": 0.0001935313061778845,
      "loss": 2.487,
      "step": 20660
    },
    {
      "epoch": 0.42522224136556613,
      "grad_norm": 0.3861473500728607,
      "learning_rate": 0.00019352344580870768,
      "loss": 2.4561,
      "step": 20670
    },
    {
      "epoch": 0.42542796088243384,
      "grad_norm": 0.3990075886249542,
      "learning_rate": 0.00019351558082652892,
      "loss": 2.4781,
      "step": 20680
    },
    {
      "epoch": 0.4256336803993016,
      "grad_norm": 0.37270599603652954,
      "learning_rate": 0.00019350771123173617,
      "loss": 2.4524,
      "step": 20690
    },
    {
      "epoch": 0.4258393999161693,
      "grad_norm": 0.4003429412841797,
      "learning_rate": 0.00019349983702471757,
      "loss": 2.4505,
      "step": 20700
    },
    {
      "epoch": 0.426045119433037,
      "grad_norm": 0.37228772044181824,
      "learning_rate": 0.00019349195820586153,
      "loss": 2.4679,
      "step": 20710
    },
    {
      "epoch": 0.42625083894990473,
      "grad_norm": 0.366750568151474,
      "learning_rate": 0.00019348407477555663,
      "loss": 2.4356,
      "step": 20720
    },
    {
      "epoch": 0.42645655846677244,
      "grad_norm": 0.35660040378570557,
      "learning_rate": 0.0001934761867341918,
      "loss": 2.4262,
      "step": 20730
    },
    {
      "epoch": 0.42666227798364015,
      "grad_norm": 0.40874817967414856,
      "learning_rate": 0.00019346829408215603,
      "loss": 2.4492,
      "step": 20740
    },
    {
      "epoch": 0.42686799750050786,
      "grad_norm": 0.39353126287460327,
      "learning_rate": 0.00019346039681983866,
      "loss": 2.4554,
      "step": 20750
    },
    {
      "epoch": 0.42707371701737556,
      "grad_norm": 0.37990328669548035,
      "learning_rate": 0.0001934524949476292,
      "loss": 2.4416,
      "step": 20760
    },
    {
      "epoch": 0.4272794365342433,
      "grad_norm": 0.37611839175224304,
      "learning_rate": 0.00019344458846591746,
      "loss": 2.4285,
      "step": 20770
    },
    {
      "epoch": 0.42748515605111104,
      "grad_norm": 0.3729201555252075,
      "learning_rate": 0.00019343667737509334,
      "loss": 2.4201,
      "step": 20780
    },
    {
      "epoch": 0.42769087556797875,
      "grad_norm": 0.3688196837902069,
      "learning_rate": 0.00019342876167554712,
      "loss": 2.4481,
      "step": 20790
    },
    {
      "epoch": 0.42789659508484645,
      "grad_norm": 0.3509818911552429,
      "learning_rate": 0.0001934208413676692,
      "loss": 2.4438,
      "step": 20800
    },
    {
      "epoch": 0.42810231460171416,
      "grad_norm": 0.4152502715587616,
      "learning_rate": 0.00019341291645185026,
      "loss": 2.4471,
      "step": 20810
    },
    {
      "epoch": 0.42830803411858187,
      "grad_norm": 0.3622313141822815,
      "learning_rate": 0.0001934049869284812,
      "loss": 2.4316,
      "step": 20820
    },
    {
      "epoch": 0.4285137536354496,
      "grad_norm": 0.429523229598999,
      "learning_rate": 0.00019339705279795312,
      "loss": 2.4274,
      "step": 20830
    },
    {
      "epoch": 0.4287194731523173,
      "grad_norm": 0.38280922174453735,
      "learning_rate": 0.00019338911406065738,
      "loss": 2.4624,
      "step": 20840
    },
    {
      "epoch": 0.428925192669185,
      "grad_norm": 0.3736419379711151,
      "learning_rate": 0.00019338117071698556,
      "loss": 2.4547,
      "step": 20850
    },
    {
      "epoch": 0.4291309121860527,
      "grad_norm": 0.3871709704399109,
      "learning_rate": 0.00019337322276732945,
      "loss": 2.4378,
      "step": 20860
    },
    {
      "epoch": 0.42933663170292047,
      "grad_norm": 0.34866294264793396,
      "learning_rate": 0.0001933652702120811,
      "loss": 2.4928,
      "step": 20870
    },
    {
      "epoch": 0.4295423512197882,
      "grad_norm": 0.3802347481250763,
      "learning_rate": 0.00019335731305163272,
      "loss": 2.4402,
      "step": 20880
    },
    {
      "epoch": 0.4297480707366559,
      "grad_norm": 0.37814491987228394,
      "learning_rate": 0.00019334935128637683,
      "loss": 2.4488,
      "step": 20890
    },
    {
      "epoch": 0.4299537902535236,
      "grad_norm": 0.37079259753227234,
      "learning_rate": 0.00019334138491670615,
      "loss": 2.4511,
      "step": 20900
    },
    {
      "epoch": 0.4301595097703913,
      "grad_norm": 0.4033206105232239,
      "learning_rate": 0.0001933334139430136,
      "loss": 2.4379,
      "step": 20910
    },
    {
      "epoch": 0.430365229287259,
      "grad_norm": 0.37810108065605164,
      "learning_rate": 0.00019332543836569232,
      "loss": 2.3673,
      "step": 20920
    },
    {
      "epoch": 0.4305709488041267,
      "grad_norm": 0.39003798365592957,
      "learning_rate": 0.00019331745818513575,
      "loss": 2.4328,
      "step": 20930
    },
    {
      "epoch": 0.4307766683209944,
      "grad_norm": 0.4150216579437256,
      "learning_rate": 0.00019330947340173748,
      "loss": 2.4953,
      "step": 20940
    },
    {
      "epoch": 0.43098238783786214,
      "grad_norm": 0.35199642181396484,
      "learning_rate": 0.00019330148401589136,
      "loss": 2.4399,
      "step": 20950
    },
    {
      "epoch": 0.4311881073547299,
      "grad_norm": 0.36966672539711,
      "learning_rate": 0.0001932934900279915,
      "loss": 2.4995,
      "step": 20960
    },
    {
      "epoch": 0.4313938268715976,
      "grad_norm": 0.35728389024734497,
      "learning_rate": 0.00019328549143843212,
      "loss": 2.4448,
      "step": 20970
    },
    {
      "epoch": 0.4315995463884653,
      "grad_norm": 0.3661412298679352,
      "learning_rate": 0.00019327748824760782,
      "loss": 2.5063,
      "step": 20980
    },
    {
      "epoch": 0.431805265905333,
      "grad_norm": 0.34205013513565063,
      "learning_rate": 0.00019326948045591332,
      "loss": 2.4021,
      "step": 20990
    },
    {
      "epoch": 0.43201098542220073,
      "grad_norm": 0.34841108322143555,
      "learning_rate": 0.0001932614680637436,
      "loss": 2.4987,
      "step": 21000
    },
    {
      "epoch": 0.43221670493906844,
      "grad_norm": 0.425040602684021,
      "learning_rate": 0.00019325345107149385,
      "loss": 2.4265,
      "step": 21010
    },
    {
      "epoch": 0.43242242445593615,
      "grad_norm": 0.34444382786750793,
      "learning_rate": 0.00019324542947955957,
      "loss": 2.4027,
      "step": 21020
    },
    {
      "epoch": 0.43262814397280386,
      "grad_norm": 0.3985115885734558,
      "learning_rate": 0.00019323740328833637,
      "loss": 2.4984,
      "step": 21030
    },
    {
      "epoch": 0.4328338634896716,
      "grad_norm": 0.34517717361450195,
      "learning_rate": 0.00019322937249822015,
      "loss": 2.4482,
      "step": 21040
    },
    {
      "epoch": 0.43303958300653933,
      "grad_norm": 0.3867475390434265,
      "learning_rate": 0.000193221337109607,
      "loss": 2.4008,
      "step": 21050
    },
    {
      "epoch": 0.43324530252340704,
      "grad_norm": 0.3956696093082428,
      "learning_rate": 0.00019321329712289331,
      "loss": 2.4856,
      "step": 21060
    },
    {
      "epoch": 0.43345102204027475,
      "grad_norm": 0.40423667430877686,
      "learning_rate": 0.00019320525253847562,
      "loss": 2.4254,
      "step": 21070
    },
    {
      "epoch": 0.43365674155714246,
      "grad_norm": 0.37650439143180847,
      "learning_rate": 0.00019319720335675072,
      "loss": 2.3916,
      "step": 21080
    },
    {
      "epoch": 0.43386246107401016,
      "grad_norm": 0.3786899447441101,
      "learning_rate": 0.00019318914957811568,
      "loss": 2.4451,
      "step": 21090
    },
    {
      "epoch": 0.4340681805908779,
      "grad_norm": 0.4353781044483185,
      "learning_rate": 0.00019318109120296768,
      "loss": 2.4455,
      "step": 21100
    },
    {
      "epoch": 0.4342739001077456,
      "grad_norm": 0.35759177803993225,
      "learning_rate": 0.00019317302823170423,
      "loss": 2.4699,
      "step": 21110
    },
    {
      "epoch": 0.4344796196246133,
      "grad_norm": 0.3768640160560608,
      "learning_rate": 0.00019316496066472302,
      "loss": 2.4548,
      "step": 21120
    },
    {
      "epoch": 0.43468533914148105,
      "grad_norm": 0.34949782490730286,
      "learning_rate": 0.000193156888502422,
      "loss": 2.4701,
      "step": 21130
    },
    {
      "epoch": 0.43489105865834876,
      "grad_norm": 0.3699665069580078,
      "learning_rate": 0.0001931488117451993,
      "loss": 2.5155,
      "step": 21140
    },
    {
      "epoch": 0.43509677817521647,
      "grad_norm": 0.37649208307266235,
      "learning_rate": 0.00019314073039345333,
      "loss": 2.5232,
      "step": 21150
    },
    {
      "epoch": 0.4353024976920842,
      "grad_norm": 0.4052368998527527,
      "learning_rate": 0.0001931326444475827,
      "loss": 2.4685,
      "step": 21160
    },
    {
      "epoch": 0.4355082172089519,
      "grad_norm": 0.4165723919868469,
      "learning_rate": 0.00019312455390798624,
      "loss": 2.4489,
      "step": 21170
    },
    {
      "epoch": 0.4357139367258196,
      "grad_norm": 0.4006737470626831,
      "learning_rate": 0.00019311645877506295,
      "loss": 2.4275,
      "step": 21180
    },
    {
      "epoch": 0.4359196562426873,
      "grad_norm": 0.390996515750885,
      "learning_rate": 0.0001931083590492122,
      "loss": 2.3945,
      "step": 21190
    },
    {
      "epoch": 0.436125375759555,
      "grad_norm": 0.37371471524238586,
      "learning_rate": 0.0001931002547308335,
      "loss": 2.4566,
      "step": 21200
    },
    {
      "epoch": 0.4363310952764227,
      "grad_norm": 0.40446752309799194,
      "learning_rate": 0.00019309214582032653,
      "loss": 2.4393,
      "step": 21210
    },
    {
      "epoch": 0.4365368147932905,
      "grad_norm": 0.3661446273326874,
      "learning_rate": 0.00019308403231809131,
      "loss": 2.4161,
      "step": 21220
    },
    {
      "epoch": 0.4367425343101582,
      "grad_norm": 0.36628803610801697,
      "learning_rate": 0.00019307591422452805,
      "loss": 2.4173,
      "step": 21230
    },
    {
      "epoch": 0.4369482538270259,
      "grad_norm": 0.4113159477710724,
      "learning_rate": 0.00019306779154003713,
      "loss": 2.5065,
      "step": 21240
    },
    {
      "epoch": 0.4371539733438936,
      "grad_norm": 0.36199724674224854,
      "learning_rate": 0.0001930596642650192,
      "loss": 2.4053,
      "step": 21250
    },
    {
      "epoch": 0.4373596928607613,
      "grad_norm": 0.37063175439834595,
      "learning_rate": 0.00019305153239987515,
      "loss": 2.534,
      "step": 21260
    },
    {
      "epoch": 0.437565412377629,
      "grad_norm": 0.39819955825805664,
      "learning_rate": 0.00019304339594500606,
      "loss": 2.5209,
      "step": 21270
    },
    {
      "epoch": 0.43777113189449673,
      "grad_norm": 0.378950834274292,
      "learning_rate": 0.0001930352549008133,
      "loss": 2.4241,
      "step": 21280
    },
    {
      "epoch": 0.43797685141136444,
      "grad_norm": 0.37348422408103943,
      "learning_rate": 0.0001930271092676984,
      "loss": 2.4733,
      "step": 21290
    },
    {
      "epoch": 0.43818257092823215,
      "grad_norm": 0.3626338839530945,
      "learning_rate": 0.00019301895904606312,
      "loss": 2.4421,
      "step": 21300
    },
    {
      "epoch": 0.4383882904450999,
      "grad_norm": 0.3596847355365753,
      "learning_rate": 0.00019301080423630952,
      "loss": 2.4245,
      "step": 21310
    },
    {
      "epoch": 0.4385940099619676,
      "grad_norm": 0.38960397243499756,
      "learning_rate": 0.00019300264483883975,
      "loss": 2.4259,
      "step": 21320
    },
    {
      "epoch": 0.43879972947883533,
      "grad_norm": 0.3762543201446533,
      "learning_rate": 0.00019299448085405632,
      "loss": 2.4674,
      "step": 21330
    },
    {
      "epoch": 0.43900544899570304,
      "grad_norm": 0.35759100317955017,
      "learning_rate": 0.00019298631228236191,
      "loss": 2.4201,
      "step": 21340
    },
    {
      "epoch": 0.43921116851257075,
      "grad_norm": 0.34529274702072144,
      "learning_rate": 0.00019297813912415944,
      "loss": 2.4374,
      "step": 21350
    },
    {
      "epoch": 0.43941688802943846,
      "grad_norm": 0.3899102807044983,
      "learning_rate": 0.00019296996137985203,
      "loss": 2.4527,
      "step": 21360
    },
    {
      "epoch": 0.43962260754630617,
      "grad_norm": 0.37336745858192444,
      "learning_rate": 0.00019296177904984307,
      "loss": 2.4929,
      "step": 21370
    },
    {
      "epoch": 0.4398283270631739,
      "grad_norm": 0.34649813175201416,
      "learning_rate": 0.0001929535921345361,
      "loss": 2.435,
      "step": 21380
    },
    {
      "epoch": 0.4400340465800416,
      "grad_norm": 0.35570472478866577,
      "learning_rate": 0.000192945400634335,
      "loss": 2.4573,
      "step": 21390
    },
    {
      "epoch": 0.44023976609690935,
      "grad_norm": 0.3620986342430115,
      "learning_rate": 0.00019293720454964376,
      "loss": 2.4564,
      "step": 21400
    },
    {
      "epoch": 0.44044548561377705,
      "grad_norm": 0.3630102574825287,
      "learning_rate": 0.00019292900388086665,
      "loss": 2.4561,
      "step": 21410
    },
    {
      "epoch": 0.44065120513064476,
      "grad_norm": 0.3334144055843353,
      "learning_rate": 0.0001929207986284082,
      "loss": 2.3693,
      "step": 21420
    },
    {
      "epoch": 0.44085692464751247,
      "grad_norm": 0.40639224648475647,
      "learning_rate": 0.00019291258879267314,
      "loss": 2.461,
      "step": 21430
    },
    {
      "epoch": 0.4410626441643802,
      "grad_norm": 0.38662219047546387,
      "learning_rate": 0.00019290437437406634,
      "loss": 2.4681,
      "step": 21440
    },
    {
      "epoch": 0.4412683636812479,
      "grad_norm": 0.3738778829574585,
      "learning_rate": 0.00019289615537299303,
      "loss": 2.4499,
      "step": 21450
    },
    {
      "epoch": 0.4414740831981156,
      "grad_norm": 0.4080217778682709,
      "learning_rate": 0.00019288793178985863,
      "loss": 2.482,
      "step": 21460
    },
    {
      "epoch": 0.4416798027149833,
      "grad_norm": 0.35305050015449524,
      "learning_rate": 0.00019287970362506869,
      "loss": 2.4342,
      "step": 21470
    },
    {
      "epoch": 0.441885522231851,
      "grad_norm": 0.37452811002731323,
      "learning_rate": 0.00019287147087902913,
      "loss": 2.4758,
      "step": 21480
    },
    {
      "epoch": 0.4420912417487188,
      "grad_norm": 0.3803325891494751,
      "learning_rate": 0.000192863233552146,
      "loss": 2.4907,
      "step": 21490
    },
    {
      "epoch": 0.4422969612655865,
      "grad_norm": 0.5930682420730591,
      "learning_rate": 0.0001928549916448256,
      "loss": 2.4102,
      "step": 21500
    },
    {
      "epoch": 0.4425026807824542,
      "grad_norm": 0.37418314814567566,
      "learning_rate": 0.00019284674515747445,
      "loss": 2.4374,
      "step": 21510
    },
    {
      "epoch": 0.4427084002993219,
      "grad_norm": 0.38648733496665955,
      "learning_rate": 0.00019283849409049934,
      "loss": 2.4462,
      "step": 21520
    },
    {
      "epoch": 0.4429141198161896,
      "grad_norm": 0.37972384691238403,
      "learning_rate": 0.0001928302384443072,
      "loss": 2.4877,
      "step": 21530
    },
    {
      "epoch": 0.4431198393330573,
      "grad_norm": 0.34320157766342163,
      "learning_rate": 0.00019282197821930526,
      "loss": 2.4094,
      "step": 21540
    },
    {
      "epoch": 0.443325558849925,
      "grad_norm": 0.37401092052459717,
      "learning_rate": 0.00019281371341590097,
      "loss": 2.4534,
      "step": 21550
    },
    {
      "epoch": 0.44353127836679274,
      "grad_norm": 0.41612085700035095,
      "learning_rate": 0.00019280544403450195,
      "loss": 2.4632,
      "step": 21560
    },
    {
      "epoch": 0.44373699788366044,
      "grad_norm": 0.38240504264831543,
      "learning_rate": 0.00019279717007551615,
      "loss": 2.4446,
      "step": 21570
    },
    {
      "epoch": 0.4439427174005282,
      "grad_norm": 0.35156142711639404,
      "learning_rate": 0.0001927888915393516,
      "loss": 2.445,
      "step": 21580
    },
    {
      "epoch": 0.4441484369173959,
      "grad_norm": 0.3513295650482178,
      "learning_rate": 0.00019278060842641665,
      "loss": 2.4989,
      "step": 21590
    },
    {
      "epoch": 0.4443541564342636,
      "grad_norm": 0.36568909883499146,
      "learning_rate": 0.0001927723207371199,
      "loss": 2.4409,
      "step": 21600
    },
    {
      "epoch": 0.44455987595113133,
      "grad_norm": 0.4061327278614044,
      "learning_rate": 0.00019276402847187015,
      "loss": 2.465,
      "step": 21610
    },
    {
      "epoch": 0.44476559546799904,
      "grad_norm": 0.3630926311016083,
      "learning_rate": 0.00019275573163107633,
      "loss": 2.4954,
      "step": 21620
    },
    {
      "epoch": 0.44497131498486675,
      "grad_norm": 0.4199475049972534,
      "learning_rate": 0.00019274743021514774,
      "loss": 2.4573,
      "step": 21630
    },
    {
      "epoch": 0.44517703450173446,
      "grad_norm": 0.3687218725681305,
      "learning_rate": 0.00019273912422449385,
      "loss": 2.3899,
      "step": 21640
    },
    {
      "epoch": 0.44538275401860217,
      "grad_norm": 0.3746643662452698,
      "learning_rate": 0.0001927308136595243,
      "loss": 2.4452,
      "step": 21650
    },
    {
      "epoch": 0.4455884735354699,
      "grad_norm": 0.41090619564056396,
      "learning_rate": 0.00019272249852064904,
      "loss": 2.4321,
      "step": 21660
    },
    {
      "epoch": 0.44579419305233764,
      "grad_norm": 0.3576410710811615,
      "learning_rate": 0.00019271417880827824,
      "loss": 2.4353,
      "step": 21670
    },
    {
      "epoch": 0.44599991256920535,
      "grad_norm": 0.46210604906082153,
      "learning_rate": 0.0001927058545228222,
      "loss": 2.4358,
      "step": 21680
    },
    {
      "epoch": 0.44620563208607306,
      "grad_norm": 0.3764689862728119,
      "learning_rate": 0.00019269752566469155,
      "loss": 2.4643,
      "step": 21690
    },
    {
      "epoch": 0.44641135160294076,
      "grad_norm": 0.38623538613319397,
      "learning_rate": 0.0001926891922342971,
      "loss": 2.457,
      "step": 21700
    },
    {
      "epoch": 0.4466170711198085,
      "grad_norm": 0.3706710934638977,
      "learning_rate": 0.0001926808542320499,
      "loss": 2.459,
      "step": 21710
    },
    {
      "epoch": 0.4468227906366762,
      "grad_norm": 0.3830585479736328,
      "learning_rate": 0.0001926725116583612,
      "loss": 2.4432,
      "step": 21720
    },
    {
      "epoch": 0.4470285101535439,
      "grad_norm": 0.3574000895023346,
      "learning_rate": 0.00019266416451364248,
      "loss": 2.4188,
      "step": 21730
    },
    {
      "epoch": 0.4472342296704116,
      "grad_norm": 0.37662115693092346,
      "learning_rate": 0.00019265581279830554,
      "loss": 2.3962,
      "step": 21740
    },
    {
      "epoch": 0.4474399491872793,
      "grad_norm": 0.3772243857383728,
      "learning_rate": 0.00019264745651276224,
      "loss": 2.4882,
      "step": 21750
    },
    {
      "epoch": 0.44764566870414707,
      "grad_norm": 0.39494800567626953,
      "learning_rate": 0.00019263909565742475,
      "loss": 2.4585,
      "step": 21760
    },
    {
      "epoch": 0.4478513882210148,
      "grad_norm": 0.3860517144203186,
      "learning_rate": 0.00019263073023270555,
      "loss": 2.425,
      "step": 21770
    },
    {
      "epoch": 0.4480571077378825,
      "grad_norm": 0.37010666728019714,
      "learning_rate": 0.00019262236023901715,
      "loss": 2.4562,
      "step": 21780
    },
    {
      "epoch": 0.4482628272547502,
      "grad_norm": 0.36376485228538513,
      "learning_rate": 0.0001926139856767725,
      "loss": 2.4635,
      "step": 21790
    },
    {
      "epoch": 0.4484685467716179,
      "grad_norm": 0.36272385716438293,
      "learning_rate": 0.0001926056065463846,
      "loss": 2.4263,
      "step": 21800
    },
    {
      "epoch": 0.4486742662884856,
      "grad_norm": 0.3515017628669739,
      "learning_rate": 0.00019259722284826672,
      "loss": 2.426,
      "step": 21810
    },
    {
      "epoch": 0.4488799858053533,
      "grad_norm": 0.3932189643383026,
      "learning_rate": 0.00019258883458283252,
      "loss": 2.4312,
      "step": 21820
    },
    {
      "epoch": 0.44908570532222103,
      "grad_norm": 0.40496718883514404,
      "learning_rate": 0.0001925804417504956,
      "loss": 2.4501,
      "step": 21830
    },
    {
      "epoch": 0.44929142483908874,
      "grad_norm": 0.338876336812973,
      "learning_rate": 0.00019257204435167,
      "loss": 2.4756,
      "step": 21840
    },
    {
      "epoch": 0.4494971443559565,
      "grad_norm": 0.3913253843784332,
      "learning_rate": 0.00019256364238676992,
      "loss": 2.4654,
      "step": 21850
    },
    {
      "epoch": 0.4497028638728242,
      "grad_norm": 0.3639850914478302,
      "learning_rate": 0.00019255523585620977,
      "loss": 2.4363,
      "step": 21860
    },
    {
      "epoch": 0.4499085833896919,
      "grad_norm": 0.3682538568973541,
      "learning_rate": 0.00019254682476040421,
      "loss": 2.4038,
      "step": 21870
    },
    {
      "epoch": 0.4501143029065596,
      "grad_norm": 0.3625568449497223,
      "learning_rate": 0.00019253840909976808,
      "loss": 2.4376,
      "step": 21880
    },
    {
      "epoch": 0.45032002242342734,
      "grad_norm": 0.38083702325820923,
      "learning_rate": 0.00019252998887471653,
      "loss": 2.4323,
      "step": 21890
    },
    {
      "epoch": 0.45052574194029504,
      "grad_norm": 0.39955079555511475,
      "learning_rate": 0.00019252156408566484,
      "loss": 2.4047,
      "step": 21900
    },
    {
      "epoch": 0.45073146145716275,
      "grad_norm": 0.3769283890724182,
      "learning_rate": 0.00019251313473302859,
      "loss": 2.4491,
      "step": 21910
    },
    {
      "epoch": 0.45093718097403046,
      "grad_norm": 0.34933942556381226,
      "learning_rate": 0.0001925047008172235,
      "loss": 2.4234,
      "step": 21920
    },
    {
      "epoch": 0.4511429004908982,
      "grad_norm": 0.35368403792381287,
      "learning_rate": 0.00019249626233866566,
      "loss": 2.4482,
      "step": 21930
    },
    {
      "epoch": 0.45134862000776593,
      "grad_norm": 0.371565580368042,
      "learning_rate": 0.00019248781929777123,
      "loss": 2.48,
      "step": 21940
    },
    {
      "epoch": 0.45155433952463364,
      "grad_norm": 0.34905970096588135,
      "learning_rate": 0.00019247937169495667,
      "loss": 2.4609,
      "step": 21950
    },
    {
      "epoch": 0.45176005904150135,
      "grad_norm": 0.3672032058238983,
      "learning_rate": 0.00019247091953063865,
      "loss": 2.4499,
      "step": 21960
    },
    {
      "epoch": 0.45196577855836906,
      "grad_norm": 0.3514031767845154,
      "learning_rate": 0.00019246246280523407,
      "loss": 2.4411,
      "step": 21970
    },
    {
      "epoch": 0.45217149807523677,
      "grad_norm": 0.34434744715690613,
      "learning_rate": 0.0001924540015191601,
      "loss": 2.4978,
      "step": 21980
    },
    {
      "epoch": 0.4523772175921045,
      "grad_norm": 0.3667899966239929,
      "learning_rate": 0.000192445535672834,
      "loss": 2.4994,
      "step": 21990
    },
    {
      "epoch": 0.4525829371089722,
      "grad_norm": 0.39791029691696167,
      "learning_rate": 0.0001924370652666734,
      "loss": 2.4707,
      "step": 22000
    },
    {
      "epoch": 0.4527886566258399,
      "grad_norm": 0.4030363857746124,
      "learning_rate": 0.0001924285903010961,
      "loss": 2.3728,
      "step": 22010
    },
    {
      "epoch": 0.45299437614270766,
      "grad_norm": 0.3319576680660248,
      "learning_rate": 0.00019242011077652012,
      "loss": 2.4411,
      "step": 22020
    },
    {
      "epoch": 0.45320009565957536,
      "grad_norm": 0.3731575310230255,
      "learning_rate": 0.0001924116266933637,
      "loss": 2.467,
      "step": 22030
    },
    {
      "epoch": 0.45340581517644307,
      "grad_norm": 0.36308911442756653,
      "learning_rate": 0.00019240313805204533,
      "loss": 2.4145,
      "step": 22040
    },
    {
      "epoch": 0.4536115346933108,
      "grad_norm": 0.36942893266677856,
      "learning_rate": 0.00019239464485298368,
      "loss": 2.4379,
      "step": 22050
    },
    {
      "epoch": 0.4538172542101785,
      "grad_norm": 0.3743901550769806,
      "learning_rate": 0.0001923861470965977,
      "loss": 2.4514,
      "step": 22060
    },
    {
      "epoch": 0.4540229737270462,
      "grad_norm": 0.3995784521102905,
      "learning_rate": 0.0001923776447833065,
      "loss": 2.4579,
      "step": 22070
    },
    {
      "epoch": 0.4542286932439139,
      "grad_norm": 0.3963177800178528,
      "learning_rate": 0.00019236913791352951,
      "loss": 2.474,
      "step": 22080
    },
    {
      "epoch": 0.4544344127607816,
      "grad_norm": 0.4001238942146301,
      "learning_rate": 0.00019236062648768628,
      "loss": 2.5153,
      "step": 22090
    },
    {
      "epoch": 0.4546401322776493,
      "grad_norm": 0.5013462901115417,
      "learning_rate": 0.00019235211050619669,
      "loss": 2.4777,
      "step": 22100
    },
    {
      "epoch": 0.4548458517945171,
      "grad_norm": 0.34825369715690613,
      "learning_rate": 0.00019234358996948072,
      "loss": 2.4878,
      "step": 22110
    },
    {
      "epoch": 0.4550515713113848,
      "grad_norm": 0.3713551163673401,
      "learning_rate": 0.00019233506487795866,
      "loss": 2.4585,
      "step": 22120
    },
    {
      "epoch": 0.4552572908282525,
      "grad_norm": 0.38024893403053284,
      "learning_rate": 0.00019232653523205102,
      "loss": 2.3795,
      "step": 22130
    },
    {
      "epoch": 0.4554630103451202,
      "grad_norm": 0.3606627881526947,
      "learning_rate": 0.00019231800103217852,
      "loss": 2.4786,
      "step": 22140
    },
    {
      "epoch": 0.4556687298619879,
      "grad_norm": 0.36208006739616394,
      "learning_rate": 0.0001923094622787621,
      "loss": 2.4382,
      "step": 22150
    },
    {
      "epoch": 0.45587444937885563,
      "grad_norm": 0.40789008140563965,
      "learning_rate": 0.00019230091897222293,
      "loss": 2.4478,
      "step": 22160
    },
    {
      "epoch": 0.45608016889572334,
      "grad_norm": 0.3725360929965973,
      "learning_rate": 0.0001922923711129824,
      "loss": 2.4543,
      "step": 22170
    },
    {
      "epoch": 0.45628588841259105,
      "grad_norm": 0.3896860182285309,
      "learning_rate": 0.00019228381870146216,
      "loss": 2.4494,
      "step": 22180
    },
    {
      "epoch": 0.45649160792945875,
      "grad_norm": 0.38741594552993774,
      "learning_rate": 0.00019227526173808403,
      "loss": 2.4695,
      "step": 22190
    },
    {
      "epoch": 0.4566973274463265,
      "grad_norm": 0.35623830556869507,
      "learning_rate": 0.00019226670022327004,
      "loss": 2.4396,
      "step": 22200
    },
    {
      "epoch": 0.4569030469631942,
      "grad_norm": 0.3846065402030945,
      "learning_rate": 0.0001922581341574426,
      "loss": 2.4785,
      "step": 22210
    },
    {
      "epoch": 0.45710876648006193,
      "grad_norm": 0.3658386766910553,
      "learning_rate": 0.00019224956354102406,
      "loss": 2.4705,
      "step": 22220
    },
    {
      "epoch": 0.45731448599692964,
      "grad_norm": 0.38067713379859924,
      "learning_rate": 0.0001922409883744373,
      "loss": 2.3806,
      "step": 22230
    },
    {
      "epoch": 0.45752020551379735,
      "grad_norm": 0.35723379254341125,
      "learning_rate": 0.0001922324086581052,
      "loss": 2.3981,
      "step": 22240
    },
    {
      "epoch": 0.45772592503066506,
      "grad_norm": 0.3875088095664978,
      "learning_rate": 0.00019222382439245103,
      "loss": 2.4456,
      "step": 22250
    },
    {
      "epoch": 0.45793164454753277,
      "grad_norm": 0.362160325050354,
      "learning_rate": 0.00019221523557789815,
      "loss": 2.4654,
      "step": 22260
    },
    {
      "epoch": 0.4581373640644005,
      "grad_norm": 0.3676320016384125,
      "learning_rate": 0.0001922066422148702,
      "loss": 2.4203,
      "step": 22270
    },
    {
      "epoch": 0.4583430835812682,
      "grad_norm": 0.4627324044704437,
      "learning_rate": 0.00019219804430379106,
      "loss": 2.5002,
      "step": 22280
    },
    {
      "epoch": 0.45854880309813595,
      "grad_norm": 0.35353943705558777,
      "learning_rate": 0.00019218944184508477,
      "loss": 2.4462,
      "step": 22290
    },
    {
      "epoch": 0.45875452261500366,
      "grad_norm": 0.39034903049468994,
      "learning_rate": 0.00019218083483917576,
      "loss": 2.3809,
      "step": 22300
    },
    {
      "epoch": 0.45896024213187137,
      "grad_norm": 0.38956889510154724,
      "learning_rate": 0.00019217222328648848,
      "loss": 2.4206,
      "step": 22310
    },
    {
      "epoch": 0.4591659616487391,
      "grad_norm": 0.3644191324710846,
      "learning_rate": 0.00019216360718744767,
      "loss": 2.473,
      "step": 22320
    },
    {
      "epoch": 0.4593716811656068,
      "grad_norm": 0.36337634921073914,
      "learning_rate": 0.00019215498654247837,
      "loss": 2.4425,
      "step": 22330
    },
    {
      "epoch": 0.4595774006824745,
      "grad_norm": 0.39044007658958435,
      "learning_rate": 0.00019214636135200574,
      "loss": 2.4244,
      "step": 22340
    },
    {
      "epoch": 0.4597831201993422,
      "grad_norm": 0.3772384524345398,
      "learning_rate": 0.00019213773161645526,
      "loss": 2.4053,
      "step": 22350
    },
    {
      "epoch": 0.4599888397162099,
      "grad_norm": 0.3627281188964844,
      "learning_rate": 0.00019212909733625256,
      "loss": 2.5031,
      "step": 22360
    },
    {
      "epoch": 0.4601945592330776,
      "grad_norm": 0.4071236550807953,
      "learning_rate": 0.00019212045851182356,
      "loss": 2.4419,
      "step": 22370
    },
    {
      "epoch": 0.4604002787499454,
      "grad_norm": 0.35974445939064026,
      "learning_rate": 0.00019211181514359432,
      "loss": 2.4818,
      "step": 22380
    },
    {
      "epoch": 0.4606059982668131,
      "grad_norm": 0.3502557575702667,
      "learning_rate": 0.0001921031672319912,
      "loss": 2.4666,
      "step": 22390
    },
    {
      "epoch": 0.4608117177836808,
      "grad_norm": 0.42132052779197693,
      "learning_rate": 0.00019209451477744075,
      "loss": 2.464,
      "step": 22400
    },
    {
      "epoch": 0.4610174373005485,
      "grad_norm": 0.37003493309020996,
      "learning_rate": 0.00019208585778036973,
      "loss": 2.4725,
      "step": 22410
    },
    {
      "epoch": 0.4612231568174162,
      "grad_norm": 0.40176576375961304,
      "learning_rate": 0.00019207719624120511,
      "loss": 2.5081,
      "step": 22420
    },
    {
      "epoch": 0.4614288763342839,
      "grad_norm": 0.3535662591457367,
      "learning_rate": 0.00019206853016037422,
      "loss": 2.4781,
      "step": 22430
    },
    {
      "epoch": 0.46163459585115163,
      "grad_norm": 0.3797140121459961,
      "learning_rate": 0.00019205985953830442,
      "loss": 2.4679,
      "step": 22440
    },
    {
      "epoch": 0.46184031536801934,
      "grad_norm": 0.35311126708984375,
      "learning_rate": 0.00019205118437542345,
      "loss": 2.4214,
      "step": 22450
    },
    {
      "epoch": 0.46204603488488705,
      "grad_norm": 0.35475245118141174,
      "learning_rate": 0.00019204250467215914,
      "loss": 2.3827,
      "step": 22460
    },
    {
      "epoch": 0.4622517544017548,
      "grad_norm": 0.3950407803058624,
      "learning_rate": 0.00019203382042893964,
      "loss": 2.4612,
      "step": 22470
    },
    {
      "epoch": 0.4624574739186225,
      "grad_norm": 0.3602442443370819,
      "learning_rate": 0.00019202513164619331,
      "loss": 2.4712,
      "step": 22480
    },
    {
      "epoch": 0.4626631934354902,
      "grad_norm": 0.3662208318710327,
      "learning_rate": 0.00019201643832434875,
      "loss": 2.424,
      "step": 22490
    },
    {
      "epoch": 0.46286891295235794,
      "grad_norm": 0.37835484743118286,
      "learning_rate": 0.00019200774046383468,
      "loss": 2.4168,
      "step": 22500
    },
    {
      "epoch": 0.46307463246922564,
      "grad_norm": 0.39769428968429565,
      "learning_rate": 0.00019199903806508016,
      "loss": 2.4221,
      "step": 22510
    },
    {
      "epoch": 0.46328035198609335,
      "grad_norm": 0.368839293718338,
      "learning_rate": 0.0001919903311285144,
      "loss": 2.4759,
      "step": 22520
    },
    {
      "epoch": 0.46348607150296106,
      "grad_norm": 0.413828045129776,
      "learning_rate": 0.00019198161965456695,
      "loss": 2.4271,
      "step": 22530
    },
    {
      "epoch": 0.46369179101982877,
      "grad_norm": 0.34218305349349976,
      "learning_rate": 0.0001919729036436674,
      "loss": 2.4009,
      "step": 22540
    },
    {
      "epoch": 0.4638975105366965,
      "grad_norm": 0.37494537234306335,
      "learning_rate": 0.00019196418309624572,
      "loss": 2.4398,
      "step": 22550
    },
    {
      "epoch": 0.46410323005356424,
      "grad_norm": 0.3909086585044861,
      "learning_rate": 0.00019195545801273202,
      "loss": 2.4141,
      "step": 22560
    },
    {
      "epoch": 0.46430894957043195,
      "grad_norm": 0.3739021122455597,
      "learning_rate": 0.0001919467283935567,
      "loss": 2.4441,
      "step": 22570
    },
    {
      "epoch": 0.46451466908729966,
      "grad_norm": 0.4084627628326416,
      "learning_rate": 0.00019193799423915028,
      "loss": 2.4076,
      "step": 22580
    },
    {
      "epoch": 0.46472038860416737,
      "grad_norm": 0.36099356412887573,
      "learning_rate": 0.00019192925554994364,
      "loss": 2.4474,
      "step": 22590
    },
    {
      "epoch": 0.4649261081210351,
      "grad_norm": 0.364352285861969,
      "learning_rate": 0.00019192051232636776,
      "loss": 2.4272,
      "step": 22600
    },
    {
      "epoch": 0.4651318276379028,
      "grad_norm": 0.36517778038978577,
      "learning_rate": 0.0001919117645688539,
      "loss": 2.4464,
      "step": 22610
    },
    {
      "epoch": 0.4653375471547705,
      "grad_norm": 0.3661791980266571,
      "learning_rate": 0.0001919030122778336,
      "loss": 2.4306,
      "step": 22620
    },
    {
      "epoch": 0.4655432666716382,
      "grad_norm": 0.4036082327365875,
      "learning_rate": 0.0001918942554537385,
      "loss": 2.4338,
      "step": 22630
    },
    {
      "epoch": 0.4657489861885059,
      "grad_norm": 0.3937651515007019,
      "learning_rate": 0.00019188549409700052,
      "loss": 2.4494,
      "step": 22640
    },
    {
      "epoch": 0.4659547057053737,
      "grad_norm": 0.38265976309776306,
      "learning_rate": 0.00019187672820805186,
      "loss": 2.4551,
      "step": 22650
    },
    {
      "epoch": 0.4661604252222414,
      "grad_norm": 0.37606608867645264,
      "learning_rate": 0.00019186795778732485,
      "loss": 2.4399,
      "step": 22660
    },
    {
      "epoch": 0.4663661447391091,
      "grad_norm": 0.34571996331214905,
      "learning_rate": 0.00019185918283525207,
      "loss": 2.4252,
      "step": 22670
    },
    {
      "epoch": 0.4665718642559768,
      "grad_norm": 0.47746339440345764,
      "learning_rate": 0.00019185040335226644,
      "loss": 2.4731,
      "step": 22680
    },
    {
      "epoch": 0.4667775837728445,
      "grad_norm": 0.36684733629226685,
      "learning_rate": 0.00019184161933880086,
      "loss": 2.4446,
      "step": 22690
    },
    {
      "epoch": 0.4669833032897122,
      "grad_norm": 0.389474481344223,
      "learning_rate": 0.00019183283079528873,
      "loss": 2.4435,
      "step": 22700
    },
    {
      "epoch": 0.4671890228065799,
      "grad_norm": 0.3888865113258362,
      "learning_rate": 0.0001918240377221635,
      "loss": 2.4657,
      "step": 22710
    },
    {
      "epoch": 0.46739474232344763,
      "grad_norm": 0.35327139496803284,
      "learning_rate": 0.00019181524011985883,
      "loss": 2.4592,
      "step": 22720
    },
    {
      "epoch": 0.4676004618403154,
      "grad_norm": 0.36821913719177246,
      "learning_rate": 0.0001918064379888087,
      "loss": 2.4197,
      "step": 22730
    },
    {
      "epoch": 0.4678061813571831,
      "grad_norm": 0.3640030324459076,
      "learning_rate": 0.00019179763132944728,
      "loss": 2.4359,
      "step": 22740
    },
    {
      "epoch": 0.4680119008740508,
      "grad_norm": 0.37323400378227234,
      "learning_rate": 0.00019178882014220895,
      "loss": 2.4384,
      "step": 22750
    },
    {
      "epoch": 0.4682176203909185,
      "grad_norm": 0.3648941218852997,
      "learning_rate": 0.0001917800044275283,
      "loss": 2.4694,
      "step": 22760
    },
    {
      "epoch": 0.46842333990778623,
      "grad_norm": 0.33275189995765686,
      "learning_rate": 0.00019177118418584017,
      "loss": 2.4629,
      "step": 22770
    },
    {
      "epoch": 0.46862905942465394,
      "grad_norm": 0.39874476194381714,
      "learning_rate": 0.00019176235941757963,
      "loss": 2.4756,
      "step": 22780
    },
    {
      "epoch": 0.46883477894152165,
      "grad_norm": 0.392324835062027,
      "learning_rate": 0.00019175353012318194,
      "loss": 2.481,
      "step": 22790
    },
    {
      "epoch": 0.46904049845838935,
      "grad_norm": 0.3504030704498291,
      "learning_rate": 0.00019174469630308262,
      "loss": 2.4322,
      "step": 22800
    },
    {
      "epoch": 0.46924621797525706,
      "grad_norm": 0.4105408489704132,
      "learning_rate": 0.00019173585795771736,
      "loss": 2.3599,
      "step": 22810
    },
    {
      "epoch": 0.4694519374921248,
      "grad_norm": 0.367506206035614,
      "learning_rate": 0.00019172701508752212,
      "loss": 2.4776,
      "step": 22820
    },
    {
      "epoch": 0.46965765700899254,
      "grad_norm": 0.36940112709999084,
      "learning_rate": 0.00019171816769293306,
      "loss": 2.4261,
      "step": 22830
    },
    {
      "epoch": 0.46986337652586024,
      "grad_norm": 0.4038340449333191,
      "learning_rate": 0.00019170931577438665,
      "loss": 2.4153,
      "step": 22840
    },
    {
      "epoch": 0.47006909604272795,
      "grad_norm": 0.3554184138774872,
      "learning_rate": 0.0001917004593323194,
      "loss": 2.4251,
      "step": 22850
    },
    {
      "epoch": 0.47027481555959566,
      "grad_norm": 0.36539581418037415,
      "learning_rate": 0.00019169159836716822,
      "loss": 2.4609,
      "step": 22860
    },
    {
      "epoch": 0.47048053507646337,
      "grad_norm": 0.37819191813468933,
      "learning_rate": 0.00019168273287937019,
      "loss": 2.4316,
      "step": 22870
    },
    {
      "epoch": 0.4706862545933311,
      "grad_norm": 0.41437840461730957,
      "learning_rate": 0.00019167386286936247,
      "loss": 2.4459,
      "step": 22880
    },
    {
      "epoch": 0.4708919741101988,
      "grad_norm": 0.3533801734447479,
      "learning_rate": 0.00019166498833758273,
      "loss": 2.4272,
      "step": 22890
    },
    {
      "epoch": 0.4710976936270665,
      "grad_norm": 0.4474467635154724,
      "learning_rate": 0.0001916561092844686,
      "loss": 2.4603,
      "step": 22900
    },
    {
      "epoch": 0.47130341314393426,
      "grad_norm": 0.3628532588481903,
      "learning_rate": 0.00019164722571045809,
      "loss": 2.3972,
      "step": 22910
    },
    {
      "epoch": 0.47150913266080197,
      "grad_norm": 0.42471882700920105,
      "learning_rate": 0.00019163833761598933,
      "loss": 2.5027,
      "step": 22920
    },
    {
      "epoch": 0.4717148521776697,
      "grad_norm": 0.3607427477836609,
      "learning_rate": 0.00019162944500150073,
      "loss": 2.4539,
      "step": 22930
    },
    {
      "epoch": 0.4719205716945374,
      "grad_norm": 0.3902522623538971,
      "learning_rate": 0.00019162054786743099,
      "loss": 2.3627,
      "step": 22940
    },
    {
      "epoch": 0.4721262912114051,
      "grad_norm": 0.3736226558685303,
      "learning_rate": 0.00019161164621421883,
      "loss": 2.4694,
      "step": 22950
    },
    {
      "epoch": 0.4723320107282728,
      "grad_norm": 0.3750371038913727,
      "learning_rate": 0.0001916027400423034,
      "loss": 2.4705,
      "step": 22960
    },
    {
      "epoch": 0.4725377302451405,
      "grad_norm": 0.3720627725124359,
      "learning_rate": 0.000191593829352124,
      "loss": 2.465,
      "step": 22970
    },
    {
      "epoch": 0.4727434497620082,
      "grad_norm": 0.3865969777107239,
      "learning_rate": 0.0001915849141441201,
      "loss": 2.4462,
      "step": 22980
    },
    {
      "epoch": 0.4729491692788759,
      "grad_norm": 0.3581872582435608,
      "learning_rate": 0.00019157599441873148,
      "loss": 2.4284,
      "step": 22990
    },
    {
      "epoch": 0.4731548887957437,
      "grad_norm": 0.3498367667198181,
      "learning_rate": 0.00019156707017639808,
      "loss": 2.4408,
      "step": 23000
    },
    {
      "epoch": 0.4733606083126114,
      "grad_norm": 0.45175808668136597,
      "learning_rate": 0.0001915581414175601,
      "loss": 2.4189,
      "step": 23010
    },
    {
      "epoch": 0.4735663278294791,
      "grad_norm": 0.40470030903816223,
      "learning_rate": 0.0001915492081426579,
      "loss": 2.4448,
      "step": 23020
    },
    {
      "epoch": 0.4737720473463468,
      "grad_norm": 0.38332557678222656,
      "learning_rate": 0.00019154027035213217,
      "loss": 2.4156,
      "step": 23030
    },
    {
      "epoch": 0.4739777668632145,
      "grad_norm": 0.3558221757411957,
      "learning_rate": 0.00019153132804642376,
      "loss": 2.4572,
      "step": 23040
    },
    {
      "epoch": 0.47418348638008223,
      "grad_norm": 0.3612760901451111,
      "learning_rate": 0.0001915223812259737,
      "loss": 2.4849,
      "step": 23050
    },
    {
      "epoch": 0.47438920589694994,
      "grad_norm": 0.3812900483608246,
      "learning_rate": 0.0001915134298912233,
      "loss": 2.493,
      "step": 23060
    },
    {
      "epoch": 0.47459492541381765,
      "grad_norm": 0.37682968378067017,
      "learning_rate": 0.0001915044740426141,
      "loss": 2.3868,
      "step": 23070
    },
    {
      "epoch": 0.47480064493068536,
      "grad_norm": 0.3579918444156647,
      "learning_rate": 0.00019149551368058786,
      "loss": 2.4189,
      "step": 23080
    },
    {
      "epoch": 0.4750063644475531,
      "grad_norm": 0.36530351638793945,
      "learning_rate": 0.0001914865488055865,
      "loss": 2.4167,
      "step": 23090
    },
    {
      "epoch": 0.47521208396442083,
      "grad_norm": 0.3536754548549652,
      "learning_rate": 0.00019147757941805223,
      "loss": 2.4092,
      "step": 23100
    },
    {
      "epoch": 0.47541780348128854,
      "grad_norm": 0.3547632396221161,
      "learning_rate": 0.00019146860551842748,
      "loss": 2.4713,
      "step": 23110
    },
    {
      "epoch": 0.47562352299815625,
      "grad_norm": 0.3713868260383606,
      "learning_rate": 0.0001914596271071548,
      "loss": 2.3777,
      "step": 23120
    },
    {
      "epoch": 0.47582924251502395,
      "grad_norm": 0.3865847885608673,
      "learning_rate": 0.00019145064418467716,
      "loss": 2.4345,
      "step": 23130
    },
    {
      "epoch": 0.47603496203189166,
      "grad_norm": 0.4092588424682617,
      "learning_rate": 0.0001914416567514376,
      "loss": 2.5139,
      "step": 23140
    },
    {
      "epoch": 0.47624068154875937,
      "grad_norm": 0.4029613435268402,
      "learning_rate": 0.00019143266480787938,
      "loss": 2.4761,
      "step": 23150
    },
    {
      "epoch": 0.4764464010656271,
      "grad_norm": 0.39697131514549255,
      "learning_rate": 0.00019142366835444606,
      "loss": 2.473,
      "step": 23160
    },
    {
      "epoch": 0.4766521205824948,
      "grad_norm": 0.3677671551704407,
      "learning_rate": 0.0001914146673915814,
      "loss": 2.4816,
      "step": 23170
    },
    {
      "epoch": 0.47685784009936255,
      "grad_norm": 0.37932276725769043,
      "learning_rate": 0.00019140566191972936,
      "loss": 2.5067,
      "step": 23180
    },
    {
      "epoch": 0.47706355961623026,
      "grad_norm": 0.3497503697872162,
      "learning_rate": 0.00019139665193933407,
      "loss": 2.4578,
      "step": 23190
    },
    {
      "epoch": 0.47726927913309797,
      "grad_norm": 0.4124462306499481,
      "learning_rate": 0.00019138763745084,
      "loss": 2.4304,
      "step": 23200
    },
    {
      "epoch": 0.4774749986499657,
      "grad_norm": 0.38077566027641296,
      "learning_rate": 0.0001913786184546918,
      "loss": 2.47,
      "step": 23210
    },
    {
      "epoch": 0.4776807181668334,
      "grad_norm": 0.3843633830547333,
      "learning_rate": 0.00019136959495133428,
      "loss": 2.4093,
      "step": 23220
    },
    {
      "epoch": 0.4778864376837011,
      "grad_norm": 0.5741747617721558,
      "learning_rate": 0.00019136056694121257,
      "loss": 2.4828,
      "step": 23230
    },
    {
      "epoch": 0.4780921572005688,
      "grad_norm": 0.37844017148017883,
      "learning_rate": 0.00019135153442477196,
      "loss": 2.4642,
      "step": 23240
    },
    {
      "epoch": 0.4782978767174365,
      "grad_norm": 0.3849114179611206,
      "learning_rate": 0.00019134249740245792,
      "loss": 2.4291,
      "step": 23250
    },
    {
      "epoch": 0.4785035962343042,
      "grad_norm": 0.4272911846637726,
      "learning_rate": 0.00019133345587471628,
      "loss": 2.4137,
      "step": 23260
    },
    {
      "epoch": 0.478709315751172,
      "grad_norm": 0.38722920417785645,
      "learning_rate": 0.00019132440984199293,
      "loss": 2.4724,
      "step": 23270
    },
    {
      "epoch": 0.4789150352680397,
      "grad_norm": 0.4417971074581146,
      "learning_rate": 0.00019131535930473414,
      "loss": 2.4552,
      "step": 23280
    },
    {
      "epoch": 0.4791207547849074,
      "grad_norm": 0.37579023838043213,
      "learning_rate": 0.00019130630426338627,
      "loss": 2.4789,
      "step": 23290
    },
    {
      "epoch": 0.4793264743017751,
      "grad_norm": 0.37073707580566406,
      "learning_rate": 0.00019129724471839596,
      "loss": 2.4265,
      "step": 23300
    },
    {
      "epoch": 0.4795321938186428,
      "grad_norm": 0.36558997631073,
      "learning_rate": 0.0001912881806702101,
      "loss": 2.4995,
      "step": 23310
    },
    {
      "epoch": 0.4797379133355105,
      "grad_norm": 0.37414073944091797,
      "learning_rate": 0.00019127911211927573,
      "loss": 2.4471,
      "step": 23320
    },
    {
      "epoch": 0.47994363285237823,
      "grad_norm": 0.3994089961051941,
      "learning_rate": 0.0001912700390660402,
      "loss": 2.4122,
      "step": 23330
    },
    {
      "epoch": 0.48014935236924594,
      "grad_norm": 0.3927798271179199,
      "learning_rate": 0.00019126096151095097,
      "loss": 2.4694,
      "step": 23340
    },
    {
      "epoch": 0.48035507188611365,
      "grad_norm": 0.36794978380203247,
      "learning_rate": 0.00019125187945445582,
      "loss": 2.4848,
      "step": 23350
    },
    {
      "epoch": 0.4805607914029814,
      "grad_norm": 0.41114574670791626,
      "learning_rate": 0.00019124279289700274,
      "loss": 2.5298,
      "step": 23360
    },
    {
      "epoch": 0.4807665109198491,
      "grad_norm": 0.35219767689704895,
      "learning_rate": 0.0001912337018390399,
      "loss": 2.436,
      "step": 23370
    },
    {
      "epoch": 0.48097223043671683,
      "grad_norm": 0.3761645555496216,
      "learning_rate": 0.0001912246062810157,
      "loss": 2.4604,
      "step": 23380
    },
    {
      "epoch": 0.48117794995358454,
      "grad_norm": 0.3715677857398987,
      "learning_rate": 0.00019121550622337879,
      "loss": 2.4066,
      "step": 23390
    },
    {
      "epoch": 0.48138366947045225,
      "grad_norm": 0.37527793645858765,
      "learning_rate": 0.000191206401666578,
      "loss": 2.4735,
      "step": 23400
    },
    {
      "epoch": 0.48158938898731996,
      "grad_norm": 0.4573603868484497,
      "learning_rate": 0.00019119729261106246,
      "loss": 2.4253,
      "step": 23410
    },
    {
      "epoch": 0.48179510850418766,
      "grad_norm": 0.3761221766471863,
      "learning_rate": 0.00019118817905728147,
      "loss": 2.3395,
      "step": 23420
    },
    {
      "epoch": 0.48200082802105537,
      "grad_norm": 0.4003273844718933,
      "learning_rate": 0.00019117906100568445,
      "loss": 2.4688,
      "step": 23430
    },
    {
      "epoch": 0.4822065475379231,
      "grad_norm": 0.40390220284461975,
      "learning_rate": 0.00019116993845672128,
      "loss": 2.4336,
      "step": 23440
    },
    {
      "epoch": 0.48241226705479084,
      "grad_norm": 0.36443033814430237,
      "learning_rate": 0.00019116081141084184,
      "loss": 2.387,
      "step": 23450
    },
    {
      "epoch": 0.48261798657165855,
      "grad_norm": 0.3984219431877136,
      "learning_rate": 0.00019115167986849633,
      "loss": 2.4122,
      "step": 23460
    },
    {
      "epoch": 0.48282370608852626,
      "grad_norm": 0.3896242678165436,
      "learning_rate": 0.00019114254383013516,
      "loss": 2.4624,
      "step": 23470
    },
    {
      "epoch": 0.48302942560539397,
      "grad_norm": 0.378885418176651,
      "learning_rate": 0.00019113340329620902,
      "loss": 2.4431,
      "step": 23480
    },
    {
      "epoch": 0.4832351451222617,
      "grad_norm": 0.3631887137889862,
      "learning_rate": 0.00019112425826716867,
      "loss": 2.4519,
      "step": 23490
    },
    {
      "epoch": 0.4834408646391294,
      "grad_norm": 0.3503328859806061,
      "learning_rate": 0.00019111510874346524,
      "loss": 2.4146,
      "step": 23500
    },
    {
      "epoch": 0.4836465841559971,
      "grad_norm": 0.38795244693756104,
      "learning_rate": 0.00019110595472555,
      "loss": 2.3692,
      "step": 23510
    },
    {
      "epoch": 0.4838523036728648,
      "grad_norm": 0.3999864459037781,
      "learning_rate": 0.0001910967962138745,
      "loss": 2.423,
      "step": 23520
    },
    {
      "epoch": 0.4840580231897325,
      "grad_norm": 0.37559911608695984,
      "learning_rate": 0.00019108763320889045,
      "loss": 2.4463,
      "step": 23530
    },
    {
      "epoch": 0.4842637427066003,
      "grad_norm": 0.37720516324043274,
      "learning_rate": 0.00019107846571104985,
      "loss": 2.4793,
      "step": 23540
    },
    {
      "epoch": 0.484469462223468,
      "grad_norm": 0.38144996762275696,
      "learning_rate": 0.00019106929372080482,
      "loss": 2.435,
      "step": 23550
    },
    {
      "epoch": 0.4846751817403357,
      "grad_norm": 0.3956502079963684,
      "learning_rate": 0.0001910601172386078,
      "loss": 2.4535,
      "step": 23560
    },
    {
      "epoch": 0.4848809012572034,
      "grad_norm": 0.34411147236824036,
      "learning_rate": 0.00019105093626491142,
      "loss": 2.4363,
      "step": 23570
    },
    {
      "epoch": 0.4850866207740711,
      "grad_norm": 0.43811696767807007,
      "learning_rate": 0.00019104175080016854,
      "loss": 2.4113,
      "step": 23580
    },
    {
      "epoch": 0.4852923402909388,
      "grad_norm": 0.36028823256492615,
      "learning_rate": 0.0001910325608448322,
      "loss": 2.4663,
      "step": 23590
    },
    {
      "epoch": 0.4854980598078065,
      "grad_norm": 0.4198399484157562,
      "learning_rate": 0.0001910233663993557,
      "loss": 2.4318,
      "step": 23600
    },
    {
      "epoch": 0.48570377932467423,
      "grad_norm": 0.3740788996219635,
      "learning_rate": 0.00019101416746419254,
      "loss": 2.4147,
      "step": 23610
    },
    {
      "epoch": 0.485909498841542,
      "grad_norm": 0.3802519142627716,
      "learning_rate": 0.0001910049640397965,
      "loss": 2.4528,
      "step": 23620
    },
    {
      "epoch": 0.4861152183584097,
      "grad_norm": 0.36929720640182495,
      "learning_rate": 0.00019099575612662145,
      "loss": 2.4959,
      "step": 23630
    },
    {
      "epoch": 0.4863209378752774,
      "grad_norm": 0.3704898953437805,
      "learning_rate": 0.00019098654372512165,
      "loss": 2.4259,
      "step": 23640
    },
    {
      "epoch": 0.4865266573921451,
      "grad_norm": 0.39906564354896545,
      "learning_rate": 0.00019097732683575147,
      "loss": 2.4515,
      "step": 23650
    },
    {
      "epoch": 0.48673237690901283,
      "grad_norm": 0.3709087371826172,
      "learning_rate": 0.0001909681054589655,
      "loss": 2.4354,
      "step": 23660
    },
    {
      "epoch": 0.48693809642588054,
      "grad_norm": 0.36611074209213257,
      "learning_rate": 0.00019095887959521863,
      "loss": 2.4189,
      "step": 23670
    },
    {
      "epoch": 0.48714381594274825,
      "grad_norm": 0.4160977005958557,
      "learning_rate": 0.00019094964924496589,
      "loss": 2.3741,
      "step": 23680
    },
    {
      "epoch": 0.48734953545961596,
      "grad_norm": 0.3998337388038635,
      "learning_rate": 0.00019094041440866256,
      "loss": 2.4764,
      "step": 23690
    },
    {
      "epoch": 0.48755525497648367,
      "grad_norm": 0.43398579955101013,
      "learning_rate": 0.00019093117508676416,
      "loss": 2.4264,
      "step": 23700
    },
    {
      "epoch": 0.48776097449335143,
      "grad_norm": 0.44847777485847473,
      "learning_rate": 0.00019092193127972641,
      "loss": 2.4159,
      "step": 23710
    },
    {
      "epoch": 0.48796669401021914,
      "grad_norm": 0.34461989998817444,
      "learning_rate": 0.00019091268298800524,
      "loss": 2.4353,
      "step": 23720
    },
    {
      "epoch": 0.48817241352708685,
      "grad_norm": 0.41161975264549255,
      "learning_rate": 0.00019090343021205687,
      "loss": 2.4317,
      "step": 23730
    },
    {
      "epoch": 0.48837813304395455,
      "grad_norm": 0.3784305155277252,
      "learning_rate": 0.00019089417295233764,
      "loss": 2.4339,
      "step": 23740
    },
    {
      "epoch": 0.48858385256082226,
      "grad_norm": 0.4181750416755676,
      "learning_rate": 0.00019088491120930413,
      "loss": 2.3985,
      "step": 23750
    },
    {
      "epoch": 0.48878957207768997,
      "grad_norm": 0.43712449073791504,
      "learning_rate": 0.00019087564498341327,
      "loss": 2.4446,
      "step": 23760
    },
    {
      "epoch": 0.4889952915945577,
      "grad_norm": 0.4147895574569702,
      "learning_rate": 0.00019086637427512203,
      "loss": 2.4321,
      "step": 23770
    },
    {
      "epoch": 0.4892010111114254,
      "grad_norm": 0.3639242649078369,
      "learning_rate": 0.00019085709908488768,
      "loss": 2.4517,
      "step": 23780
    },
    {
      "epoch": 0.4894067306282931,
      "grad_norm": 0.3595873713493347,
      "learning_rate": 0.0001908478194131678,
      "loss": 2.4108,
      "step": 23790
    },
    {
      "epoch": 0.48961245014516086,
      "grad_norm": 0.3804573714733124,
      "learning_rate": 0.00019083853526042,
      "loss": 2.4537,
      "step": 23800
    },
    {
      "epoch": 0.48981816966202857,
      "grad_norm": 0.3900098204612732,
      "learning_rate": 0.0001908292466271023,
      "loss": 2.4587,
      "step": 23810
    },
    {
      "epoch": 0.4900238891788963,
      "grad_norm": 0.39959436655044556,
      "learning_rate": 0.0001908199535136728,
      "loss": 2.4582,
      "step": 23820
    },
    {
      "epoch": 0.490229608695764,
      "grad_norm": 0.36433884501457214,
      "learning_rate": 0.00019081065592058992,
      "loss": 2.4266,
      "step": 23830
    },
    {
      "epoch": 0.4904353282126317,
      "grad_norm": 0.3679784834384918,
      "learning_rate": 0.00019080135384831224,
      "loss": 2.4387,
      "step": 23840
    },
    {
      "epoch": 0.4906410477294994,
      "grad_norm": 0.384194940328598,
      "learning_rate": 0.00019079204729729858,
      "loss": 2.4717,
      "step": 23850
    },
    {
      "epoch": 0.4908467672463671,
      "grad_norm": 0.3926975131034851,
      "learning_rate": 0.000190782736268008,
      "loss": 2.3587,
      "step": 23860
    },
    {
      "epoch": 0.4910524867632348,
      "grad_norm": 0.3608333468437195,
      "learning_rate": 0.0001907734207608997,
      "loss": 2.5028,
      "step": 23870
    },
    {
      "epoch": 0.4912582062801025,
      "grad_norm": 0.3859860897064209,
      "learning_rate": 0.00019076410077643326,
      "loss": 2.4971,
      "step": 23880
    },
    {
      "epoch": 0.4914639257969703,
      "grad_norm": 0.42109501361846924,
      "learning_rate": 0.0001907547763150683,
      "loss": 2.471,
      "step": 23890
    },
    {
      "epoch": 0.491669645313838,
      "grad_norm": 0.37956514954566956,
      "learning_rate": 0.0001907454473772648,
      "loss": 2.49,
      "step": 23900
    },
    {
      "epoch": 0.4918753648307057,
      "grad_norm": 0.36707746982574463,
      "learning_rate": 0.00019073611396348288,
      "loss": 2.4129,
      "step": 23910
    },
    {
      "epoch": 0.4920810843475734,
      "grad_norm": 0.39453861117362976,
      "learning_rate": 0.0001907267760741829,
      "loss": 2.4714,
      "step": 23920
    },
    {
      "epoch": 0.4922868038644411,
      "grad_norm": 0.38132208585739136,
      "learning_rate": 0.00019071743370982546,
      "loss": 2.4855,
      "step": 23930
    },
    {
      "epoch": 0.49249252338130883,
      "grad_norm": 0.40518292784690857,
      "learning_rate": 0.00019070808687087138,
      "loss": 2.4365,
      "step": 23940
    },
    {
      "epoch": 0.49269824289817654,
      "grad_norm": 0.3924872875213623,
      "learning_rate": 0.00019069873555778167,
      "loss": 2.428,
      "step": 23950
    },
    {
      "epoch": 0.49290396241504425,
      "grad_norm": 0.4032994508743286,
      "learning_rate": 0.00019068937977101757,
      "loss": 2.5053,
      "step": 23960
    },
    {
      "epoch": 0.49310968193191196,
      "grad_norm": 0.42338767647743225,
      "learning_rate": 0.00019068001951104058,
      "loss": 2.4298,
      "step": 23970
    },
    {
      "epoch": 0.4933154014487797,
      "grad_norm": 0.34419381618499756,
      "learning_rate": 0.00019067065477831237,
      "loss": 2.4632,
      "step": 23980
    },
    {
      "epoch": 0.49352112096564743,
      "grad_norm": 0.3745594620704651,
      "learning_rate": 0.00019066128557329487,
      "loss": 2.4481,
      "step": 23990
    },
    {
      "epoch": 0.49372684048251514,
      "grad_norm": 0.37773990631103516,
      "learning_rate": 0.00019065191189645018,
      "loss": 2.4974,
      "step": 24000
    },
    {
      "epoch": 0.49393255999938285,
      "grad_norm": 0.43355679512023926,
      "learning_rate": 0.00019064253374824066,
      "loss": 2.4461,
      "step": 24010
    },
    {
      "epoch": 0.49413827951625056,
      "grad_norm": 0.3682645857334137,
      "learning_rate": 0.0001906331511291289,
      "loss": 2.411,
      "step": 24020
    },
    {
      "epoch": 0.49434399903311826,
      "grad_norm": 0.3717178404331207,
      "learning_rate": 0.00019062376403957768,
      "loss": 2.4488,
      "step": 24030
    },
    {
      "epoch": 0.494549718549986,
      "grad_norm": 0.39253076910972595,
      "learning_rate": 0.00019061437248005003,
      "loss": 2.457,
      "step": 24040
    },
    {
      "epoch": 0.4947554380668537,
      "grad_norm": 0.3749598562717438,
      "learning_rate": 0.00019060497645100917,
      "loss": 2.468,
      "step": 24050
    },
    {
      "epoch": 0.4949611575837214,
      "grad_norm": 0.36704203486442566,
      "learning_rate": 0.00019059557595291858,
      "loss": 2.4764,
      "step": 24060
    },
    {
      "epoch": 0.49516687710058915,
      "grad_norm": 0.3866095542907715,
      "learning_rate": 0.0001905861709862419,
      "loss": 2.4826,
      "step": 24070
    },
    {
      "epoch": 0.49537259661745686,
      "grad_norm": 0.3725580871105194,
      "learning_rate": 0.00019057676155144303,
      "loss": 2.4221,
      "step": 24080
    },
    {
      "epoch": 0.49557831613432457,
      "grad_norm": 0.3506687879562378,
      "learning_rate": 0.00019056734764898612,
      "loss": 2.4109,
      "step": 24090
    },
    {
      "epoch": 0.4957840356511923,
      "grad_norm": 0.3914563059806824,
      "learning_rate": 0.00019055792927933545,
      "loss": 2.441,
      "step": 24100
    },
    {
      "epoch": 0.49598975516806,
      "grad_norm": 0.3740425109863281,
      "learning_rate": 0.00019054850644295563,
      "loss": 2.467,
      "step": 24110
    },
    {
      "epoch": 0.4961954746849277,
      "grad_norm": 0.37194398045539856,
      "learning_rate": 0.00019053907914031142,
      "loss": 2.4748,
      "step": 24120
    },
    {
      "epoch": 0.4964011942017954,
      "grad_norm": 0.3778069019317627,
      "learning_rate": 0.0001905296473718678,
      "loss": 2.4675,
      "step": 24130
    },
    {
      "epoch": 0.4966069137186631,
      "grad_norm": 0.35493776202201843,
      "learning_rate": 0.00019052021113809002,
      "loss": 2.4621,
      "step": 24140
    },
    {
      "epoch": 0.4968126332355308,
      "grad_norm": 0.35673195123672485,
      "learning_rate": 0.0001905107704394435,
      "loss": 2.4243,
      "step": 24150
    },
    {
      "epoch": 0.4970183527523986,
      "grad_norm": 0.3923870325088501,
      "learning_rate": 0.0001905013252763939,
      "loss": 2.4503,
      "step": 24160
    },
    {
      "epoch": 0.4972240722692663,
      "grad_norm": 0.395338237285614,
      "learning_rate": 0.00019049187564940709,
      "loss": 2.4673,
      "step": 24170
    },
    {
      "epoch": 0.497429791786134,
      "grad_norm": 0.40697363018989563,
      "learning_rate": 0.00019048242155894916,
      "loss": 2.4581,
      "step": 24180
    },
    {
      "epoch": 0.4976355113030017,
      "grad_norm": 0.4056661128997803,
      "learning_rate": 0.00019047296300548649,
      "loss": 2.4655,
      "step": 24190
    },
    {
      "epoch": 0.4978412308198694,
      "grad_norm": 0.36998918652534485,
      "learning_rate": 0.0001904634999894855,
      "loss": 2.4918,
      "step": 24200
    },
    {
      "epoch": 0.4980469503367371,
      "grad_norm": 0.3626502454280853,
      "learning_rate": 0.0001904540325114131,
      "loss": 2.4683,
      "step": 24210
    },
    {
      "epoch": 0.49825266985360483,
      "grad_norm": 0.3797110915184021,
      "learning_rate": 0.00019044456057173617,
      "loss": 2.4452,
      "step": 24220
    },
    {
      "epoch": 0.49845838937047254,
      "grad_norm": 0.3708201050758362,
      "learning_rate": 0.00019043508417092192,
      "loss": 2.5045,
      "step": 24230
    },
    {
      "epoch": 0.49866410888734025,
      "grad_norm": 0.3720179796218872,
      "learning_rate": 0.00019042560330943776,
      "loss": 2.4468,
      "step": 24240
    },
    {
      "epoch": 0.498869828404208,
      "grad_norm": 0.3629175126552582,
      "learning_rate": 0.00019041611798775135,
      "loss": 2.4643,
      "step": 24250
    },
    {
      "epoch": 0.4990755479210757,
      "grad_norm": 0.3965127170085907,
      "learning_rate": 0.0001904066282063306,
      "loss": 2.4552,
      "step": 24260
    },
    {
      "epoch": 0.49928126743794343,
      "grad_norm": 0.36881786584854126,
      "learning_rate": 0.00019039713396564348,
      "loss": 2.4296,
      "step": 24270
    },
    {
      "epoch": 0.49948698695481114,
      "grad_norm": 0.36732104420661926,
      "learning_rate": 0.0001903876352661584,
      "loss": 2.4208,
      "step": 24280
    },
    {
      "epoch": 0.49969270647167885,
      "grad_norm": 0.43975839018821716,
      "learning_rate": 0.0001903781321083438,
      "loss": 2.4237,
      "step": 24290
    },
    {
      "epoch": 0.49989842598854656,
      "grad_norm": 0.4130559265613556,
      "learning_rate": 0.00019036862449266844,
      "loss": 2.4196,
      "step": 24300
    },
    {
      "epoch": 0.5001041455054143,
      "grad_norm": 0.3644273281097412,
      "learning_rate": 0.00019035911241960127,
      "loss": 2.4259,
      "step": 24310
    },
    {
      "epoch": 0.500309865022282,
      "grad_norm": 0.4036881923675537,
      "learning_rate": 0.0001903495958896115,
      "loss": 2.4629,
      "step": 24320
    },
    {
      "epoch": 0.5005155845391497,
      "grad_norm": 0.36156749725341797,
      "learning_rate": 0.00019034007490316852,
      "loss": 2.4274,
      "step": 24330
    },
    {
      "epoch": 0.5007213040560174,
      "grad_norm": 0.38819801807403564,
      "learning_rate": 0.0001903305494607419,
      "loss": 2.4008,
      "step": 24340
    },
    {
      "epoch": 0.5009270235728851,
      "grad_norm": 0.3930051028728485,
      "learning_rate": 0.00019032101956280157,
      "loss": 2.4198,
      "step": 24350
    },
    {
      "epoch": 0.5011327430897529,
      "grad_norm": 0.3725009262561798,
      "learning_rate": 0.00019031148520981753,
      "loss": 2.4,
      "step": 24360
    },
    {
      "epoch": 0.5013384626066205,
      "grad_norm": 0.35834142565727234,
      "learning_rate": 0.00019030194640226003,
      "loss": 2.4362,
      "step": 24370
    },
    {
      "epoch": 0.5015441821234883,
      "grad_norm": 0.42309123277664185,
      "learning_rate": 0.00019029240314059962,
      "loss": 2.3959,
      "step": 24380
    },
    {
      "epoch": 0.501749901640356,
      "grad_norm": 0.34818440675735474,
      "learning_rate": 0.00019028285542530698,
      "loss": 2.4207,
      "step": 24390
    },
    {
      "epoch": 0.5019556211572237,
      "grad_norm": 0.38622379302978516,
      "learning_rate": 0.00019027330325685306,
      "loss": 2.4074,
      "step": 24400
    },
    {
      "epoch": 0.5021613406740915,
      "grad_norm": 0.4027600884437561,
      "learning_rate": 0.00019026374663570907,
      "loss": 2.4873,
      "step": 24410
    },
    {
      "epoch": 0.5023670601909591,
      "grad_norm": 0.4302334785461426,
      "learning_rate": 0.0001902541855623463,
      "loss": 2.4799,
      "step": 24420
    },
    {
      "epoch": 0.5025727797078269,
      "grad_norm": 0.38732224702835083,
      "learning_rate": 0.0001902446200372364,
      "loss": 2.4022,
      "step": 24430
    },
    {
      "epoch": 0.5027784992246945,
      "grad_norm": 0.36712706089019775,
      "learning_rate": 0.00019023505006085113,
      "loss": 2.4532,
      "step": 24440
    },
    {
      "epoch": 0.5029842187415623,
      "grad_norm": 0.3871903717517853,
      "learning_rate": 0.00019022547563366258,
      "loss": 2.4507,
      "step": 24450
    },
    {
      "epoch": 0.50318993825843,
      "grad_norm": 0.38326847553253174,
      "learning_rate": 0.000190215896756143,
      "loss": 2.3864,
      "step": 24460
    },
    {
      "epoch": 0.5033956577752977,
      "grad_norm": 0.46147993206977844,
      "learning_rate": 0.00019020631342876488,
      "loss": 2.4374,
      "step": 24470
    },
    {
      "epoch": 0.5036013772921655,
      "grad_norm": 0.36647555232048035,
      "learning_rate": 0.00019019672565200084,
      "loss": 2.4536,
      "step": 24480
    },
    {
      "epoch": 0.5038070968090331,
      "grad_norm": 0.5469500422477722,
      "learning_rate": 0.00019018713342632384,
      "loss": 2.3614,
      "step": 24490
    },
    {
      "epoch": 0.5040128163259009,
      "grad_norm": 0.3711521327495575,
      "learning_rate": 0.000190177536752207,
      "loss": 2.4498,
      "step": 24500
    },
    {
      "epoch": 0.5042185358427685,
      "grad_norm": 0.554725170135498,
      "learning_rate": 0.0001901679356301237,
      "loss": 2.4143,
      "step": 24510
    },
    {
      "epoch": 0.5044242553596363,
      "grad_norm": 0.3655921220779419,
      "learning_rate": 0.00019015833006054747,
      "loss": 2.4471,
      "step": 24520
    },
    {
      "epoch": 0.504629974876504,
      "grad_norm": 0.3667537569999695,
      "learning_rate": 0.00019014872004395218,
      "loss": 2.437,
      "step": 24530
    },
    {
      "epoch": 0.5048356943933717,
      "grad_norm": 0.4385596215724945,
      "learning_rate": 0.00019013910558081173,
      "loss": 2.4268,
      "step": 24540
    },
    {
      "epoch": 0.5050414139102394,
      "grad_norm": 0.3661216199398041,
      "learning_rate": 0.0001901294866716004,
      "loss": 2.4232,
      "step": 24550
    },
    {
      "epoch": 0.5052471334271071,
      "grad_norm": 0.43565836548805237,
      "learning_rate": 0.00019011986331679266,
      "loss": 2.3937,
      "step": 24560
    },
    {
      "epoch": 0.5054528529439749,
      "grad_norm": 0.42178329825401306,
      "learning_rate": 0.00019011023551686315,
      "loss": 2.4866,
      "step": 24570
    },
    {
      "epoch": 0.5056585724608426,
      "grad_norm": 0.40601637959480286,
      "learning_rate": 0.0001901006032722868,
      "loss": 2.4555,
      "step": 24580
    },
    {
      "epoch": 0.5058642919777103,
      "grad_norm": 0.3889425992965698,
      "learning_rate": 0.0001900909665835386,
      "loss": 2.4565,
      "step": 24590
    },
    {
      "epoch": 0.506070011494578,
      "grad_norm": 0.37931138277053833,
      "learning_rate": 0.000190081325451094,
      "loss": 2.4269,
      "step": 24600
    },
    {
      "epoch": 0.5062757310114457,
      "grad_norm": 0.36843064427375793,
      "learning_rate": 0.00019007167987542851,
      "loss": 2.4255,
      "step": 24610
    },
    {
      "epoch": 0.5064814505283134,
      "grad_norm": 0.391567200422287,
      "learning_rate": 0.00019006202985701786,
      "loss": 2.4313,
      "step": 24620
    },
    {
      "epoch": 0.5066871700451812,
      "grad_norm": 0.35542115569114685,
      "learning_rate": 0.00019005237539633808,
      "loss": 2.4425,
      "step": 24630
    },
    {
      "epoch": 0.5068928895620488,
      "grad_norm": 0.38348376750946045,
      "learning_rate": 0.00019004271649386532,
      "loss": 2.4947,
      "step": 24640
    },
    {
      "epoch": 0.5070986090789166,
      "grad_norm": 0.3665724992752075,
      "learning_rate": 0.00019003305315007607,
      "loss": 2.4631,
      "step": 24650
    },
    {
      "epoch": 0.5073043285957843,
      "grad_norm": 0.39730188250541687,
      "learning_rate": 0.0001900233853654469,
      "loss": 2.4178,
      "step": 24660
    },
    {
      "epoch": 0.507510048112652,
      "grad_norm": 0.3714587688446045,
      "learning_rate": 0.00019001371314045467,
      "loss": 2.405,
      "step": 24670
    },
    {
      "epoch": 0.5077157676295198,
      "grad_norm": 0.37437957525253296,
      "learning_rate": 0.00019000403647557654,
      "loss": 2.4604,
      "step": 24680
    },
    {
      "epoch": 0.5079214871463874,
      "grad_norm": 0.3755320906639099,
      "learning_rate": 0.00018999435537128974,
      "loss": 2.4025,
      "step": 24690
    },
    {
      "epoch": 0.5081272066632552,
      "grad_norm": 0.3889118731021881,
      "learning_rate": 0.00018998466982807178,
      "loss": 2.4026,
      "step": 24700
    },
    {
      "epoch": 0.5083329261801228,
      "grad_norm": 0.36038273572921753,
      "learning_rate": 0.00018997497984640045,
      "loss": 2.4758,
      "step": 24710
    },
    {
      "epoch": 0.5085386456969906,
      "grad_norm": 0.4172095060348511,
      "learning_rate": 0.00018996528542675366,
      "loss": 2.4703,
      "step": 24720
    },
    {
      "epoch": 0.5087443652138582,
      "grad_norm": 0.38449564576148987,
      "learning_rate": 0.00018995558656960957,
      "loss": 2.4302,
      "step": 24730
    },
    {
      "epoch": 0.508950084730726,
      "grad_norm": 0.3728943467140198,
      "learning_rate": 0.0001899458832754466,
      "loss": 2.418,
      "step": 24740
    },
    {
      "epoch": 0.5091558042475938,
      "grad_norm": 0.3630376160144806,
      "learning_rate": 0.00018993617554474335,
      "loss": 2.3872,
      "step": 24750
    },
    {
      "epoch": 0.5093615237644614,
      "grad_norm": 0.39603084325790405,
      "learning_rate": 0.00018992646337797866,
      "loss": 2.4384,
      "step": 24760
    },
    {
      "epoch": 0.5095672432813292,
      "grad_norm": 0.37573274970054626,
      "learning_rate": 0.00018991674677563158,
      "loss": 2.3823,
      "step": 24770
    },
    {
      "epoch": 0.5097729627981968,
      "grad_norm": 0.3595615327358246,
      "learning_rate": 0.00018990702573818137,
      "loss": 2.4118,
      "step": 24780
    },
    {
      "epoch": 0.5099786823150646,
      "grad_norm": 0.38486534357070923,
      "learning_rate": 0.0001898973002661075,
      "loss": 2.3954,
      "step": 24790
    },
    {
      "epoch": 0.5101844018319323,
      "grad_norm": 0.39741647243499756,
      "learning_rate": 0.00018988757035988968,
      "loss": 2.4883,
      "step": 24800
    },
    {
      "epoch": 0.5103901213488,
      "grad_norm": 0.3702680468559265,
      "learning_rate": 0.00018987783602000789,
      "loss": 2.4464,
      "step": 24810
    },
    {
      "epoch": 0.5105958408656677,
      "grad_norm": 0.3984440863132477,
      "learning_rate": 0.00018986809724694218,
      "loss": 2.4375,
      "step": 24820
    },
    {
      "epoch": 0.5108015603825354,
      "grad_norm": 0.3876340091228485,
      "learning_rate": 0.00018985835404117295,
      "loss": 2.4693,
      "step": 24830
    },
    {
      "epoch": 0.5110072798994032,
      "grad_norm": 0.9124240875244141,
      "learning_rate": 0.00018984860640318082,
      "loss": 2.4978,
      "step": 24840
    },
    {
      "epoch": 0.5112129994162709,
      "grad_norm": 0.3728630244731903,
      "learning_rate": 0.00018983885433344652,
      "loss": 2.453,
      "step": 24850
    },
    {
      "epoch": 0.5114187189331386,
      "grad_norm": 0.4721980392932892,
      "learning_rate": 0.00018982909783245113,
      "loss": 2.4449,
      "step": 24860
    },
    {
      "epoch": 0.5116244384500063,
      "grad_norm": 0.39371806383132935,
      "learning_rate": 0.00018981933690067585,
      "loss": 2.4332,
      "step": 24870
    },
    {
      "epoch": 0.511830157966874,
      "grad_norm": 0.38504737615585327,
      "learning_rate": 0.00018980957153860213,
      "loss": 2.4205,
      "step": 24880
    },
    {
      "epoch": 0.5120358774837417,
      "grad_norm": 0.36120331287384033,
      "learning_rate": 0.00018979980174671163,
      "loss": 2.4837,
      "step": 24890
    },
    {
      "epoch": 0.5122415970006094,
      "grad_norm": 0.399310827255249,
      "learning_rate": 0.0001897900275254863,
      "loss": 2.4126,
      "step": 24900
    },
    {
      "epoch": 0.5124473165174772,
      "grad_norm": 0.3724451959133148,
      "learning_rate": 0.00018978024887540817,
      "loss": 2.4146,
      "step": 24910
    },
    {
      "epoch": 0.5126530360343449,
      "grad_norm": 0.44575297832489014,
      "learning_rate": 0.00018977046579695962,
      "loss": 2.4158,
      "step": 24920
    },
    {
      "epoch": 0.5128587555512126,
      "grad_norm": 0.42045071721076965,
      "learning_rate": 0.00018976067829062318,
      "loss": 2.4513,
      "step": 24930
    },
    {
      "epoch": 0.5130644750680803,
      "grad_norm": 0.3914599120616913,
      "learning_rate": 0.00018975088635688164,
      "loss": 2.4527,
      "step": 24940
    },
    {
      "epoch": 0.513270194584948,
      "grad_norm": 0.3958163559436798,
      "learning_rate": 0.00018974108999621794,
      "loss": 2.3937,
      "step": 24950
    },
    {
      "epoch": 0.5134759141018157,
      "grad_norm": 0.3756549060344696,
      "learning_rate": 0.0001897312892091153,
      "loss": 2.4786,
      "step": 24960
    },
    {
      "epoch": 0.5136816336186835,
      "grad_norm": 0.37964317202568054,
      "learning_rate": 0.00018972148399605714,
      "loss": 2.4091,
      "step": 24970
    },
    {
      "epoch": 0.5138873531355511,
      "grad_norm": 0.3715061843395233,
      "learning_rate": 0.00018971167435752708,
      "loss": 2.4635,
      "step": 24980
    },
    {
      "epoch": 0.5140930726524189,
      "grad_norm": 0.36533600091934204,
      "learning_rate": 0.00018970186029400904,
      "loss": 2.4553,
      "step": 24990
    },
    {
      "epoch": 0.5142987921692866,
      "grad_norm": 0.3761371672153473,
      "learning_rate": 0.00018969204180598702,
      "loss": 2.4531,
      "step": 25000
    },
    {
      "epoch": 0.5145045116861543,
      "grad_norm": 0.4003237187862396,
      "learning_rate": 0.00018968221889394533,
      "loss": 2.448,
      "step": 25010
    },
    {
      "epoch": 0.5147102312030221,
      "grad_norm": 0.39673689007759094,
      "learning_rate": 0.00018967239155836852,
      "loss": 2.4222,
      "step": 25020
    },
    {
      "epoch": 0.5149159507198897,
      "grad_norm": 0.37891659140586853,
      "learning_rate": 0.00018966255979974126,
      "loss": 2.4629,
      "step": 25030
    },
    {
      "epoch": 0.5151216702367575,
      "grad_norm": 0.4436202943325043,
      "learning_rate": 0.00018965272361854852,
      "loss": 2.4891,
      "step": 25040
    },
    {
      "epoch": 0.5153273897536251,
      "grad_norm": 0.394908607006073,
      "learning_rate": 0.00018964288301527552,
      "loss": 2.4556,
      "step": 25050
    },
    {
      "epoch": 0.5155331092704929,
      "grad_norm": 0.3762213885784149,
      "learning_rate": 0.00018963303799040756,
      "loss": 2.4218,
      "step": 25060
    },
    {
      "epoch": 0.5157388287873605,
      "grad_norm": 0.37751978635787964,
      "learning_rate": 0.0001896231885444303,
      "loss": 2.4192,
      "step": 25070
    },
    {
      "epoch": 0.5159445483042283,
      "grad_norm": 0.44067126512527466,
      "learning_rate": 0.00018961333467782955,
      "loss": 2.4408,
      "step": 25080
    },
    {
      "epoch": 0.5161502678210961,
      "grad_norm": 0.3691501021385193,
      "learning_rate": 0.00018960347639109126,
      "loss": 2.462,
      "step": 25090
    },
    {
      "epoch": 0.5163559873379637,
      "grad_norm": 0.43080002069473267,
      "learning_rate": 0.00018959361368470182,
      "loss": 2.4409,
      "step": 25100
    },
    {
      "epoch": 0.5165617068548315,
      "grad_norm": 0.38231226801872253,
      "learning_rate": 0.00018958374655914762,
      "loss": 2.4179,
      "step": 25110
    },
    {
      "epoch": 0.5167674263716991,
      "grad_norm": 0.374629408121109,
      "learning_rate": 0.0001895738750149154,
      "loss": 2.5017,
      "step": 25120
    },
    {
      "epoch": 0.5169731458885669,
      "grad_norm": 0.40490293502807617,
      "learning_rate": 0.00018956399905249203,
      "loss": 2.4562,
      "step": 25130
    },
    {
      "epoch": 0.5171788654054346,
      "grad_norm": 0.3906741738319397,
      "learning_rate": 0.00018955411867236467,
      "loss": 2.4328,
      "step": 25140
    },
    {
      "epoch": 0.5173845849223023,
      "grad_norm": 0.3909740149974823,
      "learning_rate": 0.00018954423387502063,
      "loss": 2.4689,
      "step": 25150
    },
    {
      "epoch": 0.51759030443917,
      "grad_norm": 0.37773048877716064,
      "learning_rate": 0.00018953434466094748,
      "loss": 2.4324,
      "step": 25160
    },
    {
      "epoch": 0.5177960239560377,
      "grad_norm": 0.3900298476219177,
      "learning_rate": 0.00018952445103063303,
      "loss": 2.4756,
      "step": 25170
    },
    {
      "epoch": 0.5180017434729055,
      "grad_norm": 0.41286057233810425,
      "learning_rate": 0.00018951455298456526,
      "loss": 2.4568,
      "step": 25180
    },
    {
      "epoch": 0.5182074629897732,
      "grad_norm": 0.3958570659160614,
      "learning_rate": 0.00018950465052323238,
      "loss": 2.4654,
      "step": 25190
    },
    {
      "epoch": 0.5184131825066409,
      "grad_norm": 0.3773837983608246,
      "learning_rate": 0.00018949474364712284,
      "loss": 2.48,
      "step": 25200
    },
    {
      "epoch": 0.5186189020235086,
      "grad_norm": 0.36481574177742004,
      "learning_rate": 0.00018948483235672528,
      "loss": 2.422,
      "step": 25210
    },
    {
      "epoch": 0.5188246215403763,
      "grad_norm": 0.3642657697200775,
      "learning_rate": 0.00018947491665252856,
      "loss": 2.405,
      "step": 25220
    },
    {
      "epoch": 0.519030341057244,
      "grad_norm": 0.3812009394168854,
      "learning_rate": 0.0001894649965350218,
      "loss": 2.4676,
      "step": 25230
    },
    {
      "epoch": 0.5192360605741118,
      "grad_norm": 0.3818783462047577,
      "learning_rate": 0.00018945507200469428,
      "loss": 2.4834,
      "step": 25240
    },
    {
      "epoch": 0.5194417800909794,
      "grad_norm": 0.3718312084674835,
      "learning_rate": 0.00018944514306203553,
      "loss": 2.4872,
      "step": 25250
    },
    {
      "epoch": 0.5196474996078472,
      "grad_norm": 0.3577752709388733,
      "learning_rate": 0.00018943520970753528,
      "loss": 2.4281,
      "step": 25260
    },
    {
      "epoch": 0.5198532191247149,
      "grad_norm": 0.36772027611732483,
      "learning_rate": 0.00018942527194168348,
      "loss": 2.4189,
      "step": 25270
    },
    {
      "epoch": 0.5200589386415826,
      "grad_norm": 0.36261165142059326,
      "learning_rate": 0.0001894153297649704,
      "loss": 2.4573,
      "step": 25280
    },
    {
      "epoch": 0.5202646581584504,
      "grad_norm": 0.37841954827308655,
      "learning_rate": 0.00018940538317788628,
      "loss": 2.4516,
      "step": 25290
    },
    {
      "epoch": 0.520470377675318,
      "grad_norm": 0.456975519657135,
      "learning_rate": 0.00018939543218092183,
      "loss": 2.4209,
      "step": 25300
    },
    {
      "epoch": 0.5206760971921858,
      "grad_norm": 0.3539336621761322,
      "learning_rate": 0.00018938547677456788,
      "loss": 2.4676,
      "step": 25310
    },
    {
      "epoch": 0.5208818167090534,
      "grad_norm": 0.3885372579097748,
      "learning_rate": 0.00018937551695931542,
      "loss": 2.4427,
      "step": 25320
    },
    {
      "epoch": 0.5210875362259212,
      "grad_norm": 0.3718542754650116,
      "learning_rate": 0.0001893655527356558,
      "loss": 2.3611,
      "step": 25330
    },
    {
      "epoch": 0.5212932557427888,
      "grad_norm": 0.39563295245170593,
      "learning_rate": 0.0001893555841040804,
      "loss": 2.4734,
      "step": 25340
    },
    {
      "epoch": 0.5214989752596566,
      "grad_norm": 0.35217395424842834,
      "learning_rate": 0.00018934561106508097,
      "loss": 2.4131,
      "step": 25350
    },
    {
      "epoch": 0.5217046947765244,
      "grad_norm": 0.4796237051486969,
      "learning_rate": 0.00018933563361914942,
      "loss": 2.4782,
      "step": 25360
    },
    {
      "epoch": 0.521910414293392,
      "grad_norm": 0.4082213044166565,
      "learning_rate": 0.0001893256517667779,
      "loss": 2.4484,
      "step": 25370
    },
    {
      "epoch": 0.5221161338102598,
      "grad_norm": 0.4554308354854584,
      "learning_rate": 0.00018931566550845874,
      "loss": 2.4634,
      "step": 25380
    },
    {
      "epoch": 0.5223218533271274,
      "grad_norm": 0.3815059959888458,
      "learning_rate": 0.00018930567484468452,
      "loss": 2.4447,
      "step": 25390
    },
    {
      "epoch": 0.5225275728439952,
      "grad_norm": 0.34576931595802307,
      "learning_rate": 0.000189295679775948,
      "loss": 2.4704,
      "step": 25400
    },
    {
      "epoch": 0.5227332923608629,
      "grad_norm": 0.3582591712474823,
      "learning_rate": 0.0001892856803027422,
      "loss": 2.3674,
      "step": 25410
    },
    {
      "epoch": 0.5229390118777306,
      "grad_norm": 0.36344316601753235,
      "learning_rate": 0.00018927567642556037,
      "loss": 2.4815,
      "step": 25420
    },
    {
      "epoch": 0.5231447313945983,
      "grad_norm": 0.3714950382709503,
      "learning_rate": 0.00018926566814489587,
      "loss": 2.447,
      "step": 25430
    },
    {
      "epoch": 0.523350450911466,
      "grad_norm": 0.379854679107666,
      "learning_rate": 0.00018925565546124246,
      "loss": 2.4457,
      "step": 25440
    },
    {
      "epoch": 0.5235561704283338,
      "grad_norm": 0.4091777801513672,
      "learning_rate": 0.00018924563837509388,
      "loss": 2.4436,
      "step": 25450
    },
    {
      "epoch": 0.5237618899452015,
      "grad_norm": 0.4097766876220703,
      "learning_rate": 0.00018923561688694433,
      "loss": 2.3903,
      "step": 25460
    },
    {
      "epoch": 0.5239676094620692,
      "grad_norm": 0.3859109580516815,
      "learning_rate": 0.00018922559099728806,
      "loss": 2.4493,
      "step": 25470
    },
    {
      "epoch": 0.5241733289789369,
      "grad_norm": 0.38933536410331726,
      "learning_rate": 0.00018921556070661963,
      "loss": 2.3855,
      "step": 25480
    },
    {
      "epoch": 0.5243790484958046,
      "grad_norm": 0.3592914938926697,
      "learning_rate": 0.00018920552601543373,
      "loss": 2.4708,
      "step": 25490
    },
    {
      "epoch": 0.5245847680126723,
      "grad_norm": 0.4086747169494629,
      "learning_rate": 0.00018919548692422538,
      "loss": 2.4909,
      "step": 25500
    },
    {
      "epoch": 0.52479048752954,
      "grad_norm": 0.36085420846939087,
      "learning_rate": 0.0001891854434334897,
      "loss": 2.5054,
      "step": 25510
    },
    {
      "epoch": 0.5249962070464077,
      "grad_norm": 0.39454472064971924,
      "learning_rate": 0.00018917539554372207,
      "loss": 2.45,
      "step": 25520
    },
    {
      "epoch": 0.5252019265632755,
      "grad_norm": 0.3964739739894867,
      "learning_rate": 0.00018916534325541816,
      "loss": 2.4113,
      "step": 25530
    },
    {
      "epoch": 0.5254076460801432,
      "grad_norm": 0.3947203457355499,
      "learning_rate": 0.00018915528656907374,
      "loss": 2.4198,
      "step": 25540
    },
    {
      "epoch": 0.5256133655970109,
      "grad_norm": 0.4315751791000366,
      "learning_rate": 0.00018914522548518488,
      "loss": 2.4963,
      "step": 25550
    },
    {
      "epoch": 0.5258190851138786,
      "grad_norm": 0.37649866938591003,
      "learning_rate": 0.00018913516000424783,
      "loss": 2.4683,
      "step": 25560
    },
    {
      "epoch": 0.5260248046307463,
      "grad_norm": 0.3737703263759613,
      "learning_rate": 0.00018912509012675909,
      "loss": 2.506,
      "step": 25570
    },
    {
      "epoch": 0.5262305241476141,
      "grad_norm": 0.3624182343482971,
      "learning_rate": 0.00018911501585321526,
      "loss": 2.4183,
      "step": 25580
    },
    {
      "epoch": 0.5264362436644817,
      "grad_norm": 0.38917508721351624,
      "learning_rate": 0.00018910493718411338,
      "loss": 2.4208,
      "step": 25590
    },
    {
      "epoch": 0.5266419631813495,
      "grad_norm": 0.3617664575576782,
      "learning_rate": 0.00018909485411995047,
      "loss": 2.4295,
      "step": 25600
    },
    {
      "epoch": 0.5268476826982171,
      "grad_norm": 0.37881705164909363,
      "learning_rate": 0.00018908476666122393,
      "loss": 2.4481,
      "step": 25610
    },
    {
      "epoch": 0.5270534022150849,
      "grad_norm": 0.38143104314804077,
      "learning_rate": 0.00018907467480843134,
      "loss": 2.4622,
      "step": 25620
    },
    {
      "epoch": 0.5272591217319527,
      "grad_norm": 0.3812709450721741,
      "learning_rate": 0.0001890645785620704,
      "loss": 2.41,
      "step": 25630
    },
    {
      "epoch": 0.5274648412488203,
      "grad_norm": 0.3599015474319458,
      "learning_rate": 0.00018905447792263915,
      "loss": 2.4762,
      "step": 25640
    },
    {
      "epoch": 0.5276705607656881,
      "grad_norm": 0.3892843723297119,
      "learning_rate": 0.0001890443728906358,
      "loss": 2.4322,
      "step": 25650
    },
    {
      "epoch": 0.5278762802825557,
      "grad_norm": 0.3669866919517517,
      "learning_rate": 0.00018903426346655877,
      "loss": 2.4285,
      "step": 25660
    },
    {
      "epoch": 0.5280819997994235,
      "grad_norm": 0.4153532087802887,
      "learning_rate": 0.0001890241496509067,
      "loss": 2.4111,
      "step": 25670
    },
    {
      "epoch": 0.5282877193162911,
      "grad_norm": 0.40190446376800537,
      "learning_rate": 0.00018901403144417844,
      "loss": 2.4156,
      "step": 25680
    },
    {
      "epoch": 0.5284934388331589,
      "grad_norm": 0.391587495803833,
      "learning_rate": 0.0001890039088468731,
      "loss": 2.4319,
      "step": 25690
    },
    {
      "epoch": 0.5286991583500266,
      "grad_norm": 0.37987709045410156,
      "learning_rate": 0.00018899378185948992,
      "loss": 2.389,
      "step": 25700
    },
    {
      "epoch": 0.5289048778668943,
      "grad_norm": 0.3914327025413513,
      "learning_rate": 0.00018898365048252845,
      "loss": 2.4417,
      "step": 25710
    },
    {
      "epoch": 0.5291105973837621,
      "grad_norm": 0.4105369746685028,
      "learning_rate": 0.00018897351471648842,
      "loss": 2.418,
      "step": 25720
    },
    {
      "epoch": 0.5293163169006297,
      "grad_norm": 0.3640022873878479,
      "learning_rate": 0.00018896337456186975,
      "loss": 2.4276,
      "step": 25730
    },
    {
      "epoch": 0.5295220364174975,
      "grad_norm": 0.3376922905445099,
      "learning_rate": 0.00018895323001917258,
      "loss": 2.392,
      "step": 25740
    },
    {
      "epoch": 0.5297277559343652,
      "grad_norm": 0.3771110475063324,
      "learning_rate": 0.00018894308108889733,
      "loss": 2.4735,
      "step": 25750
    },
    {
      "epoch": 0.5299334754512329,
      "grad_norm": 0.37921664118766785,
      "learning_rate": 0.0001889329277715446,
      "loss": 2.4713,
      "step": 25760
    },
    {
      "epoch": 0.5301391949681006,
      "grad_norm": 0.39175429940223694,
      "learning_rate": 0.00018892277006761512,
      "loss": 2.4593,
      "step": 25770
    },
    {
      "epoch": 0.5303449144849683,
      "grad_norm": 0.4223167896270752,
      "learning_rate": 0.00018891260797760997,
      "loss": 2.4258,
      "step": 25780
    },
    {
      "epoch": 0.530550634001836,
      "grad_norm": 0.3668065369129181,
      "learning_rate": 0.0001889024415020304,
      "loss": 2.3904,
      "step": 25790
    },
    {
      "epoch": 0.5307563535187038,
      "grad_norm": 0.3682992160320282,
      "learning_rate": 0.00018889227064137787,
      "loss": 2.456,
      "step": 25800
    },
    {
      "epoch": 0.5309620730355715,
      "grad_norm": 0.34677714109420776,
      "learning_rate": 0.000188882095396154,
      "loss": 2.4548,
      "step": 25810
    },
    {
      "epoch": 0.5311677925524392,
      "grad_norm": 0.35894808173179626,
      "learning_rate": 0.00018887191576686071,
      "loss": 2.4913,
      "step": 25820
    },
    {
      "epoch": 0.5313735120693069,
      "grad_norm": 0.3845618665218353,
      "learning_rate": 0.00018886173175400016,
      "loss": 2.4085,
      "step": 25830
    },
    {
      "epoch": 0.5315792315861746,
      "grad_norm": 0.375167578458786,
      "learning_rate": 0.00018885154335807457,
      "loss": 2.456,
      "step": 25840
    },
    {
      "epoch": 0.5317849511030424,
      "grad_norm": 0.4027516841888428,
      "learning_rate": 0.00018884135057958655,
      "loss": 2.4437,
      "step": 25850
    },
    {
      "epoch": 0.53199067061991,
      "grad_norm": 0.44380947947502136,
      "learning_rate": 0.00018883115341903882,
      "loss": 2.4291,
      "step": 25860
    },
    {
      "epoch": 0.5321963901367778,
      "grad_norm": 0.38106128573417664,
      "learning_rate": 0.0001888209518769344,
      "loss": 2.4592,
      "step": 25870
    },
    {
      "epoch": 0.5324021096536454,
      "grad_norm": 0.41027557849884033,
      "learning_rate": 0.0001888107459537764,
      "loss": 2.4938,
      "step": 25880
    },
    {
      "epoch": 0.5326078291705132,
      "grad_norm": 0.36821648478507996,
      "learning_rate": 0.00018880053565006825,
      "loss": 2.4576,
      "step": 25890
    },
    {
      "epoch": 0.532813548687381,
      "grad_norm": 0.4212264120578766,
      "learning_rate": 0.00018879032096631363,
      "loss": 2.4335,
      "step": 25900
    },
    {
      "epoch": 0.5330192682042486,
      "grad_norm": 0.37913376092910767,
      "learning_rate": 0.0001887801019030163,
      "loss": 2.4518,
      "step": 25910
    },
    {
      "epoch": 0.5332249877211164,
      "grad_norm": 0.3691868484020233,
      "learning_rate": 0.00018876987846068035,
      "loss": 2.3767,
      "step": 25920
    },
    {
      "epoch": 0.533430707237984,
      "grad_norm": 0.3836260139942169,
      "learning_rate": 0.00018875965063981,
      "loss": 2.4009,
      "step": 25930
    },
    {
      "epoch": 0.5336364267548518,
      "grad_norm": 0.3759170174598694,
      "learning_rate": 0.00018874941844090982,
      "loss": 2.4027,
      "step": 25940
    },
    {
      "epoch": 0.5338421462717194,
      "grad_norm": 0.384425550699234,
      "learning_rate": 0.0001887391818644844,
      "loss": 2.4424,
      "step": 25950
    },
    {
      "epoch": 0.5340478657885872,
      "grad_norm": 0.3873162567615509,
      "learning_rate": 0.00018872894091103874,
      "loss": 2.428,
      "step": 25960
    },
    {
      "epoch": 0.5342535853054549,
      "grad_norm": 0.39471304416656494,
      "learning_rate": 0.00018871869558107794,
      "loss": 2.398,
      "step": 25970
    },
    {
      "epoch": 0.5344593048223226,
      "grad_norm": 0.37289345264434814,
      "learning_rate": 0.0001887084458751073,
      "loss": 2.4723,
      "step": 25980
    },
    {
      "epoch": 0.5346650243391904,
      "grad_norm": 0.40960967540740967,
      "learning_rate": 0.0001886981917936325,
      "loss": 2.476,
      "step": 25990
    },
    {
      "epoch": 0.534870743856058,
      "grad_norm": 0.369452565908432,
      "learning_rate": 0.0001886879333371592,
      "loss": 2.4461,
      "step": 26000
    },
    {
      "epoch": 0.5350764633729258,
      "grad_norm": 0.3902966380119324,
      "learning_rate": 0.00018867767050619345,
      "loss": 2.3952,
      "step": 26010
    },
    {
      "epoch": 0.5352821828897935,
      "grad_norm": 0.40932807326316833,
      "learning_rate": 0.00018866740330124144,
      "loss": 2.4198,
      "step": 26020
    },
    {
      "epoch": 0.5354879024066612,
      "grad_norm": 0.4144479036331177,
      "learning_rate": 0.00018865713172280964,
      "loss": 2.4821,
      "step": 26030
    },
    {
      "epoch": 0.5356936219235289,
      "grad_norm": 0.3665230870246887,
      "learning_rate": 0.0001886468557714046,
      "loss": 2.4275,
      "step": 26040
    },
    {
      "epoch": 0.5358993414403966,
      "grad_norm": 0.35948461294174194,
      "learning_rate": 0.0001886365754475333,
      "loss": 2.3881,
      "step": 26050
    },
    {
      "epoch": 0.5361050609572643,
      "grad_norm": 0.5105525851249695,
      "learning_rate": 0.0001886262907517027,
      "loss": 2.4585,
      "step": 26060
    },
    {
      "epoch": 0.536310780474132,
      "grad_norm": 0.38726916909217834,
      "learning_rate": 0.00018861600168442014,
      "loss": 2.4174,
      "step": 26070
    },
    {
      "epoch": 0.5365164999909998,
      "grad_norm": 0.38550832867622375,
      "learning_rate": 0.0001886057082461931,
      "loss": 2.4258,
      "step": 26080
    },
    {
      "epoch": 0.5367222195078675,
      "grad_norm": 0.3831501007080078,
      "learning_rate": 0.00018859541043752932,
      "loss": 2.4523,
      "step": 26090
    },
    {
      "epoch": 0.5369279390247352,
      "grad_norm": 0.3779900074005127,
      "learning_rate": 0.00018858510825893677,
      "loss": 2.4651,
      "step": 26100
    },
    {
      "epoch": 0.5371336585416029,
      "grad_norm": 0.39920544624328613,
      "learning_rate": 0.00018857480171092353,
      "loss": 2.4448,
      "step": 26110
    },
    {
      "epoch": 0.5373393780584707,
      "grad_norm": 0.3856554627418518,
      "learning_rate": 0.00018856449079399798,
      "loss": 2.4316,
      "step": 26120
    },
    {
      "epoch": 0.5375450975753383,
      "grad_norm": 0.3672202527523041,
      "learning_rate": 0.00018855417550866875,
      "loss": 2.4723,
      "step": 26130
    },
    {
      "epoch": 0.5377508170922061,
      "grad_norm": 0.3289141356945038,
      "learning_rate": 0.0001885438558554446,
      "loss": 2.3674,
      "step": 26140
    },
    {
      "epoch": 0.5379565366090737,
      "grad_norm": 0.3682941198348999,
      "learning_rate": 0.00018853353183483453,
      "loss": 2.4001,
      "step": 26150
    },
    {
      "epoch": 0.5381622561259415,
      "grad_norm": 0.40696772933006287,
      "learning_rate": 0.0001885232034473478,
      "loss": 2.4357,
      "step": 26160
    },
    {
      "epoch": 0.5383679756428092,
      "grad_norm": 0.3436955511569977,
      "learning_rate": 0.0001885128706934938,
      "loss": 2.4144,
      "step": 26170
    },
    {
      "epoch": 0.5385736951596769,
      "grad_norm": 0.4062771499156952,
      "learning_rate": 0.00018850253357378228,
      "loss": 2.4742,
      "step": 26180
    },
    {
      "epoch": 0.5387794146765447,
      "grad_norm": 0.3709343671798706,
      "learning_rate": 0.00018849219208872303,
      "loss": 2.4386,
      "step": 26190
    },
    {
      "epoch": 0.5389851341934123,
      "grad_norm": 0.3976139426231384,
      "learning_rate": 0.00018848184623882616,
      "loss": 2.4819,
      "step": 26200
    },
    {
      "epoch": 0.5391908537102801,
      "grad_norm": 0.3818102478981018,
      "learning_rate": 0.00018847149602460204,
      "loss": 2.4242,
      "step": 26210
    },
    {
      "epoch": 0.5393965732271477,
      "grad_norm": 0.37382110953330994,
      "learning_rate": 0.00018846114144656107,
      "loss": 2.4564,
      "step": 26220
    },
    {
      "epoch": 0.5396022927440155,
      "grad_norm": 0.38390088081359863,
      "learning_rate": 0.00018845078250521405,
      "loss": 2.4115,
      "step": 26230
    },
    {
      "epoch": 0.5398080122608832,
      "grad_norm": 0.41106393933296204,
      "learning_rate": 0.00018844041920107197,
      "loss": 2.4266,
      "step": 26240
    },
    {
      "epoch": 0.5400137317777509,
      "grad_norm": 0.40690022706985474,
      "learning_rate": 0.00018843005153464592,
      "loss": 2.4237,
      "step": 26250
    },
    {
      "epoch": 0.5402194512946187,
      "grad_norm": 0.3654024004936218,
      "learning_rate": 0.0001884196795064473,
      "loss": 2.4271,
      "step": 26260
    },
    {
      "epoch": 0.5404251708114863,
      "grad_norm": 0.36408886313438416,
      "learning_rate": 0.00018840930311698779,
      "loss": 2.4854,
      "step": 26270
    },
    {
      "epoch": 0.5406308903283541,
      "grad_norm": 0.375570148229599,
      "learning_rate": 0.00018839892236677909,
      "loss": 2.4622,
      "step": 26280
    },
    {
      "epoch": 0.5408366098452217,
      "grad_norm": 0.3943130075931549,
      "learning_rate": 0.00018838853725633325,
      "loss": 2.4139,
      "step": 26290
    },
    {
      "epoch": 0.5410423293620895,
      "grad_norm": 0.36507123708724976,
      "learning_rate": 0.00018837814778616254,
      "loss": 2.4611,
      "step": 26300
    },
    {
      "epoch": 0.5412480488789572,
      "grad_norm": 0.39189815521240234,
      "learning_rate": 0.00018836775395677938,
      "loss": 2.422,
      "step": 26310
    },
    {
      "epoch": 0.5414537683958249,
      "grad_norm": 0.4101976752281189,
      "learning_rate": 0.00018835735576869648,
      "loss": 2.4231,
      "step": 26320
    },
    {
      "epoch": 0.5416594879126926,
      "grad_norm": 0.38524508476257324,
      "learning_rate": 0.00018834695322242673,
      "loss": 2.4891,
      "step": 26330
    },
    {
      "epoch": 0.5418652074295603,
      "grad_norm": 0.3769758641719818,
      "learning_rate": 0.00018833654631848318,
      "loss": 2.3957,
      "step": 26340
    },
    {
      "epoch": 0.5420709269464281,
      "grad_norm": 0.38404685258865356,
      "learning_rate": 0.0001883261350573792,
      "loss": 2.3561,
      "step": 26350
    },
    {
      "epoch": 0.5422766464632958,
      "grad_norm": 0.3584584593772888,
      "learning_rate": 0.0001883157194396283,
      "loss": 2.473,
      "step": 26360
    },
    {
      "epoch": 0.5424823659801635,
      "grad_norm": 0.3969776928424835,
      "learning_rate": 0.0001883052994657442,
      "loss": 2.4414,
      "step": 26370
    },
    {
      "epoch": 0.5426880854970312,
      "grad_norm": 0.3845493197441101,
      "learning_rate": 0.00018829487513624088,
      "loss": 2.5116,
      "step": 26380
    },
    {
      "epoch": 0.542893805013899,
      "grad_norm": 0.3655172288417816,
      "learning_rate": 0.00018828444645163254,
      "loss": 2.4625,
      "step": 26390
    },
    {
      "epoch": 0.5430995245307666,
      "grad_norm": 0.39644914865493774,
      "learning_rate": 0.00018827401341243354,
      "loss": 2.4179,
      "step": 26400
    },
    {
      "epoch": 0.5433052440476344,
      "grad_norm": 0.425587922334671,
      "learning_rate": 0.0001882635760191585,
      "loss": 2.4699,
      "step": 26410
    },
    {
      "epoch": 0.543510963564502,
      "grad_norm": 0.3703971803188324,
      "learning_rate": 0.00018825313427232223,
      "loss": 2.4641,
      "step": 26420
    },
    {
      "epoch": 0.5437166830813698,
      "grad_norm": 0.3751305639743805,
      "learning_rate": 0.00018824268817243977,
      "loss": 2.4806,
      "step": 26430
    },
    {
      "epoch": 0.5439224025982375,
      "grad_norm": 0.3411124646663666,
      "learning_rate": 0.00018823223772002637,
      "loss": 2.4353,
      "step": 26440
    },
    {
      "epoch": 0.5441281221151052,
      "grad_norm": 0.3506609797477722,
      "learning_rate": 0.00018822178291559748,
      "loss": 2.4207,
      "step": 26450
    },
    {
      "epoch": 0.544333841631973,
      "grad_norm": 0.3695090711116791,
      "learning_rate": 0.00018821132375966882,
      "loss": 2.4125,
      "step": 26460
    },
    {
      "epoch": 0.5445395611488406,
      "grad_norm": 0.35835421085357666,
      "learning_rate": 0.0001882008602527562,
      "loss": 2.442,
      "step": 26470
    },
    {
      "epoch": 0.5447452806657084,
      "grad_norm": 0.3980492949485779,
      "learning_rate": 0.00018819039239537582,
      "loss": 2.4663,
      "step": 26480
    },
    {
      "epoch": 0.544951000182576,
      "grad_norm": 0.4025491774082184,
      "learning_rate": 0.00018817992018804397,
      "loss": 2.4504,
      "step": 26490
    },
    {
      "epoch": 0.5451567196994438,
      "grad_norm": 0.38480374217033386,
      "learning_rate": 0.00018816944363127713,
      "loss": 2.4719,
      "step": 26500
    },
    {
      "epoch": 0.5453624392163114,
      "grad_norm": 0.3670880198478699,
      "learning_rate": 0.00018815896272559215,
      "loss": 2.4841,
      "step": 26510
    },
    {
      "epoch": 0.5455681587331792,
      "grad_norm": 0.38197046518325806,
      "learning_rate": 0.00018814847747150592,
      "loss": 2.4306,
      "step": 26520
    },
    {
      "epoch": 0.545773878250047,
      "grad_norm": 0.3958851993083954,
      "learning_rate": 0.00018813798786953566,
      "loss": 2.4748,
      "step": 26530
    },
    {
      "epoch": 0.5459795977669146,
      "grad_norm": 0.39995473623275757,
      "learning_rate": 0.00018812749392019878,
      "loss": 2.3962,
      "step": 26540
    },
    {
      "epoch": 0.5461853172837824,
      "grad_norm": 0.3870236873626709,
      "learning_rate": 0.00018811699562401284,
      "loss": 2.416,
      "step": 26550
    },
    {
      "epoch": 0.54639103680065,
      "grad_norm": 0.4418366253376007,
      "learning_rate": 0.00018810649298149566,
      "loss": 2.4206,
      "step": 26560
    },
    {
      "epoch": 0.5465967563175178,
      "grad_norm": 0.3774235248565674,
      "learning_rate": 0.00018809598599316534,
      "loss": 2.483,
      "step": 26570
    },
    {
      "epoch": 0.5468024758343855,
      "grad_norm": 0.3982478678226471,
      "learning_rate": 0.00018808547465954008,
      "loss": 2.4596,
      "step": 26580
    },
    {
      "epoch": 0.5470081953512532,
      "grad_norm": 0.405482679605484,
      "learning_rate": 0.00018807495898113836,
      "loss": 2.4259,
      "step": 26590
    },
    {
      "epoch": 0.547213914868121,
      "grad_norm": 0.36071741580963135,
      "learning_rate": 0.00018806443895847888,
      "loss": 2.4551,
      "step": 26600
    },
    {
      "epoch": 0.5474196343849886,
      "grad_norm": 0.42455318570137024,
      "learning_rate": 0.0001880539145920805,
      "loss": 2.4258,
      "step": 26610
    },
    {
      "epoch": 0.5476253539018564,
      "grad_norm": 0.3928264081478119,
      "learning_rate": 0.00018804338588246237,
      "loss": 2.4288,
      "step": 26620
    },
    {
      "epoch": 0.5478310734187241,
      "grad_norm": 0.3912959396839142,
      "learning_rate": 0.00018803285283014378,
      "loss": 2.451,
      "step": 26630
    },
    {
      "epoch": 0.5480367929355918,
      "grad_norm": 0.38542166352272034,
      "learning_rate": 0.00018802231543564428,
      "loss": 2.4209,
      "step": 26640
    },
    {
      "epoch": 0.5482425124524595,
      "grad_norm": 0.39331021904945374,
      "learning_rate": 0.00018801177369948363,
      "loss": 2.4634,
      "step": 26650
    },
    {
      "epoch": 0.5484482319693272,
      "grad_norm": 0.5834528207778931,
      "learning_rate": 0.00018800122762218177,
      "loss": 2.395,
      "step": 26660
    },
    {
      "epoch": 0.5486539514861949,
      "grad_norm": 0.3792893886566162,
      "learning_rate": 0.00018799067720425892,
      "loss": 2.4892,
      "step": 26670
    },
    {
      "epoch": 0.5488596710030627,
      "grad_norm": 0.38916242122650146,
      "learning_rate": 0.00018798012244623547,
      "loss": 2.4605,
      "step": 26680
    },
    {
      "epoch": 0.5490653905199304,
      "grad_norm": 0.4028914272785187,
      "learning_rate": 0.00018796956334863197,
      "loss": 2.481,
      "step": 26690
    },
    {
      "epoch": 0.5492711100367981,
      "grad_norm": 0.39503219723701477,
      "learning_rate": 0.00018795899991196934,
      "loss": 2.4273,
      "step": 26700
    },
    {
      "epoch": 0.5494768295536658,
      "grad_norm": 0.4119515120983124,
      "learning_rate": 0.00018794843213676852,
      "loss": 2.4444,
      "step": 26710
    },
    {
      "epoch": 0.5496825490705335,
      "grad_norm": 0.3643924295902252,
      "learning_rate": 0.00018793786002355083,
      "loss": 2.4335,
      "step": 26720
    },
    {
      "epoch": 0.5498882685874013,
      "grad_norm": 0.39757850766181946,
      "learning_rate": 0.00018792728357283772,
      "loss": 2.4454,
      "step": 26730
    },
    {
      "epoch": 0.5500939881042689,
      "grad_norm": 0.389267235994339,
      "learning_rate": 0.00018791670278515082,
      "loss": 2.444,
      "step": 26740
    },
    {
      "epoch": 0.5502997076211367,
      "grad_norm": 0.4729081690311432,
      "learning_rate": 0.0001879061176610121,
      "loss": 2.4364,
      "step": 26750
    },
    {
      "epoch": 0.5505054271380043,
      "grad_norm": 0.4032052755355835,
      "learning_rate": 0.00018789552820094358,
      "loss": 2.4642,
      "step": 26760
    },
    {
      "epoch": 0.5507111466548721,
      "grad_norm": 0.39473286271095276,
      "learning_rate": 0.00018788493440546767,
      "loss": 2.4148,
      "step": 26770
    },
    {
      "epoch": 0.5509168661717398,
      "grad_norm": 0.37330928444862366,
      "learning_rate": 0.0001878743362751068,
      "loss": 2.4295,
      "step": 26780
    },
    {
      "epoch": 0.5511225856886075,
      "grad_norm": 0.37938258051872253,
      "learning_rate": 0.00018786373381038385,
      "loss": 2.4283,
      "step": 26790
    },
    {
      "epoch": 0.5513283052054753,
      "grad_norm": 0.38639071583747864,
      "learning_rate": 0.0001878531270118217,
      "loss": 2.4249,
      "step": 26800
    },
    {
      "epoch": 0.5515340247223429,
      "grad_norm": 0.3620093762874603,
      "learning_rate": 0.0001878425158799435,
      "loss": 2.4783,
      "step": 26810
    },
    {
      "epoch": 0.5517397442392107,
      "grad_norm": 0.3973589539527893,
      "learning_rate": 0.0001878319004152727,
      "loss": 2.4726,
      "step": 26820
    },
    {
      "epoch": 0.5519454637560783,
      "grad_norm": 0.3729395270347595,
      "learning_rate": 0.00018782128061833287,
      "loss": 2.4196,
      "step": 26830
    },
    {
      "epoch": 0.5521511832729461,
      "grad_norm": 0.39678147435188293,
      "learning_rate": 0.00018781065648964783,
      "loss": 2.3527,
      "step": 26840
    },
    {
      "epoch": 0.5523569027898138,
      "grad_norm": 0.4437926411628723,
      "learning_rate": 0.00018780002802974162,
      "loss": 2.4351,
      "step": 26850
    },
    {
      "epoch": 0.5525626223066815,
      "grad_norm": 0.37143662571907043,
      "learning_rate": 0.0001877893952391385,
      "loss": 2.3873,
      "step": 26860
    },
    {
      "epoch": 0.5527683418235493,
      "grad_norm": 0.36411482095718384,
      "learning_rate": 0.0001877787581183629,
      "loss": 2.4393,
      "step": 26870
    },
    {
      "epoch": 0.5529740613404169,
      "grad_norm": 0.37839818000793457,
      "learning_rate": 0.0001877681166679395,
      "loss": 2.4819,
      "step": 26880
    },
    {
      "epoch": 0.5531797808572847,
      "grad_norm": 0.38818448781967163,
      "learning_rate": 0.00018775747088839318,
      "loss": 2.4442,
      "step": 26890
    },
    {
      "epoch": 0.5533855003741524,
      "grad_norm": 0.4088214337825775,
      "learning_rate": 0.00018774682078024903,
      "loss": 2.4398,
      "step": 26900
    },
    {
      "epoch": 0.5535912198910201,
      "grad_norm": 0.39633452892303467,
      "learning_rate": 0.00018773616634403236,
      "loss": 2.4558,
      "step": 26910
    },
    {
      "epoch": 0.5537969394078878,
      "grad_norm": 0.37055641412734985,
      "learning_rate": 0.00018772550758026877,
      "loss": 2.4552,
      "step": 26920
    },
    {
      "epoch": 0.5540026589247555,
      "grad_norm": 0.3795718252658844,
      "learning_rate": 0.0001877148444894839,
      "loss": 2.435,
      "step": 26930
    },
    {
      "epoch": 0.5542083784416232,
      "grad_norm": 0.42451003193855286,
      "learning_rate": 0.0001877041770722037,
      "loss": 2.5296,
      "step": 26940
    },
    {
      "epoch": 0.554414097958491,
      "grad_norm": 0.35059893131256104,
      "learning_rate": 0.00018769350532895444,
      "loss": 2.4205,
      "step": 26950
    },
    {
      "epoch": 0.5546198174753587,
      "grad_norm": 0.4143986999988556,
      "learning_rate": 0.00018768282926026242,
      "loss": 2.4389,
      "step": 26960
    },
    {
      "epoch": 0.5548255369922264,
      "grad_norm": 0.3735322952270508,
      "learning_rate": 0.00018767214886665422,
      "loss": 2.4544,
      "step": 26970
    },
    {
      "epoch": 0.5550312565090941,
      "grad_norm": 0.3765604496002197,
      "learning_rate": 0.0001876614641486567,
      "loss": 2.4091,
      "step": 26980
    },
    {
      "epoch": 0.5552369760259618,
      "grad_norm": 0.38468489050865173,
      "learning_rate": 0.00018765077510679684,
      "loss": 2.4927,
      "step": 26990
    },
    {
      "epoch": 0.5554426955428295,
      "grad_norm": 0.37892553210258484,
      "learning_rate": 0.0001876400817416019,
      "loss": 2.4726,
      "step": 27000
    },
    {
      "epoch": 0.5556484150596972,
      "grad_norm": 0.4040905237197876,
      "learning_rate": 0.00018762938405359925,
      "loss": 2.4126,
      "step": 27010
    },
    {
      "epoch": 0.555854134576565,
      "grad_norm": 0.38174331188201904,
      "learning_rate": 0.00018761868204331665,
      "loss": 2.4781,
      "step": 27020
    },
    {
      "epoch": 0.5560598540934326,
      "grad_norm": 0.367057204246521,
      "learning_rate": 0.00018760797571128196,
      "loss": 2.4135,
      "step": 27030
    },
    {
      "epoch": 0.5562655736103004,
      "grad_norm": 0.3609898090362549,
      "learning_rate": 0.0001875972650580232,
      "loss": 2.4341,
      "step": 27040
    },
    {
      "epoch": 0.5564712931271681,
      "grad_norm": 0.3790821433067322,
      "learning_rate": 0.00018758655008406868,
      "loss": 2.4688,
      "step": 27050
    },
    {
      "epoch": 0.5566770126440358,
      "grad_norm": 0.35562998056411743,
      "learning_rate": 0.00018757583078994697,
      "loss": 2.4234,
      "step": 27060
    },
    {
      "epoch": 0.5568827321609036,
      "grad_norm": 0.4148580729961395,
      "learning_rate": 0.00018756510717618676,
      "loss": 2.4184,
      "step": 27070
    },
    {
      "epoch": 0.5570884516777712,
      "grad_norm": 0.3859809935092926,
      "learning_rate": 0.00018755437924331697,
      "loss": 2.3874,
      "step": 27080
    },
    {
      "epoch": 0.557294171194639,
      "grad_norm": 0.385621577501297,
      "learning_rate": 0.00018754364699186677,
      "loss": 2.4231,
      "step": 27090
    },
    {
      "epoch": 0.5574998907115066,
      "grad_norm": 0.3926932215690613,
      "learning_rate": 0.00018753291042236553,
      "loss": 2.4518,
      "step": 27100
    },
    {
      "epoch": 0.5577056102283744,
      "grad_norm": 0.38834264874458313,
      "learning_rate": 0.0001875221695353428,
      "loss": 2.3923,
      "step": 27110
    },
    {
      "epoch": 0.557911329745242,
      "grad_norm": 0.40062591433525085,
      "learning_rate": 0.0001875114243313284,
      "loss": 2.4136,
      "step": 27120
    },
    {
      "epoch": 0.5581170492621098,
      "grad_norm": 0.356090784072876,
      "learning_rate": 0.0001875006748108523,
      "loss": 2.404,
      "step": 27130
    },
    {
      "epoch": 0.5583227687789776,
      "grad_norm": 0.3733060956001282,
      "learning_rate": 0.00018748992097444473,
      "loss": 2.4043,
      "step": 27140
    },
    {
      "epoch": 0.5585284882958452,
      "grad_norm": 0.38164255023002625,
      "learning_rate": 0.00018747916282263612,
      "loss": 2.4785,
      "step": 27150
    },
    {
      "epoch": 0.558734207812713,
      "grad_norm": 0.38278913497924805,
      "learning_rate": 0.00018746840035595714,
      "loss": 2.4103,
      "step": 27160
    },
    {
      "epoch": 0.5589399273295806,
      "grad_norm": 0.35322508215904236,
      "learning_rate": 0.00018745763357493858,
      "loss": 2.4076,
      "step": 27170
    },
    {
      "epoch": 0.5591456468464484,
      "grad_norm": 0.3900378346443176,
      "learning_rate": 0.00018744686248011157,
      "loss": 2.4933,
      "step": 27180
    },
    {
      "epoch": 0.5593513663633161,
      "grad_norm": 0.40604689717292786,
      "learning_rate": 0.00018743608707200737,
      "loss": 2.4058,
      "step": 27190
    },
    {
      "epoch": 0.5595570858801838,
      "grad_norm": 0.36590924859046936,
      "learning_rate": 0.00018742530735115745,
      "loss": 2.4251,
      "step": 27200
    },
    {
      "epoch": 0.5597628053970515,
      "grad_norm": 0.3846677541732788,
      "learning_rate": 0.00018741452331809354,
      "loss": 2.4277,
      "step": 27210
    },
    {
      "epoch": 0.5599685249139192,
      "grad_norm": 0.4034499228000641,
      "learning_rate": 0.0001874037349733475,
      "loss": 2.4027,
      "step": 27220
    },
    {
      "epoch": 0.560174244430787,
      "grad_norm": 0.38919875025749207,
      "learning_rate": 0.00018739294231745154,
      "loss": 2.5069,
      "step": 27230
    },
    {
      "epoch": 0.5603799639476547,
      "grad_norm": 0.39084213972091675,
      "learning_rate": 0.00018738214535093798,
      "loss": 2.4742,
      "step": 27240
    },
    {
      "epoch": 0.5605856834645224,
      "grad_norm": 0.41964060068130493,
      "learning_rate": 0.0001873713440743394,
      "loss": 2.4309,
      "step": 27250
    },
    {
      "epoch": 0.5607914029813901,
      "grad_norm": 0.3832035958766937,
      "learning_rate": 0.00018736053848818847,
      "loss": 2.4469,
      "step": 27260
    },
    {
      "epoch": 0.5609971224982578,
      "grad_norm": 0.36510393023490906,
      "learning_rate": 0.00018734972859301826,
      "loss": 2.4172,
      "step": 27270
    },
    {
      "epoch": 0.5612028420151255,
      "grad_norm": 0.4144015312194824,
      "learning_rate": 0.00018733891438936192,
      "loss": 2.4478,
      "step": 27280
    },
    {
      "epoch": 0.5614085615319933,
      "grad_norm": 0.3718787729740143,
      "learning_rate": 0.00018732809587775287,
      "loss": 2.4688,
      "step": 27290
    },
    {
      "epoch": 0.5616142810488609,
      "grad_norm": 0.38913920521736145,
      "learning_rate": 0.00018731727305872474,
      "loss": 2.4468,
      "step": 27300
    },
    {
      "epoch": 0.5618200005657287,
      "grad_norm": 0.44970840215682983,
      "learning_rate": 0.0001873064459328114,
      "loss": 2.3972,
      "step": 27310
    },
    {
      "epoch": 0.5620257200825964,
      "grad_norm": 0.3812231719493866,
      "learning_rate": 0.0001872956145005468,
      "loss": 2.4468,
      "step": 27320
    },
    {
      "epoch": 0.5622314395994641,
      "grad_norm": 0.36606982350349426,
      "learning_rate": 0.00018728477876246523,
      "loss": 2.4524,
      "step": 27330
    },
    {
      "epoch": 0.5624371591163319,
      "grad_norm": 0.3987066447734833,
      "learning_rate": 0.00018727393871910118,
      "loss": 2.4421,
      "step": 27340
    },
    {
      "epoch": 0.5626428786331995,
      "grad_norm": 0.38816916942596436,
      "learning_rate": 0.00018726309437098933,
      "loss": 2.4001,
      "step": 27350
    },
    {
      "epoch": 0.5628485981500673,
      "grad_norm": 0.3714582920074463,
      "learning_rate": 0.00018725224571866455,
      "loss": 2.4461,
      "step": 27360
    },
    {
      "epoch": 0.5630543176669349,
      "grad_norm": 0.3427800238132477,
      "learning_rate": 0.00018724139276266196,
      "loss": 2.4741,
      "step": 27370
    },
    {
      "epoch": 0.5632600371838027,
      "grad_norm": 0.4092024564743042,
      "learning_rate": 0.0001872305355035169,
      "loss": 2.4765,
      "step": 27380
    },
    {
      "epoch": 0.5634657567006703,
      "grad_norm": 0.35255053639411926,
      "learning_rate": 0.00018721967394176483,
      "loss": 2.4236,
      "step": 27390
    },
    {
      "epoch": 0.5636714762175381,
      "grad_norm": 0.4377248287200928,
      "learning_rate": 0.00018720880807794157,
      "loss": 2.4061,
      "step": 27400
    },
    {
      "epoch": 0.5638771957344059,
      "grad_norm": 0.34974750876426697,
      "learning_rate": 0.000187197937912583,
      "loss": 2.3883,
      "step": 27410
    },
    {
      "epoch": 0.5640829152512735,
      "grad_norm": 0.38395997881889343,
      "learning_rate": 0.00018718706344622536,
      "loss": 2.4739,
      "step": 27420
    },
    {
      "epoch": 0.5642886347681413,
      "grad_norm": 0.3872870206832886,
      "learning_rate": 0.00018717618467940499,
      "loss": 2.3886,
      "step": 27430
    },
    {
      "epoch": 0.5644943542850089,
      "grad_norm": 0.36617904901504517,
      "learning_rate": 0.00018716530161265847,
      "loss": 2.4234,
      "step": 27440
    },
    {
      "epoch": 0.5647000738018767,
      "grad_norm": 0.40647268295288086,
      "learning_rate": 0.00018715441424652261,
      "loss": 2.4921,
      "step": 27450
    },
    {
      "epoch": 0.5649057933187444,
      "grad_norm": 0.3737659752368927,
      "learning_rate": 0.00018714352258153444,
      "loss": 2.4071,
      "step": 27460
    },
    {
      "epoch": 0.5651115128356121,
      "grad_norm": 0.38990387320518494,
      "learning_rate": 0.00018713262661823116,
      "loss": 2.4461,
      "step": 27470
    },
    {
      "epoch": 0.5653172323524798,
      "grad_norm": 0.410542756319046,
      "learning_rate": 0.00018712172635715023,
      "loss": 2.4497,
      "step": 27480
    },
    {
      "epoch": 0.5655229518693475,
      "grad_norm": 0.3541526794433594,
      "learning_rate": 0.0001871108217988293,
      "loss": 2.4396,
      "step": 27490
    },
    {
      "epoch": 0.5657286713862153,
      "grad_norm": 0.38423237204551697,
      "learning_rate": 0.00018709991294380625,
      "loss": 2.4856,
      "step": 27500
    },
    {
      "epoch": 0.565934390903083,
      "grad_norm": 0.3715328276157379,
      "learning_rate": 0.0001870889997926191,
      "loss": 2.4146,
      "step": 27510
    },
    {
      "epoch": 0.5661401104199507,
      "grad_norm": 0.3810306787490845,
      "learning_rate": 0.0001870780823458062,
      "loss": 2.4618,
      "step": 27520
    },
    {
      "epoch": 0.5663458299368184,
      "grad_norm": 0.3837217092514038,
      "learning_rate": 0.00018706716060390598,
      "loss": 2.4123,
      "step": 27530
    },
    {
      "epoch": 0.5665515494536861,
      "grad_norm": 0.37273284792900085,
      "learning_rate": 0.00018705623456745723,
      "loss": 2.4275,
      "step": 27540
    },
    {
      "epoch": 0.5667572689705538,
      "grad_norm": 0.4227108359336853,
      "learning_rate": 0.0001870453042369988,
      "loss": 2.4156,
      "step": 27550
    },
    {
      "epoch": 0.5669629884874215,
      "grad_norm": 0.38338205218315125,
      "learning_rate": 0.00018703436961306984,
      "loss": 2.4595,
      "step": 27560
    },
    {
      "epoch": 0.5671687080042892,
      "grad_norm": 0.36729738116264343,
      "learning_rate": 0.0001870234306962097,
      "loss": 2.4614,
      "step": 27570
    },
    {
      "epoch": 0.567374427521157,
      "grad_norm": 0.3877127468585968,
      "learning_rate": 0.000187012487486958,
      "loss": 2.4372,
      "step": 27580
    },
    {
      "epoch": 0.5675801470380247,
      "grad_norm": 0.37857961654663086,
      "learning_rate": 0.0001870015399858544,
      "loss": 2.4037,
      "step": 27590
    },
    {
      "epoch": 0.5677858665548924,
      "grad_norm": 0.4471276104450226,
      "learning_rate": 0.00018699058819343897,
      "loss": 2.4691,
      "step": 27600
    },
    {
      "epoch": 0.5679915860717601,
      "grad_norm": 0.3836642801761627,
      "learning_rate": 0.00018697963211025185,
      "loss": 2.4641,
      "step": 27610
    },
    {
      "epoch": 0.5681973055886278,
      "grad_norm": 0.3869706392288208,
      "learning_rate": 0.00018696867173683342,
      "loss": 2.4602,
      "step": 27620
    },
    {
      "epoch": 0.5684030251054956,
      "grad_norm": 0.3873477280139923,
      "learning_rate": 0.00018695770707372443,
      "loss": 2.4367,
      "step": 27630
    },
    {
      "epoch": 0.5686087446223632,
      "grad_norm": 0.3870142698287964,
      "learning_rate": 0.00018694673812146551,
      "loss": 2.3982,
      "step": 27640
    },
    {
      "epoch": 0.568814464139231,
      "grad_norm": 0.4263654947280884,
      "learning_rate": 0.00018693576488059788,
      "loss": 2.3948,
      "step": 27650
    },
    {
      "epoch": 0.5690201836560986,
      "grad_norm": 0.38754865527153015,
      "learning_rate": 0.00018692478735166267,
      "loss": 2.4656,
      "step": 27660
    },
    {
      "epoch": 0.5692259031729664,
      "grad_norm": 0.40308964252471924,
      "learning_rate": 0.00018691380553520143,
      "loss": 2.45,
      "step": 27670
    },
    {
      "epoch": 0.5694316226898342,
      "grad_norm": 0.3586154282093048,
      "learning_rate": 0.00018690281943175573,
      "loss": 2.397,
      "step": 27680
    },
    {
      "epoch": 0.5696373422067018,
      "grad_norm": 0.37797659635543823,
      "learning_rate": 0.00018689182904186753,
      "loss": 2.4595,
      "step": 27690
    },
    {
      "epoch": 0.5698430617235696,
      "grad_norm": 0.3662189841270447,
      "learning_rate": 0.0001868808343660789,
      "loss": 2.4318,
      "step": 27700
    },
    {
      "epoch": 0.5700487812404372,
      "grad_norm": 0.4193200469017029,
      "learning_rate": 0.00018686983540493218,
      "loss": 2.4499,
      "step": 27710
    },
    {
      "epoch": 0.570254500757305,
      "grad_norm": 0.39213886857032776,
      "learning_rate": 0.00018685883215896983,
      "loss": 2.4307,
      "step": 27720
    },
    {
      "epoch": 0.5704602202741726,
      "grad_norm": 0.38739898800849915,
      "learning_rate": 0.00018684782462873462,
      "loss": 2.4121,
      "step": 27730
    },
    {
      "epoch": 0.5706659397910404,
      "grad_norm": 0.40539661049842834,
      "learning_rate": 0.0001868368128147695,
      "loss": 2.4114,
      "step": 27740
    },
    {
      "epoch": 0.5708716593079081,
      "grad_norm": 0.3836956024169922,
      "learning_rate": 0.0001868257967176176,
      "loss": 2.3948,
      "step": 27750
    },
    {
      "epoch": 0.5710773788247758,
      "grad_norm": 0.36763134598731995,
      "learning_rate": 0.0001868147763378223,
      "loss": 2.3797,
      "step": 27760
    },
    {
      "epoch": 0.5712830983416436,
      "grad_norm": 0.481731116771698,
      "learning_rate": 0.00018680375167592716,
      "loss": 2.4531,
      "step": 27770
    },
    {
      "epoch": 0.5714888178585112,
      "grad_norm": 0.37089988589286804,
      "learning_rate": 0.000186792722732476,
      "loss": 2.4445,
      "step": 27780
    },
    {
      "epoch": 0.571694537375379,
      "grad_norm": 0.4181384742259979,
      "learning_rate": 0.00018678168950801276,
      "loss": 2.3944,
      "step": 27790
    },
    {
      "epoch": 0.5719002568922467,
      "grad_norm": 0.3651382029056549,
      "learning_rate": 0.00018677065200308168,
      "loss": 2.4594,
      "step": 27800
    },
    {
      "epoch": 0.5721059764091144,
      "grad_norm": 0.46182146668434143,
      "learning_rate": 0.00018675961021822718,
      "loss": 2.4455,
      "step": 27810
    },
    {
      "epoch": 0.5723116959259821,
      "grad_norm": 0.37519827485084534,
      "learning_rate": 0.0001867485641539939,
      "loss": 2.4478,
      "step": 27820
    },
    {
      "epoch": 0.5725174154428498,
      "grad_norm": 0.36289381980895996,
      "learning_rate": 0.0001867375138109267,
      "loss": 2.4753,
      "step": 27830
    },
    {
      "epoch": 0.5727231349597175,
      "grad_norm": 0.42358148097991943,
      "learning_rate": 0.00018672645918957055,
      "loss": 2.4005,
      "step": 27840
    },
    {
      "epoch": 0.5729288544765853,
      "grad_norm": 0.4250965118408203,
      "learning_rate": 0.0001867154002904708,
      "loss": 2.4632,
      "step": 27850
    },
    {
      "epoch": 0.573134573993453,
      "grad_norm": 0.3778582811355591,
      "learning_rate": 0.0001867043371141729,
      "loss": 2.4536,
      "step": 27860
    },
    {
      "epoch": 0.5733402935103207,
      "grad_norm": 0.3936448097229004,
      "learning_rate": 0.0001866932696612225,
      "loss": 2.4119,
      "step": 27870
    },
    {
      "epoch": 0.5735460130271884,
      "grad_norm": 0.4267238676548004,
      "learning_rate": 0.00018668219793216558,
      "loss": 2.4042,
      "step": 27880
    },
    {
      "epoch": 0.5737517325440561,
      "grad_norm": 0.387642502784729,
      "learning_rate": 0.00018667112192754818,
      "loss": 2.4922,
      "step": 27890
    },
    {
      "epoch": 0.5739574520609239,
      "grad_norm": 0.3900313377380371,
      "learning_rate": 0.00018666004164791665,
      "loss": 2.4325,
      "step": 27900
    },
    {
      "epoch": 0.5741631715777915,
      "grad_norm": 0.3577161431312561,
      "learning_rate": 0.00018664895709381745,
      "loss": 2.4962,
      "step": 27910
    },
    {
      "epoch": 0.5743688910946593,
      "grad_norm": 0.3423645496368408,
      "learning_rate": 0.00018663786826579744,
      "loss": 2.4986,
      "step": 27920
    },
    {
      "epoch": 0.5745746106115269,
      "grad_norm": 0.36680373549461365,
      "learning_rate": 0.0001866267751644035,
      "loss": 2.4428,
      "step": 27930
    },
    {
      "epoch": 0.5747803301283947,
      "grad_norm": 0.3786846697330475,
      "learning_rate": 0.00018661567779018273,
      "loss": 2.4787,
      "step": 27940
    },
    {
      "epoch": 0.5749860496452625,
      "grad_norm": 0.4051438868045807,
      "learning_rate": 0.00018660457614368264,
      "loss": 2.373,
      "step": 27950
    },
    {
      "epoch": 0.5751917691621301,
      "grad_norm": 0.4951363503932953,
      "learning_rate": 0.00018659347022545073,
      "loss": 2.393,
      "step": 27960
    },
    {
      "epoch": 0.5753974886789979,
      "grad_norm": 0.40068063139915466,
      "learning_rate": 0.0001865823600360348,
      "loss": 2.4514,
      "step": 27970
    },
    {
      "epoch": 0.5756032081958655,
      "grad_norm": 0.3725915253162384,
      "learning_rate": 0.0001865712455759829,
      "loss": 2.4439,
      "step": 27980
    },
    {
      "epoch": 0.5758089277127333,
      "grad_norm": 0.3890083134174347,
      "learning_rate": 0.00018656012684584319,
      "loss": 2.4432,
      "step": 27990
    },
    {
      "epoch": 0.5760146472296009,
      "grad_norm": 0.39217594265937805,
      "learning_rate": 0.00018654900384616413,
      "loss": 2.4227,
      "step": 28000
    },
    {
      "epoch": 0.5762203667464687,
      "grad_norm": 0.38255494832992554,
      "learning_rate": 0.00018653787657749434,
      "loss": 2.4654,
      "step": 28010
    },
    {
      "epoch": 0.5764260862633364,
      "grad_norm": 0.3891073763370514,
      "learning_rate": 0.00018652674504038264,
      "loss": 2.4044,
      "step": 28020
    },
    {
      "epoch": 0.5766318057802041,
      "grad_norm": 0.391955703496933,
      "learning_rate": 0.00018651560923537816,
      "loss": 2.4262,
      "step": 28030
    },
    {
      "epoch": 0.5768375252970719,
      "grad_norm": 0.3719145655632019,
      "learning_rate": 0.00018650446916303014,
      "loss": 2.3917,
      "step": 28040
    },
    {
      "epoch": 0.5770432448139395,
      "grad_norm": 0.39948034286499023,
      "learning_rate": 0.000186493324823888,
      "loss": 2.3957,
      "step": 28050
    },
    {
      "epoch": 0.5772489643308073,
      "grad_norm": 0.370720773935318,
      "learning_rate": 0.00018648217621850154,
      "loss": 2.4027,
      "step": 28060
    },
    {
      "epoch": 0.577454683847675,
      "grad_norm": 0.3820042312145233,
      "learning_rate": 0.00018647102334742055,
      "loss": 2.417,
      "step": 28070
    },
    {
      "epoch": 0.5776604033645427,
      "grad_norm": 0.38614925742149353,
      "learning_rate": 0.0001864598662111952,
      "loss": 2.442,
      "step": 28080
    },
    {
      "epoch": 0.5778661228814104,
      "grad_norm": 0.3887157738208771,
      "learning_rate": 0.0001864487048103758,
      "loss": 2.454,
      "step": 28090
    },
    {
      "epoch": 0.5780718423982781,
      "grad_norm": 0.4056462049484253,
      "learning_rate": 0.00018643753914551292,
      "loss": 2.4127,
      "step": 28100
    },
    {
      "epoch": 0.5782775619151458,
      "grad_norm": 0.370373398065567,
      "learning_rate": 0.0001864263692171572,
      "loss": 2.4735,
      "step": 28110
    },
    {
      "epoch": 0.5784832814320136,
      "grad_norm": 0.4323677122592926,
      "learning_rate": 0.0001864151950258597,
      "loss": 2.4656,
      "step": 28120
    },
    {
      "epoch": 0.5786890009488813,
      "grad_norm": 0.4127415120601654,
      "learning_rate": 0.00018640401657217152,
      "loss": 2.4747,
      "step": 28130
    },
    {
      "epoch": 0.578894720465749,
      "grad_norm": 0.3824217915534973,
      "learning_rate": 0.00018639283385664404,
      "loss": 2.3895,
      "step": 28140
    },
    {
      "epoch": 0.5791004399826167,
      "grad_norm": 0.38901838660240173,
      "learning_rate": 0.00018638164687982887,
      "loss": 2.4105,
      "step": 28150
    },
    {
      "epoch": 0.5793061594994844,
      "grad_norm": 0.3707382380962372,
      "learning_rate": 0.00018637045564227775,
      "loss": 2.5027,
      "step": 28160
    },
    {
      "epoch": 0.5795118790163521,
      "grad_norm": 0.39468520879745483,
      "learning_rate": 0.00018635926014454274,
      "loss": 2.3978,
      "step": 28170
    },
    {
      "epoch": 0.5797175985332198,
      "grad_norm": 0.37942785024642944,
      "learning_rate": 0.00018634806038717603,
      "loss": 2.4156,
      "step": 28180
    },
    {
      "epoch": 0.5799233180500876,
      "grad_norm": 0.38186341524124146,
      "learning_rate": 0.00018633685637073005,
      "loss": 2.3903,
      "step": 28190
    },
    {
      "epoch": 0.5801290375669552,
      "grad_norm": 0.399541974067688,
      "learning_rate": 0.00018632564809575742,
      "loss": 2.5055,
      "step": 28200
    },
    {
      "epoch": 0.580334757083823,
      "grad_norm": 0.34846827387809753,
      "learning_rate": 0.00018631443556281095,
      "loss": 2.4475,
      "step": 28210
    },
    {
      "epoch": 0.5805404766006907,
      "grad_norm": 0.389178603887558,
      "learning_rate": 0.00018630321877244376,
      "loss": 2.4106,
      "step": 28220
    },
    {
      "epoch": 0.5807461961175584,
      "grad_norm": 0.47549471259117126,
      "learning_rate": 0.0001862919977252091,
      "loss": 2.446,
      "step": 28230
    },
    {
      "epoch": 0.5809519156344262,
      "grad_norm": 0.38400155305862427,
      "learning_rate": 0.0001862807724216604,
      "loss": 2.4378,
      "step": 28240
    },
    {
      "epoch": 0.5811576351512938,
      "grad_norm": 0.3598034679889679,
      "learning_rate": 0.00018626954286235137,
      "loss": 2.4537,
      "step": 28250
    },
    {
      "epoch": 0.5813633546681616,
      "grad_norm": 0.3919657766819,
      "learning_rate": 0.00018625830904783592,
      "loss": 2.4722,
      "step": 28260
    },
    {
      "epoch": 0.5815690741850292,
      "grad_norm": 0.3854912519454956,
      "learning_rate": 0.00018624707097866813,
      "loss": 2.44,
      "step": 28270
    },
    {
      "epoch": 0.581774793701897,
      "grad_norm": 0.36998340487480164,
      "learning_rate": 0.00018623582865540233,
      "loss": 2.4504,
      "step": 28280
    },
    {
      "epoch": 0.5819805132187648,
      "grad_norm": 0.4427245557308197,
      "learning_rate": 0.000186224582078593,
      "loss": 2.4851,
      "step": 28290
    },
    {
      "epoch": 0.5821862327356324,
      "grad_norm": 0.36490991711616516,
      "learning_rate": 0.00018621333124879496,
      "loss": 2.3741,
      "step": 28300
    },
    {
      "epoch": 0.5823919522525002,
      "grad_norm": 0.3637121915817261,
      "learning_rate": 0.00018620207616656305,
      "loss": 2.427,
      "step": 28310
    },
    {
      "epoch": 0.5825976717693678,
      "grad_norm": 0.42644572257995605,
      "learning_rate": 0.00018619081683245246,
      "loss": 2.456,
      "step": 28320
    },
    {
      "epoch": 0.5828033912862356,
      "grad_norm": 0.4202239513397217,
      "learning_rate": 0.00018617955324701856,
      "loss": 2.4562,
      "step": 28330
    },
    {
      "epoch": 0.5830091108031032,
      "grad_norm": 0.382277250289917,
      "learning_rate": 0.00018616828541081695,
      "loss": 2.5463,
      "step": 28340
    },
    {
      "epoch": 0.583214830319971,
      "grad_norm": 0.39335840940475464,
      "learning_rate": 0.00018615701332440338,
      "loss": 2.4394,
      "step": 28350
    },
    {
      "epoch": 0.5834205498368387,
      "grad_norm": 0.3737272024154663,
      "learning_rate": 0.0001861457369883338,
      "loss": 2.4441,
      "step": 28360
    },
    {
      "epoch": 0.5836262693537064,
      "grad_norm": 0.3725162446498871,
      "learning_rate": 0.0001861344564031645,
      "loss": 2.4845,
      "step": 28370
    },
    {
      "epoch": 0.5838319888705742,
      "grad_norm": 0.4006703794002533,
      "learning_rate": 0.00018612317156945182,
      "loss": 2.4247,
      "step": 28380
    },
    {
      "epoch": 0.5840377083874418,
      "grad_norm": 0.3812839686870575,
      "learning_rate": 0.0001861118824877524,
      "loss": 2.4152,
      "step": 28390
    },
    {
      "epoch": 0.5842434279043096,
      "grad_norm": 0.4100683629512787,
      "learning_rate": 0.0001861005891586231,
      "loss": 2.4553,
      "step": 28400
    },
    {
      "epoch": 0.5844491474211773,
      "grad_norm": 0.39011213183403015,
      "learning_rate": 0.00018608929158262088,
      "loss": 2.459,
      "step": 28410
    },
    {
      "epoch": 0.584654866938045,
      "grad_norm": 0.3818870484828949,
      "learning_rate": 0.00018607798976030305,
      "loss": 2.4291,
      "step": 28420
    },
    {
      "epoch": 0.5848605864549127,
      "grad_norm": 0.3938046097755432,
      "learning_rate": 0.00018606668369222708,
      "loss": 2.4924,
      "step": 28430
    },
    {
      "epoch": 0.5850663059717804,
      "grad_norm": 0.40695634484291077,
      "learning_rate": 0.0001860553733789506,
      "loss": 2.4338,
      "step": 28440
    },
    {
      "epoch": 0.5852720254886481,
      "grad_norm": 0.38105881214141846,
      "learning_rate": 0.0001860440588210315,
      "loss": 2.4803,
      "step": 28450
    },
    {
      "epoch": 0.5854777450055159,
      "grad_norm": 0.3878714144229889,
      "learning_rate": 0.00018603274001902786,
      "loss": 2.4318,
      "step": 28460
    },
    {
      "epoch": 0.5856834645223836,
      "grad_norm": 0.3938128352165222,
      "learning_rate": 0.00018602141697349797,
      "loss": 2.4719,
      "step": 28470
    },
    {
      "epoch": 0.5858891840392513,
      "grad_norm": 0.39059019088745117,
      "learning_rate": 0.00018601008968500033,
      "loss": 2.4332,
      "step": 28480
    },
    {
      "epoch": 0.586094903556119,
      "grad_norm": 0.4048420190811157,
      "learning_rate": 0.00018599875815409372,
      "loss": 2.4406,
      "step": 28490
    },
    {
      "epoch": 0.5863006230729867,
      "grad_norm": 0.44178515672683716,
      "learning_rate": 0.00018598742238133692,
      "loss": 2.4053,
      "step": 28500
    },
    {
      "epoch": 0.5865063425898545,
      "grad_norm": 0.410972535610199,
      "learning_rate": 0.00018597608236728923,
      "loss": 2.4018,
      "step": 28510
    },
    {
      "epoch": 0.5867120621067221,
      "grad_norm": 0.3655951917171478,
      "learning_rate": 0.00018596473811250986,
      "loss": 2.4655,
      "step": 28520
    },
    {
      "epoch": 0.5869177816235899,
      "grad_norm": 0.3767746686935425,
      "learning_rate": 0.00018595338961755844,
      "loss": 2.4046,
      "step": 28530
    },
    {
      "epoch": 0.5871235011404575,
      "grad_norm": 0.4238992929458618,
      "learning_rate": 0.00018594203688299468,
      "loss": 2.4671,
      "step": 28540
    },
    {
      "epoch": 0.5873292206573253,
      "grad_norm": 0.36958077549934387,
      "learning_rate": 0.00018593067990937856,
      "loss": 2.4804,
      "step": 28550
    },
    {
      "epoch": 0.587534940174193,
      "grad_norm": 0.4170093238353729,
      "learning_rate": 0.00018591931869727028,
      "loss": 2.4359,
      "step": 28560
    },
    {
      "epoch": 0.5877406596910607,
      "grad_norm": 0.3594343662261963,
      "learning_rate": 0.00018590795324723022,
      "loss": 2.4153,
      "step": 28570
    },
    {
      "epoch": 0.5879463792079285,
      "grad_norm": 0.3870394825935364,
      "learning_rate": 0.00018589658355981897,
      "loss": 2.4031,
      "step": 28580
    },
    {
      "epoch": 0.5881520987247961,
      "grad_norm": 0.3765479624271393,
      "learning_rate": 0.00018588520963559732,
      "loss": 2.4577,
      "step": 28590
    },
    {
      "epoch": 0.5883578182416639,
      "grad_norm": 0.43050411343574524,
      "learning_rate": 0.00018587383147512633,
      "loss": 2.4773,
      "step": 28600
    },
    {
      "epoch": 0.5885635377585315,
      "grad_norm": 0.4183051288127899,
      "learning_rate": 0.00018586244907896715,
      "loss": 2.5028,
      "step": 28610
    },
    {
      "epoch": 0.5887692572753993,
      "grad_norm": 0.4136037528514862,
      "learning_rate": 0.00018585106244768126,
      "loss": 2.4431,
      "step": 28620
    },
    {
      "epoch": 0.588974976792267,
      "grad_norm": 0.39377495646476746,
      "learning_rate": 0.00018583967158183028,
      "loss": 2.4142,
      "step": 28630
    },
    {
      "epoch": 0.5891806963091347,
      "grad_norm": 0.3621053099632263,
      "learning_rate": 0.0001858282764819761,
      "loss": 2.4467,
      "step": 28640
    },
    {
      "epoch": 0.5893864158260025,
      "grad_norm": 0.36634495854377747,
      "learning_rate": 0.00018581687714868073,
      "loss": 2.4336,
      "step": 28650
    },
    {
      "epoch": 0.5895921353428701,
      "grad_norm": 0.36196067929267883,
      "learning_rate": 0.00018580547358250645,
      "loss": 2.4047,
      "step": 28660
    },
    {
      "epoch": 0.5897978548597379,
      "grad_norm": 0.3797409236431122,
      "learning_rate": 0.00018579406578401577,
      "loss": 2.4662,
      "step": 28670
    },
    {
      "epoch": 0.5900035743766056,
      "grad_norm": 0.3800143599510193,
      "learning_rate": 0.00018578265375377134,
      "loss": 2.4489,
      "step": 28680
    },
    {
      "epoch": 0.5902092938934733,
      "grad_norm": 0.4042907953262329,
      "learning_rate": 0.000185771237492336,
      "loss": 2.4763,
      "step": 28690
    },
    {
      "epoch": 0.590415013410341,
      "grad_norm": 0.38349318504333496,
      "learning_rate": 0.00018575981700027298,
      "loss": 2.4578,
      "step": 28700
    },
    {
      "epoch": 0.5906207329272087,
      "grad_norm": 0.38773781061172485,
      "learning_rate": 0.0001857483922781455,
      "loss": 2.4199,
      "step": 28710
    },
    {
      "epoch": 0.5908264524440764,
      "grad_norm": 0.3818555772304535,
      "learning_rate": 0.00018573696332651712,
      "loss": 2.4736,
      "step": 28720
    },
    {
      "epoch": 0.5910321719609442,
      "grad_norm": 0.36563557386398315,
      "learning_rate": 0.00018572553014595153,
      "loss": 2.4614,
      "step": 28730
    },
    {
      "epoch": 0.5912378914778119,
      "grad_norm": 0.3714592158794403,
      "learning_rate": 0.00018571409273701267,
      "loss": 2.4246,
      "step": 28740
    },
    {
      "epoch": 0.5914436109946796,
      "grad_norm": 0.4242003262042999,
      "learning_rate": 0.00018570265110026473,
      "loss": 2.455,
      "step": 28750
    },
    {
      "epoch": 0.5916493305115473,
      "grad_norm": 0.3738420009613037,
      "learning_rate": 0.00018569120523627205,
      "loss": 2.3996,
      "step": 28760
    },
    {
      "epoch": 0.591855050028415,
      "grad_norm": 0.4164637327194214,
      "learning_rate": 0.00018567975514559914,
      "loss": 2.4091,
      "step": 28770
    },
    {
      "epoch": 0.5920607695452828,
      "grad_norm": 0.3627977669239044,
      "learning_rate": 0.0001856683008288108,
      "loss": 2.4466,
      "step": 28780
    },
    {
      "epoch": 0.5922664890621504,
      "grad_norm": 0.46340212225914,
      "learning_rate": 0.00018565684228647204,
      "loss": 2.4205,
      "step": 28790
    },
    {
      "epoch": 0.5924722085790182,
      "grad_norm": 0.4326539933681488,
      "learning_rate": 0.00018564537951914803,
      "loss": 2.4762,
      "step": 28800
    },
    {
      "epoch": 0.5926779280958858,
      "grad_norm": 0.39426249265670776,
      "learning_rate": 0.00018563391252740416,
      "loss": 2.4462,
      "step": 28810
    },
    {
      "epoch": 0.5928836476127536,
      "grad_norm": 0.4218806326389313,
      "learning_rate": 0.000185622441311806,
      "loss": 2.4521,
      "step": 28820
    },
    {
      "epoch": 0.5930893671296213,
      "grad_norm": 0.3745236098766327,
      "learning_rate": 0.00018561096587291944,
      "loss": 2.4718,
      "step": 28830
    },
    {
      "epoch": 0.593295086646489,
      "grad_norm": 0.3974398672580719,
      "learning_rate": 0.00018559948621131044,
      "loss": 2.4572,
      "step": 28840
    },
    {
      "epoch": 0.5935008061633568,
      "grad_norm": 0.37394222617149353,
      "learning_rate": 0.00018558800232754523,
      "loss": 2.4509,
      "step": 28850
    },
    {
      "epoch": 0.5937065256802244,
      "grad_norm": 0.36673155426979065,
      "learning_rate": 0.00018557651422219026,
      "loss": 2.4561,
      "step": 28860
    },
    {
      "epoch": 0.5939122451970922,
      "grad_norm": 0.4114181697368622,
      "learning_rate": 0.0001855650218958122,
      "loss": 2.4749,
      "step": 28870
    },
    {
      "epoch": 0.5941179647139598,
      "grad_norm": 0.4180186688899994,
      "learning_rate": 0.0001855535253489779,
      "loss": 2.4295,
      "step": 28880
    },
    {
      "epoch": 0.5943236842308276,
      "grad_norm": 0.37237587571144104,
      "learning_rate": 0.00018554202458225438,
      "loss": 2.4432,
      "step": 28890
    },
    {
      "epoch": 0.5945294037476953,
      "grad_norm": 0.4022301733493805,
      "learning_rate": 0.00018553051959620893,
      "loss": 2.4265,
      "step": 28900
    },
    {
      "epoch": 0.594735123264563,
      "grad_norm": 0.37567952275276184,
      "learning_rate": 0.00018551901039140907,
      "loss": 2.4076,
      "step": 28910
    },
    {
      "epoch": 0.5949408427814308,
      "grad_norm": 0.3690734803676605,
      "learning_rate": 0.00018550749696842243,
      "loss": 2.4314,
      "step": 28920
    },
    {
      "epoch": 0.5951465622982984,
      "grad_norm": 0.4300696551799774,
      "learning_rate": 0.0001854959793278169,
      "loss": 2.4757,
      "step": 28930
    },
    {
      "epoch": 0.5953522818151662,
      "grad_norm": 0.38230881094932556,
      "learning_rate": 0.00018548445747016063,
      "loss": 2.4592,
      "step": 28940
    },
    {
      "epoch": 0.5955580013320338,
      "grad_norm": 0.382525771856308,
      "learning_rate": 0.00018547293139602192,
      "loss": 2.4028,
      "step": 28950
    },
    {
      "epoch": 0.5957637208489016,
      "grad_norm": 0.39920759201049805,
      "learning_rate": 0.00018546140110596926,
      "loss": 2.4839,
      "step": 28960
    },
    {
      "epoch": 0.5959694403657693,
      "grad_norm": 0.38064873218536377,
      "learning_rate": 0.00018544986660057138,
      "loss": 2.4642,
      "step": 28970
    },
    {
      "epoch": 0.596175159882637,
      "grad_norm": 0.38787606358528137,
      "learning_rate": 0.00018543832788039724,
      "loss": 2.4191,
      "step": 28980
    },
    {
      "epoch": 0.5963808793995047,
      "grad_norm": 0.502334475517273,
      "learning_rate": 0.00018542678494601596,
      "loss": 2.4162,
      "step": 28990
    },
    {
      "epoch": 0.5965865989163724,
      "grad_norm": 0.3745792806148529,
      "learning_rate": 0.00018541523779799692,
      "loss": 2.3949,
      "step": 29000
    },
    {
      "epoch": 0.5967923184332402,
      "grad_norm": 0.4311508536338806,
      "learning_rate": 0.00018540368643690965,
      "loss": 2.4312,
      "step": 29010
    },
    {
      "epoch": 0.5969980379501079,
      "grad_norm": 0.42395347356796265,
      "learning_rate": 0.0001853921308633239,
      "loss": 2.4169,
      "step": 29020
    },
    {
      "epoch": 0.5972037574669756,
      "grad_norm": 0.39464589953422546,
      "learning_rate": 0.00018538057107780968,
      "loss": 2.4716,
      "step": 29030
    },
    {
      "epoch": 0.5974094769838433,
      "grad_norm": 0.36778029799461365,
      "learning_rate": 0.00018536900708093716,
      "loss": 2.4589,
      "step": 29040
    },
    {
      "epoch": 0.597615196500711,
      "grad_norm": 0.38921698927879333,
      "learning_rate": 0.00018535743887327673,
      "loss": 2.4409,
      "step": 29050
    },
    {
      "epoch": 0.5978209160175787,
      "grad_norm": 0.35520681738853455,
      "learning_rate": 0.00018534586645539895,
      "loss": 2.4876,
      "step": 29060
    },
    {
      "epoch": 0.5980266355344465,
      "grad_norm": 0.36619922518730164,
      "learning_rate": 0.0001853342898278747,
      "loss": 2.4063,
      "step": 29070
    },
    {
      "epoch": 0.5982323550513141,
      "grad_norm": 0.3853239417076111,
      "learning_rate": 0.00018532270899127492,
      "loss": 2.441,
      "step": 29080
    },
    {
      "epoch": 0.5984380745681819,
      "grad_norm": 0.42635560035705566,
      "learning_rate": 0.0001853111239461709,
      "loss": 2.4484,
      "step": 29090
    },
    {
      "epoch": 0.5986437940850496,
      "grad_norm": 0.40890178084373474,
      "learning_rate": 0.00018529953469313397,
      "loss": 2.427,
      "step": 29100
    },
    {
      "epoch": 0.5988495136019173,
      "grad_norm": 0.3835512399673462,
      "learning_rate": 0.00018528794123273587,
      "loss": 2.4194,
      "step": 29110
    },
    {
      "epoch": 0.5990552331187851,
      "grad_norm": 0.4136093556880951,
      "learning_rate": 0.00018527634356554836,
      "loss": 2.4048,
      "step": 29120
    },
    {
      "epoch": 0.5992609526356527,
      "grad_norm": 0.4248165488243103,
      "learning_rate": 0.00018526474169214353,
      "loss": 2.4174,
      "step": 29130
    },
    {
      "epoch": 0.5994666721525205,
      "grad_norm": 0.36343055963516235,
      "learning_rate": 0.00018525313561309364,
      "loss": 2.4654,
      "step": 29140
    },
    {
      "epoch": 0.5996723916693881,
      "grad_norm": 0.42993098497390747,
      "learning_rate": 0.00018524152532897113,
      "loss": 2.4051,
      "step": 29150
    },
    {
      "epoch": 0.5998781111862559,
      "grad_norm": 0.37812677025794983,
      "learning_rate": 0.0001852299108403487,
      "loss": 2.5009,
      "step": 29160
    },
    {
      "epoch": 0.6000838307031235,
      "grad_norm": 0.36531150341033936,
      "learning_rate": 0.0001852182921477992,
      "loss": 2.4738,
      "step": 29170
    },
    {
      "epoch": 0.6002895502199913,
      "grad_norm": 0.39656564593315125,
      "learning_rate": 0.00018520666925189572,
      "loss": 2.3695,
      "step": 29180
    },
    {
      "epoch": 0.6004952697368591,
      "grad_norm": 0.3551873564720154,
      "learning_rate": 0.0001851950421532116,
      "loss": 2.4218,
      "step": 29190
    },
    {
      "epoch": 0.6007009892537267,
      "grad_norm": 0.37866777181625366,
      "learning_rate": 0.0001851834108523203,
      "loss": 2.439,
      "step": 29200
    },
    {
      "epoch": 0.6009067087705945,
      "grad_norm": 0.39087268710136414,
      "learning_rate": 0.00018517177534979552,
      "loss": 2.449,
      "step": 29210
    },
    {
      "epoch": 0.6011124282874621,
      "grad_norm": 0.4313814342021942,
      "learning_rate": 0.0001851601356462112,
      "loss": 2.3744,
      "step": 29220
    },
    {
      "epoch": 0.6013181478043299,
      "grad_norm": 0.410460889339447,
      "learning_rate": 0.00018514849174214147,
      "loss": 2.4626,
      "step": 29230
    },
    {
      "epoch": 0.6015238673211976,
      "grad_norm": 0.42497992515563965,
      "learning_rate": 0.00018513684363816063,
      "loss": 2.4503,
      "step": 29240
    },
    {
      "epoch": 0.6017295868380653,
      "grad_norm": 0.386336088180542,
      "learning_rate": 0.00018512519133484325,
      "loss": 2.4484,
      "step": 29250
    },
    {
      "epoch": 0.601935306354933,
      "grad_norm": 0.3774840235710144,
      "learning_rate": 0.00018511353483276403,
      "loss": 2.44,
      "step": 29260
    },
    {
      "epoch": 0.6021410258718007,
      "grad_norm": 0.38267678022384644,
      "learning_rate": 0.00018510187413249795,
      "loss": 2.4523,
      "step": 29270
    },
    {
      "epoch": 0.6023467453886685,
      "grad_norm": 0.4401903748512268,
      "learning_rate": 0.0001850902092346202,
      "loss": 2.4368,
      "step": 29280
    },
    {
      "epoch": 0.6025524649055362,
      "grad_norm": 0.3797702193260193,
      "learning_rate": 0.0001850785401397061,
      "loss": 2.4672,
      "step": 29290
    },
    {
      "epoch": 0.6027581844224039,
      "grad_norm": 0.38247358798980713,
      "learning_rate": 0.00018506686684833124,
      "loss": 2.5033,
      "step": 29300
    },
    {
      "epoch": 0.6029639039392716,
      "grad_norm": 0.3728758692741394,
      "learning_rate": 0.0001850551893610714,
      "loss": 2.465,
      "step": 29310
    },
    {
      "epoch": 0.6031696234561393,
      "grad_norm": 0.3927392065525055,
      "learning_rate": 0.00018504350767850256,
      "loss": 2.4176,
      "step": 29320
    },
    {
      "epoch": 0.603375342973007,
      "grad_norm": 0.34969016909599304,
      "learning_rate": 0.0001850318218012009,
      "loss": 2.4671,
      "step": 29330
    },
    {
      "epoch": 0.6035810624898748,
      "grad_norm": 0.4101261794567108,
      "learning_rate": 0.00018502013172974285,
      "loss": 2.4278,
      "step": 29340
    },
    {
      "epoch": 0.6037867820067424,
      "grad_norm": 0.43554696440696716,
      "learning_rate": 0.00018500843746470502,
      "loss": 2.4507,
      "step": 29350
    },
    {
      "epoch": 0.6039925015236102,
      "grad_norm": 0.42630791664123535,
      "learning_rate": 0.0001849967390066642,
      "loss": 2.3968,
      "step": 29360
    },
    {
      "epoch": 0.6041982210404779,
      "grad_norm": 0.3781091272830963,
      "learning_rate": 0.00018498503635619744,
      "loss": 2.459,
      "step": 29370
    },
    {
      "epoch": 0.6044039405573456,
      "grad_norm": 0.44689008593559265,
      "learning_rate": 0.0001849733295138819,
      "loss": 2.3999,
      "step": 29380
    },
    {
      "epoch": 0.6046096600742134,
      "grad_norm": 0.43367403745651245,
      "learning_rate": 0.0001849616184802951,
      "loss": 2.4395,
      "step": 29390
    },
    {
      "epoch": 0.604815379591081,
      "grad_norm": 0.3634333610534668,
      "learning_rate": 0.00018494990325601464,
      "loss": 2.3796,
      "step": 29400
    },
    {
      "epoch": 0.6050210991079488,
      "grad_norm": 0.3960282802581787,
      "learning_rate": 0.00018493818384161838,
      "loss": 2.3858,
      "step": 29410
    },
    {
      "epoch": 0.6052268186248164,
      "grad_norm": 0.39983460307121277,
      "learning_rate": 0.00018492646023768436,
      "loss": 2.3984,
      "step": 29420
    },
    {
      "epoch": 0.6054325381416842,
      "grad_norm": 0.40092790126800537,
      "learning_rate": 0.00018491473244479085,
      "loss": 2.4884,
      "step": 29430
    },
    {
      "epoch": 0.6056382576585518,
      "grad_norm": 0.3679734170436859,
      "learning_rate": 0.00018490300046351634,
      "loss": 2.4738,
      "step": 29440
    },
    {
      "epoch": 0.6058439771754196,
      "grad_norm": 0.39325469732284546,
      "learning_rate": 0.00018489126429443946,
      "loss": 2.4455,
      "step": 29450
    },
    {
      "epoch": 0.6060496966922874,
      "grad_norm": 0.38754698634147644,
      "learning_rate": 0.00018487952393813912,
      "loss": 2.4289,
      "step": 29460
    },
    {
      "epoch": 0.606255416209155,
      "grad_norm": 0.36736395955085754,
      "learning_rate": 0.0001848677793951944,
      "loss": 2.4824,
      "step": 29470
    },
    {
      "epoch": 0.6064611357260228,
      "grad_norm": 0.38493308424949646,
      "learning_rate": 0.00018485603066618458,
      "loss": 2.4201,
      "step": 29480
    },
    {
      "epoch": 0.6066668552428904,
      "grad_norm": 0.38405632972717285,
      "learning_rate": 0.00018484427775168922,
      "loss": 2.4369,
      "step": 29490
    },
    {
      "epoch": 0.6068725747597582,
      "grad_norm": 0.35073086619377136,
      "learning_rate": 0.00018483252065228792,
      "loss": 2.432,
      "step": 29500
    },
    {
      "epoch": 0.6070782942766259,
      "grad_norm": 0.3897987902164459,
      "learning_rate": 0.00018482075936856074,
      "loss": 2.4253,
      "step": 29510
    },
    {
      "epoch": 0.6072840137934936,
      "grad_norm": 0.36504167318344116,
      "learning_rate": 0.00018480899390108766,
      "loss": 2.4147,
      "step": 29520
    },
    {
      "epoch": 0.6074897333103613,
      "grad_norm": 0.3892647624015808,
      "learning_rate": 0.00018479722425044907,
      "loss": 2.4622,
      "step": 29530
    },
    {
      "epoch": 0.607695452827229,
      "grad_norm": 0.40929171442985535,
      "learning_rate": 0.0001847854504172255,
      "loss": 2.4207,
      "step": 29540
    },
    {
      "epoch": 0.6079011723440968,
      "grad_norm": 0.40716150403022766,
      "learning_rate": 0.00018477367240199772,
      "loss": 2.4596,
      "step": 29550
    },
    {
      "epoch": 0.6081068918609644,
      "grad_norm": 0.3896031677722931,
      "learning_rate": 0.0001847618902053466,
      "loss": 2.4137,
      "step": 29560
    },
    {
      "epoch": 0.6083126113778322,
      "grad_norm": 0.41482964158058167,
      "learning_rate": 0.00018475010382785337,
      "loss": 2.4694,
      "step": 29570
    },
    {
      "epoch": 0.6085183308946999,
      "grad_norm": 0.39955946803092957,
      "learning_rate": 0.0001847383132700993,
      "loss": 2.4603,
      "step": 29580
    },
    {
      "epoch": 0.6087240504115676,
      "grad_norm": 0.3897315263748169,
      "learning_rate": 0.00018472651853266604,
      "loss": 2.4626,
      "step": 29590
    },
    {
      "epoch": 0.6089297699284353,
      "grad_norm": 0.3540264368057251,
      "learning_rate": 0.0001847147196161353,
      "loss": 2.4252,
      "step": 29600
    },
    {
      "epoch": 0.609135489445303,
      "grad_norm": 0.37822404503822327,
      "learning_rate": 0.00018470291652108913,
      "loss": 2.4938,
      "step": 29610
    },
    {
      "epoch": 0.6093412089621707,
      "grad_norm": 0.40737247467041016,
      "learning_rate": 0.00018469110924810962,
      "loss": 2.4615,
      "step": 29620
    },
    {
      "epoch": 0.6095469284790385,
      "grad_norm": 0.5015671849250793,
      "learning_rate": 0.0001846792977977792,
      "loss": 2.4703,
      "step": 29630
    },
    {
      "epoch": 0.6097526479959062,
      "grad_norm": 0.3675505220890045,
      "learning_rate": 0.0001846674821706805,
      "loss": 2.4229,
      "step": 29640
    },
    {
      "epoch": 0.6099583675127739,
      "grad_norm": 0.39638349413871765,
      "learning_rate": 0.00018465566236739627,
      "loss": 2.4552,
      "step": 29650
    },
    {
      "epoch": 0.6101640870296416,
      "grad_norm": 0.39728137850761414,
      "learning_rate": 0.00018464383838850954,
      "loss": 2.4587,
      "step": 29660
    },
    {
      "epoch": 0.6103698065465093,
      "grad_norm": 0.40159374475479126,
      "learning_rate": 0.00018463201023460348,
      "loss": 2.4586,
      "step": 29670
    },
    {
      "epoch": 0.6105755260633771,
      "grad_norm": 0.37432748079299927,
      "learning_rate": 0.00018462017790626159,
      "loss": 2.3953,
      "step": 29680
    },
    {
      "epoch": 0.6107812455802447,
      "grad_norm": 0.3896721303462982,
      "learning_rate": 0.00018460834140406743,
      "loss": 2.4789,
      "step": 29690
    },
    {
      "epoch": 0.6109869650971125,
      "grad_norm": 0.40025565028190613,
      "learning_rate": 0.00018459650072860483,
      "loss": 2.4558,
      "step": 29700
    },
    {
      "epoch": 0.6111926846139801,
      "grad_norm": 0.3888491094112396,
      "learning_rate": 0.0001845846558804579,
      "loss": 2.4469,
      "step": 29710
    },
    {
      "epoch": 0.6113984041308479,
      "grad_norm": 0.37575042247772217,
      "learning_rate": 0.0001845728068602108,
      "loss": 2.4309,
      "step": 29720
    },
    {
      "epoch": 0.6116041236477157,
      "grad_norm": 0.42504236102104187,
      "learning_rate": 0.00018456095366844796,
      "loss": 2.3962,
      "step": 29730
    },
    {
      "epoch": 0.6118098431645833,
      "grad_norm": 0.36882710456848145,
      "learning_rate": 0.00018454909630575411,
      "loss": 2.4142,
      "step": 29740
    },
    {
      "epoch": 0.6120155626814511,
      "grad_norm": 0.3794386386871338,
      "learning_rate": 0.00018453723477271407,
      "loss": 2.411,
      "step": 29750
    },
    {
      "epoch": 0.6122212821983187,
      "grad_norm": 0.3883907198905945,
      "learning_rate": 0.00018452536906991292,
      "loss": 2.4939,
      "step": 29760
    },
    {
      "epoch": 0.6124270017151865,
      "grad_norm": 0.37407606840133667,
      "learning_rate": 0.00018451349919793595,
      "loss": 2.4419,
      "step": 29770
    },
    {
      "epoch": 0.6126327212320541,
      "grad_norm": 0.3988915979862213,
      "learning_rate": 0.00018450162515736858,
      "loss": 2.5016,
      "step": 29780
    },
    {
      "epoch": 0.6128384407489219,
      "grad_norm": 0.4055638313293457,
      "learning_rate": 0.00018448974694879652,
      "loss": 2.4705,
      "step": 29790
    },
    {
      "epoch": 0.6130441602657896,
      "grad_norm": 0.4042050838470459,
      "learning_rate": 0.00018447786457280566,
      "loss": 2.4293,
      "step": 29800
    },
    {
      "epoch": 0.6132498797826573,
      "grad_norm": 0.3801405131816864,
      "learning_rate": 0.00018446597802998206,
      "loss": 2.5378,
      "step": 29810
    },
    {
      "epoch": 0.6134555992995251,
      "grad_norm": 0.38958480954170227,
      "learning_rate": 0.0001844540873209121,
      "loss": 2.4053,
      "step": 29820
    },
    {
      "epoch": 0.6136613188163927,
      "grad_norm": 0.41480395197868347,
      "learning_rate": 0.0001844421924461822,
      "loss": 2.4231,
      "step": 29830
    },
    {
      "epoch": 0.6138670383332605,
      "grad_norm": 0.465008407831192,
      "learning_rate": 0.0001844302934063791,
      "loss": 2.4537,
      "step": 29840
    },
    {
      "epoch": 0.6140727578501282,
      "grad_norm": 0.38356736302375793,
      "learning_rate": 0.00018441839020208974,
      "loss": 2.4227,
      "step": 29850
    },
    {
      "epoch": 0.6142784773669959,
      "grad_norm": 0.4454136788845062,
      "learning_rate": 0.00018440648283390117,
      "loss": 2.4431,
      "step": 29860
    },
    {
      "epoch": 0.6144841968838636,
      "grad_norm": 0.35630908608436584,
      "learning_rate": 0.0001843945713024008,
      "loss": 2.4282,
      "step": 29870
    },
    {
      "epoch": 0.6146899164007313,
      "grad_norm": 0.4541659951210022,
      "learning_rate": 0.00018438265560817612,
      "loss": 2.4635,
      "step": 29880
    },
    {
      "epoch": 0.614895635917599,
      "grad_norm": 0.3826372027397156,
      "learning_rate": 0.00018437073575181486,
      "loss": 2.4306,
      "step": 29890
    },
    {
      "epoch": 0.6151013554344668,
      "grad_norm": 0.3687438368797302,
      "learning_rate": 0.00018435881173390497,
      "loss": 2.4217,
      "step": 29900
    },
    {
      "epoch": 0.6153070749513345,
      "grad_norm": 0.41912397742271423,
      "learning_rate": 0.0001843468835550346,
      "loss": 2.3935,
      "step": 29910
    },
    {
      "epoch": 0.6155127944682022,
      "grad_norm": 0.386364221572876,
      "learning_rate": 0.0001843349512157921,
      "loss": 2.3947,
      "step": 29920
    },
    {
      "epoch": 0.6157185139850699,
      "grad_norm": 0.40262463688850403,
      "learning_rate": 0.00018432301471676604,
      "loss": 2.381,
      "step": 29930
    },
    {
      "epoch": 0.6159242335019376,
      "grad_norm": 0.38708752393722534,
      "learning_rate": 0.00018431107405854515,
      "loss": 2.4322,
      "step": 29940
    },
    {
      "epoch": 0.6161299530188054,
      "grad_norm": 0.37056466937065125,
      "learning_rate": 0.00018429912924171845,
      "loss": 2.3793,
      "step": 29950
    },
    {
      "epoch": 0.616335672535673,
      "grad_norm": 0.3854159116744995,
      "learning_rate": 0.00018428718026687504,
      "loss": 2.4295,
      "step": 29960
    },
    {
      "epoch": 0.6165413920525408,
      "grad_norm": 0.40006783604621887,
      "learning_rate": 0.00018427522713460436,
      "loss": 2.4396,
      "step": 29970
    },
    {
      "epoch": 0.6167471115694084,
      "grad_norm": 0.38834643363952637,
      "learning_rate": 0.00018426326984549596,
      "loss": 2.3632,
      "step": 29980
    },
    {
      "epoch": 0.6169528310862762,
      "grad_norm": 0.41227462887763977,
      "learning_rate": 0.00018425130840013965,
      "loss": 2.4717,
      "step": 29990
    },
    {
      "epoch": 0.617158550603144,
      "grad_norm": 0.42581337690353394,
      "learning_rate": 0.00018423934279912537,
      "loss": 2.4761,
      "step": 30000
    },
    {
      "epoch": 0.6173642701200116,
      "grad_norm": 0.3599620759487152,
      "learning_rate": 0.0001842273730430434,
      "loss": 2.4311,
      "step": 30010
    },
    {
      "epoch": 0.6175699896368794,
      "grad_norm": 0.40914395451545715,
      "learning_rate": 0.0001842153991324841,
      "loss": 2.4719,
      "step": 30020
    },
    {
      "epoch": 0.617775709153747,
      "grad_norm": 0.38924935460090637,
      "learning_rate": 0.00018420342106803807,
      "loss": 2.4495,
      "step": 30030
    },
    {
      "epoch": 0.6179814286706148,
      "grad_norm": 0.37133342027664185,
      "learning_rate": 0.00018419143885029616,
      "loss": 2.4405,
      "step": 30040
    },
    {
      "epoch": 0.6181871481874824,
      "grad_norm": 0.3833346664905548,
      "learning_rate": 0.0001841794524798493,
      "loss": 2.4113,
      "step": 30050
    },
    {
      "epoch": 0.6183928677043502,
      "grad_norm": 0.3795502185821533,
      "learning_rate": 0.00018416746195728883,
      "loss": 2.4287,
      "step": 30060
    },
    {
      "epoch": 0.618598587221218,
      "grad_norm": 0.44286176562309265,
      "learning_rate": 0.00018415546728320608,
      "loss": 2.4396,
      "step": 30070
    },
    {
      "epoch": 0.6188043067380856,
      "grad_norm": 0.4344915747642517,
      "learning_rate": 0.00018414346845819273,
      "loss": 2.4822,
      "step": 30080
    },
    {
      "epoch": 0.6190100262549534,
      "grad_norm": 0.3817327618598938,
      "learning_rate": 0.00018413146548284064,
      "loss": 2.45,
      "step": 30090
    },
    {
      "epoch": 0.619215745771821,
      "grad_norm": 0.3830251693725586,
      "learning_rate": 0.0001841194583577418,
      "loss": 2.4813,
      "step": 30100
    },
    {
      "epoch": 0.6194214652886888,
      "grad_norm": 0.3812251687049866,
      "learning_rate": 0.00018410744708348843,
      "loss": 2.3886,
      "step": 30110
    },
    {
      "epoch": 0.6196271848055565,
      "grad_norm": 0.3463844358921051,
      "learning_rate": 0.00018409543166067309,
      "loss": 2.4806,
      "step": 30120
    },
    {
      "epoch": 0.6198329043224242,
      "grad_norm": 0.3720387816429138,
      "learning_rate": 0.00018408341208988834,
      "loss": 2.4722,
      "step": 30130
    },
    {
      "epoch": 0.6200386238392919,
      "grad_norm": 0.39534905552864075,
      "learning_rate": 0.0001840713883717271,
      "loss": 2.4188,
      "step": 30140
    },
    {
      "epoch": 0.6202443433561596,
      "grad_norm": 0.5069396495819092,
      "learning_rate": 0.00018405936050678237,
      "loss": 2.3737,
      "step": 30150
    },
    {
      "epoch": 0.6204500628730274,
      "grad_norm": 0.38768789172172546,
      "learning_rate": 0.00018404732849564747,
      "loss": 2.4408,
      "step": 30160
    },
    {
      "epoch": 0.620655782389895,
      "grad_norm": 0.4005047380924225,
      "learning_rate": 0.00018403529233891586,
      "loss": 2.4277,
      "step": 30170
    },
    {
      "epoch": 0.6208615019067628,
      "grad_norm": 0.36865633726119995,
      "learning_rate": 0.0001840232520371812,
      "loss": 2.4256,
      "step": 30180
    },
    {
      "epoch": 0.6210672214236305,
      "grad_norm": 0.3798815608024597,
      "learning_rate": 0.00018401120759103742,
      "loss": 2.4456,
      "step": 30190
    },
    {
      "epoch": 0.6212729409404982,
      "grad_norm": 0.4431755840778351,
      "learning_rate": 0.00018399915900107856,
      "loss": 2.4351,
      "step": 30200
    },
    {
      "epoch": 0.6214786604573659,
      "grad_norm": 0.40674862265586853,
      "learning_rate": 0.00018398710626789893,
      "loss": 2.4522,
      "step": 30210
    },
    {
      "epoch": 0.6216843799742336,
      "grad_norm": 0.3572362959384918,
      "learning_rate": 0.000183975049392093,
      "loss": 2.4494,
      "step": 30220
    },
    {
      "epoch": 0.6218900994911013,
      "grad_norm": 0.35979798436164856,
      "learning_rate": 0.0001839629883742555,
      "loss": 2.42,
      "step": 30230
    },
    {
      "epoch": 0.6220958190079691,
      "grad_norm": 0.40796616673469543,
      "learning_rate": 0.00018395092321498135,
      "loss": 2.4068,
      "step": 30240
    },
    {
      "epoch": 0.6223015385248368,
      "grad_norm": 0.3786047101020813,
      "learning_rate": 0.00018393885391486563,
      "loss": 2.3834,
      "step": 30250
    },
    {
      "epoch": 0.6225072580417045,
      "grad_norm": 0.3968081474304199,
      "learning_rate": 0.00018392678047450366,
      "loss": 2.4272,
      "step": 30260
    },
    {
      "epoch": 0.6227129775585722,
      "grad_norm": 0.39061495661735535,
      "learning_rate": 0.00018391470289449093,
      "loss": 2.4537,
      "step": 30270
    },
    {
      "epoch": 0.6229186970754399,
      "grad_norm": 0.37505170702934265,
      "learning_rate": 0.0001839026211754232,
      "loss": 2.3805,
      "step": 30280
    },
    {
      "epoch": 0.6231244165923077,
      "grad_norm": 0.4109443128108978,
      "learning_rate": 0.0001838905353178964,
      "loss": 2.4422,
      "step": 30290
    },
    {
      "epoch": 0.6233301361091753,
      "grad_norm": 0.39706483483314514,
      "learning_rate": 0.00018387844532250663,
      "loss": 2.4184,
      "step": 30300
    },
    {
      "epoch": 0.6235358556260431,
      "grad_norm": 0.41603440046310425,
      "learning_rate": 0.00018386635118985024,
      "loss": 2.4574,
      "step": 30310
    },
    {
      "epoch": 0.6237415751429107,
      "grad_norm": 0.3580484390258789,
      "learning_rate": 0.00018385425292052375,
      "loss": 2.4046,
      "step": 30320
    },
    {
      "epoch": 0.6239472946597785,
      "grad_norm": 0.41499045491218567,
      "learning_rate": 0.0001838421505151239,
      "loss": 2.3829,
      "step": 30330
    },
    {
      "epoch": 0.6241530141766463,
      "grad_norm": 0.39143112301826477,
      "learning_rate": 0.00018383004397424764,
      "loss": 2.4555,
      "step": 30340
    },
    {
      "epoch": 0.6243587336935139,
      "grad_norm": 0.45885565876960754,
      "learning_rate": 0.00018381793329849217,
      "loss": 2.4096,
      "step": 30350
    },
    {
      "epoch": 0.6245644532103817,
      "grad_norm": 0.39441561698913574,
      "learning_rate": 0.0001838058184884548,
      "loss": 2.4282,
      "step": 30360
    },
    {
      "epoch": 0.6247701727272493,
      "grad_norm": 0.3773910105228424,
      "learning_rate": 0.0001837936995447331,
      "loss": 2.4776,
      "step": 30370
    },
    {
      "epoch": 0.6249758922441171,
      "grad_norm": 0.43560823798179626,
      "learning_rate": 0.0001837815764679248,
      "loss": 2.411,
      "step": 30380
    },
    {
      "epoch": 0.6251816117609847,
      "grad_norm": 0.3928109109401703,
      "learning_rate": 0.00018376944925862788,
      "loss": 2.3951,
      "step": 30390
    },
    {
      "epoch": 0.6253873312778525,
      "grad_norm": 0.38050520420074463,
      "learning_rate": 0.00018375731791744052,
      "loss": 2.4112,
      "step": 30400
    },
    {
      "epoch": 0.6255930507947202,
      "grad_norm": 0.37686043977737427,
      "learning_rate": 0.0001837451824449611,
      "loss": 2.4572,
      "step": 30410
    },
    {
      "epoch": 0.6257987703115879,
      "grad_norm": 0.38649797439575195,
      "learning_rate": 0.0001837330428417882,
      "loss": 2.4042,
      "step": 30420
    },
    {
      "epoch": 0.6260044898284557,
      "grad_norm": 0.38884827494621277,
      "learning_rate": 0.00018372089910852058,
      "loss": 2.5025,
      "step": 30430
    },
    {
      "epoch": 0.6262102093453233,
      "grad_norm": 0.4120415449142456,
      "learning_rate": 0.0001837087512457572,
      "loss": 2.4306,
      "step": 30440
    },
    {
      "epoch": 0.6264159288621911,
      "grad_norm": 0.38232696056365967,
      "learning_rate": 0.0001836965992540973,
      "loss": 2.4359,
      "step": 30450
    },
    {
      "epoch": 0.6266216483790588,
      "grad_norm": 0.41218405961990356,
      "learning_rate": 0.0001836844431341403,
      "loss": 2.4625,
      "step": 30460
    },
    {
      "epoch": 0.6268273678959265,
      "grad_norm": 0.37055546045303345,
      "learning_rate": 0.00018367228288648568,
      "loss": 2.4419,
      "step": 30470
    },
    {
      "epoch": 0.6270330874127942,
      "grad_norm": 0.38815876841545105,
      "learning_rate": 0.0001836601185117333,
      "loss": 2.4311,
      "step": 30480
    },
    {
      "epoch": 0.6272388069296619,
      "grad_norm": 0.3886885941028595,
      "learning_rate": 0.0001836479500104832,
      "loss": 2.4421,
      "step": 30490
    },
    {
      "epoch": 0.6274445264465296,
      "grad_norm": 0.384259432554245,
      "learning_rate": 0.00018363577738333552,
      "loss": 2.4435,
      "step": 30500
    },
    {
      "epoch": 0.6276502459633974,
      "grad_norm": 0.3842802345752716,
      "learning_rate": 0.00018362360063089075,
      "loss": 2.3945,
      "step": 30510
    },
    {
      "epoch": 0.6278559654802651,
      "grad_norm": 0.4281342029571533,
      "learning_rate": 0.00018361141975374942,
      "loss": 2.4419,
      "step": 30520
    },
    {
      "epoch": 0.6280616849971328,
      "grad_norm": 0.37590157985687256,
      "learning_rate": 0.0001835992347525124,
      "loss": 2.4259,
      "step": 30530
    },
    {
      "epoch": 0.6282674045140005,
      "grad_norm": 0.42104166746139526,
      "learning_rate": 0.0001835870456277807,
      "loss": 2.3812,
      "step": 30540
    },
    {
      "epoch": 0.6284731240308682,
      "grad_norm": 0.36649996042251587,
      "learning_rate": 0.0001835748523801555,
      "loss": 2.4698,
      "step": 30550
    },
    {
      "epoch": 0.628678843547736,
      "grad_norm": 0.3575754165649414,
      "learning_rate": 0.00018356265501023828,
      "loss": 2.4237,
      "step": 30560
    },
    {
      "epoch": 0.6288845630646036,
      "grad_norm": 0.38083896040916443,
      "learning_rate": 0.00018355045351863063,
      "loss": 2.4295,
      "step": 30570
    },
    {
      "epoch": 0.6290902825814714,
      "grad_norm": 0.3833199143409729,
      "learning_rate": 0.00018353824790593443,
      "loss": 2.3957,
      "step": 30580
    },
    {
      "epoch": 0.629296002098339,
      "grad_norm": 0.3907511234283447,
      "learning_rate": 0.00018352603817275168,
      "loss": 2.4606,
      "step": 30590
    },
    {
      "epoch": 0.6295017216152068,
      "grad_norm": 0.4132033586502075,
      "learning_rate": 0.00018351382431968462,
      "loss": 2.4382,
      "step": 30600
    },
    {
      "epoch": 0.6297074411320746,
      "grad_norm": 0.396755188703537,
      "learning_rate": 0.0001835016063473357,
      "loss": 2.4215,
      "step": 30610
    },
    {
      "epoch": 0.6299131606489422,
      "grad_norm": 0.36839234828948975,
      "learning_rate": 0.0001834893842563076,
      "loss": 2.4269,
      "step": 30620
    },
    {
      "epoch": 0.63011888016581,
      "grad_norm": 0.36838042736053467,
      "learning_rate": 0.00018347715804720312,
      "loss": 2.4968,
      "step": 30630
    },
    {
      "epoch": 0.6303245996826776,
      "grad_norm": 0.4888404905796051,
      "learning_rate": 0.0001834649277206253,
      "loss": 2.3934,
      "step": 30640
    },
    {
      "epoch": 0.6305303191995454,
      "grad_norm": 0.370294988155365,
      "learning_rate": 0.00018345269327717746,
      "loss": 2.4633,
      "step": 30650
    },
    {
      "epoch": 0.630736038716413,
      "grad_norm": 0.38386446237564087,
      "learning_rate": 0.00018344045471746303,
      "loss": 2.413,
      "step": 30660
    },
    {
      "epoch": 0.6309417582332808,
      "grad_norm": 0.4391036629676819,
      "learning_rate": 0.00018342821204208565,
      "loss": 2.4849,
      "step": 30670
    },
    {
      "epoch": 0.6311474777501485,
      "grad_norm": 0.42379435896873474,
      "learning_rate": 0.0001834159652516492,
      "loss": 2.4645,
      "step": 30680
    },
    {
      "epoch": 0.6313531972670162,
      "grad_norm": 0.37605953216552734,
      "learning_rate": 0.00018340371434675775,
      "loss": 2.4796,
      "step": 30690
    },
    {
      "epoch": 0.631558916783884,
      "grad_norm": 0.39489665627479553,
      "learning_rate": 0.00018339145932801554,
      "loss": 2.4698,
      "step": 30700
    },
    {
      "epoch": 0.6317646363007516,
      "grad_norm": 0.39027532935142517,
      "learning_rate": 0.0001833792001960271,
      "loss": 2.451,
      "step": 30710
    },
    {
      "epoch": 0.6319703558176194,
      "grad_norm": 0.40117496252059937,
      "learning_rate": 0.00018336693695139705,
      "loss": 2.47,
      "step": 30720
    },
    {
      "epoch": 0.632176075334487,
      "grad_norm": 0.4304233193397522,
      "learning_rate": 0.00018335466959473032,
      "loss": 2.4752,
      "step": 30730
    },
    {
      "epoch": 0.6323817948513548,
      "grad_norm": 0.35184311866760254,
      "learning_rate": 0.00018334239812663197,
      "loss": 2.4215,
      "step": 30740
    },
    {
      "epoch": 0.6325875143682225,
      "grad_norm": 0.3933085799217224,
      "learning_rate": 0.00018333012254770727,
      "loss": 2.4287,
      "step": 30750
    },
    {
      "epoch": 0.6327932338850902,
      "grad_norm": 0.38404303789138794,
      "learning_rate": 0.0001833178428585617,
      "loss": 2.4434,
      "step": 30760
    },
    {
      "epoch": 0.6329989534019579,
      "grad_norm": 0.4088100790977478,
      "learning_rate": 0.000183305559059801,
      "loss": 2.4184,
      "step": 30770
    },
    {
      "epoch": 0.6332046729188257,
      "grad_norm": 0.38176220655441284,
      "learning_rate": 0.00018329327115203104,
      "loss": 2.4149,
      "step": 30780
    },
    {
      "epoch": 0.6334103924356934,
      "grad_norm": 0.4249875843524933,
      "learning_rate": 0.0001832809791358579,
      "loss": 2.4479,
      "step": 30790
    },
    {
      "epoch": 0.6336161119525611,
      "grad_norm": 0.3730480372905731,
      "learning_rate": 0.00018326868301188787,
      "loss": 2.474,
      "step": 30800
    },
    {
      "epoch": 0.6338218314694288,
      "grad_norm": 0.41780728101730347,
      "learning_rate": 0.00018325638278072747,
      "loss": 2.4748,
      "step": 30810
    },
    {
      "epoch": 0.6340275509862965,
      "grad_norm": 0.42061203718185425,
      "learning_rate": 0.00018324407844298341,
      "loss": 2.396,
      "step": 30820
    },
    {
      "epoch": 0.6342332705031642,
      "grad_norm": 0.4074746072292328,
      "learning_rate": 0.00018323176999926257,
      "loss": 2.4667,
      "step": 30830
    },
    {
      "epoch": 0.6344389900200319,
      "grad_norm": 0.3937627971172333,
      "learning_rate": 0.00018321945745017207,
      "loss": 2.4704,
      "step": 30840
    },
    {
      "epoch": 0.6346447095368997,
      "grad_norm": 0.4214900732040405,
      "learning_rate": 0.00018320714079631927,
      "loss": 2.4305,
      "step": 30850
    },
    {
      "epoch": 0.6348504290537673,
      "grad_norm": 0.43081170320510864,
      "learning_rate": 0.0001831948200383116,
      "loss": 2.4662,
      "step": 30860
    },
    {
      "epoch": 0.6350561485706351,
      "grad_norm": 0.44051164388656616,
      "learning_rate": 0.00018318249517675683,
      "loss": 2.4033,
      "step": 30870
    },
    {
      "epoch": 0.6352618680875028,
      "grad_norm": 0.4305107295513153,
      "learning_rate": 0.00018317016621226286,
      "loss": 2.3864,
      "step": 30880
    },
    {
      "epoch": 0.6354675876043705,
      "grad_norm": 0.37840813398361206,
      "learning_rate": 0.00018315783314543782,
      "loss": 2.3139,
      "step": 30890
    },
    {
      "epoch": 0.6356733071212383,
      "grad_norm": 0.40225011110305786,
      "learning_rate": 0.00018314549597689,
      "loss": 2.4183,
      "step": 30900
    },
    {
      "epoch": 0.6358790266381059,
      "grad_norm": 0.39458784461021423,
      "learning_rate": 0.000183133154707228,
      "loss": 2.4703,
      "step": 30910
    },
    {
      "epoch": 0.6360847461549737,
      "grad_norm": 0.3868529796600342,
      "learning_rate": 0.00018312080933706045,
      "loss": 2.4197,
      "step": 30920
    },
    {
      "epoch": 0.6362904656718413,
      "grad_norm": 0.3603861331939697,
      "learning_rate": 0.00018310845986699637,
      "loss": 2.3791,
      "step": 30930
    },
    {
      "epoch": 0.6364961851887091,
      "grad_norm": 0.4009430408477783,
      "learning_rate": 0.0001830961062976448,
      "loss": 2.4803,
      "step": 30940
    },
    {
      "epoch": 0.6367019047055767,
      "grad_norm": 0.36840152740478516,
      "learning_rate": 0.00018308374862961518,
      "loss": 2.4521,
      "step": 30950
    },
    {
      "epoch": 0.6369076242224445,
      "grad_norm": 0.39982521533966064,
      "learning_rate": 0.00018307138686351695,
      "loss": 2.5339,
      "step": 30960
    },
    {
      "epoch": 0.6371133437393123,
      "grad_norm": 0.40768134593963623,
      "learning_rate": 0.00018305902099995992,
      "loss": 2.512,
      "step": 30970
    },
    {
      "epoch": 0.6373190632561799,
      "grad_norm": 0.3907618224620819,
      "learning_rate": 0.000183046651039554,
      "loss": 2.4825,
      "step": 30980
    },
    {
      "epoch": 0.6375247827730477,
      "grad_norm": 0.3928670883178711,
      "learning_rate": 0.00018303427698290934,
      "loss": 2.4443,
      "step": 30990
    },
    {
      "epoch": 0.6377305022899153,
      "grad_norm": 0.387521892786026,
      "learning_rate": 0.00018302189883063626,
      "loss": 2.4228,
      "step": 31000
    },
    {
      "epoch": 0.6379362218067831,
      "grad_norm": 0.4247501492500305,
      "learning_rate": 0.00018300951658334534,
      "loss": 2.3797,
      "step": 31010
    },
    {
      "epoch": 0.6381419413236508,
      "grad_norm": 0.4066753685474396,
      "learning_rate": 0.00018299713024164728,
      "loss": 2.4691,
      "step": 31020
    },
    {
      "epoch": 0.6383476608405185,
      "grad_norm": 0.37518590688705444,
      "learning_rate": 0.0001829847398061531,
      "loss": 2.463,
      "step": 31030
    },
    {
      "epoch": 0.6385533803573862,
      "grad_norm": 0.39059436321258545,
      "learning_rate": 0.0001829723452774739,
      "loss": 2.4274,
      "step": 31040
    },
    {
      "epoch": 0.6387590998742539,
      "grad_norm": 0.3900120258331299,
      "learning_rate": 0.00018295994665622106,
      "loss": 2.4393,
      "step": 31050
    },
    {
      "epoch": 0.6389648193911217,
      "grad_norm": 0.3884585499763489,
      "learning_rate": 0.00018294754394300612,
      "loss": 2.4219,
      "step": 31060
    },
    {
      "epoch": 0.6391705389079894,
      "grad_norm": 0.397116482257843,
      "learning_rate": 0.00018293513713844089,
      "loss": 2.4524,
      "step": 31070
    },
    {
      "epoch": 0.6393762584248571,
      "grad_norm": 0.5639687180519104,
      "learning_rate": 0.00018292272624313724,
      "loss": 2.4033,
      "step": 31080
    },
    {
      "epoch": 0.6395819779417248,
      "grad_norm": 0.3976890742778778,
      "learning_rate": 0.0001829103112577074,
      "loss": 2.4166,
      "step": 31090
    },
    {
      "epoch": 0.6397876974585925,
      "grad_norm": 0.43240055441856384,
      "learning_rate": 0.0001828978921827637,
      "loss": 2.4046,
      "step": 31100
    },
    {
      "epoch": 0.6399934169754602,
      "grad_norm": 0.4125562608242035,
      "learning_rate": 0.00018288546901891874,
      "loss": 2.4189,
      "step": 31110
    },
    {
      "epoch": 0.640199136492328,
      "grad_norm": 0.3782895505428314,
      "learning_rate": 0.00018287304176678524,
      "loss": 2.4367,
      "step": 31120
    },
    {
      "epoch": 0.6404048560091956,
      "grad_norm": 0.3875442445278168,
      "learning_rate": 0.00018286061042697619,
      "loss": 2.3803,
      "step": 31130
    },
    {
      "epoch": 0.6406105755260634,
      "grad_norm": 0.44770851731300354,
      "learning_rate": 0.00018284817500010482,
      "loss": 2.4878,
      "step": 31140
    },
    {
      "epoch": 0.6408162950429311,
      "grad_norm": 0.3993864953517914,
      "learning_rate": 0.00018283573548678436,
      "loss": 2.439,
      "step": 31150
    },
    {
      "epoch": 0.6410220145597988,
      "grad_norm": 0.4023670554161072,
      "learning_rate": 0.00018282329188762852,
      "loss": 2.4576,
      "step": 31160
    },
    {
      "epoch": 0.6412277340766666,
      "grad_norm": 0.4191879630088806,
      "learning_rate": 0.00018281084420325103,
      "loss": 2.4508,
      "step": 31170
    },
    {
      "epoch": 0.6414334535935342,
      "grad_norm": 0.4304000437259674,
      "learning_rate": 0.00018279839243426585,
      "loss": 2.4434,
      "step": 31180
    },
    {
      "epoch": 0.641639173110402,
      "grad_norm": 0.4867490231990814,
      "learning_rate": 0.00018278593658128714,
      "loss": 2.4049,
      "step": 31190
    },
    {
      "epoch": 0.6418448926272696,
      "grad_norm": 0.3931492865085602,
      "learning_rate": 0.00018277347664492934,
      "loss": 2.408,
      "step": 31200
    },
    {
      "epoch": 0.6420506121441374,
      "grad_norm": 0.423790842294693,
      "learning_rate": 0.000182761012625807,
      "loss": 2.4397,
      "step": 31210
    },
    {
      "epoch": 0.642256331661005,
      "grad_norm": 0.3849693238735199,
      "learning_rate": 0.00018274854452453488,
      "loss": 2.4783,
      "step": 31220
    },
    {
      "epoch": 0.6424620511778728,
      "grad_norm": 0.4290071725845337,
      "learning_rate": 0.000182736072341728,
      "loss": 2.4564,
      "step": 31230
    },
    {
      "epoch": 0.6426677706947406,
      "grad_norm": 0.3972611725330353,
      "learning_rate": 0.00018272359607800153,
      "loss": 2.4756,
      "step": 31240
    },
    {
      "epoch": 0.6428734902116082,
      "grad_norm": 0.3784301280975342,
      "learning_rate": 0.00018271111573397083,
      "loss": 2.4466,
      "step": 31250
    },
    {
      "epoch": 0.643079209728476,
      "grad_norm": 0.3913850784301758,
      "learning_rate": 0.00018269863131025154,
      "loss": 2.4064,
      "step": 31260
    },
    {
      "epoch": 0.6432849292453436,
      "grad_norm": 0.411100298166275,
      "learning_rate": 0.00018268614280745943,
      "loss": 2.3633,
      "step": 31270
    },
    {
      "epoch": 0.6434906487622114,
      "grad_norm": 0.36639419198036194,
      "learning_rate": 0.00018267365022621047,
      "loss": 2.4064,
      "step": 31280
    },
    {
      "epoch": 0.6436963682790791,
      "grad_norm": 0.3955020606517792,
      "learning_rate": 0.00018266115356712086,
      "loss": 2.453,
      "step": 31290
    },
    {
      "epoch": 0.6439020877959468,
      "grad_norm": 0.4055512845516205,
      "learning_rate": 0.00018264865283080698,
      "loss": 2.4125,
      "step": 31300
    },
    {
      "epoch": 0.6441078073128145,
      "grad_norm": 0.41097012162208557,
      "learning_rate": 0.00018263614801788546,
      "loss": 2.4634,
      "step": 31310
    },
    {
      "epoch": 0.6443135268296822,
      "grad_norm": 0.4914601743221283,
      "learning_rate": 0.00018262363912897306,
      "loss": 2.4767,
      "step": 31320
    },
    {
      "epoch": 0.64451924634655,
      "grad_norm": 0.3933935761451721,
      "learning_rate": 0.00018261112616468677,
      "loss": 2.4551,
      "step": 31330
    },
    {
      "epoch": 0.6447249658634177,
      "grad_norm": 0.399190753698349,
      "learning_rate": 0.0001825986091256438,
      "loss": 2.4719,
      "step": 31340
    },
    {
      "epoch": 0.6449306853802854,
      "grad_norm": 0.3639136552810669,
      "learning_rate": 0.00018258608801246157,
      "loss": 2.3602,
      "step": 31350
    },
    {
      "epoch": 0.6451364048971531,
      "grad_norm": 0.375212699174881,
      "learning_rate": 0.00018257356282575765,
      "loss": 2.3989,
      "step": 31360
    },
    {
      "epoch": 0.6453421244140208,
      "grad_norm": 0.36835309863090515,
      "learning_rate": 0.0001825610335661499,
      "loss": 2.4055,
      "step": 31370
    },
    {
      "epoch": 0.6455478439308885,
      "grad_norm": 0.384754478931427,
      "learning_rate": 0.00018254850023425619,
      "loss": 2.4325,
      "step": 31380
    },
    {
      "epoch": 0.6457535634477563,
      "grad_norm": 0.3830329477787018,
      "learning_rate": 0.00018253596283069484,
      "loss": 2.4303,
      "step": 31390
    },
    {
      "epoch": 0.6459592829646239,
      "grad_norm": 0.39608973264694214,
      "learning_rate": 0.0001825234213560842,
      "loss": 2.4323,
      "step": 31400
    },
    {
      "epoch": 0.6461650024814917,
      "grad_norm": 0.4276471436023712,
      "learning_rate": 0.00018251087581104287,
      "loss": 2.4424,
      "step": 31410
    },
    {
      "epoch": 0.6463707219983594,
      "grad_norm": 0.3970215618610382,
      "learning_rate": 0.00018249832619618967,
      "loss": 2.4433,
      "step": 31420
    },
    {
      "epoch": 0.6465764415152271,
      "grad_norm": 0.3704990744590759,
      "learning_rate": 0.00018248577251214364,
      "loss": 2.4161,
      "step": 31430
    },
    {
      "epoch": 0.6467821610320948,
      "grad_norm": 0.38747644424438477,
      "learning_rate": 0.0001824732147595239,
      "loss": 2.4173,
      "step": 31440
    },
    {
      "epoch": 0.6469878805489625,
      "grad_norm": 0.4176204800605774,
      "learning_rate": 0.0001824606529389499,
      "loss": 2.4404,
      "step": 31450
    },
    {
      "epoch": 0.6471936000658303,
      "grad_norm": 0.4025680124759674,
      "learning_rate": 0.00018244808705104127,
      "loss": 2.3897,
      "step": 31460
    },
    {
      "epoch": 0.6473993195826979,
      "grad_norm": 0.37308794260025024,
      "learning_rate": 0.00018243551709641782,
      "loss": 2.4112,
      "step": 31470
    },
    {
      "epoch": 0.6476050390995657,
      "grad_norm": 0.3882266879081726,
      "learning_rate": 0.0001824229430756995,
      "loss": 2.4161,
      "step": 31480
    },
    {
      "epoch": 0.6478107586164333,
      "grad_norm": 0.3985980153083801,
      "learning_rate": 0.00018241036498950654,
      "loss": 2.4487,
      "step": 31490
    },
    {
      "epoch": 0.6480164781333011,
      "grad_norm": 0.4086713194847107,
      "learning_rate": 0.00018239778283845936,
      "loss": 2.4676,
      "step": 31500
    },
    {
      "epoch": 0.6482221976501689,
      "grad_norm": 0.38566112518310547,
      "learning_rate": 0.00018238519662317856,
      "loss": 2.4408,
      "step": 31510
    },
    {
      "epoch": 0.6484279171670365,
      "grad_norm": 0.3960205018520355,
      "learning_rate": 0.00018237260634428498,
      "loss": 2.4313,
      "step": 31520
    },
    {
      "epoch": 0.6486336366839043,
      "grad_norm": 0.3851509988307953,
      "learning_rate": 0.0001823600120023996,
      "loss": 2.3988,
      "step": 31530
    },
    {
      "epoch": 0.6488393562007719,
      "grad_norm": 0.40544334053993225,
      "learning_rate": 0.00018234741359814366,
      "loss": 2.4417,
      "step": 31540
    },
    {
      "epoch": 0.6490450757176397,
      "grad_norm": 0.3783136308193207,
      "learning_rate": 0.00018233481113213849,
      "loss": 2.4589,
      "step": 31550
    },
    {
      "epoch": 0.6492507952345073,
      "grad_norm": 0.373760849237442,
      "learning_rate": 0.0001823222046050058,
      "loss": 2.4267,
      "step": 31560
    },
    {
      "epoch": 0.6494565147513751,
      "grad_norm": 0.4048673212528229,
      "learning_rate": 0.00018230959401736732,
      "loss": 2.4517,
      "step": 31570
    },
    {
      "epoch": 0.6496622342682428,
      "grad_norm": 0.4090200960636139,
      "learning_rate": 0.00018229697936984515,
      "loss": 2.3842,
      "step": 31580
    },
    {
      "epoch": 0.6498679537851105,
      "grad_norm": 0.4315798580646515,
      "learning_rate": 0.00018228436066306143,
      "loss": 2.4469,
      "step": 31590
    },
    {
      "epoch": 0.6500736733019783,
      "grad_norm": 0.3798850476741791,
      "learning_rate": 0.00018227173789763857,
      "loss": 2.4025,
      "step": 31600
    },
    {
      "epoch": 0.650279392818846,
      "grad_norm": 0.40969642996788025,
      "learning_rate": 0.00018225911107419925,
      "loss": 2.4305,
      "step": 31610
    },
    {
      "epoch": 0.6504851123357137,
      "grad_norm": 0.39331763982772827,
      "learning_rate": 0.00018224648019336622,
      "loss": 2.4822,
      "step": 31620
    },
    {
      "epoch": 0.6506908318525814,
      "grad_norm": 0.43466201424598694,
      "learning_rate": 0.0001822338452557625,
      "loss": 2.4221,
      "step": 31630
    },
    {
      "epoch": 0.6508965513694491,
      "grad_norm": 0.37645667791366577,
      "learning_rate": 0.00018222120626201134,
      "loss": 2.4045,
      "step": 31640
    },
    {
      "epoch": 0.6511022708863168,
      "grad_norm": 0.3747856914997101,
      "learning_rate": 0.0001822085632127361,
      "loss": 2.4565,
      "step": 31650
    },
    {
      "epoch": 0.6513079904031845,
      "grad_norm": 0.4065949022769928,
      "learning_rate": 0.00018219591610856044,
      "loss": 2.4498,
      "step": 31660
    },
    {
      "epoch": 0.6515137099200522,
      "grad_norm": 0.3760444223880768,
      "learning_rate": 0.00018218326495010815,
      "loss": 2.4279,
      "step": 31670
    },
    {
      "epoch": 0.65171942943692,
      "grad_norm": 0.42080146074295044,
      "learning_rate": 0.00018217060973800325,
      "loss": 2.4197,
      "step": 31680
    },
    {
      "epoch": 0.6519251489537877,
      "grad_norm": 0.38877570629119873,
      "learning_rate": 0.0001821579504728699,
      "loss": 2.4588,
      "step": 31690
    },
    {
      "epoch": 0.6521308684706554,
      "grad_norm": 0.41303035616874695,
      "learning_rate": 0.00018214528715533262,
      "loss": 2.428,
      "step": 31700
    },
    {
      "epoch": 0.6523365879875231,
      "grad_norm": 0.3782491683959961,
      "learning_rate": 0.00018213261978601594,
      "loss": 2.4367,
      "step": 31710
    },
    {
      "epoch": 0.6525423075043908,
      "grad_norm": 0.36412709951400757,
      "learning_rate": 0.0001821199483655447,
      "loss": 2.4279,
      "step": 31720
    },
    {
      "epoch": 0.6527480270212586,
      "grad_norm": 0.3600834608078003,
      "learning_rate": 0.00018210727289454389,
      "loss": 2.4191,
      "step": 31730
    },
    {
      "epoch": 0.6529537465381262,
      "grad_norm": 0.3574801981449127,
      "learning_rate": 0.00018209459337363874,
      "loss": 2.4656,
      "step": 31740
    },
    {
      "epoch": 0.653159466054994,
      "grad_norm": 0.3758185803890228,
      "learning_rate": 0.0001820819098034547,
      "loss": 2.4286,
      "step": 31750
    },
    {
      "epoch": 0.6533651855718617,
      "grad_norm": 0.4163688123226166,
      "learning_rate": 0.00018206922218461728,
      "loss": 2.4136,
      "step": 31760
    },
    {
      "epoch": 0.6535709050887294,
      "grad_norm": 0.5189310312271118,
      "learning_rate": 0.0001820565305177524,
      "loss": 2.4585,
      "step": 31770
    },
    {
      "epoch": 0.6537766246055972,
      "grad_norm": 0.3736894130706787,
      "learning_rate": 0.000182043834803486,
      "loss": 2.4104,
      "step": 31780
    },
    {
      "epoch": 0.6539823441224648,
      "grad_norm": 0.4013892710208893,
      "learning_rate": 0.00018203113504244436,
      "loss": 2.3859,
      "step": 31790
    },
    {
      "epoch": 0.6541880636393326,
      "grad_norm": 0.3911066949367523,
      "learning_rate": 0.0001820184312352538,
      "loss": 2.4457,
      "step": 31800
    },
    {
      "epoch": 0.6543937831562002,
      "grad_norm": 0.4243726134300232,
      "learning_rate": 0.000182005723382541,
      "loss": 2.46,
      "step": 31810
    },
    {
      "epoch": 0.654599502673068,
      "grad_norm": 0.37105056643486023,
      "learning_rate": 0.00018199301148493272,
      "loss": 2.4957,
      "step": 31820
    },
    {
      "epoch": 0.6548052221899356,
      "grad_norm": 0.4006940424442291,
      "learning_rate": 0.00018198029554305597,
      "loss": 2.4156,
      "step": 31830
    },
    {
      "epoch": 0.6550109417068034,
      "grad_norm": 0.4078596532344818,
      "learning_rate": 0.00018196757555753802,
      "loss": 2.4402,
      "step": 31840
    },
    {
      "epoch": 0.6552166612236712,
      "grad_norm": 0.4579009711742401,
      "learning_rate": 0.00018195485152900626,
      "loss": 2.4854,
      "step": 31850
    },
    {
      "epoch": 0.6554223807405388,
      "grad_norm": 0.38106569647789,
      "learning_rate": 0.00018194212345808822,
      "loss": 2.4931,
      "step": 31860
    },
    {
      "epoch": 0.6556281002574066,
      "grad_norm": 0.3824596405029297,
      "learning_rate": 0.00018192939134541178,
      "loss": 2.4702,
      "step": 31870
    },
    {
      "epoch": 0.6558338197742742,
      "grad_norm": 0.39858371019363403,
      "learning_rate": 0.00018191665519160492,
      "loss": 2.5258,
      "step": 31880
    },
    {
      "epoch": 0.656039539291142,
      "grad_norm": 0.4079461991786957,
      "learning_rate": 0.00018190391499729586,
      "loss": 2.4121,
      "step": 31890
    },
    {
      "epoch": 0.6562452588080097,
      "grad_norm": 0.4271887540817261,
      "learning_rate": 0.00018189117076311298,
      "loss": 2.438,
      "step": 31900
    },
    {
      "epoch": 0.6564509783248774,
      "grad_norm": 0.3920520544052124,
      "learning_rate": 0.00018187842248968493,
      "loss": 2.4178,
      "step": 31910
    },
    {
      "epoch": 0.6566566978417451,
      "grad_norm": 0.37835827469825745,
      "learning_rate": 0.00018186567017764045,
      "loss": 2.4514,
      "step": 31920
    },
    {
      "epoch": 0.6568624173586128,
      "grad_norm": 0.38605421781539917,
      "learning_rate": 0.00018185291382760862,
      "loss": 2.3944,
      "step": 31930
    },
    {
      "epoch": 0.6570681368754806,
      "grad_norm": 0.38287681341171265,
      "learning_rate": 0.00018184015344021857,
      "loss": 2.4133,
      "step": 31940
    },
    {
      "epoch": 0.6572738563923483,
      "grad_norm": 0.3837150037288666,
      "learning_rate": 0.00018182738901609973,
      "loss": 2.4888,
      "step": 31950
    },
    {
      "epoch": 0.657479575909216,
      "grad_norm": 0.3746720850467682,
      "learning_rate": 0.00018181462055588167,
      "loss": 2.4286,
      "step": 31960
    },
    {
      "epoch": 0.6576852954260837,
      "grad_norm": 0.38737547397613525,
      "learning_rate": 0.00018180184806019424,
      "loss": 2.4581,
      "step": 31970
    },
    {
      "epoch": 0.6578910149429514,
      "grad_norm": 0.36134985089302063,
      "learning_rate": 0.00018178907152966743,
      "loss": 2.4505,
      "step": 31980
    },
    {
      "epoch": 0.6580967344598191,
      "grad_norm": 0.420884907245636,
      "learning_rate": 0.00018177629096493138,
      "loss": 2.4153,
      "step": 31990
    },
    {
      "epoch": 0.6583024539766869,
      "grad_norm": 0.38500094413757324,
      "learning_rate": 0.00018176350636661654,
      "loss": 2.3962,
      "step": 32000
    },
    {
      "epoch": 0.6585081734935545,
      "grad_norm": 0.3989892303943634,
      "learning_rate": 0.0001817507177353535,
      "loss": 2.4767,
      "step": 32010
    },
    {
      "epoch": 0.6587138930104223,
      "grad_norm": 0.3932584226131439,
      "learning_rate": 0.00018173792507177302,
      "loss": 2.4334,
      "step": 32020
    },
    {
      "epoch": 0.65891961252729,
      "grad_norm": 0.4094153940677643,
      "learning_rate": 0.0001817251283765061,
      "loss": 2.4873,
      "step": 32030
    },
    {
      "epoch": 0.6591253320441577,
      "grad_norm": 0.36514514684677124,
      "learning_rate": 0.00018171232765018397,
      "loss": 2.408,
      "step": 32040
    },
    {
      "epoch": 0.6593310515610254,
      "grad_norm": 0.38743236660957336,
      "learning_rate": 0.000181699522893438,
      "loss": 2.4579,
      "step": 32050
    },
    {
      "epoch": 0.6595367710778931,
      "grad_norm": 0.3845030665397644,
      "learning_rate": 0.00018168671410689977,
      "loss": 2.4277,
      "step": 32060
    },
    {
      "epoch": 0.6597424905947609,
      "grad_norm": 0.4344560503959656,
      "learning_rate": 0.00018167390129120108,
      "loss": 2.4723,
      "step": 32070
    },
    {
      "epoch": 0.6599482101116285,
      "grad_norm": 0.3978974521160126,
      "learning_rate": 0.0001816610844469739,
      "loss": 2.3945,
      "step": 32080
    },
    {
      "epoch": 0.6601539296284963,
      "grad_norm": 0.38266056776046753,
      "learning_rate": 0.00018164826357485044,
      "loss": 2.4694,
      "step": 32090
    },
    {
      "epoch": 0.6603596491453639,
      "grad_norm": 0.3812323808670044,
      "learning_rate": 0.00018163543867546304,
      "loss": 2.4162,
      "step": 32100
    },
    {
      "epoch": 0.6605653686622317,
      "grad_norm": 0.35620376467704773,
      "learning_rate": 0.0001816226097494443,
      "loss": 2.4345,
      "step": 32110
    },
    {
      "epoch": 0.6607710881790995,
      "grad_norm": 0.442793607711792,
      "learning_rate": 0.00018160977679742706,
      "loss": 2.4608,
      "step": 32120
    },
    {
      "epoch": 0.6609768076959671,
      "grad_norm": 0.3822553753852844,
      "learning_rate": 0.00018159693982004423,
      "loss": 2.4285,
      "step": 32130
    },
    {
      "epoch": 0.6611825272128349,
      "grad_norm": 0.3874679505825043,
      "learning_rate": 0.000181584098817929,
      "loss": 2.4748,
      "step": 32140
    },
    {
      "epoch": 0.6613882467297025,
      "grad_norm": 0.3945654332637787,
      "learning_rate": 0.00018157125379171472,
      "loss": 2.4679,
      "step": 32150
    },
    {
      "epoch": 0.6615939662465703,
      "grad_norm": 0.424717515707016,
      "learning_rate": 0.00018155840474203507,
      "loss": 2.4293,
      "step": 32160
    },
    {
      "epoch": 0.661799685763438,
      "grad_norm": 0.36403918266296387,
      "learning_rate": 0.00018154555166952373,
      "loss": 2.4282,
      "step": 32170
    },
    {
      "epoch": 0.6620054052803057,
      "grad_norm": 0.4117281436920166,
      "learning_rate": 0.00018153269457481467,
      "loss": 2.4377,
      "step": 32180
    },
    {
      "epoch": 0.6622111247971734,
      "grad_norm": 0.41573667526245117,
      "learning_rate": 0.00018151983345854214,
      "loss": 2.4628,
      "step": 32190
    },
    {
      "epoch": 0.6624168443140411,
      "grad_norm": 0.40752550959587097,
      "learning_rate": 0.0001815069683213404,
      "loss": 2.4431,
      "step": 32200
    },
    {
      "epoch": 0.6626225638309089,
      "grad_norm": 0.4214719831943512,
      "learning_rate": 0.00018149409916384408,
      "loss": 2.473,
      "step": 32210
    },
    {
      "epoch": 0.6628282833477765,
      "grad_norm": 0.38957148790359497,
      "learning_rate": 0.00018148122598668796,
      "loss": 2.4346,
      "step": 32220
    },
    {
      "epoch": 0.6630340028646443,
      "grad_norm": 0.40917810797691345,
      "learning_rate": 0.00018146834879050698,
      "loss": 2.3906,
      "step": 32230
    },
    {
      "epoch": 0.663239722381512,
      "grad_norm": 0.3866139352321625,
      "learning_rate": 0.0001814554675759363,
      "loss": 2.438,
      "step": 32240
    },
    {
      "epoch": 0.6634454418983797,
      "grad_norm": 0.41143175959587097,
      "learning_rate": 0.00018144258234361127,
      "loss": 2.4623,
      "step": 32250
    },
    {
      "epoch": 0.6636511614152474,
      "grad_norm": 0.34869083762168884,
      "learning_rate": 0.00018142969309416747,
      "loss": 2.474,
      "step": 32260
    },
    {
      "epoch": 0.6638568809321151,
      "grad_norm": 0.37570199370384216,
      "learning_rate": 0.00018141679982824064,
      "loss": 2.4122,
      "step": 32270
    },
    {
      "epoch": 0.6640626004489828,
      "grad_norm": 0.38892853260040283,
      "learning_rate": 0.00018140390254646675,
      "loss": 2.4557,
      "step": 32280
    },
    {
      "epoch": 0.6642683199658506,
      "grad_norm": 0.47246816754341125,
      "learning_rate": 0.00018139100124948195,
      "loss": 2.4346,
      "step": 32290
    },
    {
      "epoch": 0.6644740394827183,
      "grad_norm": 0.4226776957511902,
      "learning_rate": 0.0001813780959379226,
      "loss": 2.4929,
      "step": 32300
    },
    {
      "epoch": 0.664679758999586,
      "grad_norm": 0.4045863151550293,
      "learning_rate": 0.0001813651866124252,
      "loss": 2.4234,
      "step": 32310
    },
    {
      "epoch": 0.6648854785164537,
      "grad_norm": 0.45015329122543335,
      "learning_rate": 0.00018135227327362657,
      "loss": 2.4883,
      "step": 32320
    },
    {
      "epoch": 0.6650911980333214,
      "grad_norm": 0.3947961628437042,
      "learning_rate": 0.00018133935592216357,
      "loss": 2.4494,
      "step": 32330
    },
    {
      "epoch": 0.6652969175501892,
      "grad_norm": 0.3695177733898163,
      "learning_rate": 0.0001813264345586734,
      "loss": 2.4675,
      "step": 32340
    },
    {
      "epoch": 0.6655026370670568,
      "grad_norm": 0.40132173895835876,
      "learning_rate": 0.00018131350918379344,
      "loss": 2.4448,
      "step": 32350
    },
    {
      "epoch": 0.6657083565839246,
      "grad_norm": 0.3866599500179291,
      "learning_rate": 0.00018130057979816116,
      "loss": 2.4414,
      "step": 32360
    },
    {
      "epoch": 0.6659140761007922,
      "grad_norm": 0.45160406827926636,
      "learning_rate": 0.00018128764640241426,
      "loss": 2.4195,
      "step": 32370
    },
    {
      "epoch": 0.66611979561766,
      "grad_norm": 0.35473954677581787,
      "learning_rate": 0.00018127470899719078,
      "loss": 2.3937,
      "step": 32380
    },
    {
      "epoch": 0.6663255151345278,
      "grad_norm": 0.41192397475242615,
      "learning_rate": 0.00018126176758312878,
      "loss": 2.4456,
      "step": 32390
    },
    {
      "epoch": 0.6665312346513954,
      "grad_norm": 0.3859054744243622,
      "learning_rate": 0.0001812488221608666,
      "loss": 2.4137,
      "step": 32400
    },
    {
      "epoch": 0.6667369541682632,
      "grad_norm": 0.3790111541748047,
      "learning_rate": 0.0001812358727310428,
      "loss": 2.3816,
      "step": 32410
    },
    {
      "epoch": 0.6669426736851308,
      "grad_norm": 0.38135626912117004,
      "learning_rate": 0.00018122291929429608,
      "loss": 2.3668,
      "step": 32420
    },
    {
      "epoch": 0.6671483932019986,
      "grad_norm": 0.40101704001426697,
      "learning_rate": 0.00018120996185126532,
      "loss": 2.4599,
      "step": 32430
    },
    {
      "epoch": 0.6673541127188662,
      "grad_norm": 0.3831178843975067,
      "learning_rate": 0.0001811970004025897,
      "loss": 2.4419,
      "step": 32440
    },
    {
      "epoch": 0.667559832235734,
      "grad_norm": 0.3842070996761322,
      "learning_rate": 0.00018118403494890855,
      "loss": 2.4396,
      "step": 32450
    },
    {
      "epoch": 0.6677655517526017,
      "grad_norm": 0.38434046506881714,
      "learning_rate": 0.00018117106549086134,
      "loss": 2.4494,
      "step": 32460
    },
    {
      "epoch": 0.6679712712694694,
      "grad_norm": 0.3623722493648529,
      "learning_rate": 0.0001811580920290878,
      "loss": 2.418,
      "step": 32470
    },
    {
      "epoch": 0.6681769907863372,
      "grad_norm": 0.4362773895263672,
      "learning_rate": 0.00018114511456422786,
      "loss": 2.4116,
      "step": 32480
    },
    {
      "epoch": 0.6683827103032048,
      "grad_norm": 0.3918570876121521,
      "learning_rate": 0.00018113213309692156,
      "loss": 2.3932,
      "step": 32490
    },
    {
      "epoch": 0.6685884298200726,
      "grad_norm": 0.3908328413963318,
      "learning_rate": 0.0001811191476278093,
      "loss": 2.4106,
      "step": 32500
    },
    {
      "epoch": 0.6687941493369403,
      "grad_norm": 0.4089934825897217,
      "learning_rate": 0.0001811061581575315,
      "loss": 2.4419,
      "step": 32510
    },
    {
      "epoch": 0.668999868853808,
      "grad_norm": 0.40066665410995483,
      "learning_rate": 0.0001810931646867289,
      "loss": 2.4379,
      "step": 32520
    },
    {
      "epoch": 0.6692055883706757,
      "grad_norm": 0.39808881282806396,
      "learning_rate": 0.00018108016721604238,
      "loss": 2.447,
      "step": 32530
    },
    {
      "epoch": 0.6694113078875434,
      "grad_norm": 0.42487001419067383,
      "learning_rate": 0.00018106716574611308,
      "loss": 2.4408,
      "step": 32540
    },
    {
      "epoch": 0.6696170274044111,
      "grad_norm": 0.4006194472312927,
      "learning_rate": 0.0001810541602775822,
      "loss": 2.3855,
      "step": 32550
    },
    {
      "epoch": 0.6698227469212789,
      "grad_norm": 0.39565709233283997,
      "learning_rate": 0.00018104115081109135,
      "loss": 2.4084,
      "step": 32560
    },
    {
      "epoch": 0.6700284664381466,
      "grad_norm": 0.3904688358306885,
      "learning_rate": 0.0001810281373472821,
      "loss": 2.4972,
      "step": 32570
    },
    {
      "epoch": 0.6702341859550143,
      "grad_norm": 0.36379939317703247,
      "learning_rate": 0.00018101511988679643,
      "loss": 2.4397,
      "step": 32580
    },
    {
      "epoch": 0.670439905471882,
      "grad_norm": 0.39405617117881775,
      "learning_rate": 0.00018100209843027634,
      "loss": 2.4418,
      "step": 32590
    },
    {
      "epoch": 0.6706456249887497,
      "grad_norm": 0.41929522156715393,
      "learning_rate": 0.00018098907297836415,
      "loss": 2.397,
      "step": 32600
    },
    {
      "epoch": 0.6708513445056175,
      "grad_norm": 0.38189011812210083,
      "learning_rate": 0.00018097604353170231,
      "loss": 2.4333,
      "step": 32610
    },
    {
      "epoch": 0.6710570640224851,
      "grad_norm": 0.39517828822135925,
      "learning_rate": 0.00018096301009093354,
      "loss": 2.4336,
      "step": 32620
    },
    {
      "epoch": 0.6712627835393529,
      "grad_norm": 0.43236666917800903,
      "learning_rate": 0.00018094997265670067,
      "loss": 2.4214,
      "step": 32630
    },
    {
      "epoch": 0.6714685030562205,
      "grad_norm": 0.45511680841445923,
      "learning_rate": 0.0001809369312296468,
      "loss": 2.4134,
      "step": 32640
    },
    {
      "epoch": 0.6716742225730883,
      "grad_norm": 0.3878018260002136,
      "learning_rate": 0.00018092388581041516,
      "loss": 2.4104,
      "step": 32650
    },
    {
      "epoch": 0.671879942089956,
      "grad_norm": 0.3949885070323944,
      "learning_rate": 0.00018091083639964917,
      "loss": 2.4683,
      "step": 32660
    },
    {
      "epoch": 0.6720856616068237,
      "grad_norm": 0.38226398825645447,
      "learning_rate": 0.00018089778299799255,
      "loss": 2.4671,
      "step": 32670
    },
    {
      "epoch": 0.6722913811236915,
      "grad_norm": 0.4089620113372803,
      "learning_rate": 0.00018088472560608915,
      "loss": 2.4255,
      "step": 32680
    },
    {
      "epoch": 0.6724971006405591,
      "grad_norm": 0.39803609251976013,
      "learning_rate": 0.00018087166422458298,
      "loss": 2.4512,
      "step": 32690
    },
    {
      "epoch": 0.6727028201574269,
      "grad_norm": 0.42325443029403687,
      "learning_rate": 0.00018085859885411836,
      "loss": 2.4634,
      "step": 32700
    },
    {
      "epoch": 0.6729085396742945,
      "grad_norm": 0.3633739948272705,
      "learning_rate": 0.00018084552949533964,
      "loss": 2.4562,
      "step": 32710
    },
    {
      "epoch": 0.6731142591911623,
      "grad_norm": 0.41570866107940674,
      "learning_rate": 0.0001808324561488915,
      "loss": 2.4582,
      "step": 32720
    },
    {
      "epoch": 0.67331997870803,
      "grad_norm": 0.4677615165710449,
      "learning_rate": 0.0001808193788154188,
      "loss": 2.4179,
      "step": 32730
    },
    {
      "epoch": 0.6735256982248977,
      "grad_norm": 0.42686769366264343,
      "learning_rate": 0.00018080629749556656,
      "loss": 2.5083,
      "step": 32740
    },
    {
      "epoch": 0.6737314177417655,
      "grad_norm": 0.37559157609939575,
      "learning_rate": 0.00018079321218998003,
      "loss": 2.4409,
      "step": 32750
    },
    {
      "epoch": 0.6739371372586331,
      "grad_norm": 0.3820463716983795,
      "learning_rate": 0.00018078012289930454,
      "loss": 2.4297,
      "step": 32760
    },
    {
      "epoch": 0.6741428567755009,
      "grad_norm": 0.3782232999801636,
      "learning_rate": 0.00018076702962418586,
      "loss": 2.4215,
      "step": 32770
    },
    {
      "epoch": 0.6743485762923686,
      "grad_norm": 0.42218413949012756,
      "learning_rate": 0.0001807539323652697,
      "loss": 2.4196,
      "step": 32780
    },
    {
      "epoch": 0.6745542958092363,
      "grad_norm": 0.40641576051712036,
      "learning_rate": 0.00018074083112320212,
      "loss": 2.4172,
      "step": 32790
    },
    {
      "epoch": 0.674760015326104,
      "grad_norm": 0.37449565529823303,
      "learning_rate": 0.0001807277258986293,
      "loss": 2.3985,
      "step": 32800
    },
    {
      "epoch": 0.6749657348429717,
      "grad_norm": 0.4496915340423584,
      "learning_rate": 0.00018071461669219772,
      "loss": 2.4422,
      "step": 32810
    },
    {
      "epoch": 0.6751714543598394,
      "grad_norm": 0.39457643032073975,
      "learning_rate": 0.00018070150350455393,
      "loss": 2.4369,
      "step": 32820
    },
    {
      "epoch": 0.6753771738767071,
      "grad_norm": 0.40132826566696167,
      "learning_rate": 0.00018068838633634473,
      "loss": 2.3804,
      "step": 32830
    },
    {
      "epoch": 0.6755828933935749,
      "grad_norm": 0.42822766304016113,
      "learning_rate": 0.00018067526518821713,
      "loss": 2.4159,
      "step": 32840
    },
    {
      "epoch": 0.6757886129104426,
      "grad_norm": 0.43995508551597595,
      "learning_rate": 0.00018066214006081833,
      "loss": 2.4664,
      "step": 32850
    },
    {
      "epoch": 0.6759943324273103,
      "grad_norm": 0.4045553505420685,
      "learning_rate": 0.00018064901095479573,
      "loss": 2.4062,
      "step": 32860
    },
    {
      "epoch": 0.676200051944178,
      "grad_norm": 0.3780114948749542,
      "learning_rate": 0.00018063587787079687,
      "loss": 2.4665,
      "step": 32870
    },
    {
      "epoch": 0.6764057714610457,
      "grad_norm": 0.4140525460243225,
      "learning_rate": 0.0001806227408094696,
      "loss": 2.3952,
      "step": 32880
    },
    {
      "epoch": 0.6766114909779134,
      "grad_norm": 0.380340039730072,
      "learning_rate": 0.00018060959977146186,
      "loss": 2.4205,
      "step": 32890
    },
    {
      "epoch": 0.6768172104947812,
      "grad_norm": 0.3573397099971771,
      "learning_rate": 0.00018059645475742181,
      "loss": 2.4377,
      "step": 32900
    },
    {
      "epoch": 0.6770229300116488,
      "grad_norm": 0.3724111318588257,
      "learning_rate": 0.00018058330576799786,
      "loss": 2.4112,
      "step": 32910
    },
    {
      "epoch": 0.6772286495285166,
      "grad_norm": 0.4205867052078247,
      "learning_rate": 0.00018057015280383858,
      "loss": 2.4901,
      "step": 32920
    },
    {
      "epoch": 0.6774343690453843,
      "grad_norm": 0.351205974817276,
      "learning_rate": 0.0001805569958655927,
      "loss": 2.4673,
      "step": 32930
    },
    {
      "epoch": 0.677640088562252,
      "grad_norm": 0.41080623865127563,
      "learning_rate": 0.0001805438349539092,
      "loss": 2.4413,
      "step": 32940
    },
    {
      "epoch": 0.6778458080791198,
      "grad_norm": 0.3961770534515381,
      "learning_rate": 0.00018053067006943723,
      "loss": 2.4546,
      "step": 32950
    },
    {
      "epoch": 0.6780515275959874,
      "grad_norm": 0.39871785044670105,
      "learning_rate": 0.00018051750121282615,
      "loss": 2.4584,
      "step": 32960
    },
    {
      "epoch": 0.6782572471128552,
      "grad_norm": 0.3909696638584137,
      "learning_rate": 0.0001805043283847255,
      "loss": 2.4035,
      "step": 32970
    },
    {
      "epoch": 0.6784629666297228,
      "grad_norm": 0.4191798269748688,
      "learning_rate": 0.000180491151585785,
      "loss": 2.4982,
      "step": 32980
    },
    {
      "epoch": 0.6786686861465906,
      "grad_norm": 0.3827713429927826,
      "learning_rate": 0.00018047797081665467,
      "loss": 2.453,
      "step": 32990
    },
    {
      "epoch": 0.6788744056634582,
      "grad_norm": 0.384298175573349,
      "learning_rate": 0.00018046478607798455,
      "loss": 2.4537,
      "step": 33000
    },
    {
      "epoch": 0.679080125180326,
      "grad_norm": 0.4274574816226959,
      "learning_rate": 0.00018045159737042506,
      "loss": 2.4621,
      "step": 33010
    },
    {
      "epoch": 0.6792858446971938,
      "grad_norm": 0.36313509941101074,
      "learning_rate": 0.00018043840469462663,
      "loss": 2.4087,
      "step": 33020
    },
    {
      "epoch": 0.6794915642140614,
      "grad_norm": 0.4183594882488251,
      "learning_rate": 0.00018042520805124006,
      "loss": 2.4568,
      "step": 33030
    },
    {
      "epoch": 0.6796972837309292,
      "grad_norm": 0.4139261841773987,
      "learning_rate": 0.00018041200744091623,
      "loss": 2.4165,
      "step": 33040
    },
    {
      "epoch": 0.6799030032477968,
      "grad_norm": 0.37697362899780273,
      "learning_rate": 0.00018039880286430625,
      "loss": 2.4117,
      "step": 33050
    },
    {
      "epoch": 0.6801087227646646,
      "grad_norm": 0.41314128041267395,
      "learning_rate": 0.00018038559432206148,
      "loss": 2.4584,
      "step": 33060
    },
    {
      "epoch": 0.6803144422815323,
      "grad_norm": 0.37735050916671753,
      "learning_rate": 0.00018037238181483337,
      "loss": 2.4034,
      "step": 33070
    },
    {
      "epoch": 0.6805201617984,
      "grad_norm": 0.402106910943985,
      "learning_rate": 0.00018035916534327366,
      "loss": 2.4309,
      "step": 33080
    },
    {
      "epoch": 0.6807258813152677,
      "grad_norm": 0.40286219120025635,
      "learning_rate": 0.0001803459449080342,
      "loss": 2.4056,
      "step": 33090
    },
    {
      "epoch": 0.6809316008321354,
      "grad_norm": 0.38784950971603394,
      "learning_rate": 0.00018033272050976712,
      "loss": 2.3684,
      "step": 33100
    },
    {
      "epoch": 0.6811373203490032,
      "grad_norm": 0.4198208451271057,
      "learning_rate": 0.00018031949214912472,
      "loss": 2.5103,
      "step": 33110
    },
    {
      "epoch": 0.6813430398658709,
      "grad_norm": 0.3770025074481964,
      "learning_rate": 0.00018030625982675945,
      "loss": 2.423,
      "step": 33120
    },
    {
      "epoch": 0.6815487593827386,
      "grad_norm": 0.4036147892475128,
      "learning_rate": 0.000180293023543324,
      "loss": 2.4197,
      "step": 33130
    },
    {
      "epoch": 0.6817544788996063,
      "grad_norm": 0.40462517738342285,
      "learning_rate": 0.00018027978329947125,
      "loss": 2.4113,
      "step": 33140
    },
    {
      "epoch": 0.681960198416474,
      "grad_norm": 0.39325815439224243,
      "learning_rate": 0.00018026653909585422,
      "loss": 2.4024,
      "step": 33150
    },
    {
      "epoch": 0.6821659179333417,
      "grad_norm": 0.37599286437034607,
      "learning_rate": 0.00018025329093312626,
      "loss": 2.4456,
      "step": 33160
    },
    {
      "epoch": 0.6823716374502095,
      "grad_norm": 0.39016181230545044,
      "learning_rate": 0.00018024003881194077,
      "loss": 2.3682,
      "step": 33170
    },
    {
      "epoch": 0.6825773569670771,
      "grad_norm": 0.40762409567832947,
      "learning_rate": 0.00018022678273295144,
      "loss": 2.435,
      "step": 33180
    },
    {
      "epoch": 0.6827830764839449,
      "grad_norm": 0.3869098126888275,
      "learning_rate": 0.00018021352269681208,
      "loss": 2.4682,
      "step": 33190
    },
    {
      "epoch": 0.6829887960008126,
      "grad_norm": 0.37249428033828735,
      "learning_rate": 0.00018020025870417675,
      "loss": 2.375,
      "step": 33200
    },
    {
      "epoch": 0.6831945155176803,
      "grad_norm": 0.425498902797699,
      "learning_rate": 0.00018018699075569972,
      "loss": 2.4387,
      "step": 33210
    },
    {
      "epoch": 0.683400235034548,
      "grad_norm": 0.3730759620666504,
      "learning_rate": 0.0001801737188520354,
      "loss": 2.4652,
      "step": 33220
    },
    {
      "epoch": 0.6836059545514157,
      "grad_norm": 0.42373791337013245,
      "learning_rate": 0.0001801604429938384,
      "loss": 2.5174,
      "step": 33230
    },
    {
      "epoch": 0.6838116740682835,
      "grad_norm": 0.3705046772956848,
      "learning_rate": 0.0001801471631817636,
      "loss": 2.4327,
      "step": 33240
    },
    {
      "epoch": 0.6840173935851511,
      "grad_norm": 0.38972124457359314,
      "learning_rate": 0.000180133879416466,
      "loss": 2.4875,
      "step": 33250
    },
    {
      "epoch": 0.6842231131020189,
      "grad_norm": 0.4359895586967468,
      "learning_rate": 0.00018012059169860076,
      "loss": 2.4226,
      "step": 33260
    },
    {
      "epoch": 0.6844288326188865,
      "grad_norm": 0.3952939510345459,
      "learning_rate": 0.00018010730002882336,
      "loss": 2.4175,
      "step": 33270
    },
    {
      "epoch": 0.6846345521357543,
      "grad_norm": 0.4031475782394409,
      "learning_rate": 0.00018009400440778937,
      "loss": 2.4374,
      "step": 33280
    },
    {
      "epoch": 0.6848402716526221,
      "grad_norm": 0.3727231025695801,
      "learning_rate": 0.00018008070483615463,
      "loss": 2.416,
      "step": 33290
    },
    {
      "epoch": 0.6850459911694897,
      "grad_norm": 0.3892841637134552,
      "learning_rate": 0.00018006740131457506,
      "loss": 2.4279,
      "step": 33300
    },
    {
      "epoch": 0.6852517106863575,
      "grad_norm": 0.3965650498867035,
      "learning_rate": 0.00018005409384370694,
      "loss": 2.4605,
      "step": 33310
    },
    {
      "epoch": 0.6854574302032251,
      "grad_norm": 0.41055330634117126,
      "learning_rate": 0.0001800407824242066,
      "loss": 2.4578,
      "step": 33320
    },
    {
      "epoch": 0.6856631497200929,
      "grad_norm": 0.3918803036212921,
      "learning_rate": 0.00018002746705673064,
      "loss": 2.4332,
      "step": 33330
    },
    {
      "epoch": 0.6858688692369606,
      "grad_norm": 0.4625098407268524,
      "learning_rate": 0.00018001414774193583,
      "loss": 2.4022,
      "step": 33340
    },
    {
      "epoch": 0.6860745887538283,
      "grad_norm": 0.40346023440361023,
      "learning_rate": 0.00018000082448047913,
      "loss": 2.415,
      "step": 33350
    },
    {
      "epoch": 0.686280308270696,
      "grad_norm": 0.3792831599712372,
      "learning_rate": 0.00017998749727301768,
      "loss": 2.4916,
      "step": 33360
    },
    {
      "epoch": 0.6864860277875637,
      "grad_norm": 0.4088153839111328,
      "learning_rate": 0.00017997416612020893,
      "loss": 2.482,
      "step": 33370
    },
    {
      "epoch": 0.6866917473044315,
      "grad_norm": 0.39923417568206787,
      "learning_rate": 0.00017996083102271034,
      "loss": 2.4579,
      "step": 33380
    },
    {
      "epoch": 0.6868974668212992,
      "grad_norm": 0.4080408811569214,
      "learning_rate": 0.0001799474919811797,
      "loss": 2.4442,
      "step": 33390
    },
    {
      "epoch": 0.6871031863381669,
      "grad_norm": 0.39661040902137756,
      "learning_rate": 0.00017993414899627493,
      "loss": 2.4637,
      "step": 33400
    },
    {
      "epoch": 0.6873089058550346,
      "grad_norm": 0.39582347869873047,
      "learning_rate": 0.00017992080206865417,
      "loss": 2.3922,
      "step": 33410
    },
    {
      "epoch": 0.6875146253719023,
      "grad_norm": 0.3682098090648651,
      "learning_rate": 0.00017990745119897576,
      "loss": 2.3681,
      "step": 33420
    },
    {
      "epoch": 0.68772034488877,
      "grad_norm": 0.4554656744003296,
      "learning_rate": 0.00017989409638789826,
      "loss": 2.4573,
      "step": 33430
    },
    {
      "epoch": 0.6879260644056377,
      "grad_norm": 0.3903571665287018,
      "learning_rate": 0.0001798807376360803,
      "loss": 2.3965,
      "step": 33440
    },
    {
      "epoch": 0.6881317839225055,
      "grad_norm": 0.3872247040271759,
      "learning_rate": 0.0001798673749441809,
      "loss": 2.4426,
      "step": 33450
    },
    {
      "epoch": 0.6883375034393732,
      "grad_norm": 0.3739507496356964,
      "learning_rate": 0.0001798540083128591,
      "loss": 2.4199,
      "step": 33460
    },
    {
      "epoch": 0.6885432229562409,
      "grad_norm": 0.4155949652194977,
      "learning_rate": 0.00017984063774277424,
      "loss": 2.4667,
      "step": 33470
    },
    {
      "epoch": 0.6887489424731086,
      "grad_norm": 0.3728228807449341,
      "learning_rate": 0.0001798272632345858,
      "loss": 2.4026,
      "step": 33480
    },
    {
      "epoch": 0.6889546619899763,
      "grad_norm": 0.3813602030277252,
      "learning_rate": 0.00017981388478895342,
      "loss": 2.3972,
      "step": 33490
    },
    {
      "epoch": 0.689160381506844,
      "grad_norm": 0.39910805225372314,
      "learning_rate": 0.0001798005024065371,
      "loss": 2.3516,
      "step": 33500
    },
    {
      "epoch": 0.6893661010237118,
      "grad_norm": 0.4495798349380493,
      "learning_rate": 0.0001797871160879968,
      "loss": 2.4056,
      "step": 33510
    },
    {
      "epoch": 0.6895718205405794,
      "grad_norm": 0.4276101589202881,
      "learning_rate": 0.0001797737258339929,
      "loss": 2.4211,
      "step": 33520
    },
    {
      "epoch": 0.6897775400574472,
      "grad_norm": 0.4269320070743561,
      "learning_rate": 0.0001797603316451858,
      "loss": 2.456,
      "step": 33530
    },
    {
      "epoch": 0.6899832595743149,
      "grad_norm": 0.38868436217308044,
      "learning_rate": 0.00017974693352223615,
      "loss": 2.4405,
      "step": 33540
    },
    {
      "epoch": 0.6901889790911826,
      "grad_norm": 0.39568477869033813,
      "learning_rate": 0.00017973353146580489,
      "loss": 2.4679,
      "step": 33550
    },
    {
      "epoch": 0.6903946986080504,
      "grad_norm": 0.4267224371433258,
      "learning_rate": 0.000179720125476553,
      "loss": 2.4152,
      "step": 33560
    },
    {
      "epoch": 0.690600418124918,
      "grad_norm": 0.46873390674591064,
      "learning_rate": 0.00017970671555514174,
      "loss": 2.4503,
      "step": 33570
    },
    {
      "epoch": 0.6908061376417858,
      "grad_norm": 0.38477659225463867,
      "learning_rate": 0.00017969330170223256,
      "loss": 2.4588,
      "step": 33580
    },
    {
      "epoch": 0.6910118571586534,
      "grad_norm": 0.408286988735199,
      "learning_rate": 0.00017967988391848704,
      "loss": 2.4636,
      "step": 33590
    },
    {
      "epoch": 0.6912175766755212,
      "grad_norm": 0.3870900869369507,
      "learning_rate": 0.00017966646220456708,
      "loss": 2.4252,
      "step": 33600
    },
    {
      "epoch": 0.6914232961923888,
      "grad_norm": 0.4065626561641693,
      "learning_rate": 0.00017965303656113468,
      "loss": 2.4587,
      "step": 33610
    },
    {
      "epoch": 0.6916290157092566,
      "grad_norm": 0.41532790660858154,
      "learning_rate": 0.00017963960698885205,
      "loss": 2.4398,
      "step": 33620
    },
    {
      "epoch": 0.6918347352261244,
      "grad_norm": 0.40083232522010803,
      "learning_rate": 0.00017962617348838157,
      "loss": 2.3617,
      "step": 33630
    },
    {
      "epoch": 0.692040454742992,
      "grad_norm": 0.3900175392627716,
      "learning_rate": 0.00017961273606038587,
      "loss": 2.4617,
      "step": 33640
    },
    {
      "epoch": 0.6922461742598598,
      "grad_norm": 0.4654987156391144,
      "learning_rate": 0.00017959929470552774,
      "loss": 2.428,
      "step": 33650
    },
    {
      "epoch": 0.6924518937767274,
      "grad_norm": 0.39875781536102295,
      "learning_rate": 0.00017958584942447017,
      "loss": 2.4781,
      "step": 33660
    },
    {
      "epoch": 0.6926576132935952,
      "grad_norm": 0.40841296315193176,
      "learning_rate": 0.00017957240021787633,
      "loss": 2.4377,
      "step": 33670
    },
    {
      "epoch": 0.6928633328104629,
      "grad_norm": 0.408732533454895,
      "learning_rate": 0.00017955894708640961,
      "loss": 2.4351,
      "step": 33680
    },
    {
      "epoch": 0.6930690523273306,
      "grad_norm": 0.42848050594329834,
      "learning_rate": 0.00017954549003073358,
      "loss": 2.3788,
      "step": 33690
    },
    {
      "epoch": 0.6932747718441983,
      "grad_norm": 0.4001046121120453,
      "learning_rate": 0.000179532029051512,
      "loss": 2.3989,
      "step": 33700
    },
    {
      "epoch": 0.693480491361066,
      "grad_norm": 0.3885462284088135,
      "learning_rate": 0.00017951856414940879,
      "loss": 2.4285,
      "step": 33710
    },
    {
      "epoch": 0.6936862108779338,
      "grad_norm": 0.3860659599304199,
      "learning_rate": 0.00017950509532508818,
      "loss": 2.5218,
      "step": 33720
    },
    {
      "epoch": 0.6938919303948015,
      "grad_norm": 0.4211541712284088,
      "learning_rate": 0.00017949162257921444,
      "loss": 2.4793,
      "step": 33730
    },
    {
      "epoch": 0.6940976499116692,
      "grad_norm": 0.40851470828056335,
      "learning_rate": 0.00017947814591245213,
      "loss": 2.458,
      "step": 33740
    },
    {
      "epoch": 0.6943033694285369,
      "grad_norm": 0.39916953444480896,
      "learning_rate": 0.000179464665325466,
      "loss": 2.4487,
      "step": 33750
    },
    {
      "epoch": 0.6945090889454046,
      "grad_norm": 0.39009803533554077,
      "learning_rate": 0.00017945118081892097,
      "loss": 2.4245,
      "step": 33760
    },
    {
      "epoch": 0.6947148084622723,
      "grad_norm": 0.38292232155799866,
      "learning_rate": 0.00017943769239348213,
      "loss": 2.4119,
      "step": 33770
    },
    {
      "epoch": 0.6949205279791401,
      "grad_norm": 0.3940700888633728,
      "learning_rate": 0.00017942420004981482,
      "loss": 2.4253,
      "step": 33780
    },
    {
      "epoch": 0.6951262474960077,
      "grad_norm": 0.4388367831707001,
      "learning_rate": 0.0001794107037885845,
      "loss": 2.4609,
      "step": 33790
    },
    {
      "epoch": 0.6953319670128755,
      "grad_norm": 0.36551424860954285,
      "learning_rate": 0.00017939720361045695,
      "loss": 2.4411,
      "step": 33800
    },
    {
      "epoch": 0.6955376865297432,
      "grad_norm": 0.39116430282592773,
      "learning_rate": 0.00017938369951609796,
      "loss": 2.4231,
      "step": 33810
    },
    {
      "epoch": 0.6957434060466109,
      "grad_norm": 0.431508332490921,
      "learning_rate": 0.00017937019150617372,
      "loss": 2.4345,
      "step": 33820
    },
    {
      "epoch": 0.6959491255634787,
      "grad_norm": 0.4510482847690582,
      "learning_rate": 0.0001793566795813504,
      "loss": 2.4555,
      "step": 33830
    },
    {
      "epoch": 0.6961548450803463,
      "grad_norm": 0.4061945080757141,
      "learning_rate": 0.0001793431637422945,
      "loss": 2.439,
      "step": 33840
    },
    {
      "epoch": 0.6963605645972141,
      "grad_norm": 0.4032038450241089,
      "learning_rate": 0.00017932964398967278,
      "loss": 2.4184,
      "step": 33850
    },
    {
      "epoch": 0.6965662841140817,
      "grad_norm": 0.418805867433548,
      "learning_rate": 0.00017931612032415196,
      "loss": 2.4152,
      "step": 33860
    },
    {
      "epoch": 0.6967720036309495,
      "grad_norm": 0.3719031512737274,
      "learning_rate": 0.00017930259274639916,
      "loss": 2.4704,
      "step": 33870
    },
    {
      "epoch": 0.6969777231478171,
      "grad_norm": 0.4063299000263214,
      "learning_rate": 0.00017928906125708158,
      "loss": 2.4696,
      "step": 33880
    },
    {
      "epoch": 0.6971834426646849,
      "grad_norm": 0.3874526619911194,
      "learning_rate": 0.0001792755258568667,
      "loss": 2.4061,
      "step": 33890
    },
    {
      "epoch": 0.6973891621815527,
      "grad_norm": 0.3694319725036621,
      "learning_rate": 0.00017926198654642214,
      "loss": 2.4974,
      "step": 33900
    },
    {
      "epoch": 0.6975948816984203,
      "grad_norm": 0.39972707629203796,
      "learning_rate": 0.00017924844332641571,
      "loss": 2.4807,
      "step": 33910
    },
    {
      "epoch": 0.6978006012152881,
      "grad_norm": 0.39231833815574646,
      "learning_rate": 0.00017923489619751544,
      "loss": 2.4095,
      "step": 33920
    },
    {
      "epoch": 0.6980063207321557,
      "grad_norm": 0.4187150299549103,
      "learning_rate": 0.0001792213451603895,
      "loss": 2.4618,
      "step": 33930
    },
    {
      "epoch": 0.6982120402490235,
      "grad_norm": 0.4012087285518646,
      "learning_rate": 0.00017920779021570629,
      "loss": 2.4564,
      "step": 33940
    },
    {
      "epoch": 0.6984177597658912,
      "grad_norm": 0.4236144721508026,
      "learning_rate": 0.00017919423136413443,
      "loss": 2.4592,
      "step": 33950
    },
    {
      "epoch": 0.6986234792827589,
      "grad_norm": 0.38354846835136414,
      "learning_rate": 0.00017918066860634268,
      "loss": 2.4234,
      "step": 33960
    },
    {
      "epoch": 0.6988291987996266,
      "grad_norm": 0.4259386658668518,
      "learning_rate": 0.00017916710194300006,
      "loss": 2.4442,
      "step": 33970
    },
    {
      "epoch": 0.6990349183164943,
      "grad_norm": 0.4041038155555725,
      "learning_rate": 0.00017915353137477568,
      "loss": 2.4215,
      "step": 33980
    },
    {
      "epoch": 0.6992406378333621,
      "grad_norm": 0.3999313414096832,
      "learning_rate": 0.00017913995690233895,
      "loss": 2.4621,
      "step": 33990
    },
    {
      "epoch": 0.6994463573502298,
      "grad_norm": 0.41026538610458374,
      "learning_rate": 0.00017912637852635942,
      "loss": 2.3563,
      "step": 34000
    },
    {
      "epoch": 0.6996520768670975,
      "grad_norm": 0.4008523225784302,
      "learning_rate": 0.0001791127962475068,
      "loss": 2.4156,
      "step": 34010
    },
    {
      "epoch": 0.6998577963839652,
      "grad_norm": 0.41314390301704407,
      "learning_rate": 0.00017909921006645108,
      "loss": 2.398,
      "step": 34020
    },
    {
      "epoch": 0.7000635159008329,
      "grad_norm": 0.41028398275375366,
      "learning_rate": 0.00017908561998386234,
      "loss": 2.4555,
      "step": 34030
    },
    {
      "epoch": 0.7002692354177006,
      "grad_norm": 0.3826255202293396,
      "learning_rate": 0.00017907202600041095,
      "loss": 2.4276,
      "step": 34040
    },
    {
      "epoch": 0.7004749549345684,
      "grad_norm": 0.4152386784553528,
      "learning_rate": 0.0001790584281167674,
      "loss": 2.3712,
      "step": 34050
    },
    {
      "epoch": 0.700680674451436,
      "grad_norm": 0.4183124899864197,
      "learning_rate": 0.0001790448263336024,
      "loss": 2.5132,
      "step": 34060
    },
    {
      "epoch": 0.7008863939683038,
      "grad_norm": 0.43093088269233704,
      "learning_rate": 0.00017903122065158686,
      "loss": 2.4629,
      "step": 34070
    },
    {
      "epoch": 0.7010921134851715,
      "grad_norm": 0.3778016269207001,
      "learning_rate": 0.00017901761107139188,
      "loss": 2.4504,
      "step": 34080
    },
    {
      "epoch": 0.7012978330020392,
      "grad_norm": 0.40717899799346924,
      "learning_rate": 0.00017900399759368876,
      "loss": 2.4691,
      "step": 34090
    },
    {
      "epoch": 0.701503552518907,
      "grad_norm": 0.4336594343185425,
      "learning_rate": 0.00017899038021914894,
      "loss": 2.3976,
      "step": 34100
    },
    {
      "epoch": 0.7017092720357746,
      "grad_norm": 0.3820144832134247,
      "learning_rate": 0.00017897675894844412,
      "loss": 2.4789,
      "step": 34110
    },
    {
      "epoch": 0.7019149915526424,
      "grad_norm": 0.45575961470603943,
      "learning_rate": 0.0001789631337822461,
      "loss": 2.3647,
      "step": 34120
    },
    {
      "epoch": 0.70212071106951,
      "grad_norm": 0.4089339077472687,
      "learning_rate": 0.00017894950472122708,
      "loss": 2.4496,
      "step": 34130
    },
    {
      "epoch": 0.7023264305863778,
      "grad_norm": 0.422296404838562,
      "learning_rate": 0.00017893587176605916,
      "loss": 2.4548,
      "step": 34140
    },
    {
      "epoch": 0.7025321501032454,
      "grad_norm": 0.3965893089771271,
      "learning_rate": 0.00017892223491741482,
      "loss": 2.4478,
      "step": 34150
    },
    {
      "epoch": 0.7027378696201132,
      "grad_norm": 0.4128219485282898,
      "learning_rate": 0.00017890859417596673,
      "loss": 2.4126,
      "step": 34160
    },
    {
      "epoch": 0.702943589136981,
      "grad_norm": 0.4003490209579468,
      "learning_rate": 0.00017889494954238767,
      "loss": 2.4472,
      "step": 34170
    },
    {
      "epoch": 0.7031493086538486,
      "grad_norm": 0.39146408438682556,
      "learning_rate": 0.00017888130101735073,
      "loss": 2.401,
      "step": 34180
    },
    {
      "epoch": 0.7033550281707164,
      "grad_norm": 0.3806702494621277,
      "learning_rate": 0.000178867648601529,
      "loss": 2.4064,
      "step": 34190
    },
    {
      "epoch": 0.703560747687584,
      "grad_norm": 0.40873217582702637,
      "learning_rate": 0.00017885399229559598,
      "loss": 2.419,
      "step": 34200
    },
    {
      "epoch": 0.7037664672044518,
      "grad_norm": 0.38523051142692566,
      "learning_rate": 0.0001788403321002252,
      "loss": 2.4898,
      "step": 34210
    },
    {
      "epoch": 0.7039721867213194,
      "grad_norm": 0.3905585706233978,
      "learning_rate": 0.00017882666801609048,
      "loss": 2.4017,
      "step": 34220
    },
    {
      "epoch": 0.7041779062381872,
      "grad_norm": 0.42341771721839905,
      "learning_rate": 0.0001788130000438658,
      "loss": 2.403,
      "step": 34230
    },
    {
      "epoch": 0.7043836257550549,
      "grad_norm": 0.35461464524269104,
      "learning_rate": 0.00017879932818422526,
      "loss": 2.4519,
      "step": 34240
    },
    {
      "epoch": 0.7045893452719226,
      "grad_norm": 0.37344124913215637,
      "learning_rate": 0.0001787856524378433,
      "loss": 2.4339,
      "step": 34250
    },
    {
      "epoch": 0.7047950647887904,
      "grad_norm": 0.40850555896759033,
      "learning_rate": 0.00017877197280539439,
      "loss": 2.4699,
      "step": 34260
    },
    {
      "epoch": 0.705000784305658,
      "grad_norm": 0.3825722932815552,
      "learning_rate": 0.00017875828928755336,
      "loss": 2.4541,
      "step": 34270
    },
    {
      "epoch": 0.7052065038225258,
      "grad_norm": 0.40666913986206055,
      "learning_rate": 0.0001787446018849951,
      "loss": 2.4116,
      "step": 34280
    },
    {
      "epoch": 0.7054122233393935,
      "grad_norm": 0.4230305850505829,
      "learning_rate": 0.00017873091059839472,
      "loss": 2.4888,
      "step": 34290
    },
    {
      "epoch": 0.7056179428562612,
      "grad_norm": 0.39274880290031433,
      "learning_rate": 0.00017871721542842757,
      "loss": 2.495,
      "step": 34300
    },
    {
      "epoch": 0.7058236623731289,
      "grad_norm": 0.4049130082130432,
      "learning_rate": 0.00017870351637576913,
      "loss": 2.4101,
      "step": 34310
    },
    {
      "epoch": 0.7060293818899966,
      "grad_norm": 0.40417781472206116,
      "learning_rate": 0.0001786898134410951,
      "loss": 2.384,
      "step": 34320
    },
    {
      "epoch": 0.7062351014068643,
      "grad_norm": 0.4218243956565857,
      "learning_rate": 0.0001786761066250814,
      "loss": 2.4649,
      "step": 34330
    },
    {
      "epoch": 0.7064408209237321,
      "grad_norm": 0.39953652024269104,
      "learning_rate": 0.00017866239592840408,
      "loss": 2.3678,
      "step": 34340
    },
    {
      "epoch": 0.7066465404405998,
      "grad_norm": 0.4179273843765259,
      "learning_rate": 0.00017864868135173943,
      "loss": 2.3886,
      "step": 34350
    },
    {
      "epoch": 0.7068522599574675,
      "grad_norm": 0.5639117956161499,
      "learning_rate": 0.0001786349628957639,
      "loss": 2.4352,
      "step": 34360
    },
    {
      "epoch": 0.7070579794743352,
      "grad_norm": 0.3818844258785248,
      "learning_rate": 0.00017862124056115418,
      "loss": 2.3929,
      "step": 34370
    },
    {
      "epoch": 0.7072636989912029,
      "grad_norm": 0.38117295503616333,
      "learning_rate": 0.00017860751434858708,
      "loss": 2.3997,
      "step": 34380
    },
    {
      "epoch": 0.7074694185080707,
      "grad_norm": 0.5602639317512512,
      "learning_rate": 0.00017859378425873969,
      "loss": 2.4116,
      "step": 34390
    },
    {
      "epoch": 0.7076751380249383,
      "grad_norm": 0.4168339669704437,
      "learning_rate": 0.00017858005029228917,
      "loss": 2.4609,
      "step": 34400
    },
    {
      "epoch": 0.7078808575418061,
      "grad_norm": 0.3923102915287018,
      "learning_rate": 0.000178566312449913,
      "loss": 2.3813,
      "step": 34410
    },
    {
      "epoch": 0.7080865770586737,
      "grad_norm": 0.4326731264591217,
      "learning_rate": 0.00017855257073228878,
      "loss": 2.3857,
      "step": 34420
    },
    {
      "epoch": 0.7082922965755415,
      "grad_norm": 0.3555700182914734,
      "learning_rate": 0.00017853882514009428,
      "loss": 2.4126,
      "step": 34430
    },
    {
      "epoch": 0.7084980160924093,
      "grad_norm": 0.39148885011672974,
      "learning_rate": 0.00017852507567400752,
      "loss": 2.4856,
      "step": 34440
    },
    {
      "epoch": 0.7087037356092769,
      "grad_norm": 0.41786468029022217,
      "learning_rate": 0.0001785113223347067,
      "loss": 2.5049,
      "step": 34450
    },
    {
      "epoch": 0.7089094551261447,
      "grad_norm": 0.3576831519603729,
      "learning_rate": 0.0001784975651228702,
      "loss": 2.3799,
      "step": 34460
    },
    {
      "epoch": 0.7091151746430123,
      "grad_norm": 0.37732627987861633,
      "learning_rate": 0.00017848380403917655,
      "loss": 2.392,
      "step": 34470
    },
    {
      "epoch": 0.7093208941598801,
      "grad_norm": 0.41469037532806396,
      "learning_rate": 0.00017847003908430455,
      "loss": 2.4644,
      "step": 34480
    },
    {
      "epoch": 0.7095266136767477,
      "grad_norm": 0.4320369064807892,
      "learning_rate": 0.0001784562702589331,
      "loss": 2.4015,
      "step": 34490
    },
    {
      "epoch": 0.7097323331936155,
      "grad_norm": 0.38000163435935974,
      "learning_rate": 0.0001784424975637414,
      "loss": 2.4597,
      "step": 34500
    },
    {
      "epoch": 0.7099380527104832,
      "grad_norm": 0.39368435740470886,
      "learning_rate": 0.00017842872099940878,
      "loss": 2.4566,
      "step": 34510
    },
    {
      "epoch": 0.7101437722273509,
      "grad_norm": 0.39359015226364136,
      "learning_rate": 0.00017841494056661472,
      "loss": 2.4216,
      "step": 34520
    },
    {
      "epoch": 0.7103494917442187,
      "grad_norm": 0.37256914377212524,
      "learning_rate": 0.00017840115626603892,
      "loss": 2.4471,
      "step": 34530
    },
    {
      "epoch": 0.7105552112610863,
      "grad_norm": 0.4219917356967926,
      "learning_rate": 0.00017838736809836135,
      "loss": 2.4661,
      "step": 34540
    },
    {
      "epoch": 0.7107609307779541,
      "grad_norm": 0.3861921429634094,
      "learning_rate": 0.00017837357606426203,
      "loss": 2.4351,
      "step": 34550
    },
    {
      "epoch": 0.7109666502948218,
      "grad_norm": 0.3946413993835449,
      "learning_rate": 0.00017835978016442135,
      "loss": 2.376,
      "step": 34560
    },
    {
      "epoch": 0.7111723698116895,
      "grad_norm": 0.4058631360530853,
      "learning_rate": 0.00017834598039951968,
      "loss": 2.439,
      "step": 34570
    },
    {
      "epoch": 0.7113780893285572,
      "grad_norm": 0.36921969056129456,
      "learning_rate": 0.00017833217677023773,
      "loss": 2.4728,
      "step": 34580
    },
    {
      "epoch": 0.7115838088454249,
      "grad_norm": 0.4071304500102997,
      "learning_rate": 0.00017831836927725638,
      "loss": 2.3801,
      "step": 34590
    },
    {
      "epoch": 0.7117895283622926,
      "grad_norm": 0.37052100896835327,
      "learning_rate": 0.00017830455792125664,
      "loss": 2.5049,
      "step": 34600
    },
    {
      "epoch": 0.7119952478791604,
      "grad_norm": 0.4254659414291382,
      "learning_rate": 0.00017829074270291978,
      "loss": 2.4551,
      "step": 34610
    },
    {
      "epoch": 0.7122009673960281,
      "grad_norm": 0.3813524842262268,
      "learning_rate": 0.00017827692362292722,
      "loss": 2.4028,
      "step": 34620
    },
    {
      "epoch": 0.7124066869128958,
      "grad_norm": 0.3678814768791199,
      "learning_rate": 0.00017826310068196058,
      "loss": 2.386,
      "step": 34630
    },
    {
      "epoch": 0.7126124064297635,
      "grad_norm": 0.378222793340683,
      "learning_rate": 0.00017824927388070165,
      "loss": 2.4273,
      "step": 34640
    },
    {
      "epoch": 0.7128181259466312,
      "grad_norm": 0.4135814607143402,
      "learning_rate": 0.00017823544321983245,
      "loss": 2.4715,
      "step": 34650
    },
    {
      "epoch": 0.713023845463499,
      "grad_norm": 0.3991040587425232,
      "learning_rate": 0.00017822160870003515,
      "loss": 2.4351,
      "step": 34660
    },
    {
      "epoch": 0.7132295649803666,
      "grad_norm": 0.3939184248447418,
      "learning_rate": 0.00017820777032199217,
      "loss": 2.348,
      "step": 34670
    },
    {
      "epoch": 0.7134352844972344,
      "grad_norm": 0.3909313678741455,
      "learning_rate": 0.00017819392808638606,
      "loss": 2.4414,
      "step": 34680
    },
    {
      "epoch": 0.713641004014102,
      "grad_norm": 0.40283942222595215,
      "learning_rate": 0.0001781800819938996,
      "loss": 2.3605,
      "step": 34690
    },
    {
      "epoch": 0.7138467235309698,
      "grad_norm": 0.4162224233150482,
      "learning_rate": 0.0001781662320452157,
      "loss": 2.4881,
      "step": 34700
    },
    {
      "epoch": 0.7140524430478375,
      "grad_norm": 0.39536020159721375,
      "learning_rate": 0.00017815237824101752,
      "loss": 2.448,
      "step": 34710
    },
    {
      "epoch": 0.7142581625647052,
      "grad_norm": 0.3975452184677124,
      "learning_rate": 0.00017813852058198843,
      "loss": 2.4193,
      "step": 34720
    },
    {
      "epoch": 0.714463882081573,
      "grad_norm": 0.38808393478393555,
      "learning_rate": 0.00017812465906881194,
      "loss": 2.3943,
      "step": 34730
    },
    {
      "epoch": 0.7146696015984406,
      "grad_norm": 0.40997111797332764,
      "learning_rate": 0.00017811079370217168,
      "loss": 2.3888,
      "step": 34740
    },
    {
      "epoch": 0.7148753211153084,
      "grad_norm": 0.4260624051094055,
      "learning_rate": 0.00017809692448275165,
      "loss": 2.4196,
      "step": 34750
    },
    {
      "epoch": 0.715081040632176,
      "grad_norm": 0.4457428753376007,
      "learning_rate": 0.00017808305141123594,
      "loss": 2.4073,
      "step": 34760
    },
    {
      "epoch": 0.7152867601490438,
      "grad_norm": 0.3716523349285126,
      "learning_rate": 0.00017806917448830878,
      "loss": 2.4195,
      "step": 34770
    },
    {
      "epoch": 0.7154924796659115,
      "grad_norm": 0.38516977429389954,
      "learning_rate": 0.0001780552937146547,
      "loss": 2.4926,
      "step": 34780
    },
    {
      "epoch": 0.7156981991827792,
      "grad_norm": 0.3810339868068695,
      "learning_rate": 0.00017804140909095828,
      "loss": 2.4191,
      "step": 34790
    },
    {
      "epoch": 0.715903918699647,
      "grad_norm": 0.38052329421043396,
      "learning_rate": 0.00017802752061790445,
      "loss": 2.4518,
      "step": 34800
    },
    {
      "epoch": 0.7161096382165146,
      "grad_norm": 0.3835500180721283,
      "learning_rate": 0.00017801362829617822,
      "loss": 2.4316,
      "step": 34810
    },
    {
      "epoch": 0.7163153577333824,
      "grad_norm": 0.3713811933994293,
      "learning_rate": 0.00017799973212646484,
      "loss": 2.3947,
      "step": 34820
    },
    {
      "epoch": 0.71652107725025,
      "grad_norm": 0.39168357849121094,
      "learning_rate": 0.00017798583210944973,
      "loss": 2.4847,
      "step": 34830
    },
    {
      "epoch": 0.7167267967671178,
      "grad_norm": 0.36603185534477234,
      "learning_rate": 0.00017797192824581845,
      "loss": 2.4704,
      "step": 34840
    },
    {
      "epoch": 0.7169325162839855,
      "grad_norm": 0.3823910653591156,
      "learning_rate": 0.0001779580205362569,
      "loss": 2.3851,
      "step": 34850
    },
    {
      "epoch": 0.7171382358008532,
      "grad_norm": 0.386938214302063,
      "learning_rate": 0.000177944108981451,
      "loss": 2.4415,
      "step": 34860
    },
    {
      "epoch": 0.7173439553177209,
      "grad_norm": 0.371310830116272,
      "learning_rate": 0.0001779301935820869,
      "loss": 2.4459,
      "step": 34870
    },
    {
      "epoch": 0.7175496748345886,
      "grad_norm": 0.3711734712123871,
      "learning_rate": 0.00017791627433885108,
      "loss": 2.4646,
      "step": 34880
    },
    {
      "epoch": 0.7177553943514564,
      "grad_norm": 0.40747177600860596,
      "learning_rate": 0.00017790235125243002,
      "loss": 2.447,
      "step": 34890
    },
    {
      "epoch": 0.7179611138683241,
      "grad_norm": 0.42918866872787476,
      "learning_rate": 0.0001778884243235105,
      "loss": 2.4262,
      "step": 34900
    },
    {
      "epoch": 0.7181668333851918,
      "grad_norm": 0.3888656497001648,
      "learning_rate": 0.00017787449355277944,
      "loss": 2.4365,
      "step": 34910
    },
    {
      "epoch": 0.7183725529020595,
      "grad_norm": 0.39028871059417725,
      "learning_rate": 0.00017786055894092396,
      "loss": 2.3972,
      "step": 34920
    },
    {
      "epoch": 0.7185782724189272,
      "grad_norm": 0.4016173481941223,
      "learning_rate": 0.0001778466204886314,
      "loss": 2.4323,
      "step": 34930
    },
    {
      "epoch": 0.7187839919357949,
      "grad_norm": 0.37326329946517944,
      "learning_rate": 0.0001778326781965893,
      "loss": 2.4597,
      "step": 34940
    },
    {
      "epoch": 0.7189897114526627,
      "grad_norm": 0.4017931818962097,
      "learning_rate": 0.0001778187320654853,
      "loss": 2.4511,
      "step": 34950
    },
    {
      "epoch": 0.7191954309695303,
      "grad_norm": 0.43424585461616516,
      "learning_rate": 0.00017780478209600734,
      "loss": 2.407,
      "step": 34960
    },
    {
      "epoch": 0.7194011504863981,
      "grad_norm": 0.3682149052619934,
      "learning_rate": 0.0001777908282888434,
      "loss": 2.4234,
      "step": 34970
    },
    {
      "epoch": 0.7196068700032658,
      "grad_norm": 0.4193657636642456,
      "learning_rate": 0.00017777687064468185,
      "loss": 2.414,
      "step": 34980
    },
    {
      "epoch": 0.7198125895201335,
      "grad_norm": 0.3985189199447632,
      "learning_rate": 0.0001777629091642111,
      "loss": 2.3839,
      "step": 34990
    },
    {
      "epoch": 0.7200183090370013,
      "grad_norm": 0.4884990453720093,
      "learning_rate": 0.00017774894384811978,
      "loss": 2.4869,
      "step": 35000
    },
    {
      "epoch": 0.7202240285538689,
      "grad_norm": 0.4041549563407898,
      "learning_rate": 0.00017773497469709677,
      "loss": 2.4438,
      "step": 35010
    },
    {
      "epoch": 0.7204297480707367,
      "grad_norm": 0.42173829674720764,
      "learning_rate": 0.00017772100171183105,
      "loss": 2.4455,
      "step": 35020
    },
    {
      "epoch": 0.7206354675876043,
      "grad_norm": 0.3907637298107147,
      "learning_rate": 0.00017770702489301184,
      "loss": 2.3945,
      "step": 35030
    },
    {
      "epoch": 0.7208411871044721,
      "grad_norm": 0.3786861300468445,
      "learning_rate": 0.00017769304424132854,
      "loss": 2.4144,
      "step": 35040
    },
    {
      "epoch": 0.7210469066213397,
      "grad_norm": 0.41712817549705505,
      "learning_rate": 0.00017767905975747076,
      "loss": 2.4839,
      "step": 35050
    },
    {
      "epoch": 0.7212526261382075,
      "grad_norm": 0.4635882079601288,
      "learning_rate": 0.00017766507144212826,
      "loss": 2.416,
      "step": 35060
    },
    {
      "epoch": 0.7214583456550753,
      "grad_norm": 0.4296986162662506,
      "learning_rate": 0.000177651079295991,
      "loss": 2.4239,
      "step": 35070
    },
    {
      "epoch": 0.7216640651719429,
      "grad_norm": 0.3632844388484955,
      "learning_rate": 0.00017763708331974915,
      "loss": 2.4908,
      "step": 35080
    },
    {
      "epoch": 0.7218697846888107,
      "grad_norm": 0.38928595185279846,
      "learning_rate": 0.00017762308351409304,
      "loss": 2.4463,
      "step": 35090
    },
    {
      "epoch": 0.7220755042056783,
      "grad_norm": 0.44764554500579834,
      "learning_rate": 0.00017760907987971324,
      "loss": 2.3849,
      "step": 35100
    },
    {
      "epoch": 0.7222812237225461,
      "grad_norm": 0.4111475944519043,
      "learning_rate": 0.0001775950724173004,
      "loss": 2.4732,
      "step": 35110
    },
    {
      "epoch": 0.7224869432394138,
      "grad_norm": 0.36170971393585205,
      "learning_rate": 0.00017758106112754555,
      "loss": 2.391,
      "step": 35120
    },
    {
      "epoch": 0.7226926627562815,
      "grad_norm": 0.46681758761405945,
      "learning_rate": 0.0001775670460111397,
      "loss": 2.4266,
      "step": 35130
    },
    {
      "epoch": 0.7228983822731493,
      "grad_norm": 0.40955284237861633,
      "learning_rate": 0.00017755302706877411,
      "loss": 2.3885,
      "step": 35140
    },
    {
      "epoch": 0.7231041017900169,
      "grad_norm": 0.43702808022499084,
      "learning_rate": 0.00017753900430114036,
      "loss": 2.3862,
      "step": 35150
    },
    {
      "epoch": 0.7233098213068847,
      "grad_norm": 0.42746785283088684,
      "learning_rate": 0.00017752497770893007,
      "loss": 2.4034,
      "step": 35160
    },
    {
      "epoch": 0.7235155408237524,
      "grad_norm": 0.37534692883491516,
      "learning_rate": 0.00017751094729283505,
      "loss": 2.3815,
      "step": 35170
    },
    {
      "epoch": 0.7237212603406201,
      "grad_norm": 0.4248310625553131,
      "learning_rate": 0.0001774969130535474,
      "loss": 2.4098,
      "step": 35180
    },
    {
      "epoch": 0.7239269798574878,
      "grad_norm": 0.39658331871032715,
      "learning_rate": 0.00017748287499175932,
      "loss": 2.4051,
      "step": 35190
    },
    {
      "epoch": 0.7241326993743555,
      "grad_norm": 0.37393665313720703,
      "learning_rate": 0.00017746883310816326,
      "loss": 2.4627,
      "step": 35200
    },
    {
      "epoch": 0.7243384188912232,
      "grad_norm": 0.38505229353904724,
      "learning_rate": 0.00017745478740345186,
      "loss": 2.4112,
      "step": 35210
    },
    {
      "epoch": 0.724544138408091,
      "grad_norm": 0.43813014030456543,
      "learning_rate": 0.00017744073787831784,
      "loss": 2.4745,
      "step": 35220
    },
    {
      "epoch": 0.7247498579249587,
      "grad_norm": 0.40467938780784607,
      "learning_rate": 0.00017742668453345418,
      "loss": 2.4406,
      "step": 35230
    },
    {
      "epoch": 0.7249555774418264,
      "grad_norm": 0.4094107747077942,
      "learning_rate": 0.00017741262736955418,
      "loss": 2.4298,
      "step": 35240
    },
    {
      "epoch": 0.7251612969586941,
      "grad_norm": 0.399017333984375,
      "learning_rate": 0.00017739856638731105,
      "loss": 2.436,
      "step": 35250
    },
    {
      "epoch": 0.7253670164755618,
      "grad_norm": 0.4042903780937195,
      "learning_rate": 0.00017738450158741843,
      "loss": 2.4262,
      "step": 35260
    },
    {
      "epoch": 0.7255727359924296,
      "grad_norm": 0.37636709213256836,
      "learning_rate": 0.00017737043297057009,
      "loss": 2.4379,
      "step": 35270
    },
    {
      "epoch": 0.7257784555092972,
      "grad_norm": 0.3953472375869751,
      "learning_rate": 0.00017735636053745985,
      "loss": 2.393,
      "step": 35280
    },
    {
      "epoch": 0.725984175026165,
      "grad_norm": 0.40429458022117615,
      "learning_rate": 0.00017734228428878193,
      "loss": 2.4101,
      "step": 35290
    },
    {
      "epoch": 0.7261898945430326,
      "grad_norm": 0.4038713276386261,
      "learning_rate": 0.00017732820422523054,
      "loss": 2.4873,
      "step": 35300
    },
    {
      "epoch": 0.7263956140599004,
      "grad_norm": 0.38610804080963135,
      "learning_rate": 0.00017731412034750025,
      "loss": 2.4339,
      "step": 35310
    },
    {
      "epoch": 0.7266013335767681,
      "grad_norm": 0.38864749670028687,
      "learning_rate": 0.00017730003265628574,
      "loss": 2.3783,
      "step": 35320
    },
    {
      "epoch": 0.7268070530936358,
      "grad_norm": 0.40223029255867004,
      "learning_rate": 0.00017728594115228182,
      "loss": 2.4502,
      "step": 35330
    },
    {
      "epoch": 0.7270127726105036,
      "grad_norm": 0.40859514474868774,
      "learning_rate": 0.0001772718458361836,
      "loss": 2.4273,
      "step": 35340
    },
    {
      "epoch": 0.7272184921273712,
      "grad_norm": 0.7815976142883301,
      "learning_rate": 0.00017725774670868628,
      "loss": 2.4313,
      "step": 35350
    },
    {
      "epoch": 0.727424211644239,
      "grad_norm": 0.3795619606971741,
      "learning_rate": 0.00017724364377048535,
      "loss": 2.4291,
      "step": 35360
    },
    {
      "epoch": 0.7276299311611066,
      "grad_norm": 0.42289796471595764,
      "learning_rate": 0.00017722953702227639,
      "loss": 2.4346,
      "step": 35370
    },
    {
      "epoch": 0.7278356506779744,
      "grad_norm": 0.48224103450775146,
      "learning_rate": 0.0001772154264647552,
      "loss": 2.4566,
      "step": 35380
    },
    {
      "epoch": 0.728041370194842,
      "grad_norm": 0.4454711377620697,
      "learning_rate": 0.00017720131209861782,
      "loss": 2.4557,
      "step": 35390
    },
    {
      "epoch": 0.7282470897117098,
      "grad_norm": 0.4070412516593933,
      "learning_rate": 0.0001771871939245604,
      "loss": 2.4855,
      "step": 35400
    },
    {
      "epoch": 0.7284528092285776,
      "grad_norm": 0.4004136919975281,
      "learning_rate": 0.00017717307194327933,
      "loss": 2.4088,
      "step": 35410
    },
    {
      "epoch": 0.7286585287454452,
      "grad_norm": 0.4076795279979706,
      "learning_rate": 0.00017715894615547115,
      "loss": 2.4184,
      "step": 35420
    },
    {
      "epoch": 0.728864248262313,
      "grad_norm": 0.3966805040836334,
      "learning_rate": 0.00017714481656183265,
      "loss": 2.445,
      "step": 35430
    },
    {
      "epoch": 0.7290699677791806,
      "grad_norm": 0.3695114850997925,
      "learning_rate": 0.00017713068316306072,
      "loss": 2.415,
      "step": 35440
    },
    {
      "epoch": 0.7292756872960484,
      "grad_norm": 0.39029067754745483,
      "learning_rate": 0.0001771165459598525,
      "loss": 2.4578,
      "step": 35450
    },
    {
      "epoch": 0.7294814068129161,
      "grad_norm": 0.4401032626628876,
      "learning_rate": 0.0001771024049529053,
      "loss": 2.4003,
      "step": 35460
    },
    {
      "epoch": 0.7296871263297838,
      "grad_norm": 0.43112677335739136,
      "learning_rate": 0.00017708826014291662,
      "loss": 2.3943,
      "step": 35470
    },
    {
      "epoch": 0.7298928458466515,
      "grad_norm": 0.39598536491394043,
      "learning_rate": 0.00017707411153058416,
      "loss": 2.4158,
      "step": 35480
    },
    {
      "epoch": 0.7300985653635192,
      "grad_norm": 0.39941203594207764,
      "learning_rate": 0.00017705995911660574,
      "loss": 2.4825,
      "step": 35490
    },
    {
      "epoch": 0.730304284880387,
      "grad_norm": 0.37123847007751465,
      "learning_rate": 0.00017704580290167955,
      "loss": 2.4131,
      "step": 35500
    },
    {
      "epoch": 0.7305100043972547,
      "grad_norm": 0.39406710863113403,
      "learning_rate": 0.00017703164288650366,
      "loss": 2.4505,
      "step": 35510
    },
    {
      "epoch": 0.7307157239141224,
      "grad_norm": 0.37107622623443604,
      "learning_rate": 0.00017701747907177665,
      "loss": 2.4703,
      "step": 35520
    },
    {
      "epoch": 0.7309214434309901,
      "grad_norm": 0.4545035660266876,
      "learning_rate": 0.00017700331145819708,
      "loss": 2.5016,
      "step": 35530
    },
    {
      "epoch": 0.7311271629478578,
      "grad_norm": 0.39190366864204407,
      "learning_rate": 0.00017698914004646377,
      "loss": 2.5017,
      "step": 35540
    },
    {
      "epoch": 0.7313328824647255,
      "grad_norm": 0.3653889000415802,
      "learning_rate": 0.00017697496483727574,
      "loss": 2.3978,
      "step": 35550
    },
    {
      "epoch": 0.7315386019815933,
      "grad_norm": 0.4077860713005066,
      "learning_rate": 0.00017696078583133215,
      "loss": 2.3795,
      "step": 35560
    },
    {
      "epoch": 0.7317443214984609,
      "grad_norm": 0.41392040252685547,
      "learning_rate": 0.00017694660302933234,
      "loss": 2.4461,
      "step": 35570
    },
    {
      "epoch": 0.7319500410153287,
      "grad_norm": 0.4462350010871887,
      "learning_rate": 0.00017693241643197599,
      "loss": 2.455,
      "step": 35580
    },
    {
      "epoch": 0.7321557605321964,
      "grad_norm": 0.3984822928905487,
      "learning_rate": 0.0001769182260399627,
      "loss": 2.4141,
      "step": 35590
    },
    {
      "epoch": 0.7323614800490641,
      "grad_norm": 0.39473092555999756,
      "learning_rate": 0.0001769040318539925,
      "loss": 2.4319,
      "step": 35600
    },
    {
      "epoch": 0.7325671995659319,
      "grad_norm": 0.43227359652519226,
      "learning_rate": 0.0001768898338747655,
      "loss": 2.4041,
      "step": 35610
    },
    {
      "epoch": 0.7327729190827995,
      "grad_norm": 0.4076046645641327,
      "learning_rate": 0.00017687563210298194,
      "loss": 2.4946,
      "step": 35620
    },
    {
      "epoch": 0.7329786385996673,
      "grad_norm": 0.42757394909858704,
      "learning_rate": 0.00017686142653934245,
      "loss": 2.4708,
      "step": 35630
    },
    {
      "epoch": 0.7331843581165349,
      "grad_norm": 0.3849504292011261,
      "learning_rate": 0.00017684721718454758,
      "loss": 2.4268,
      "step": 35640
    },
    {
      "epoch": 0.7333900776334027,
      "grad_norm": 0.43316882848739624,
      "learning_rate": 0.00017683300403929827,
      "loss": 2.5138,
      "step": 35650
    },
    {
      "epoch": 0.7335957971502703,
      "grad_norm": 0.425386518239975,
      "learning_rate": 0.00017681878710429553,
      "loss": 2.4474,
      "step": 35660
    },
    {
      "epoch": 0.7338015166671381,
      "grad_norm": 0.44547224044799805,
      "learning_rate": 0.00017680456638024066,
      "loss": 2.3818,
      "step": 35670
    },
    {
      "epoch": 0.7340072361840059,
      "grad_norm": 0.419147789478302,
      "learning_rate": 0.00017679034186783509,
      "loss": 2.4644,
      "step": 35680
    },
    {
      "epoch": 0.7342129557008735,
      "grad_norm": 0.39448341727256775,
      "learning_rate": 0.00017677611356778039,
      "loss": 2.4625,
      "step": 35690
    },
    {
      "epoch": 0.7344186752177413,
      "grad_norm": 0.38698768615722656,
      "learning_rate": 0.00017676188148077837,
      "loss": 2.416,
      "step": 35700
    },
    {
      "epoch": 0.7346243947346089,
      "grad_norm": 0.38081738352775574,
      "learning_rate": 0.00017674764560753108,
      "loss": 2.4052,
      "step": 35710
    },
    {
      "epoch": 0.7348301142514767,
      "grad_norm": 0.3744221031665802,
      "learning_rate": 0.00017673340594874062,
      "loss": 2.4388,
      "step": 35720
    },
    {
      "epoch": 0.7350358337683444,
      "grad_norm": 0.4106505513191223,
      "learning_rate": 0.0001767191625051094,
      "loss": 2.4399,
      "step": 35730
    },
    {
      "epoch": 0.7352415532852121,
      "grad_norm": 0.3829362988471985,
      "learning_rate": 0.00017670491527733994,
      "loss": 2.4651,
      "step": 35740
    },
    {
      "epoch": 0.7354472728020798,
      "grad_norm": 0.3797805905342102,
      "learning_rate": 0.00017669066426613506,
      "loss": 2.3734,
      "step": 35750
    },
    {
      "epoch": 0.7356529923189475,
      "grad_norm": 0.410049706697464,
      "learning_rate": 0.00017667640947219756,
      "loss": 2.4652,
      "step": 35760
    },
    {
      "epoch": 0.7358587118358153,
      "grad_norm": 0.3715755045413971,
      "learning_rate": 0.0001766621508962307,
      "loss": 2.4096,
      "step": 35770
    },
    {
      "epoch": 0.736064431352683,
      "grad_norm": 0.38118791580200195,
      "learning_rate": 0.0001766478885389376,
      "loss": 2.4546,
      "step": 35780
    },
    {
      "epoch": 0.7362701508695507,
      "grad_norm": 0.4132520854473114,
      "learning_rate": 0.00017663362240102188,
      "loss": 2.4092,
      "step": 35790
    },
    {
      "epoch": 0.7364758703864184,
      "grad_norm": 0.42232298851013184,
      "learning_rate": 0.00017661935248318718,
      "loss": 2.4502,
      "step": 35800
    },
    {
      "epoch": 0.7366815899032861,
      "grad_norm": 0.38343071937561035,
      "learning_rate": 0.00017660507878613734,
      "loss": 2.4399,
      "step": 35810
    },
    {
      "epoch": 0.7368873094201538,
      "grad_norm": 0.4421609342098236,
      "learning_rate": 0.00017659080131057642,
      "loss": 2.374,
      "step": 35820
    },
    {
      "epoch": 0.7370930289370216,
      "grad_norm": 0.40525341033935547,
      "learning_rate": 0.00017657652005720863,
      "loss": 2.4928,
      "step": 35830
    },
    {
      "epoch": 0.7372987484538892,
      "grad_norm": 0.3910978138446808,
      "learning_rate": 0.00017656223502673842,
      "loss": 2.4762,
      "step": 35840
    },
    {
      "epoch": 0.737504467970757,
      "grad_norm": 0.4771210253238678,
      "learning_rate": 0.00017654794621987035,
      "loss": 2.4098,
      "step": 35850
    },
    {
      "epoch": 0.7377101874876247,
      "grad_norm": 0.3671867251396179,
      "learning_rate": 0.0001765336536373092,
      "loss": 2.4422,
      "step": 35860
    },
    {
      "epoch": 0.7379159070044924,
      "grad_norm": 0.3758281171321869,
      "learning_rate": 0.00017651935727976,
      "loss": 2.4002,
      "step": 35870
    },
    {
      "epoch": 0.7381216265213602,
      "grad_norm": 0.3873968720436096,
      "learning_rate": 0.00017650505714792792,
      "loss": 2.4027,
      "step": 35880
    },
    {
      "epoch": 0.7383273460382278,
      "grad_norm": 0.4536260962486267,
      "learning_rate": 0.00017649075324251823,
      "loss": 2.4234,
      "step": 35890
    },
    {
      "epoch": 0.7385330655550956,
      "grad_norm": 0.3897192180156708,
      "learning_rate": 0.00017647644556423653,
      "loss": 2.4043,
      "step": 35900
    },
    {
      "epoch": 0.7387387850719632,
      "grad_norm": 0.36288750171661377,
      "learning_rate": 0.00017646213411378852,
      "loss": 2.4073,
      "step": 35910
    },
    {
      "epoch": 0.738944504588831,
      "grad_norm": 0.4282447099685669,
      "learning_rate": 0.00017644781889188007,
      "loss": 2.4262,
      "step": 35920
    },
    {
      "epoch": 0.7391502241056986,
      "grad_norm": 0.3850639760494232,
      "learning_rate": 0.00017643349989921733,
      "loss": 2.3864,
      "step": 35930
    },
    {
      "epoch": 0.7393559436225664,
      "grad_norm": 0.40396779775619507,
      "learning_rate": 0.00017641917713650656,
      "loss": 2.3986,
      "step": 35940
    },
    {
      "epoch": 0.7395616631394342,
      "grad_norm": 0.4120926558971405,
      "learning_rate": 0.00017640485060445418,
      "loss": 2.4241,
      "step": 35950
    },
    {
      "epoch": 0.7397673826563018,
      "grad_norm": 0.42741140723228455,
      "learning_rate": 0.0001763905203037669,
      "loss": 2.5042,
      "step": 35960
    },
    {
      "epoch": 0.7399731021731696,
      "grad_norm": 0.44242623448371887,
      "learning_rate": 0.00017637618623515152,
      "loss": 2.4488,
      "step": 35970
    },
    {
      "epoch": 0.7401788216900372,
      "grad_norm": 0.41228336095809937,
      "learning_rate": 0.00017636184839931512,
      "loss": 2.4285,
      "step": 35980
    },
    {
      "epoch": 0.740384541206905,
      "grad_norm": 0.35582831501960754,
      "learning_rate": 0.0001763475067969648,
      "loss": 2.4079,
      "step": 35990
    },
    {
      "epoch": 0.7405902607237727,
      "grad_norm": 0.418212354183197,
      "learning_rate": 0.00017633316142880805,
      "loss": 2.3699,
      "step": 36000
    },
    {
      "epoch": 0.7407959802406404,
      "grad_norm": 0.37620222568511963,
      "learning_rate": 0.0001763188122955524,
      "loss": 2.5008,
      "step": 36010
    },
    {
      "epoch": 0.7410016997575081,
      "grad_norm": 0.36346951127052307,
      "learning_rate": 0.00017630445939790563,
      "loss": 2.4301,
      "step": 36020
    },
    {
      "epoch": 0.7412074192743758,
      "grad_norm": 0.37440475821495056,
      "learning_rate": 0.00017629010273657568,
      "loss": 2.378,
      "step": 36030
    },
    {
      "epoch": 0.7414131387912436,
      "grad_norm": 0.3991711139678955,
      "learning_rate": 0.00017627574231227072,
      "loss": 2.427,
      "step": 36040
    },
    {
      "epoch": 0.7416188583081113,
      "grad_norm": 0.3803052008152008,
      "learning_rate": 0.00017626137812569903,
      "loss": 2.4749,
      "step": 36050
    },
    {
      "epoch": 0.741824577824979,
      "grad_norm": 0.40094828605651855,
      "learning_rate": 0.0001762470101775691,
      "loss": 2.4051,
      "step": 36060
    },
    {
      "epoch": 0.7420302973418467,
      "grad_norm": 0.46469810605049133,
      "learning_rate": 0.00017623263846858967,
      "loss": 2.4883,
      "step": 36070
    },
    {
      "epoch": 0.7422360168587144,
      "grad_norm": 0.43797212839126587,
      "learning_rate": 0.00017621826299946965,
      "loss": 2.41,
      "step": 36080
    },
    {
      "epoch": 0.7424417363755821,
      "grad_norm": 0.5545824766159058,
      "learning_rate": 0.000176203883770918,
      "loss": 2.4324,
      "step": 36090
    },
    {
      "epoch": 0.7426474558924498,
      "grad_norm": 0.42632168531417847,
      "learning_rate": 0.00017618950078364403,
      "loss": 2.3819,
      "step": 36100
    },
    {
      "epoch": 0.7428531754093175,
      "grad_norm": 0.3816260099411011,
      "learning_rate": 0.00017617511403835722,
      "loss": 2.4643,
      "step": 36110
    },
    {
      "epoch": 0.7430588949261853,
      "grad_norm": 0.4113142192363739,
      "learning_rate": 0.0001761607235357671,
      "loss": 2.4333,
      "step": 36120
    },
    {
      "epoch": 0.743264614443053,
      "grad_norm": 0.3815055787563324,
      "learning_rate": 0.00017614632927658353,
      "loss": 2.4606,
      "step": 36130
    },
    {
      "epoch": 0.7434703339599207,
      "grad_norm": 0.4318229556083679,
      "learning_rate": 0.00017613193126151644,
      "loss": 2.4266,
      "step": 36140
    },
    {
      "epoch": 0.7436760534767884,
      "grad_norm": 0.3613649010658264,
      "learning_rate": 0.0001761175294912761,
      "loss": 2.4415,
      "step": 36150
    },
    {
      "epoch": 0.7438817729936561,
      "grad_norm": 0.3988684117794037,
      "learning_rate": 0.00017610312396657278,
      "loss": 2.4817,
      "step": 36160
    },
    {
      "epoch": 0.7440874925105239,
      "grad_norm": 0.42550623416900635,
      "learning_rate": 0.00017608871468811712,
      "loss": 2.4019,
      "step": 36170
    },
    {
      "epoch": 0.7442932120273915,
      "grad_norm": 0.45930391550064087,
      "learning_rate": 0.00017607430165661976,
      "loss": 2.4066,
      "step": 36180
    },
    {
      "epoch": 0.7444989315442593,
      "grad_norm": 0.3899829685688019,
      "learning_rate": 0.00017605988487279166,
      "loss": 2.477,
      "step": 36190
    },
    {
      "epoch": 0.7447046510611269,
      "grad_norm": 0.390216201543808,
      "learning_rate": 0.0001760454643373439,
      "loss": 2.4536,
      "step": 36200
    },
    {
      "epoch": 0.7449103705779947,
      "grad_norm": 0.37851375341415405,
      "learning_rate": 0.00017603104005098783,
      "loss": 2.4,
      "step": 36210
    },
    {
      "epoch": 0.7451160900948625,
      "grad_norm": 0.4086046814918518,
      "learning_rate": 0.00017601661201443482,
      "loss": 2.4528,
      "step": 36220
    },
    {
      "epoch": 0.7453218096117301,
      "grad_norm": 0.44097229838371277,
      "learning_rate": 0.0001760021802283966,
      "loss": 2.4427,
      "step": 36230
    },
    {
      "epoch": 0.7455275291285979,
      "grad_norm": 0.3900730311870575,
      "learning_rate": 0.00017598774469358502,
      "loss": 2.4525,
      "step": 36240
    },
    {
      "epoch": 0.7457332486454655,
      "grad_norm": 0.3715152442455292,
      "learning_rate": 0.00017597330541071205,
      "loss": 2.3982,
      "step": 36250
    },
    {
      "epoch": 0.7459389681623333,
      "grad_norm": 0.3854343891143799,
      "learning_rate": 0.00017595886238048992,
      "loss": 2.4152,
      "step": 36260
    },
    {
      "epoch": 0.746144687679201,
      "grad_norm": 0.4178374707698822,
      "learning_rate": 0.00017594441560363105,
      "loss": 2.4464,
      "step": 36270
    },
    {
      "epoch": 0.7463504071960687,
      "grad_norm": 0.42493170499801636,
      "learning_rate": 0.000175929965080848,
      "loss": 2.4322,
      "step": 36280
    },
    {
      "epoch": 0.7465561267129364,
      "grad_norm": 0.3680817186832428,
      "learning_rate": 0.00017591551081285356,
      "loss": 2.425,
      "step": 36290
    },
    {
      "epoch": 0.7467618462298041,
      "grad_norm": 0.4139913320541382,
      "learning_rate": 0.0001759010528003606,
      "loss": 2.3979,
      "step": 36300
    },
    {
      "epoch": 0.7469675657466719,
      "grad_norm": 0.3902120888233185,
      "learning_rate": 0.00017588659104408237,
      "loss": 2.4313,
      "step": 36310
    },
    {
      "epoch": 0.7471732852635395,
      "grad_norm": 0.38798391819000244,
      "learning_rate": 0.00017587212554473215,
      "loss": 2.4309,
      "step": 36320
    },
    {
      "epoch": 0.7473790047804073,
      "grad_norm": 0.3940717577934265,
      "learning_rate": 0.00017585765630302342,
      "loss": 2.4509,
      "step": 36330
    },
    {
      "epoch": 0.747584724297275,
      "grad_norm": 0.37358784675598145,
      "learning_rate": 0.00017584318331966987,
      "loss": 2.4659,
      "step": 36340
    },
    {
      "epoch": 0.7477904438141427,
      "grad_norm": 0.3620857298374176,
      "learning_rate": 0.00017582870659538536,
      "loss": 2.4502,
      "step": 36350
    },
    {
      "epoch": 0.7479961633310104,
      "grad_norm": 0.3637099266052246,
      "learning_rate": 0.000175814226130884,
      "loss": 2.3791,
      "step": 36360
    },
    {
      "epoch": 0.7482018828478781,
      "grad_norm": 0.38731977343559265,
      "learning_rate": 0.00017579974192688001,
      "loss": 2.4227,
      "step": 36370
    },
    {
      "epoch": 0.7484076023647458,
      "grad_norm": 0.3930925726890564,
      "learning_rate": 0.00017578525398408777,
      "loss": 2.4161,
      "step": 36380
    },
    {
      "epoch": 0.7486133218816136,
      "grad_norm": 0.46342992782592773,
      "learning_rate": 0.00017577076230322196,
      "loss": 2.4803,
      "step": 36390
    },
    {
      "epoch": 0.7488190413984813,
      "grad_norm": 0.41216108202934265,
      "learning_rate": 0.00017575626688499737,
      "loss": 2.3806,
      "step": 36400
    },
    {
      "epoch": 0.749024760915349,
      "grad_norm": 0.4233231842517853,
      "learning_rate": 0.0001757417677301289,
      "loss": 2.434,
      "step": 36410
    },
    {
      "epoch": 0.7492304804322167,
      "grad_norm": 0.4354385435581207,
      "learning_rate": 0.00017572726483933181,
      "loss": 2.4035,
      "step": 36420
    },
    {
      "epoch": 0.7494361999490844,
      "grad_norm": 0.3810231387615204,
      "learning_rate": 0.0001757127582133214,
      "loss": 2.4254,
      "step": 36430
    },
    {
      "epoch": 0.7496419194659522,
      "grad_norm": 0.37515324354171753,
      "learning_rate": 0.0001756982478528132,
      "loss": 2.4073,
      "step": 36440
    },
    {
      "epoch": 0.7498476389828198,
      "grad_norm": 0.46453768014907837,
      "learning_rate": 0.00017568373375852295,
      "loss": 2.4491,
      "step": 36450
    },
    {
      "epoch": 0.7500533584996876,
      "grad_norm": 0.38709819316864014,
      "learning_rate": 0.00017566921593116657,
      "loss": 2.4599,
      "step": 36460
    },
    {
      "epoch": 0.7502590780165552,
      "grad_norm": 0.41031208634376526,
      "learning_rate": 0.00017565469437146007,
      "loss": 2.4053,
      "step": 36470
    },
    {
      "epoch": 0.750464797533423,
      "grad_norm": 0.3798382878303528,
      "learning_rate": 0.0001756401690801198,
      "loss": 2.4572,
      "step": 36480
    },
    {
      "epoch": 0.7506705170502908,
      "grad_norm": 0.4049011468887329,
      "learning_rate": 0.00017562564005786215,
      "loss": 2.4148,
      "step": 36490
    },
    {
      "epoch": 0.7508762365671584,
      "grad_norm": 0.3954615294933319,
      "learning_rate": 0.0001756111073054038,
      "loss": 2.4632,
      "step": 36500
    },
    {
      "epoch": 0.7510819560840262,
      "grad_norm": 0.46915164589881897,
      "learning_rate": 0.00017559657082346158,
      "loss": 2.3938,
      "step": 36510
    },
    {
      "epoch": 0.7512876756008938,
      "grad_norm": 0.39493414759635925,
      "learning_rate": 0.00017558203061275243,
      "loss": 2.5111,
      "step": 36520
    },
    {
      "epoch": 0.7514933951177616,
      "grad_norm": 0.4582572877407074,
      "learning_rate": 0.0001755674866739936,
      "loss": 2.4301,
      "step": 36530
    },
    {
      "epoch": 0.7516991146346292,
      "grad_norm": 0.3959786891937256,
      "learning_rate": 0.00017555293900790243,
      "loss": 2.477,
      "step": 36540
    },
    {
      "epoch": 0.751904834151497,
      "grad_norm": 0.38637152314186096,
      "learning_rate": 0.00017553838761519652,
      "loss": 2.4608,
      "step": 36550
    },
    {
      "epoch": 0.7521105536683647,
      "grad_norm": 0.3905600309371948,
      "learning_rate": 0.0001755238324965936,
      "loss": 2.4846,
      "step": 36560
    },
    {
      "epoch": 0.7523162731852324,
      "grad_norm": 0.3964928388595581,
      "learning_rate": 0.00017550927365281153,
      "loss": 2.4219,
      "step": 36570
    },
    {
      "epoch": 0.7525219927021002,
      "grad_norm": 0.39217862486839294,
      "learning_rate": 0.00017549471108456848,
      "loss": 2.4244,
      "step": 36580
    },
    {
      "epoch": 0.7527277122189678,
      "grad_norm": 0.40533530712127686,
      "learning_rate": 0.00017548014479258276,
      "loss": 2.45,
      "step": 36590
    },
    {
      "epoch": 0.7529334317358356,
      "grad_norm": 0.49506911635398865,
      "learning_rate": 0.00017546557477757276,
      "loss": 2.3555,
      "step": 36600
    },
    {
      "epoch": 0.7531391512527033,
      "grad_norm": 0.42751121520996094,
      "learning_rate": 0.00017545100104025723,
      "loss": 2.4285,
      "step": 36610
    },
    {
      "epoch": 0.753344870769571,
      "grad_norm": 0.40933936834335327,
      "learning_rate": 0.000175436423581355,
      "loss": 2.3878,
      "step": 36620
    },
    {
      "epoch": 0.7535505902864387,
      "grad_norm": 0.45392560958862305,
      "learning_rate": 0.00017542184240158503,
      "loss": 2.3887,
      "step": 36630
    },
    {
      "epoch": 0.7537563098033064,
      "grad_norm": 0.3739238679409027,
      "learning_rate": 0.0001754072575016666,
      "loss": 2.4076,
      "step": 36640
    },
    {
      "epoch": 0.7539620293201741,
      "grad_norm": 0.40991854667663574,
      "learning_rate": 0.0001753926688823191,
      "loss": 2.4784,
      "step": 36650
    },
    {
      "epoch": 0.7541677488370419,
      "grad_norm": 0.3960050642490387,
      "learning_rate": 0.00017537807654426205,
      "loss": 2.4015,
      "step": 36660
    },
    {
      "epoch": 0.7543734683539096,
      "grad_norm": 0.41756629943847656,
      "learning_rate": 0.00017536348048821524,
      "loss": 2.4273,
      "step": 36670
    },
    {
      "epoch": 0.7545791878707773,
      "grad_norm": 0.4131343364715576,
      "learning_rate": 0.00017534888071489864,
      "loss": 2.3995,
      "step": 36680
    },
    {
      "epoch": 0.754784907387645,
      "grad_norm": 0.38147398829460144,
      "learning_rate": 0.00017533427722503232,
      "loss": 2.4083,
      "step": 36690
    },
    {
      "epoch": 0.7549906269045127,
      "grad_norm": 0.4027383625507355,
      "learning_rate": 0.00017531967001933668,
      "loss": 2.4781,
      "step": 36700
    },
    {
      "epoch": 0.7551963464213804,
      "grad_norm": 0.38184985518455505,
      "learning_rate": 0.00017530505909853211,
      "loss": 2.4062,
      "step": 36710
    },
    {
      "epoch": 0.7554020659382481,
      "grad_norm": 0.43961894512176514,
      "learning_rate": 0.00017529044446333938,
      "loss": 2.4637,
      "step": 36720
    },
    {
      "epoch": 0.7556077854551159,
      "grad_norm": 0.4027673900127411,
      "learning_rate": 0.0001752758261144793,
      "loss": 2.378,
      "step": 36730
    },
    {
      "epoch": 0.7558135049719835,
      "grad_norm": 0.43471789360046387,
      "learning_rate": 0.00017526120405267289,
      "loss": 2.4674,
      "step": 36740
    },
    {
      "epoch": 0.7560192244888513,
      "grad_norm": 0.4489729404449463,
      "learning_rate": 0.00017524657827864144,
      "loss": 2.3965,
      "step": 36750
    },
    {
      "epoch": 0.756224944005719,
      "grad_norm": 0.41092708706855774,
      "learning_rate": 0.0001752319487931063,
      "loss": 2.4018,
      "step": 36760
    },
    {
      "epoch": 0.7564306635225867,
      "grad_norm": 0.42879271507263184,
      "learning_rate": 0.0001752173155967891,
      "loss": 2.4121,
      "step": 36770
    },
    {
      "epoch": 0.7566363830394545,
      "grad_norm": 0.3926053047180176,
      "learning_rate": 0.00017520267869041166,
      "loss": 2.4004,
      "step": 36780
    },
    {
      "epoch": 0.7568421025563221,
      "grad_norm": 0.3814272880554199,
      "learning_rate": 0.00017518803807469583,
      "loss": 2.4384,
      "step": 36790
    },
    {
      "epoch": 0.7570478220731899,
      "grad_norm": 0.404358834028244,
      "learning_rate": 0.00017517339375036381,
      "loss": 2.4037,
      "step": 36800
    },
    {
      "epoch": 0.7572535415900575,
      "grad_norm": 0.40958043932914734,
      "learning_rate": 0.00017515874571813792,
      "loss": 2.4669,
      "step": 36810
    },
    {
      "epoch": 0.7574592611069253,
      "grad_norm": 0.4331452250480652,
      "learning_rate": 0.0001751440939787407,
      "loss": 2.4437,
      "step": 36820
    },
    {
      "epoch": 0.757664980623793,
      "grad_norm": 0.38950493931770325,
      "learning_rate": 0.0001751294385328948,
      "loss": 2.4827,
      "step": 36830
    },
    {
      "epoch": 0.7578707001406607,
      "grad_norm": 0.3917253017425537,
      "learning_rate": 0.0001751147793813231,
      "loss": 2.4544,
      "step": 36840
    },
    {
      "epoch": 0.7580764196575285,
      "grad_norm": 0.38841477036476135,
      "learning_rate": 0.00017510011652474865,
      "loss": 2.4242,
      "step": 36850
    },
    {
      "epoch": 0.7582821391743961,
      "grad_norm": 0.4089798927307129,
      "learning_rate": 0.0001750854499638947,
      "loss": 2.4693,
      "step": 36860
    },
    {
      "epoch": 0.7584878586912639,
      "grad_norm": 0.3981212079524994,
      "learning_rate": 0.00017507077969948465,
      "loss": 2.4196,
      "step": 36870
    },
    {
      "epoch": 0.7586935782081315,
      "grad_norm": 0.382609099149704,
      "learning_rate": 0.00017505610573224217,
      "loss": 2.4191,
      "step": 36880
    },
    {
      "epoch": 0.7588992977249993,
      "grad_norm": 0.4729822278022766,
      "learning_rate": 0.00017504142806289098,
      "loss": 2.4443,
      "step": 36890
    },
    {
      "epoch": 0.759105017241867,
      "grad_norm": 0.4362547993659973,
      "learning_rate": 0.00017502674669215508,
      "loss": 2.4923,
      "step": 36900
    },
    {
      "epoch": 0.7593107367587347,
      "grad_norm": 0.38665589690208435,
      "learning_rate": 0.0001750120616207586,
      "loss": 2.4231,
      "step": 36910
    },
    {
      "epoch": 0.7595164562756025,
      "grad_norm": 0.4749554395675659,
      "learning_rate": 0.0001749973728494259,
      "loss": 2.422,
      "step": 36920
    },
    {
      "epoch": 0.7597221757924701,
      "grad_norm": 0.40552854537963867,
      "learning_rate": 0.00017498268037888145,
      "loss": 2.4059,
      "step": 36930
    },
    {
      "epoch": 0.7599278953093379,
      "grad_norm": 0.3828674256801605,
      "learning_rate": 0.00017496798420985,
      "loss": 2.4407,
      "step": 36940
    },
    {
      "epoch": 0.7601336148262056,
      "grad_norm": 0.394014447927475,
      "learning_rate": 0.00017495328434305644,
      "loss": 2.4827,
      "step": 36950
    },
    {
      "epoch": 0.7603393343430733,
      "grad_norm": 0.3811364471912384,
      "learning_rate": 0.0001749385807792258,
      "loss": 2.3696,
      "step": 36960
    },
    {
      "epoch": 0.760545053859941,
      "grad_norm": 0.44262582063674927,
      "learning_rate": 0.00017492387351908333,
      "loss": 2.4205,
      "step": 36970
    },
    {
      "epoch": 0.7607507733768087,
      "grad_norm": 0.39811915159225464,
      "learning_rate": 0.00017490916256335446,
      "loss": 2.4788,
      "step": 36980
    },
    {
      "epoch": 0.7609564928936764,
      "grad_norm": 0.41426852345466614,
      "learning_rate": 0.0001748944479127648,
      "loss": 2.4167,
      "step": 36990
    },
    {
      "epoch": 0.7611622124105442,
      "grad_norm": 0.4542537033557892,
      "learning_rate": 0.00017487972956804017,
      "loss": 2.4651,
      "step": 37000
    },
    {
      "epoch": 0.7613679319274119,
      "grad_norm": 0.3935979902744293,
      "learning_rate": 0.00017486500752990648,
      "loss": 2.4141,
      "step": 37010
    },
    {
      "epoch": 0.7615736514442796,
      "grad_norm": 0.39689865708351135,
      "learning_rate": 0.00017485028179909,
      "loss": 2.3819,
      "step": 37020
    },
    {
      "epoch": 0.7617793709611473,
      "grad_norm": 0.38074588775634766,
      "learning_rate": 0.00017483555237631697,
      "loss": 2.4596,
      "step": 37030
    },
    {
      "epoch": 0.761985090478015,
      "grad_norm": 0.37867382168769836,
      "learning_rate": 0.00017482081926231395,
      "loss": 2.4404,
      "step": 37040
    },
    {
      "epoch": 0.7621908099948828,
      "grad_norm": 0.3811699151992798,
      "learning_rate": 0.00017480608245780762,
      "loss": 2.3908,
      "step": 37050
    },
    {
      "epoch": 0.7623965295117504,
      "grad_norm": 0.3894108831882477,
      "learning_rate": 0.00017479134196352495,
      "loss": 2.4017,
      "step": 37060
    },
    {
      "epoch": 0.7626022490286182,
      "grad_norm": 0.4084959626197815,
      "learning_rate": 0.00017477659778019285,
      "loss": 2.4836,
      "step": 37070
    },
    {
      "epoch": 0.7628079685454858,
      "grad_norm": 0.3804239332675934,
      "learning_rate": 0.00017476184990853873,
      "loss": 2.4045,
      "step": 37080
    },
    {
      "epoch": 0.7630136880623536,
      "grad_norm": 0.3952306807041168,
      "learning_rate": 0.00017474709834928995,
      "loss": 2.382,
      "step": 37090
    },
    {
      "epoch": 0.7632194075792214,
      "grad_norm": 0.4430486559867859,
      "learning_rate": 0.00017473234310317413,
      "loss": 2.439,
      "step": 37100
    },
    {
      "epoch": 0.763425127096089,
      "grad_norm": 0.3798421025276184,
      "learning_rate": 0.0001747175841709191,
      "loss": 2.423,
      "step": 37110
    },
    {
      "epoch": 0.7636308466129568,
      "grad_norm": 0.5825676321983337,
      "learning_rate": 0.00017470282155325272,
      "loss": 2.4119,
      "step": 37120
    },
    {
      "epoch": 0.7638365661298244,
      "grad_norm": 0.40957891941070557,
      "learning_rate": 0.0001746880552509033,
      "loss": 2.4503,
      "step": 37130
    },
    {
      "epoch": 0.7640422856466922,
      "grad_norm": 0.4595411419868469,
      "learning_rate": 0.0001746732852645991,
      "loss": 2.4591,
      "step": 37140
    },
    {
      "epoch": 0.7642480051635598,
      "grad_norm": 0.38112330436706543,
      "learning_rate": 0.00017465851159506864,
      "loss": 2.4019,
      "step": 37150
    },
    {
      "epoch": 0.7644537246804276,
      "grad_norm": 0.39368677139282227,
      "learning_rate": 0.00017464373424304068,
      "loss": 2.4342,
      "step": 37160
    },
    {
      "epoch": 0.7646594441972953,
      "grad_norm": 0.4205947518348694,
      "learning_rate": 0.00017462895320924404,
      "loss": 2.5045,
      "step": 37170
    },
    {
      "epoch": 0.764865163714163,
      "grad_norm": 0.431439608335495,
      "learning_rate": 0.00017461416849440782,
      "loss": 2.4083,
      "step": 37180
    },
    {
      "epoch": 0.7650708832310308,
      "grad_norm": 0.3940739929676056,
      "learning_rate": 0.00017459938009926126,
      "loss": 2.4381,
      "step": 37190
    },
    {
      "epoch": 0.7652766027478984,
      "grad_norm": 0.3814457952976227,
      "learning_rate": 0.00017458458802453383,
      "loss": 2.438,
      "step": 37200
    },
    {
      "epoch": 0.7654823222647662,
      "grad_norm": 0.4139944314956665,
      "learning_rate": 0.0001745697922709551,
      "loss": 2.4503,
      "step": 37210
    },
    {
      "epoch": 0.7656880417816339,
      "grad_norm": 0.4004404842853546,
      "learning_rate": 0.00017455499283925484,
      "loss": 2.4515,
      "step": 37220
    },
    {
      "epoch": 0.7658937612985016,
      "grad_norm": 0.4370574951171875,
      "learning_rate": 0.0001745401897301631,
      "loss": 2.4755,
      "step": 37230
    },
    {
      "epoch": 0.7660994808153693,
      "grad_norm": 0.39834466576576233,
      "learning_rate": 0.00017452538294440996,
      "loss": 2.4277,
      "step": 37240
    },
    {
      "epoch": 0.766305200332237,
      "grad_norm": 0.4315599799156189,
      "learning_rate": 0.0001745105724827258,
      "loss": 2.3974,
      "step": 37250
    },
    {
      "epoch": 0.7665109198491047,
      "grad_norm": 0.43481793999671936,
      "learning_rate": 0.00017449575834584116,
      "loss": 2.4168,
      "step": 37260
    },
    {
      "epoch": 0.7667166393659725,
      "grad_norm": 0.4053425192832947,
      "learning_rate": 0.00017448094053448672,
      "loss": 2.4048,
      "step": 37270
    },
    {
      "epoch": 0.7669223588828402,
      "grad_norm": 0.40858209133148193,
      "learning_rate": 0.00017446611904939333,
      "loss": 2.448,
      "step": 37280
    },
    {
      "epoch": 0.7671280783997079,
      "grad_norm": 0.40447112917900085,
      "learning_rate": 0.00017445129389129208,
      "loss": 2.4494,
      "step": 37290
    },
    {
      "epoch": 0.7673337979165756,
      "grad_norm": 0.400086909532547,
      "learning_rate": 0.0001744364650609142,
      "loss": 2.4894,
      "step": 37300
    },
    {
      "epoch": 0.7675395174334433,
      "grad_norm": 0.374267578125,
      "learning_rate": 0.00017442163255899118,
      "loss": 2.4901,
      "step": 37310
    },
    {
      "epoch": 0.767745236950311,
      "grad_norm": 0.3974860608577728,
      "learning_rate": 0.00017440679638625455,
      "loss": 2.4833,
      "step": 37320
    },
    {
      "epoch": 0.7679509564671787,
      "grad_norm": 0.431705117225647,
      "learning_rate": 0.0001743919565434361,
      "loss": 2.457,
      "step": 37330
    },
    {
      "epoch": 0.7681566759840465,
      "grad_norm": 0.4536283016204834,
      "learning_rate": 0.00017437711303126783,
      "loss": 2.385,
      "step": 37340
    },
    {
      "epoch": 0.7683623955009141,
      "grad_norm": 0.4154655337333679,
      "learning_rate": 0.0001743622658504819,
      "loss": 2.4153,
      "step": 37350
    },
    {
      "epoch": 0.7685681150177819,
      "grad_norm": 0.4233620762825012,
      "learning_rate": 0.0001743474150018106,
      "loss": 2.4342,
      "step": 37360
    },
    {
      "epoch": 0.7687738345346496,
      "grad_norm": 0.3869701623916626,
      "learning_rate": 0.00017433256048598648,
      "loss": 2.3817,
      "step": 37370
    },
    {
      "epoch": 0.7689795540515173,
      "grad_norm": 0.5458946824073792,
      "learning_rate": 0.0001743177023037422,
      "loss": 2.4449,
      "step": 37380
    },
    {
      "epoch": 0.7691852735683851,
      "grad_norm": 0.3666141927242279,
      "learning_rate": 0.00017430284045581065,
      "loss": 2.3933,
      "step": 37390
    },
    {
      "epoch": 0.7693909930852527,
      "grad_norm": 0.3875284790992737,
      "learning_rate": 0.0001742879749429249,
      "loss": 2.3805,
      "step": 37400
    },
    {
      "epoch": 0.7695967126021205,
      "grad_norm": 0.4168837070465088,
      "learning_rate": 0.00017427310576581816,
      "loss": 2.485,
      "step": 37410
    },
    {
      "epoch": 0.7698024321189881,
      "grad_norm": 0.44785159826278687,
      "learning_rate": 0.00017425823292522384,
      "loss": 2.4558,
      "step": 37420
    },
    {
      "epoch": 0.7700081516358559,
      "grad_norm": 0.40981850028038025,
      "learning_rate": 0.00017424335642187555,
      "loss": 2.4383,
      "step": 37430
    },
    {
      "epoch": 0.7702138711527236,
      "grad_norm": 0.3790605664253235,
      "learning_rate": 0.00017422847625650706,
      "loss": 2.4602,
      "step": 37440
    },
    {
      "epoch": 0.7704195906695913,
      "grad_norm": 0.39905107021331787,
      "learning_rate": 0.00017421359242985235,
      "loss": 2.4286,
      "step": 37450
    },
    {
      "epoch": 0.7706253101864591,
      "grad_norm": 0.4000406861305237,
      "learning_rate": 0.00017419870494264555,
      "loss": 2.4268,
      "step": 37460
    },
    {
      "epoch": 0.7708310297033267,
      "grad_norm": 0.4412916302680969,
      "learning_rate": 0.00017418381379562093,
      "loss": 2.4328,
      "step": 37470
    },
    {
      "epoch": 0.7710367492201945,
      "grad_norm": 0.3772265613079071,
      "learning_rate": 0.00017416891898951306,
      "loss": 2.3995,
      "step": 37480
    },
    {
      "epoch": 0.7712424687370621,
      "grad_norm": 0.3943732976913452,
      "learning_rate": 0.00017415402052505658,
      "loss": 2.4623,
      "step": 37490
    },
    {
      "epoch": 0.7714481882539299,
      "grad_norm": 0.3691692352294922,
      "learning_rate": 0.00017413911840298636,
      "loss": 2.4767,
      "step": 37500
    },
    {
      "epoch": 0.7716539077707976,
      "grad_norm": 0.41677504777908325,
      "learning_rate": 0.00017412421262403744,
      "loss": 2.3932,
      "step": 37510
    },
    {
      "epoch": 0.7718596272876653,
      "grad_norm": 0.3882853388786316,
      "learning_rate": 0.00017410930318894504,
      "loss": 2.4007,
      "step": 37520
    },
    {
      "epoch": 0.772065346804533,
      "grad_norm": 0.3933160901069641,
      "learning_rate": 0.00017409439009844456,
      "loss": 2.3851,
      "step": 37530
    },
    {
      "epoch": 0.7722710663214007,
      "grad_norm": 0.42462360858917236,
      "learning_rate": 0.00017407947335327156,
      "loss": 2.47,
      "step": 37540
    },
    {
      "epoch": 0.7724767858382685,
      "grad_norm": 0.41236764192581177,
      "learning_rate": 0.00017406455295416188,
      "loss": 2.4829,
      "step": 37550
    },
    {
      "epoch": 0.7726825053551362,
      "grad_norm": 0.48317858576774597,
      "learning_rate": 0.00017404962890185135,
      "loss": 2.4229,
      "step": 37560
    },
    {
      "epoch": 0.7728882248720039,
      "grad_norm": 0.3885059952735901,
      "learning_rate": 0.00017403470119707617,
      "loss": 2.429,
      "step": 37570
    },
    {
      "epoch": 0.7730939443888716,
      "grad_norm": 0.39318329095840454,
      "learning_rate": 0.00017401976984057264,
      "loss": 2.4088,
      "step": 37580
    },
    {
      "epoch": 0.7732996639057393,
      "grad_norm": 0.38275718688964844,
      "learning_rate": 0.0001740048348330772,
      "loss": 2.3807,
      "step": 37590
    },
    {
      "epoch": 0.773505383422607,
      "grad_norm": 0.3764900863170624,
      "learning_rate": 0.00017398989617532654,
      "loss": 2.377,
      "step": 37600
    },
    {
      "epoch": 0.7737111029394748,
      "grad_norm": 0.42567798495292664,
      "learning_rate": 0.00017397495386805748,
      "loss": 2.4338,
      "step": 37610
    },
    {
      "epoch": 0.7739168224563424,
      "grad_norm": 0.40634021162986755,
      "learning_rate": 0.0001739600079120071,
      "loss": 2.4391,
      "step": 37620
    },
    {
      "epoch": 0.7741225419732102,
      "grad_norm": 0.39178580045700073,
      "learning_rate": 0.0001739450583079125,
      "loss": 2.4745,
      "step": 37630
    },
    {
      "epoch": 0.7743282614900779,
      "grad_norm": 0.4246146082878113,
      "learning_rate": 0.0001739301050565112,
      "loss": 2.4664,
      "step": 37640
    },
    {
      "epoch": 0.7745339810069456,
      "grad_norm": 0.44674456119537354,
      "learning_rate": 0.00017391514815854063,
      "loss": 2.3989,
      "step": 37650
    },
    {
      "epoch": 0.7747397005238134,
      "grad_norm": 0.38738179206848145,
      "learning_rate": 0.00017390018761473866,
      "loss": 2.4462,
      "step": 37660
    },
    {
      "epoch": 0.774945420040681,
      "grad_norm": 0.40172508358955383,
      "learning_rate": 0.0001738852234258431,
      "loss": 2.4193,
      "step": 37670
    },
    {
      "epoch": 0.7751511395575488,
      "grad_norm": 0.3813512921333313,
      "learning_rate": 0.00017387025559259208,
      "loss": 2.4749,
      "step": 37680
    },
    {
      "epoch": 0.7753568590744164,
      "grad_norm": 0.363126665353775,
      "learning_rate": 0.00017385528411572392,
      "loss": 2.3364,
      "step": 37690
    },
    {
      "epoch": 0.7755625785912842,
      "grad_norm": 0.40824732184410095,
      "learning_rate": 0.00017384030899597703,
      "loss": 2.4998,
      "step": 37700
    },
    {
      "epoch": 0.7757682981081518,
      "grad_norm": 0.4150768518447876,
      "learning_rate": 0.0001738253302340901,
      "loss": 2.4533,
      "step": 37710
    },
    {
      "epoch": 0.7759740176250196,
      "grad_norm": 0.376786470413208,
      "learning_rate": 0.0001738103478308019,
      "loss": 2.4389,
      "step": 37720
    },
    {
      "epoch": 0.7761797371418874,
      "grad_norm": 0.37308529019355774,
      "learning_rate": 0.0001737953617868515,
      "loss": 2.432,
      "step": 37730
    },
    {
      "epoch": 0.776385456658755,
      "grad_norm": 0.4145103693008423,
      "learning_rate": 0.00017378037210297805,
      "loss": 2.4258,
      "step": 37740
    },
    {
      "epoch": 0.7765911761756228,
      "grad_norm": 0.4160437285900116,
      "learning_rate": 0.00017376537877992088,
      "loss": 2.4615,
      "step": 37750
    },
    {
      "epoch": 0.7767968956924904,
      "grad_norm": 0.4091853201389313,
      "learning_rate": 0.00017375038181841954,
      "loss": 2.4441,
      "step": 37760
    },
    {
      "epoch": 0.7770026152093582,
      "grad_norm": 0.4549654722213745,
      "learning_rate": 0.00017373538121921375,
      "loss": 2.4105,
      "step": 37770
    },
    {
      "epoch": 0.7772083347262259,
      "grad_norm": 0.4191645383834839,
      "learning_rate": 0.00017372037698304343,
      "loss": 2.3921,
      "step": 37780
    },
    {
      "epoch": 0.7774140542430936,
      "grad_norm": 0.6330524682998657,
      "learning_rate": 0.00017370536911064864,
      "loss": 2.3782,
      "step": 37790
    },
    {
      "epoch": 0.7776197737599613,
      "grad_norm": 0.46350499987602234,
      "learning_rate": 0.0001736903576027696,
      "loss": 2.4364,
      "step": 37800
    },
    {
      "epoch": 0.777825493276829,
      "grad_norm": 0.4057106375694275,
      "learning_rate": 0.00017367534246014684,
      "loss": 2.4009,
      "step": 37810
    },
    {
      "epoch": 0.7780312127936968,
      "grad_norm": 0.41358688473701477,
      "learning_rate": 0.0001736603236835209,
      "loss": 2.4789,
      "step": 37820
    },
    {
      "epoch": 0.7782369323105645,
      "grad_norm": 0.37420088052749634,
      "learning_rate": 0.00017364530127363258,
      "loss": 2.4463,
      "step": 37830
    },
    {
      "epoch": 0.7784426518274322,
      "grad_norm": 0.45821258425712585,
      "learning_rate": 0.00017363027523122288,
      "loss": 2.4812,
      "step": 37840
    },
    {
      "epoch": 0.7786483713442999,
      "grad_norm": 0.35986608266830444,
      "learning_rate": 0.00017361524555703292,
      "loss": 2.4377,
      "step": 37850
    },
    {
      "epoch": 0.7788540908611676,
      "grad_norm": 0.3761785328388214,
      "learning_rate": 0.00017360021225180407,
      "loss": 2.4276,
      "step": 37860
    },
    {
      "epoch": 0.7790598103780353,
      "grad_norm": 0.38418301939964294,
      "learning_rate": 0.00017358517531627782,
      "loss": 2.4492,
      "step": 37870
    },
    {
      "epoch": 0.779265529894903,
      "grad_norm": 0.4311221241950989,
      "learning_rate": 0.00017357013475119586,
      "loss": 2.4369,
      "step": 37880
    },
    {
      "epoch": 0.7794712494117707,
      "grad_norm": 0.592932939529419,
      "learning_rate": 0.00017355509055730006,
      "loss": 2.4708,
      "step": 37890
    },
    {
      "epoch": 0.7796769689286385,
      "grad_norm": 0.36366623640060425,
      "learning_rate": 0.0001735400427353325,
      "loss": 2.4006,
      "step": 37900
    },
    {
      "epoch": 0.7798826884455062,
      "grad_norm": 0.40340662002563477,
      "learning_rate": 0.0001735249912860353,
      "loss": 2.424,
      "step": 37910
    },
    {
      "epoch": 0.7800884079623739,
      "grad_norm": 0.38958922028541565,
      "learning_rate": 0.000173509936210151,
      "loss": 2.4707,
      "step": 37920
    },
    {
      "epoch": 0.7802941274792417,
      "grad_norm": 0.3935920298099518,
      "learning_rate": 0.00017349487750842216,
      "loss": 2.3986,
      "step": 37930
    },
    {
      "epoch": 0.7804998469961093,
      "grad_norm": 0.4369279742240906,
      "learning_rate": 0.00017347981518159146,
      "loss": 2.4118,
      "step": 37940
    },
    {
      "epoch": 0.7807055665129771,
      "grad_norm": 0.4477655589580536,
      "learning_rate": 0.0001734647492304019,
      "loss": 2.4074,
      "step": 37950
    },
    {
      "epoch": 0.7809112860298447,
      "grad_norm": 0.4667353928089142,
      "learning_rate": 0.0001734496796555966,
      "loss": 2.4427,
      "step": 37960
    },
    {
      "epoch": 0.7811170055467125,
      "grad_norm": 0.4068475365638733,
      "learning_rate": 0.00017343460645791886,
      "loss": 2.4896,
      "step": 37970
    },
    {
      "epoch": 0.7813227250635801,
      "grad_norm": 0.395815908908844,
      "learning_rate": 0.00017341952963811216,
      "loss": 2.4385,
      "step": 37980
    },
    {
      "epoch": 0.7815284445804479,
      "grad_norm": 0.40397247672080994,
      "learning_rate": 0.00017340444919692013,
      "loss": 2.3818,
      "step": 37990
    },
    {
      "epoch": 0.7817341640973157,
      "grad_norm": 0.395256370306015,
      "learning_rate": 0.00017338936513508667,
      "loss": 2.4268,
      "step": 38000
    },
    {
      "epoch": 0.7819398836141833,
      "grad_norm": 0.3957657814025879,
      "learning_rate": 0.00017337427745335574,
      "loss": 2.4136,
      "step": 38010
    },
    {
      "epoch": 0.7821456031310511,
      "grad_norm": 0.4209919571876526,
      "learning_rate": 0.0001733591861524715,
      "loss": 2.4503,
      "step": 38020
    },
    {
      "epoch": 0.7823513226479187,
      "grad_norm": 0.3672387897968292,
      "learning_rate": 0.00017334409123317837,
      "loss": 2.4247,
      "step": 38030
    },
    {
      "epoch": 0.7825570421647865,
      "grad_norm": 0.3880953788757324,
      "learning_rate": 0.00017332899269622092,
      "loss": 2.4157,
      "step": 38040
    },
    {
      "epoch": 0.7827627616816542,
      "grad_norm": 0.4036361277103424,
      "learning_rate": 0.0001733138905423439,
      "loss": 2.4608,
      "step": 38050
    },
    {
      "epoch": 0.7829684811985219,
      "grad_norm": 0.40265414118766785,
      "learning_rate": 0.0001732987847722921,
      "loss": 2.4649,
      "step": 38060
    },
    {
      "epoch": 0.7831742007153896,
      "grad_norm": 0.4082372188568115,
      "learning_rate": 0.00017328367538681073,
      "loss": 2.4506,
      "step": 38070
    },
    {
      "epoch": 0.7833799202322573,
      "grad_norm": 0.4301729202270508,
      "learning_rate": 0.00017326856238664498,
      "loss": 2.4528,
      "step": 38080
    },
    {
      "epoch": 0.7835856397491251,
      "grad_norm": 0.3954101800918579,
      "learning_rate": 0.0001732534457725403,
      "loss": 2.3709,
      "step": 38090
    },
    {
      "epoch": 0.7837913592659927,
      "grad_norm": 0.39636269211769104,
      "learning_rate": 0.00017323832554524233,
      "loss": 2.4235,
      "step": 38100
    },
    {
      "epoch": 0.7839970787828605,
      "grad_norm": 0.4696553349494934,
      "learning_rate": 0.00017322320170549686,
      "loss": 2.4507,
      "step": 38110
    },
    {
      "epoch": 0.7842027982997282,
      "grad_norm": 0.42657002806663513,
      "learning_rate": 0.00017320807425404988,
      "loss": 2.4591,
      "step": 38120
    },
    {
      "epoch": 0.7844085178165959,
      "grad_norm": 0.42142221331596375,
      "learning_rate": 0.00017319294319164752,
      "loss": 2.4056,
      "step": 38130
    },
    {
      "epoch": 0.7846142373334636,
      "grad_norm": 0.37910565733909607,
      "learning_rate": 0.00017317780851903614,
      "loss": 2.4427,
      "step": 38140
    },
    {
      "epoch": 0.7848199568503313,
      "grad_norm": 0.37953752279281616,
      "learning_rate": 0.00017316267023696224,
      "loss": 2.4535,
      "step": 38150
    },
    {
      "epoch": 0.785025676367199,
      "grad_norm": 0.39278969168663025,
      "learning_rate": 0.0001731475283461725,
      "loss": 2.4035,
      "step": 38160
    },
    {
      "epoch": 0.7852313958840668,
      "grad_norm": 0.39821499586105347,
      "learning_rate": 0.0001731323828474138,
      "loss": 2.4326,
      "step": 38170
    },
    {
      "epoch": 0.7854371154009345,
      "grad_norm": 0.42786848545074463,
      "learning_rate": 0.00017311723374143317,
      "loss": 2.4484,
      "step": 38180
    },
    {
      "epoch": 0.7856428349178022,
      "grad_norm": 0.5069872140884399,
      "learning_rate": 0.00017310208102897784,
      "loss": 2.408,
      "step": 38190
    },
    {
      "epoch": 0.7858485544346699,
      "grad_norm": 0.3964107930660248,
      "learning_rate": 0.00017308692471079516,
      "loss": 2.5358,
      "step": 38200
    },
    {
      "epoch": 0.7860542739515376,
      "grad_norm": 0.410315603017807,
      "learning_rate": 0.00017307176478763284,
      "loss": 2.4848,
      "step": 38210
    },
    {
      "epoch": 0.7862599934684054,
      "grad_norm": 0.38321104645729065,
      "learning_rate": 0.00017305660126023853,
      "loss": 2.4446,
      "step": 38220
    },
    {
      "epoch": 0.786465712985273,
      "grad_norm": 0.4021995961666107,
      "learning_rate": 0.00017304143412936017,
      "loss": 2.3728,
      "step": 38230
    },
    {
      "epoch": 0.7866714325021408,
      "grad_norm": 0.4229431450366974,
      "learning_rate": 0.00017302626339574592,
      "loss": 2.461,
      "step": 38240
    },
    {
      "epoch": 0.7868771520190084,
      "grad_norm": 0.4018111526966095,
      "learning_rate": 0.00017301108906014404,
      "loss": 2.4957,
      "step": 38250
    },
    {
      "epoch": 0.7870828715358762,
      "grad_norm": 0.3733677566051483,
      "learning_rate": 0.00017299591112330298,
      "loss": 2.475,
      "step": 38260
    },
    {
      "epoch": 0.787288591052744,
      "grad_norm": 0.3963709771633148,
      "learning_rate": 0.00017298072958597143,
      "loss": 2.3846,
      "step": 38270
    },
    {
      "epoch": 0.7874943105696116,
      "grad_norm": 0.3839915692806244,
      "learning_rate": 0.00017296554444889818,
      "loss": 2.4262,
      "step": 38280
    },
    {
      "epoch": 0.7877000300864794,
      "grad_norm": 0.39676403999328613,
      "learning_rate": 0.0001729503557128322,
      "loss": 2.3706,
      "step": 38290
    },
    {
      "epoch": 0.787905749603347,
      "grad_norm": 0.43990814685821533,
      "learning_rate": 0.00017293516337852274,
      "loss": 2.4288,
      "step": 38300
    },
    {
      "epoch": 0.7881114691202148,
      "grad_norm": 0.41124624013900757,
      "learning_rate": 0.0001729199674467191,
      "loss": 2.4249,
      "step": 38310
    },
    {
      "epoch": 0.7883171886370824,
      "grad_norm": 0.40227270126342773,
      "learning_rate": 0.00017290476791817088,
      "loss": 2.4518,
      "step": 38320
    },
    {
      "epoch": 0.7885229081539502,
      "grad_norm": 0.4225839078426361,
      "learning_rate": 0.00017288956479362768,
      "loss": 2.4464,
      "step": 38330
    },
    {
      "epoch": 0.7887286276708179,
      "grad_norm": 0.4554867446422577,
      "learning_rate": 0.00017287435807383949,
      "loss": 2.4097,
      "step": 38340
    },
    {
      "epoch": 0.7889343471876856,
      "grad_norm": 0.3993096649646759,
      "learning_rate": 0.0001728591477595563,
      "loss": 2.413,
      "step": 38350
    },
    {
      "epoch": 0.7891400667045534,
      "grad_norm": 0.39735299348831177,
      "learning_rate": 0.0001728439338515284,
      "loss": 2.4354,
      "step": 38360
    },
    {
      "epoch": 0.789345786221421,
      "grad_norm": 0.4110225439071655,
      "learning_rate": 0.0001728287163505062,
      "loss": 2.4078,
      "step": 38370
    },
    {
      "epoch": 0.7895515057382888,
      "grad_norm": 0.43097230792045593,
      "learning_rate": 0.00017281349525724032,
      "loss": 2.4629,
      "step": 38380
    },
    {
      "epoch": 0.7897572252551565,
      "grad_norm": 0.37600013613700867,
      "learning_rate": 0.00017279827057248146,
      "loss": 2.4215,
      "step": 38390
    },
    {
      "epoch": 0.7899629447720242,
      "grad_norm": 0.395654559135437,
      "learning_rate": 0.00017278304229698064,
      "loss": 2.3923,
      "step": 38400
    },
    {
      "epoch": 0.7901686642888919,
      "grad_norm": 0.37542393803596497,
      "learning_rate": 0.00017276781043148897,
      "loss": 2.4394,
      "step": 38410
    },
    {
      "epoch": 0.7903743838057596,
      "grad_norm": 0.43764325976371765,
      "learning_rate": 0.00017275257497675772,
      "loss": 2.4378,
      "step": 38420
    },
    {
      "epoch": 0.7905801033226273,
      "grad_norm": 0.4108993411064148,
      "learning_rate": 0.00017273733593353843,
      "loss": 2.4291,
      "step": 38430
    },
    {
      "epoch": 0.7907858228394951,
      "grad_norm": 0.4121476709842682,
      "learning_rate": 0.0001727220933025827,
      "loss": 2.4467,
      "step": 38440
    },
    {
      "epoch": 0.7909915423563628,
      "grad_norm": 0.3941313326358795,
      "learning_rate": 0.00017270684708464243,
      "loss": 2.4655,
      "step": 38450
    },
    {
      "epoch": 0.7911972618732305,
      "grad_norm": 0.4809333384037018,
      "learning_rate": 0.00017269159728046954,
      "loss": 2.4141,
      "step": 38460
    },
    {
      "epoch": 0.7914029813900982,
      "grad_norm": 0.4187997579574585,
      "learning_rate": 0.00017267634389081632,
      "loss": 2.4568,
      "step": 38470
    },
    {
      "epoch": 0.7916087009069659,
      "grad_norm": 0.4781021177768707,
      "learning_rate": 0.0001726610869164351,
      "loss": 2.4511,
      "step": 38480
    },
    {
      "epoch": 0.7918144204238337,
      "grad_norm": 0.41664406657218933,
      "learning_rate": 0.0001726458263580784,
      "loss": 2.4846,
      "step": 38490
    },
    {
      "epoch": 0.7920201399407013,
      "grad_norm": 0.3827773928642273,
      "learning_rate": 0.00017263056221649897,
      "loss": 2.4597,
      "step": 38500
    },
    {
      "epoch": 0.7922258594575691,
      "grad_norm": 0.4011608362197876,
      "learning_rate": 0.0001726152944924497,
      "loss": 2.4557,
      "step": 38510
    },
    {
      "epoch": 0.7924315789744367,
      "grad_norm": 0.37168455123901367,
      "learning_rate": 0.00017260002318668363,
      "loss": 2.4575,
      "step": 38520
    },
    {
      "epoch": 0.7926372984913045,
      "grad_norm": 0.38466930389404297,
      "learning_rate": 0.00017258474829995408,
      "loss": 2.4165,
      "step": 38530
    },
    {
      "epoch": 0.7928430180081723,
      "grad_norm": 0.4402051568031311,
      "learning_rate": 0.00017256946983301442,
      "loss": 2.4264,
      "step": 38540
    },
    {
      "epoch": 0.7930487375250399,
      "grad_norm": 0.4138297140598297,
      "learning_rate": 0.00017255418778661826,
      "loss": 2.451,
      "step": 38550
    },
    {
      "epoch": 0.7932544570419077,
      "grad_norm": 0.40963155031204224,
      "learning_rate": 0.00017253890216151937,
      "loss": 2.3994,
      "step": 38560
    },
    {
      "epoch": 0.7934601765587753,
      "grad_norm": 0.4141581058502197,
      "learning_rate": 0.00017252361295847177,
      "loss": 2.3838,
      "step": 38570
    },
    {
      "epoch": 0.7936658960756431,
      "grad_norm": 0.40816256403923035,
      "learning_rate": 0.0001725083201782295,
      "loss": 2.4388,
      "step": 38580
    },
    {
      "epoch": 0.7938716155925107,
      "grad_norm": 0.4327602684497833,
      "learning_rate": 0.00017249302382154697,
      "loss": 2.4184,
      "step": 38590
    },
    {
      "epoch": 0.7940773351093785,
      "grad_norm": 0.4039839804172516,
      "learning_rate": 0.0001724777238891786,
      "loss": 2.405,
      "step": 38600
    },
    {
      "epoch": 0.7942830546262463,
      "grad_norm": 0.4674563407897949,
      "learning_rate": 0.00017246242038187904,
      "loss": 2.4361,
      "step": 38610
    },
    {
      "epoch": 0.7944887741431139,
      "grad_norm": 0.41016241908073425,
      "learning_rate": 0.00017244711330040318,
      "loss": 2.3417,
      "step": 38620
    },
    {
      "epoch": 0.7946944936599817,
      "grad_norm": 0.3894825875759125,
      "learning_rate": 0.000172431802645506,
      "loss": 2.4173,
      "step": 38630
    },
    {
      "epoch": 0.7949002131768493,
      "grad_norm": 0.38934433460235596,
      "learning_rate": 0.00017241648841794272,
      "loss": 2.3928,
      "step": 38640
    },
    {
      "epoch": 0.7951059326937171,
      "grad_norm": 0.3928334712982178,
      "learning_rate": 0.00017240117061846869,
      "loss": 2.4169,
      "step": 38650
    },
    {
      "epoch": 0.7953116522105848,
      "grad_norm": 0.41868162155151367,
      "learning_rate": 0.0001723858492478394,
      "loss": 2.4309,
      "step": 38660
    },
    {
      "epoch": 0.7955173717274525,
      "grad_norm": 0.3671940267086029,
      "learning_rate": 0.00017237052430681067,
      "loss": 2.4311,
      "step": 38670
    },
    {
      "epoch": 0.7957230912443202,
      "grad_norm": 0.44816842675209045,
      "learning_rate": 0.00017235519579613832,
      "loss": 2.3695,
      "step": 38680
    },
    {
      "epoch": 0.7959288107611879,
      "grad_norm": 0.39834532141685486,
      "learning_rate": 0.00017233986371657846,
      "loss": 2.4122,
      "step": 38690
    },
    {
      "epoch": 0.7961345302780557,
      "grad_norm": 0.42294004559516907,
      "learning_rate": 0.00017232452806888735,
      "loss": 2.4366,
      "step": 38700
    },
    {
      "epoch": 0.7963402497949233,
      "grad_norm": 0.4084824323654175,
      "learning_rate": 0.00017230918885382138,
      "loss": 2.4511,
      "step": 38710
    },
    {
      "epoch": 0.7965459693117911,
      "grad_norm": 0.38436225056648254,
      "learning_rate": 0.00017229384607213714,
      "loss": 2.4104,
      "step": 38720
    },
    {
      "epoch": 0.7967516888286588,
      "grad_norm": 0.4090651571750641,
      "learning_rate": 0.00017227849972459148,
      "loss": 2.4792,
      "step": 38730
    },
    {
      "epoch": 0.7969574083455265,
      "grad_norm": 0.36986640095710754,
      "learning_rate": 0.00017226314981194123,
      "loss": 2.4083,
      "step": 38740
    },
    {
      "epoch": 0.7971631278623942,
      "grad_norm": 0.4287416338920593,
      "learning_rate": 0.00017224779633494364,
      "loss": 2.4711,
      "step": 38750
    },
    {
      "epoch": 0.797368847379262,
      "grad_norm": 0.34074172377586365,
      "learning_rate": 0.00017223243929435594,
      "loss": 2.4024,
      "step": 38760
    },
    {
      "epoch": 0.7975745668961296,
      "grad_norm": 0.5172719359397888,
      "learning_rate": 0.0001722170786909356,
      "loss": 2.4198,
      "step": 38770
    },
    {
      "epoch": 0.7977802864129974,
      "grad_norm": 0.43107226490974426,
      "learning_rate": 0.00017220171452544036,
      "loss": 2.3595,
      "step": 38780
    },
    {
      "epoch": 0.7979860059298651,
      "grad_norm": 0.5078887939453125,
      "learning_rate": 0.00017218634679862796,
      "loss": 2.4095,
      "step": 38790
    },
    {
      "epoch": 0.7981917254467328,
      "grad_norm": 0.42393267154693604,
      "learning_rate": 0.00017217097551125644,
      "loss": 2.4411,
      "step": 38800
    },
    {
      "epoch": 0.7983974449636005,
      "grad_norm": 0.3866899013519287,
      "learning_rate": 0.00017215560066408402,
      "loss": 2.4442,
      "step": 38810
    },
    {
      "epoch": 0.7986031644804682,
      "grad_norm": 0.3832302391529083,
      "learning_rate": 0.00017214022225786897,
      "loss": 2.3747,
      "step": 38820
    },
    {
      "epoch": 0.798808883997336,
      "grad_norm": 0.416774183511734,
      "learning_rate": 0.00017212484029336991,
      "loss": 2.5055,
      "step": 38830
    },
    {
      "epoch": 0.7990146035142036,
      "grad_norm": 0.39509958028793335,
      "learning_rate": 0.0001721094547713455,
      "loss": 2.4636,
      "step": 38840
    },
    {
      "epoch": 0.7992203230310714,
      "grad_norm": 0.387245237827301,
      "learning_rate": 0.0001720940656925546,
      "loss": 2.4038,
      "step": 38850
    },
    {
      "epoch": 0.799426042547939,
      "grad_norm": 0.39765146374702454,
      "learning_rate": 0.00017207867305775638,
      "loss": 2.4569,
      "step": 38860
    },
    {
      "epoch": 0.7996317620648068,
      "grad_norm": 0.4175686836242676,
      "learning_rate": 0.00017206327686770996,
      "loss": 2.3955,
      "step": 38870
    },
    {
      "epoch": 0.7998374815816746,
      "grad_norm": 0.4224880039691925,
      "learning_rate": 0.00017204787712317475,
      "loss": 2.5099,
      "step": 38880
    },
    {
      "epoch": 0.8000432010985422,
      "grad_norm": 0.38077864050865173,
      "learning_rate": 0.00017203247382491043,
      "loss": 2.429,
      "step": 38890
    },
    {
      "epoch": 0.80024892061541,
      "grad_norm": 0.393677294254303,
      "learning_rate": 0.0001720170669736767,
      "loss": 2.4345,
      "step": 38900
    },
    {
      "epoch": 0.8004546401322776,
      "grad_norm": 0.44580769538879395,
      "learning_rate": 0.00017200165657023348,
      "loss": 2.4381,
      "step": 38910
    },
    {
      "epoch": 0.8006603596491454,
      "grad_norm": 0.3813837468624115,
      "learning_rate": 0.0001719862426153409,
      "loss": 2.4157,
      "step": 38920
    },
    {
      "epoch": 0.800866079166013,
      "grad_norm": 0.4172939956188202,
      "learning_rate": 0.0001719708251097593,
      "loss": 2.4086,
      "step": 38930
    },
    {
      "epoch": 0.8010717986828808,
      "grad_norm": 0.397286981344223,
      "learning_rate": 0.00017195540405424905,
      "loss": 2.4025,
      "step": 38940
    },
    {
      "epoch": 0.8012775181997485,
      "grad_norm": 0.45282360911369324,
      "learning_rate": 0.00017193997944957081,
      "loss": 2.462,
      "step": 38950
    },
    {
      "epoch": 0.8014832377166162,
      "grad_norm": 0.3851903975009918,
      "learning_rate": 0.00017192455129648543,
      "loss": 2.4524,
      "step": 38960
    },
    {
      "epoch": 0.801688957233484,
      "grad_norm": 0.5174961090087891,
      "learning_rate": 0.00017190911959575388,
      "loss": 2.4209,
      "step": 38970
    },
    {
      "epoch": 0.8018946767503516,
      "grad_norm": 0.39681875705718994,
      "learning_rate": 0.0001718936843481373,
      "loss": 2.4261,
      "step": 38980
    },
    {
      "epoch": 0.8021003962672194,
      "grad_norm": 0.4073902666568756,
      "learning_rate": 0.00017187824555439708,
      "loss": 2.4751,
      "step": 38990
    },
    {
      "epoch": 0.8023061157840871,
      "grad_norm": 0.37750229239463806,
      "learning_rate": 0.00017186280321529468,
      "loss": 2.4367,
      "step": 39000
    },
    {
      "epoch": 0.8025118353009548,
      "grad_norm": 0.3883015513420105,
      "learning_rate": 0.00017184735733159182,
      "loss": 2.4564,
      "step": 39010
    },
    {
      "epoch": 0.8027175548178225,
      "grad_norm": 0.4134208559989929,
      "learning_rate": 0.0001718319079040503,
      "loss": 2.3996,
      "step": 39020
    },
    {
      "epoch": 0.8029232743346902,
      "grad_norm": 0.3928743004798889,
      "learning_rate": 0.00017181645493343224,
      "loss": 2.3791,
      "step": 39030
    },
    {
      "epoch": 0.8031289938515579,
      "grad_norm": 0.3995404541492462,
      "learning_rate": 0.0001718009984204998,
      "loss": 2.37,
      "step": 39040
    },
    {
      "epoch": 0.8033347133684257,
      "grad_norm": 0.5144380331039429,
      "learning_rate": 0.00017178553836601535,
      "loss": 2.4273,
      "step": 39050
    },
    {
      "epoch": 0.8035404328852934,
      "grad_norm": 0.40097442269325256,
      "learning_rate": 0.0001717700747707415,
      "loss": 2.421,
      "step": 39060
    },
    {
      "epoch": 0.8037461524021611,
      "grad_norm": 0.38029393553733826,
      "learning_rate": 0.00017175460763544097,
      "loss": 2.4137,
      "step": 39070
    },
    {
      "epoch": 0.8039518719190288,
      "grad_norm": 0.3901830315589905,
      "learning_rate": 0.00017173913696087666,
      "loss": 2.4247,
      "step": 39080
    },
    {
      "epoch": 0.8041575914358965,
      "grad_norm": 0.4508652091026306,
      "learning_rate": 0.00017172366274781164,
      "loss": 2.4441,
      "step": 39090
    },
    {
      "epoch": 0.8043633109527643,
      "grad_norm": 0.43478336930274963,
      "learning_rate": 0.0001717081849970092,
      "loss": 2.4046,
      "step": 39100
    },
    {
      "epoch": 0.8045690304696319,
      "grad_norm": 0.3969298303127289,
      "learning_rate": 0.00017169270370923274,
      "loss": 2.4146,
      "step": 39110
    },
    {
      "epoch": 0.8047747499864997,
      "grad_norm": 0.36506420373916626,
      "learning_rate": 0.00017167721888524588,
      "loss": 2.3841,
      "step": 39120
    },
    {
      "epoch": 0.8049804695033673,
      "grad_norm": 0.4088422358036041,
      "learning_rate": 0.00017166173052581243,
      "loss": 2.4306,
      "step": 39130
    },
    {
      "epoch": 0.8051861890202351,
      "grad_norm": 0.5348560810089111,
      "learning_rate": 0.0001716462386316963,
      "loss": 2.475,
      "step": 39140
    },
    {
      "epoch": 0.8053919085371029,
      "grad_norm": 0.39942869544029236,
      "learning_rate": 0.00017163074320366165,
      "loss": 2.3865,
      "step": 39150
    },
    {
      "epoch": 0.8055976280539705,
      "grad_norm": 0.37857523560523987,
      "learning_rate": 0.0001716152442424728,
      "loss": 2.4272,
      "step": 39160
    },
    {
      "epoch": 0.8058033475708383,
      "grad_norm": 0.43469640612602234,
      "learning_rate": 0.0001715997417488942,
      "loss": 2.4311,
      "step": 39170
    },
    {
      "epoch": 0.8060090670877059,
      "grad_norm": 0.3789338171482086,
      "learning_rate": 0.0001715842357236905,
      "loss": 2.424,
      "step": 39180
    },
    {
      "epoch": 0.8062147866045737,
      "grad_norm": 0.42721253633499146,
      "learning_rate": 0.00017156872616762653,
      "loss": 2.4387,
      "step": 39190
    },
    {
      "epoch": 0.8064205061214413,
      "grad_norm": 0.3825128376483917,
      "learning_rate": 0.0001715532130814673,
      "loss": 2.429,
      "step": 39200
    },
    {
      "epoch": 0.8066262256383091,
      "grad_norm": 0.3921798765659332,
      "learning_rate": 0.00017153769646597802,
      "loss": 2.4247,
      "step": 39210
    },
    {
      "epoch": 0.8068319451551768,
      "grad_norm": 0.4043186604976654,
      "learning_rate": 0.00017152217632192398,
      "loss": 2.4088,
      "step": 39220
    },
    {
      "epoch": 0.8070376646720445,
      "grad_norm": 0.4208466112613678,
      "learning_rate": 0.00017150665265007074,
      "loss": 2.4673,
      "step": 39230
    },
    {
      "epoch": 0.8072433841889123,
      "grad_norm": 0.38531139492988586,
      "learning_rate": 0.000171491125451184,
      "loss": 2.3666,
      "step": 39240
    },
    {
      "epoch": 0.8074491037057799,
      "grad_norm": 0.3782695233821869,
      "learning_rate": 0.00017147559472602958,
      "loss": 2.4068,
      "step": 39250
    },
    {
      "epoch": 0.8076548232226477,
      "grad_norm": 0.3983974754810333,
      "learning_rate": 0.00017146006047537358,
      "loss": 2.4342,
      "step": 39260
    },
    {
      "epoch": 0.8078605427395154,
      "grad_norm": 0.39819765090942383,
      "learning_rate": 0.00017144452269998224,
      "loss": 2.4285,
      "step": 39270
    },
    {
      "epoch": 0.8080662622563831,
      "grad_norm": 0.3845010995864868,
      "learning_rate": 0.0001714289814006219,
      "loss": 2.4128,
      "step": 39280
    },
    {
      "epoch": 0.8082719817732508,
      "grad_norm": 0.40789714455604553,
      "learning_rate": 0.00017141343657805917,
      "loss": 2.4847,
      "step": 39290
    },
    {
      "epoch": 0.8084777012901185,
      "grad_norm": 0.35933005809783936,
      "learning_rate": 0.00017139788823306072,
      "loss": 2.4282,
      "step": 39300
    },
    {
      "epoch": 0.8086834208069862,
      "grad_norm": 0.3951205313205719,
      "learning_rate": 0.00017138233636639357,
      "loss": 2.4596,
      "step": 39310
    },
    {
      "epoch": 0.808889140323854,
      "grad_norm": 0.39043793082237244,
      "learning_rate": 0.00017136678097882471,
      "loss": 2.4,
      "step": 39320
    },
    {
      "epoch": 0.8090948598407217,
      "grad_norm": 0.39010390639305115,
      "learning_rate": 0.00017135122207112148,
      "loss": 2.4475,
      "step": 39330
    },
    {
      "epoch": 0.8093005793575894,
      "grad_norm": 0.42979294061660767,
      "learning_rate": 0.00017133565964405127,
      "loss": 2.375,
      "step": 39340
    },
    {
      "epoch": 0.8095062988744571,
      "grad_norm": 0.37436968088150024,
      "learning_rate": 0.00017132009369838174,
      "loss": 2.3971,
      "step": 39350
    },
    {
      "epoch": 0.8097120183913248,
      "grad_norm": 0.40739038586616516,
      "learning_rate": 0.00017130452423488057,
      "loss": 2.3883,
      "step": 39360
    },
    {
      "epoch": 0.8099177379081925,
      "grad_norm": 0.4103219211101532,
      "learning_rate": 0.00017128895125431584,
      "loss": 2.4373,
      "step": 39370
    },
    {
      "epoch": 0.8101234574250602,
      "grad_norm": 0.44027870893478394,
      "learning_rate": 0.00017127337475745556,
      "loss": 2.42,
      "step": 39380
    },
    {
      "epoch": 0.810329176941928,
      "grad_norm": 0.39584386348724365,
      "learning_rate": 0.00017125779474506817,
      "loss": 2.4616,
      "step": 39390
    },
    {
      "epoch": 0.8105348964587956,
      "grad_norm": 0.4678243100643158,
      "learning_rate": 0.000171242211217922,
      "loss": 2.4399,
      "step": 39400
    },
    {
      "epoch": 0.8107406159756634,
      "grad_norm": 0.48004966974258423,
      "learning_rate": 0.00017122662417678582,
      "loss": 2.4754,
      "step": 39410
    },
    {
      "epoch": 0.8109463354925311,
      "grad_norm": 0.3809528648853302,
      "learning_rate": 0.0001712110336224284,
      "loss": 2.3995,
      "step": 39420
    },
    {
      "epoch": 0.8111520550093988,
      "grad_norm": 0.38061514496803284,
      "learning_rate": 0.00017119543955561875,
      "loss": 2.3709,
      "step": 39430
    },
    {
      "epoch": 0.8113577745262666,
      "grad_norm": 0.39243653416633606,
      "learning_rate": 0.00017117984197712602,
      "loss": 2.42,
      "step": 39440
    },
    {
      "epoch": 0.8115634940431342,
      "grad_norm": 0.4205206334590912,
      "learning_rate": 0.00017116424088771956,
      "loss": 2.4611,
      "step": 39450
    },
    {
      "epoch": 0.811769213560002,
      "grad_norm": 0.4545735716819763,
      "learning_rate": 0.00017114863628816892,
      "loss": 2.4636,
      "step": 39460
    },
    {
      "epoch": 0.8119749330768696,
      "grad_norm": 0.4411112368106842,
      "learning_rate": 0.00017113302817924376,
      "loss": 2.4239,
      "step": 39470
    },
    {
      "epoch": 0.8121806525937374,
      "grad_norm": 0.40751662850379944,
      "learning_rate": 0.00017111741656171395,
      "loss": 2.4287,
      "step": 39480
    },
    {
      "epoch": 0.812386372110605,
      "grad_norm": 0.39147526025772095,
      "learning_rate": 0.0001711018014363495,
      "loss": 2.4377,
      "step": 39490
    },
    {
      "epoch": 0.8125920916274728,
      "grad_norm": 0.44790586829185486,
      "learning_rate": 0.0001710861828039207,
      "loss": 2.4619,
      "step": 39500
    },
    {
      "epoch": 0.8127978111443406,
      "grad_norm": 0.4217648506164551,
      "learning_rate": 0.0001710705606651978,
      "loss": 2.4195,
      "step": 39510
    },
    {
      "epoch": 0.8130035306612082,
      "grad_norm": 0.38963934779167175,
      "learning_rate": 0.0001710549350209515,
      "loss": 2.3453,
      "step": 39520
    },
    {
      "epoch": 0.813209250178076,
      "grad_norm": 0.40607911348342896,
      "learning_rate": 0.00017103930587195241,
      "loss": 2.4535,
      "step": 39530
    },
    {
      "epoch": 0.8134149696949436,
      "grad_norm": 0.39266350865364075,
      "learning_rate": 0.0001710236732189715,
      "loss": 2.4181,
      "step": 39540
    },
    {
      "epoch": 0.8136206892118114,
      "grad_norm": 0.428368479013443,
      "learning_rate": 0.00017100803706277985,
      "loss": 2.3965,
      "step": 39550
    },
    {
      "epoch": 0.8138264087286791,
      "grad_norm": 0.3857499659061432,
      "learning_rate": 0.00017099239740414864,
      "loss": 2.4274,
      "step": 39560
    },
    {
      "epoch": 0.8140321282455468,
      "grad_norm": 0.44171860814094543,
      "learning_rate": 0.0001709767542438493,
      "loss": 2.4203,
      "step": 39570
    },
    {
      "epoch": 0.8142378477624145,
      "grad_norm": 0.42607733607292175,
      "learning_rate": 0.00017096110758265352,
      "loss": 2.4816,
      "step": 39580
    },
    {
      "epoch": 0.8144435672792822,
      "grad_norm": 0.40604880452156067,
      "learning_rate": 0.00017094545742133295,
      "loss": 2.4422,
      "step": 39590
    },
    {
      "epoch": 0.81464928679615,
      "grad_norm": 0.4064731001853943,
      "learning_rate": 0.0001709298037606596,
      "loss": 2.4265,
      "step": 39600
    },
    {
      "epoch": 0.8148550063130177,
      "grad_norm": 0.4007600247859955,
      "learning_rate": 0.00017091414660140553,
      "loss": 2.4469,
      "step": 39610
    },
    {
      "epoch": 0.8150607258298854,
      "grad_norm": 0.3825855851173401,
      "learning_rate": 0.00017089848594434304,
      "loss": 2.4178,
      "step": 39620
    },
    {
      "epoch": 0.8152664453467531,
      "grad_norm": 0.4090779721736908,
      "learning_rate": 0.0001708828217902446,
      "loss": 2.413,
      "step": 39630
    },
    {
      "epoch": 0.8154721648636208,
      "grad_norm": 0.38560158014297485,
      "learning_rate": 0.00017086715413988283,
      "loss": 2.4726,
      "step": 39640
    },
    {
      "epoch": 0.8156778843804885,
      "grad_norm": 0.43142110109329224,
      "learning_rate": 0.0001708514829940305,
      "loss": 2.4315,
      "step": 39650
    },
    {
      "epoch": 0.8158836038973563,
      "grad_norm": 0.39612695574760437,
      "learning_rate": 0.00017083580835346065,
      "loss": 2.4062,
      "step": 39660
    },
    {
      "epoch": 0.8160893234142239,
      "grad_norm": 0.38868069648742676,
      "learning_rate": 0.00017082013021894635,
      "loss": 2.3957,
      "step": 39670
    },
    {
      "epoch": 0.8162950429310917,
      "grad_norm": 0.3816138505935669,
      "learning_rate": 0.00017080444859126097,
      "loss": 2.4527,
      "step": 39680
    },
    {
      "epoch": 0.8165007624479594,
      "grad_norm": 0.4469115436077118,
      "learning_rate": 0.00017078876347117797,
      "loss": 2.4028,
      "step": 39690
    },
    {
      "epoch": 0.8167064819648271,
      "grad_norm": 0.40331459045410156,
      "learning_rate": 0.000170773074859471,
      "loss": 2.4159,
      "step": 39700
    },
    {
      "epoch": 0.8169122014816949,
      "grad_norm": 0.40099620819091797,
      "learning_rate": 0.00017075738275691395,
      "loss": 2.4363,
      "step": 39710
    },
    {
      "epoch": 0.8171179209985625,
      "grad_norm": 0.4603644907474518,
      "learning_rate": 0.0001707416871642808,
      "loss": 2.4602,
      "step": 39720
    },
    {
      "epoch": 0.8173236405154303,
      "grad_norm": 0.4059174358844757,
      "learning_rate": 0.00017072598808234567,
      "loss": 2.4378,
      "step": 39730
    },
    {
      "epoch": 0.8175293600322979,
      "grad_norm": 0.3904784917831421,
      "learning_rate": 0.00017071028551188299,
      "loss": 2.3792,
      "step": 39740
    },
    {
      "epoch": 0.8177350795491657,
      "grad_norm": 0.4037612974643707,
      "learning_rate": 0.00017069457945366722,
      "loss": 2.4081,
      "step": 39750
    },
    {
      "epoch": 0.8179407990660333,
      "grad_norm": 0.37563931941986084,
      "learning_rate": 0.00017067886990847312,
      "loss": 2.3871,
      "step": 39760
    },
    {
      "epoch": 0.8181465185829011,
      "grad_norm": 0.3838493227958679,
      "learning_rate": 0.00017066315687707553,
      "loss": 2.3413,
      "step": 39770
    },
    {
      "epoch": 0.8183522380997689,
      "grad_norm": 0.40773382782936096,
      "learning_rate": 0.0001706474403602494,
      "loss": 2.3979,
      "step": 39780
    },
    {
      "epoch": 0.8185579576166365,
      "grad_norm": 0.4301631450653076,
      "learning_rate": 0.0001706317203587701,
      "loss": 2.4572,
      "step": 39790
    },
    {
      "epoch": 0.8187636771335043,
      "grad_norm": 0.40666189789772034,
      "learning_rate": 0.0001706159968734129,
      "loss": 2.4303,
      "step": 39800
    },
    {
      "epoch": 0.8189693966503719,
      "grad_norm": 0.3900110423564911,
      "learning_rate": 0.00017060026990495342,
      "loss": 2.4544,
      "step": 39810
    },
    {
      "epoch": 0.8191751161672397,
      "grad_norm": 0.3683977723121643,
      "learning_rate": 0.0001705845394541673,
      "loss": 2.4469,
      "step": 39820
    },
    {
      "epoch": 0.8193808356841074,
      "grad_norm": 0.3601406216621399,
      "learning_rate": 0.0001705688055218305,
      "loss": 2.4038,
      "step": 39830
    },
    {
      "epoch": 0.8195865552009751,
      "grad_norm": 0.411740243434906,
      "learning_rate": 0.00017055306810871912,
      "loss": 2.4666,
      "step": 39840
    },
    {
      "epoch": 0.8197922747178428,
      "grad_norm": 0.432537317276001,
      "learning_rate": 0.00017053732721560933,
      "loss": 2.4591,
      "step": 39850
    },
    {
      "epoch": 0.8199979942347105,
      "grad_norm": 0.3982638120651245,
      "learning_rate": 0.00017052158284327758,
      "loss": 2.4775,
      "step": 39860
    },
    {
      "epoch": 0.8202037137515783,
      "grad_norm": 0.3867229223251343,
      "learning_rate": 0.0001705058349925004,
      "loss": 2.5058,
      "step": 39870
    },
    {
      "epoch": 0.820409433268446,
      "grad_norm": 0.3963509798049927,
      "learning_rate": 0.00017049008366405466,
      "loss": 2.446,
      "step": 39880
    },
    {
      "epoch": 0.8206151527853137,
      "grad_norm": 0.44299450516700745,
      "learning_rate": 0.00017047432885871716,
      "loss": 2.4321,
      "step": 39890
    },
    {
      "epoch": 0.8208208723021814,
      "grad_norm": 0.4156801402568817,
      "learning_rate": 0.00017045857057726508,
      "loss": 2.3938,
      "step": 39900
    },
    {
      "epoch": 0.8210265918190491,
      "grad_norm": 0.4093468487262726,
      "learning_rate": 0.00017044280882047566,
      "loss": 2.4409,
      "step": 39910
    },
    {
      "epoch": 0.8212323113359168,
      "grad_norm": 0.3896854519844055,
      "learning_rate": 0.00017042704358912635,
      "loss": 2.4506,
      "step": 39920
    },
    {
      "epoch": 0.8214380308527846,
      "grad_norm": 0.3966057598590851,
      "learning_rate": 0.00017041127488399478,
      "loss": 2.416,
      "step": 39930
    },
    {
      "epoch": 0.8216437503696522,
      "grad_norm": 0.6404225826263428,
      "learning_rate": 0.00017039550270585868,
      "loss": 2.4517,
      "step": 39940
    },
    {
      "epoch": 0.82184946988652,
      "grad_norm": 0.3910057246685028,
      "learning_rate": 0.00017037972705549607,
      "loss": 2.4256,
      "step": 39950
    },
    {
      "epoch": 0.8220551894033877,
      "grad_norm": 0.4388618767261505,
      "learning_rate": 0.00017036394793368503,
      "loss": 2.4765,
      "step": 39960
    },
    {
      "epoch": 0.8222609089202554,
      "grad_norm": 0.37746915221214294,
      "learning_rate": 0.0001703481653412039,
      "loss": 2.3986,
      "step": 39970
    },
    {
      "epoch": 0.8224666284371231,
      "grad_norm": 0.409506618976593,
      "learning_rate": 0.0001703323792788311,
      "loss": 2.5099,
      "step": 39980
    },
    {
      "epoch": 0.8226723479539908,
      "grad_norm": 0.4329909384250641,
      "learning_rate": 0.00017031658974734533,
      "loss": 2.4287,
      "step": 39990
    },
    {
      "epoch": 0.8228780674708586,
      "grad_norm": 0.4018583595752716,
      "learning_rate": 0.00017030079674752532,
      "loss": 2.457,
      "step": 40000
    },
    {
      "epoch": 0.8230837869877262,
      "grad_norm": 0.385593980550766,
      "learning_rate": 0.00017028500028015014,
      "loss": 2.4324,
      "step": 40010
    },
    {
      "epoch": 0.823289506504594,
      "grad_norm": 0.40379098057746887,
      "learning_rate": 0.00017026920034599887,
      "loss": 2.419,
      "step": 40020
    },
    {
      "epoch": 0.8234952260214616,
      "grad_norm": 0.39120039343833923,
      "learning_rate": 0.0001702533969458509,
      "loss": 2.4743,
      "step": 40030
    },
    {
      "epoch": 0.8237009455383294,
      "grad_norm": 0.4600154161453247,
      "learning_rate": 0.00017023759008048564,
      "loss": 2.4361,
      "step": 40040
    },
    {
      "epoch": 0.8239066650551972,
      "grad_norm": 0.4792023301124573,
      "learning_rate": 0.00017022177975068288,
      "loss": 2.4598,
      "step": 40050
    },
    {
      "epoch": 0.8241123845720648,
      "grad_norm": 0.4092542231082916,
      "learning_rate": 0.00017020596595722235,
      "loss": 2.4149,
      "step": 40060
    },
    {
      "epoch": 0.8243181040889326,
      "grad_norm": 0.43348756432533264,
      "learning_rate": 0.0001701901487008841,
      "loss": 2.3708,
      "step": 40070
    },
    {
      "epoch": 0.8245238236058002,
      "grad_norm": 0.3947940766811371,
      "learning_rate": 0.0001701743279824483,
      "loss": 2.4437,
      "step": 40080
    },
    {
      "epoch": 0.824729543122668,
      "grad_norm": 0.4367409348487854,
      "learning_rate": 0.00017015850380269534,
      "loss": 2.3946,
      "step": 40090
    },
    {
      "epoch": 0.8249352626395356,
      "grad_norm": 0.40085574984550476,
      "learning_rate": 0.00017014267616240563,
      "loss": 2.4605,
      "step": 40100
    },
    {
      "epoch": 0.8251409821564034,
      "grad_norm": 0.4114835858345032,
      "learning_rate": 0.00017012684506236,
      "loss": 2.3874,
      "step": 40110
    },
    {
      "epoch": 0.8253467016732711,
      "grad_norm": 0.4064192473888397,
      "learning_rate": 0.00017011101050333924,
      "loss": 2.4008,
      "step": 40120
    },
    {
      "epoch": 0.8255524211901388,
      "grad_norm": 0.3875398635864258,
      "learning_rate": 0.00017009517248612439,
      "loss": 2.4386,
      "step": 40130
    },
    {
      "epoch": 0.8257581407070066,
      "grad_norm": 0.38715627789497375,
      "learning_rate": 0.00017007933101149662,
      "loss": 2.4177,
      "step": 40140
    },
    {
      "epoch": 0.8259638602238742,
      "grad_norm": 0.3741092085838318,
      "learning_rate": 0.0001700634860802374,
      "loss": 2.4223,
      "step": 40150
    },
    {
      "epoch": 0.826169579740742,
      "grad_norm": 0.42223766446113586,
      "learning_rate": 0.00017004763769312814,
      "loss": 2.4121,
      "step": 40160
    },
    {
      "epoch": 0.8263752992576097,
      "grad_norm": 0.401297003030777,
      "learning_rate": 0.0001700317858509507,
      "loss": 2.344,
      "step": 40170
    },
    {
      "epoch": 0.8265810187744774,
      "grad_norm": 0.39832645654678345,
      "learning_rate": 0.00017001593055448682,
      "loss": 2.4626,
      "step": 40180
    },
    {
      "epoch": 0.8267867382913451,
      "grad_norm": 0.4397091567516327,
      "learning_rate": 0.00017000007180451867,
      "loss": 2.4512,
      "step": 40190
    },
    {
      "epoch": 0.8269924578082128,
      "grad_norm": 0.40657109022140503,
      "learning_rate": 0.00016998420960182844,
      "loss": 2.3824,
      "step": 40200
    },
    {
      "epoch": 0.8271981773250805,
      "grad_norm": 0.39073091745376587,
      "learning_rate": 0.0001699683439471985,
      "loss": 2.4383,
      "step": 40210
    },
    {
      "epoch": 0.8274038968419483,
      "grad_norm": 0.37953606247901917,
      "learning_rate": 0.00016995247484141143,
      "loss": 2.4791,
      "step": 40220
    },
    {
      "epoch": 0.827609616358816,
      "grad_norm": 0.4031623601913452,
      "learning_rate": 0.00016993660228525,
      "loss": 2.431,
      "step": 40230
    },
    {
      "epoch": 0.8278153358756837,
      "grad_norm": 0.38898101449012756,
      "learning_rate": 0.00016992072627949707,
      "loss": 2.4225,
      "step": 40240
    },
    {
      "epoch": 0.8280210553925514,
      "grad_norm": 0.4370169937610626,
      "learning_rate": 0.0001699048468249357,
      "loss": 2.4323,
      "step": 40250
    },
    {
      "epoch": 0.8282267749094191,
      "grad_norm": 0.397172749042511,
      "learning_rate": 0.00016988896392234923,
      "loss": 2.4476,
      "step": 40260
    },
    {
      "epoch": 0.8284324944262869,
      "grad_norm": 0.3765253722667694,
      "learning_rate": 0.000169873077572521,
      "loss": 2.404,
      "step": 40270
    },
    {
      "epoch": 0.8286382139431545,
      "grad_norm": 0.43006497621536255,
      "learning_rate": 0.0001698571877762346,
      "loss": 2.4424,
      "step": 40280
    },
    {
      "epoch": 0.8288439334600223,
      "grad_norm": 0.46475157141685486,
      "learning_rate": 0.00016984129453427382,
      "loss": 2.4602,
      "step": 40290
    },
    {
      "epoch": 0.82904965297689,
      "grad_norm": 0.4226767420768738,
      "learning_rate": 0.00016982539784742256,
      "loss": 2.4638,
      "step": 40300
    },
    {
      "epoch": 0.8292553724937577,
      "grad_norm": 0.39193668961524963,
      "learning_rate": 0.0001698094977164649,
      "loss": 2.4045,
      "step": 40310
    },
    {
      "epoch": 0.8294610920106255,
      "grad_norm": 0.38414546847343445,
      "learning_rate": 0.0001697935941421852,
      "loss": 2.4597,
      "step": 40320
    },
    {
      "epoch": 0.8296668115274931,
      "grad_norm": 0.4115855097770691,
      "learning_rate": 0.00016977768712536778,
      "loss": 2.3664,
      "step": 40330
    },
    {
      "epoch": 0.8298725310443609,
      "grad_norm": 0.39225584268569946,
      "learning_rate": 0.0001697617766667973,
      "loss": 2.4449,
      "step": 40340
    },
    {
      "epoch": 0.8300782505612285,
      "grad_norm": 0.4856293499469757,
      "learning_rate": 0.00016974586276725855,
      "loss": 2.4036,
      "step": 40350
    },
    {
      "epoch": 0.8302839700780963,
      "grad_norm": 0.41146060824394226,
      "learning_rate": 0.00016972994542753647,
      "loss": 2.4363,
      "step": 40360
    },
    {
      "epoch": 0.8304896895949639,
      "grad_norm": 0.39653727412223816,
      "learning_rate": 0.00016971402464841613,
      "loss": 2.4211,
      "step": 40370
    },
    {
      "epoch": 0.8306954091118317,
      "grad_norm": 0.5343612432479858,
      "learning_rate": 0.0001696981004306829,
      "loss": 2.4103,
      "step": 40380
    },
    {
      "epoch": 0.8309011286286995,
      "grad_norm": 0.39142271876335144,
      "learning_rate": 0.0001696821727751221,
      "loss": 2.4925,
      "step": 40390
    },
    {
      "epoch": 0.8311068481455671,
      "grad_norm": 0.4479515850543976,
      "learning_rate": 0.0001696662416825195,
      "loss": 2.4558,
      "step": 40400
    },
    {
      "epoch": 0.8313125676624349,
      "grad_norm": 0.37207600474357605,
      "learning_rate": 0.00016965030715366083,
      "loss": 2.4121,
      "step": 40410
    },
    {
      "epoch": 0.8315182871793025,
      "grad_norm": 0.4183083772659302,
      "learning_rate": 0.00016963436918933207,
      "loss": 2.3823,
      "step": 40420
    },
    {
      "epoch": 0.8317240066961703,
      "grad_norm": 0.4250900149345398,
      "learning_rate": 0.0001696184277903193,
      "loss": 2.4448,
      "step": 40430
    },
    {
      "epoch": 0.831929726213038,
      "grad_norm": 0.4152457118034363,
      "learning_rate": 0.00016960248295740888,
      "loss": 2.4846,
      "step": 40440
    },
    {
      "epoch": 0.8321354457299057,
      "grad_norm": 0.41386404633522034,
      "learning_rate": 0.00016958653469138728,
      "loss": 2.4806,
      "step": 40450
    },
    {
      "epoch": 0.8323411652467734,
      "grad_norm": 0.3648427724838257,
      "learning_rate": 0.0001695705829930411,
      "loss": 2.3909,
      "step": 40460
    },
    {
      "epoch": 0.8325468847636411,
      "grad_norm": 0.36755919456481934,
      "learning_rate": 0.0001695546278631572,
      "loss": 2.3589,
      "step": 40470
    },
    {
      "epoch": 0.8327526042805089,
      "grad_norm": 0.49589207768440247,
      "learning_rate": 0.00016953866930252248,
      "loss": 2.4397,
      "step": 40480
    },
    {
      "epoch": 0.8329583237973766,
      "grad_norm": 0.4226298928260803,
      "learning_rate": 0.00016952270731192422,
      "loss": 2.465,
      "step": 40490
    },
    {
      "epoch": 0.8331640433142443,
      "grad_norm": 0.38689470291137695,
      "learning_rate": 0.00016950674189214962,
      "loss": 2.3389,
      "step": 40500
    },
    {
      "epoch": 0.833369762831112,
      "grad_norm": 0.44147640466690063,
      "learning_rate": 0.00016949077304398622,
      "loss": 2.4103,
      "step": 40510
    },
    {
      "epoch": 0.8335754823479797,
      "grad_norm": 0.4788903295993805,
      "learning_rate": 0.00016947480076822164,
      "loss": 2.4226,
      "step": 40520
    },
    {
      "epoch": 0.8337812018648474,
      "grad_norm": 0.40556398034095764,
      "learning_rate": 0.00016945882506564378,
      "loss": 2.3922,
      "step": 40530
    },
    {
      "epoch": 0.8339869213817152,
      "grad_norm": 0.3780299425125122,
      "learning_rate": 0.00016944284593704056,
      "loss": 2.4227,
      "step": 40540
    },
    {
      "epoch": 0.8341926408985828,
      "grad_norm": 0.36428380012512207,
      "learning_rate": 0.00016942686338320014,
      "loss": 2.3797,
      "step": 40550
    },
    {
      "epoch": 0.8343983604154506,
      "grad_norm": 0.41068434715270996,
      "learning_rate": 0.0001694108774049109,
      "loss": 2.4469,
      "step": 40560
    },
    {
      "epoch": 0.8346040799323183,
      "grad_norm": 0.4270353615283966,
      "learning_rate": 0.00016939488800296135,
      "loss": 2.4465,
      "step": 40570
    },
    {
      "epoch": 0.834809799449186,
      "grad_norm": 0.38961827754974365,
      "learning_rate": 0.0001693788951781401,
      "loss": 2.3948,
      "step": 40580
    },
    {
      "epoch": 0.8350155189660537,
      "grad_norm": 0.41599544882774353,
      "learning_rate": 0.000169362898931236,
      "loss": 2.4244,
      "step": 40590
    },
    {
      "epoch": 0.8352212384829214,
      "grad_norm": 0.4091412425041199,
      "learning_rate": 0.00016934689926303813,
      "loss": 2.4434,
      "step": 40600
    },
    {
      "epoch": 0.8354269579997892,
      "grad_norm": 0.3787643611431122,
      "learning_rate": 0.00016933089617433557,
      "loss": 2.3966,
      "step": 40610
    },
    {
      "epoch": 0.8356326775166568,
      "grad_norm": 0.4034017324447632,
      "learning_rate": 0.00016931488966591776,
      "loss": 2.4727,
      "step": 40620
    },
    {
      "epoch": 0.8358383970335246,
      "grad_norm": 0.3641555607318878,
      "learning_rate": 0.00016929887973857413,
      "loss": 2.4579,
      "step": 40630
    },
    {
      "epoch": 0.8360441165503922,
      "grad_norm": 0.4113048315048218,
      "learning_rate": 0.00016928286639309438,
      "loss": 2.4172,
      "step": 40640
    },
    {
      "epoch": 0.83624983606726,
      "grad_norm": 0.391681969165802,
      "learning_rate": 0.00016926684963026843,
      "loss": 2.4246,
      "step": 40650
    },
    {
      "epoch": 0.8364555555841278,
      "grad_norm": 0.4293072819709778,
      "learning_rate": 0.0001692508294508862,
      "loss": 2.4295,
      "step": 40660
    },
    {
      "epoch": 0.8366612751009954,
      "grad_norm": 0.3920993208885193,
      "learning_rate": 0.00016923480585573792,
      "loss": 2.5049,
      "step": 40670
    },
    {
      "epoch": 0.8368669946178632,
      "grad_norm": 0.394353985786438,
      "learning_rate": 0.00016921877884561396,
      "loss": 2.4565,
      "step": 40680
    },
    {
      "epoch": 0.8370727141347308,
      "grad_norm": 0.41846784949302673,
      "learning_rate": 0.00016920274842130485,
      "loss": 2.4582,
      "step": 40690
    },
    {
      "epoch": 0.8372784336515986,
      "grad_norm": 0.3727341890335083,
      "learning_rate": 0.0001691867145836013,
      "loss": 2.5162,
      "step": 40700
    },
    {
      "epoch": 0.8374841531684662,
      "grad_norm": 0.3888701796531677,
      "learning_rate": 0.0001691706773332941,
      "loss": 2.4509,
      "step": 40710
    },
    {
      "epoch": 0.837689872685334,
      "grad_norm": 0.39237016439437866,
      "learning_rate": 0.0001691546366711743,
      "loss": 2.4381,
      "step": 40720
    },
    {
      "epoch": 0.8378955922022017,
      "grad_norm": 0.4221893548965454,
      "learning_rate": 0.00016913859259803316,
      "loss": 2.4285,
      "step": 40730
    },
    {
      "epoch": 0.8381013117190694,
      "grad_norm": 0.40465888381004333,
      "learning_rate": 0.00016912254511466203,
      "loss": 2.4398,
      "step": 40740
    },
    {
      "epoch": 0.8383070312359372,
      "grad_norm": 0.4100026488304138,
      "learning_rate": 0.00016910649422185236,
      "loss": 2.4332,
      "step": 40750
    },
    {
      "epoch": 0.8385127507528048,
      "grad_norm": 0.3631651997566223,
      "learning_rate": 0.00016909043992039595,
      "loss": 2.3794,
      "step": 40760
    },
    {
      "epoch": 0.8387184702696726,
      "grad_norm": 0.3643149137496948,
      "learning_rate": 0.00016907438221108466,
      "loss": 2.4611,
      "step": 40770
    },
    {
      "epoch": 0.8389241897865403,
      "grad_norm": 0.41539883613586426,
      "learning_rate": 0.00016905832109471048,
      "loss": 2.4261,
      "step": 40780
    },
    {
      "epoch": 0.839129909303408,
      "grad_norm": 0.40503981709480286,
      "learning_rate": 0.0001690422565720656,
      "loss": 2.4362,
      "step": 40790
    },
    {
      "epoch": 0.8393356288202757,
      "grad_norm": 0.4164401888847351,
      "learning_rate": 0.0001690261886439425,
      "loss": 2.4298,
      "step": 40800
    },
    {
      "epoch": 0.8395413483371434,
      "grad_norm": 0.4030853509902954,
      "learning_rate": 0.00016901011731113364,
      "loss": 2.4328,
      "step": 40810
    },
    {
      "epoch": 0.8397470678540111,
      "grad_norm": 0.40313440561294556,
      "learning_rate": 0.00016899404257443177,
      "loss": 2.4854,
      "step": 40820
    },
    {
      "epoch": 0.8399527873708789,
      "grad_norm": 0.3762631118297577,
      "learning_rate": 0.0001689779644346297,
      "loss": 2.453,
      "step": 40830
    },
    {
      "epoch": 0.8401585068877466,
      "grad_norm": 0.4411385953426361,
      "learning_rate": 0.0001689618828925206,
      "loss": 2.4482,
      "step": 40840
    },
    {
      "epoch": 0.8403642264046143,
      "grad_norm": 0.3695039451122284,
      "learning_rate": 0.00016894579794889757,
      "loss": 2.4564,
      "step": 40850
    },
    {
      "epoch": 0.840569945921482,
      "grad_norm": 0.4023747146129608,
      "learning_rate": 0.00016892970960455405,
      "loss": 2.4845,
      "step": 40860
    },
    {
      "epoch": 0.8407756654383497,
      "grad_norm": 0.4180873930454254,
      "learning_rate": 0.00016891361786028358,
      "loss": 2.4851,
      "step": 40870
    },
    {
      "epoch": 0.8409813849552175,
      "grad_norm": 0.3559517562389374,
      "learning_rate": 0.00016889752271687986,
      "loss": 2.4303,
      "step": 40880
    },
    {
      "epoch": 0.8411871044720851,
      "grad_norm": 0.4073050320148468,
      "learning_rate": 0.0001688814241751368,
      "loss": 2.4353,
      "step": 40890
    },
    {
      "epoch": 0.8413928239889529,
      "grad_norm": 0.4303934574127197,
      "learning_rate": 0.00016886532223584847,
      "loss": 2.449,
      "step": 40900
    },
    {
      "epoch": 0.8415985435058205,
      "grad_norm": 0.4677688181400299,
      "learning_rate": 0.000168849216899809,
      "loss": 2.4752,
      "step": 40910
    },
    {
      "epoch": 0.8418042630226883,
      "grad_norm": 0.34093332290649414,
      "learning_rate": 0.0001688331081678129,
      "loss": 2.4048,
      "step": 40920
    },
    {
      "epoch": 0.8420099825395561,
      "grad_norm": 0.35300567746162415,
      "learning_rate": 0.00016881699604065465,
      "loss": 2.4441,
      "step": 40930
    },
    {
      "epoch": 0.8422157020564237,
      "grad_norm": 0.3933780789375305,
      "learning_rate": 0.000168800880519129,
      "loss": 2.429,
      "step": 40940
    },
    {
      "epoch": 0.8424214215732915,
      "grad_norm": 0.4123627841472626,
      "learning_rate": 0.00016878476160403082,
      "loss": 2.3478,
      "step": 40950
    },
    {
      "epoch": 0.8426271410901591,
      "grad_norm": 0.38788720965385437,
      "learning_rate": 0.0001687686392961552,
      "loss": 2.4947,
      "step": 40960
    },
    {
      "epoch": 0.8428328606070269,
      "grad_norm": 0.4188247621059418,
      "learning_rate": 0.00016875251359629734,
      "loss": 2.412,
      "step": 40970
    },
    {
      "epoch": 0.8430385801238945,
      "grad_norm": 0.4631035327911377,
      "learning_rate": 0.00016873638450525265,
      "loss": 2.4475,
      "step": 40980
    },
    {
      "epoch": 0.8432442996407623,
      "grad_norm": 0.4099596440792084,
      "learning_rate": 0.00016872025202381666,
      "loss": 2.4269,
      "step": 40990
    },
    {
      "epoch": 0.84345001915763,
      "grad_norm": 0.46350181102752686,
      "learning_rate": 0.00016870411615278513,
      "loss": 2.4328,
      "step": 41000
    },
    {
      "epoch": 0.8436557386744977,
      "grad_norm": 0.4651540517807007,
      "learning_rate": 0.00016868797689295395,
      "loss": 2.4641,
      "step": 41010
    },
    {
      "epoch": 0.8438614581913655,
      "grad_norm": 0.45169463753700256,
      "learning_rate": 0.00016867183424511918,
      "loss": 2.434,
      "step": 41020
    },
    {
      "epoch": 0.8440671777082331,
      "grad_norm": 0.3902173638343811,
      "learning_rate": 0.000168655688210077,
      "loss": 2.4165,
      "step": 41030
    },
    {
      "epoch": 0.8442728972251009,
      "grad_norm": 0.462861567735672,
      "learning_rate": 0.00016863953878862387,
      "loss": 2.4171,
      "step": 41040
    },
    {
      "epoch": 0.8444786167419686,
      "grad_norm": 0.4831250309944153,
      "learning_rate": 0.00016862338598155636,
      "loss": 2.4177,
      "step": 41050
    },
    {
      "epoch": 0.8446843362588363,
      "grad_norm": 0.4250912666320801,
      "learning_rate": 0.00016860722978967116,
      "loss": 2.379,
      "step": 41060
    },
    {
      "epoch": 0.844890055775704,
      "grad_norm": 0.4074157178401947,
      "learning_rate": 0.00016859107021376514,
      "loss": 2.4785,
      "step": 41070
    },
    {
      "epoch": 0.8450957752925717,
      "grad_norm": 0.4094683527946472,
      "learning_rate": 0.00016857490725463546,
      "loss": 2.4436,
      "step": 41080
    },
    {
      "epoch": 0.8453014948094394,
      "grad_norm": 0.39476829767227173,
      "learning_rate": 0.00016855874091307928,
      "loss": 2.4138,
      "step": 41090
    },
    {
      "epoch": 0.8455072143263072,
      "grad_norm": 0.41323623061180115,
      "learning_rate": 0.00016854257118989398,
      "loss": 2.3964,
      "step": 41100
    },
    {
      "epoch": 0.8457129338431749,
      "grad_norm": 0.46577492356300354,
      "learning_rate": 0.00016852639808587717,
      "loss": 2.4047,
      "step": 41110
    },
    {
      "epoch": 0.8459186533600426,
      "grad_norm": 0.41201722621917725,
      "learning_rate": 0.00016851022160182657,
      "loss": 2.4467,
      "step": 41120
    },
    {
      "epoch": 0.8461243728769103,
      "grad_norm": 0.42754876613616943,
      "learning_rate": 0.00016849404173854008,
      "loss": 2.4194,
      "step": 41130
    },
    {
      "epoch": 0.846330092393778,
      "grad_norm": 0.4371104836463928,
      "learning_rate": 0.00016847785849681576,
      "loss": 2.4781,
      "step": 41140
    },
    {
      "epoch": 0.8465358119106458,
      "grad_norm": 0.4084160327911377,
      "learning_rate": 0.00016846167187745183,
      "loss": 2.4449,
      "step": 41150
    },
    {
      "epoch": 0.8467415314275134,
      "grad_norm": 0.43535611033439636,
      "learning_rate": 0.00016844548188124672,
      "loss": 2.3914,
      "step": 41160
    },
    {
      "epoch": 0.8469472509443812,
      "grad_norm": 0.40912115573883057,
      "learning_rate": 0.00016842928850899894,
      "loss": 2.3966,
      "step": 41170
    },
    {
      "epoch": 0.8471529704612488,
      "grad_norm": 0.3841608464717865,
      "learning_rate": 0.0001684130917615073,
      "loss": 2.4675,
      "step": 41180
    },
    {
      "epoch": 0.8473586899781166,
      "grad_norm": 0.36554402112960815,
      "learning_rate": 0.00016839689163957057,
      "loss": 2.3719,
      "step": 41190
    },
    {
      "epoch": 0.8475644094949843,
      "grad_norm": 0.4383796751499176,
      "learning_rate": 0.00016838068814398796,
      "loss": 2.389,
      "step": 41200
    },
    {
      "epoch": 0.847770129011852,
      "grad_norm": 0.39621421694755554,
      "learning_rate": 0.0001683644812755586,
      "loss": 2.4571,
      "step": 41210
    },
    {
      "epoch": 0.8479758485287198,
      "grad_norm": 0.39630362391471863,
      "learning_rate": 0.00016834827103508196,
      "loss": 2.4205,
      "step": 41220
    },
    {
      "epoch": 0.8481815680455874,
      "grad_norm": 0.38103196024894714,
      "learning_rate": 0.0001683320574233575,
      "loss": 2.4187,
      "step": 41230
    },
    {
      "epoch": 0.8483872875624552,
      "grad_norm": 0.38112905621528625,
      "learning_rate": 0.0001683158404411851,
      "loss": 2.4136,
      "step": 41240
    },
    {
      "epoch": 0.8485930070793228,
      "grad_norm": 0.3774906098842621,
      "learning_rate": 0.0001682996200893645,
      "loss": 2.3793,
      "step": 41250
    },
    {
      "epoch": 0.8487987265961906,
      "grad_norm": 0.4026030898094177,
      "learning_rate": 0.00016828339636869584,
      "loss": 2.4203,
      "step": 41260
    },
    {
      "epoch": 0.8490044461130583,
      "grad_norm": 0.40873056650161743,
      "learning_rate": 0.00016826716927997935,
      "loss": 2.4865,
      "step": 41270
    },
    {
      "epoch": 0.849210165629926,
      "grad_norm": 0.4153454303741455,
      "learning_rate": 0.00016825093882401538,
      "loss": 2.4046,
      "step": 41280
    },
    {
      "epoch": 0.8494158851467938,
      "grad_norm": 0.37633150815963745,
      "learning_rate": 0.00016823470500160456,
      "loss": 2.4152,
      "step": 41290
    },
    {
      "epoch": 0.8496216046636614,
      "grad_norm": 0.3955783247947693,
      "learning_rate": 0.00016821846781354757,
      "loss": 2.4611,
      "step": 41300
    },
    {
      "epoch": 0.8498273241805292,
      "grad_norm": 0.4493129551410675,
      "learning_rate": 0.0001682022272606453,
      "loss": 2.4602,
      "step": 41310
    },
    {
      "epoch": 0.8500330436973969,
      "grad_norm": 0.4080365002155304,
      "learning_rate": 0.00016818598334369882,
      "loss": 2.383,
      "step": 41320
    },
    {
      "epoch": 0.8502387632142646,
      "grad_norm": 0.4262036085128784,
      "learning_rate": 0.00016816973606350937,
      "loss": 2.4137,
      "step": 41330
    },
    {
      "epoch": 0.8504444827311323,
      "grad_norm": 0.3715951442718506,
      "learning_rate": 0.00016815348542087829,
      "loss": 2.4041,
      "step": 41340
    },
    {
      "epoch": 0.850650202248,
      "grad_norm": 0.43035468459129333,
      "learning_rate": 0.00016813723141660716,
      "loss": 2.3916,
      "step": 41350
    },
    {
      "epoch": 0.8508559217648677,
      "grad_norm": 0.41177302598953247,
      "learning_rate": 0.00016812097405149773,
      "loss": 2.4054,
      "step": 41360
    },
    {
      "epoch": 0.8510616412817354,
      "grad_norm": 0.4016868472099304,
      "learning_rate": 0.00016810471332635185,
      "loss": 2.4144,
      "step": 41370
    },
    {
      "epoch": 0.8512673607986032,
      "grad_norm": 0.39190247654914856,
      "learning_rate": 0.0001680884492419716,
      "loss": 2.4165,
      "step": 41380
    },
    {
      "epoch": 0.8514730803154709,
      "grad_norm": 0.5027175545692444,
      "learning_rate": 0.00016807218179915918,
      "loss": 2.4399,
      "step": 41390
    },
    {
      "epoch": 0.8516787998323386,
      "grad_norm": 0.39652058482170105,
      "learning_rate": 0.000168055910998717,
      "loss": 2.5021,
      "step": 41400
    },
    {
      "epoch": 0.8518845193492063,
      "grad_norm": 0.38931068778038025,
      "learning_rate": 0.00016803963684144756,
      "loss": 2.3784,
      "step": 41410
    },
    {
      "epoch": 0.852090238866074,
      "grad_norm": 0.39859962463378906,
      "learning_rate": 0.00016802335932815363,
      "loss": 2.4112,
      "step": 41420
    },
    {
      "epoch": 0.8522959583829417,
      "grad_norm": 0.4253412187099457,
      "learning_rate": 0.00016800707845963806,
      "loss": 2.4547,
      "step": 41430
    },
    {
      "epoch": 0.8525016778998095,
      "grad_norm": 0.4107581377029419,
      "learning_rate": 0.0001679907942367039,
      "loss": 2.4193,
      "step": 41440
    },
    {
      "epoch": 0.8527073974166771,
      "grad_norm": 0.40354788303375244,
      "learning_rate": 0.00016797450666015436,
      "loss": 2.4144,
      "step": 41450
    },
    {
      "epoch": 0.8529131169335449,
      "grad_norm": 0.4259631931781769,
      "learning_rate": 0.00016795821573079286,
      "loss": 2.4153,
      "step": 41460
    },
    {
      "epoch": 0.8531188364504126,
      "grad_norm": 0.3915850818157196,
      "learning_rate": 0.00016794192144942285,
      "loss": 2.3974,
      "step": 41470
    },
    {
      "epoch": 0.8533245559672803,
      "grad_norm": 0.42153772711753845,
      "learning_rate": 0.00016792562381684811,
      "loss": 2.4425,
      "step": 41480
    },
    {
      "epoch": 0.8535302754841481,
      "grad_norm": 0.3981510102748871,
      "learning_rate": 0.00016790932283387253,
      "loss": 2.4503,
      "step": 41490
    },
    {
      "epoch": 0.8537359950010157,
      "grad_norm": 0.6443663835525513,
      "learning_rate": 0.0001678930185013001,
      "loss": 2.413,
      "step": 41500
    },
    {
      "epoch": 0.8539417145178835,
      "grad_norm": 0.38978761434555054,
      "learning_rate": 0.00016787671081993502,
      "loss": 2.4487,
      "step": 41510
    },
    {
      "epoch": 0.8541474340347511,
      "grad_norm": 0.41976407170295715,
      "learning_rate": 0.00016786039979058167,
      "loss": 2.4201,
      "step": 41520
    },
    {
      "epoch": 0.8543531535516189,
      "grad_norm": 0.45430684089660645,
      "learning_rate": 0.00016784408541404463,
      "loss": 2.4082,
      "step": 41530
    },
    {
      "epoch": 0.8545588730684865,
      "grad_norm": 0.3847786486148834,
      "learning_rate": 0.00016782776769112853,
      "loss": 2.4846,
      "step": 41540
    },
    {
      "epoch": 0.8547645925853543,
      "grad_norm": 0.42839235067367554,
      "learning_rate": 0.00016781144662263834,
      "loss": 2.4353,
      "step": 41550
    },
    {
      "epoch": 0.8549703121022221,
      "grad_norm": 0.3773078918457031,
      "learning_rate": 0.00016779512220937894,
      "loss": 2.4457,
      "step": 41560
    },
    {
      "epoch": 0.8551760316190897,
      "grad_norm": 0.39551836252212524,
      "learning_rate": 0.0001677787944521556,
      "loss": 2.416,
      "step": 41570
    },
    {
      "epoch": 0.8553817511359575,
      "grad_norm": 0.3888527452945709,
      "learning_rate": 0.00016776246335177374,
      "loss": 2.4046,
      "step": 41580
    },
    {
      "epoch": 0.8555874706528251,
      "grad_norm": 0.41444554924964905,
      "learning_rate": 0.00016774612890903875,
      "loss": 2.3938,
      "step": 41590
    },
    {
      "epoch": 0.8557931901696929,
      "grad_norm": 0.4290482699871063,
      "learning_rate": 0.00016772979112475645,
      "loss": 2.3789,
      "step": 41600
    },
    {
      "epoch": 0.8559989096865606,
      "grad_norm": 0.4752805531024933,
      "learning_rate": 0.0001677134499997326,
      "loss": 2.4367,
      "step": 41610
    },
    {
      "epoch": 0.8562046292034283,
      "grad_norm": 0.3880115747451782,
      "learning_rate": 0.00016769710553477327,
      "loss": 2.4511,
      "step": 41620
    },
    {
      "epoch": 0.856410348720296,
      "grad_norm": 0.38588640093803406,
      "learning_rate": 0.00016768075773068464,
      "loss": 2.3687,
      "step": 41630
    },
    {
      "epoch": 0.8566160682371637,
      "grad_norm": 0.4096691906452179,
      "learning_rate": 0.00016766440658827302,
      "loss": 2.4153,
      "step": 41640
    },
    {
      "epoch": 0.8568217877540315,
      "grad_norm": 0.41778355836868286,
      "learning_rate": 0.00016764805210834496,
      "loss": 2.4365,
      "step": 41650
    },
    {
      "epoch": 0.8570275072708992,
      "grad_norm": 0.4149138927459717,
      "learning_rate": 0.00016763169429170714,
      "loss": 2.4859,
      "step": 41660
    },
    {
      "epoch": 0.8572332267877669,
      "grad_norm": 0.41776013374328613,
      "learning_rate": 0.00016761533313916637,
      "loss": 2.4409,
      "step": 41670
    },
    {
      "epoch": 0.8574389463046346,
      "grad_norm": 0.4062735140323639,
      "learning_rate": 0.0001675989686515297,
      "loss": 2.4315,
      "step": 41680
    },
    {
      "epoch": 0.8576446658215023,
      "grad_norm": 0.377184122800827,
      "learning_rate": 0.0001675826008296043,
      "loss": 2.407,
      "step": 41690
    },
    {
      "epoch": 0.85785038533837,
      "grad_norm": 0.4325494170188904,
      "learning_rate": 0.00016756622967419743,
      "loss": 2.4001,
      "step": 41700
    },
    {
      "epoch": 0.8580561048552378,
      "grad_norm": 0.4108354151248932,
      "learning_rate": 0.00016754985518611666,
      "loss": 2.4099,
      "step": 41710
    },
    {
      "epoch": 0.8582618243721054,
      "grad_norm": 0.415132999420166,
      "learning_rate": 0.00016753347736616963,
      "loss": 2.3919,
      "step": 41720
    },
    {
      "epoch": 0.8584675438889732,
      "grad_norm": 0.3758947551250458,
      "learning_rate": 0.00016751709621516415,
      "loss": 2.4253,
      "step": 41730
    },
    {
      "epoch": 0.8586732634058409,
      "grad_norm": 0.4340355396270752,
      "learning_rate": 0.00016750071173390828,
      "loss": 2.4611,
      "step": 41740
    },
    {
      "epoch": 0.8588789829227086,
      "grad_norm": 0.37367457151412964,
      "learning_rate": 0.00016748432392321014,
      "loss": 2.4081,
      "step": 41750
    },
    {
      "epoch": 0.8590847024395764,
      "grad_norm": 0.35623031854629517,
      "learning_rate": 0.000167467932783878,
      "loss": 2.4465,
      "step": 41760
    },
    {
      "epoch": 0.859290421956444,
      "grad_norm": 0.4491395354270935,
      "learning_rate": 0.00016745153831672046,
      "loss": 2.4527,
      "step": 41770
    },
    {
      "epoch": 0.8594961414733118,
      "grad_norm": 0.4951644241809845,
      "learning_rate": 0.00016743514052254606,
      "loss": 2.4876,
      "step": 41780
    },
    {
      "epoch": 0.8597018609901794,
      "grad_norm": 0.394779235124588,
      "learning_rate": 0.00016741873940216367,
      "loss": 2.4379,
      "step": 41790
    },
    {
      "epoch": 0.8599075805070472,
      "grad_norm": 0.3834611773490906,
      "learning_rate": 0.0001674023349563822,
      "loss": 2.4299,
      "step": 41800
    },
    {
      "epoch": 0.8601133000239148,
      "grad_norm": 0.39571693539619446,
      "learning_rate": 0.0001673859271860109,
      "loss": 2.4258,
      "step": 41810
    },
    {
      "epoch": 0.8603190195407826,
      "grad_norm": 0.4137577414512634,
      "learning_rate": 0.000167369516091859,
      "loss": 2.4433,
      "step": 41820
    },
    {
      "epoch": 0.8605247390576504,
      "grad_norm": 0.3954731523990631,
      "learning_rate": 0.000167353101674736,
      "loss": 2.3989,
      "step": 41830
    },
    {
      "epoch": 0.860730458574518,
      "grad_norm": 0.37835392355918884,
      "learning_rate": 0.0001673366839354515,
      "loss": 2.3886,
      "step": 41840
    },
    {
      "epoch": 0.8609361780913858,
      "grad_norm": 0.4244231879711151,
      "learning_rate": 0.00016732026287481538,
      "loss": 2.4502,
      "step": 41850
    },
    {
      "epoch": 0.8611418976082534,
      "grad_norm": 0.4025072157382965,
      "learning_rate": 0.0001673038384936375,
      "loss": 2.421,
      "step": 41860
    },
    {
      "epoch": 0.8613476171251212,
      "grad_norm": 0.4254845082759857,
      "learning_rate": 0.00016728741079272805,
      "loss": 2.3961,
      "step": 41870
    },
    {
      "epoch": 0.8615533366419889,
      "grad_norm": 0.4283907413482666,
      "learning_rate": 0.0001672709797728973,
      "loss": 2.3739,
      "step": 41880
    },
    {
      "epoch": 0.8617590561588566,
      "grad_norm": 0.38890552520751953,
      "learning_rate": 0.00016725454543495567,
      "loss": 2.4083,
      "step": 41890
    },
    {
      "epoch": 0.8619647756757243,
      "grad_norm": 0.47472214698791504,
      "learning_rate": 0.00016723810777971384,
      "loss": 2.4443,
      "step": 41900
    },
    {
      "epoch": 0.862170495192592,
      "grad_norm": 0.4289970397949219,
      "learning_rate": 0.00016722166680798256,
      "loss": 2.4554,
      "step": 41910
    },
    {
      "epoch": 0.8623762147094598,
      "grad_norm": 0.38868409395217896,
      "learning_rate": 0.0001672052225205727,
      "loss": 2.4609,
      "step": 41920
    },
    {
      "epoch": 0.8625819342263275,
      "grad_norm": 0.39342182874679565,
      "learning_rate": 0.00016718877491829552,
      "loss": 2.4127,
      "step": 41930
    },
    {
      "epoch": 0.8627876537431952,
      "grad_norm": 0.414669930934906,
      "learning_rate": 0.00016717232400196216,
      "loss": 2.4325,
      "step": 41940
    },
    {
      "epoch": 0.8629933732600629,
      "grad_norm": 0.3812859058380127,
      "learning_rate": 0.00016715586977238413,
      "loss": 2.3832,
      "step": 41950
    },
    {
      "epoch": 0.8631990927769306,
      "grad_norm": 0.37702885270118713,
      "learning_rate": 0.00016713941223037298,
      "loss": 2.3998,
      "step": 41960
    },
    {
      "epoch": 0.8634048122937983,
      "grad_norm": 0.4079362750053406,
      "learning_rate": 0.0001671229513767405,
      "loss": 2.4204,
      "step": 41970
    },
    {
      "epoch": 0.863610531810666,
      "grad_norm": 0.37783193588256836,
      "learning_rate": 0.00016710648721229862,
      "loss": 2.4068,
      "step": 41980
    },
    {
      "epoch": 0.8638162513275337,
      "grad_norm": 0.42457136511802673,
      "learning_rate": 0.0001670900197378594,
      "loss": 2.4519,
      "step": 41990
    },
    {
      "epoch": 0.8640219708444015,
      "grad_norm": 0.3901348114013672,
      "learning_rate": 0.00016707354895423507,
      "loss": 2.4718,
      "step": 42000
    },
    {
      "epoch": 0.8642276903612692,
      "grad_norm": 0.38864579796791077,
      "learning_rate": 0.00016705707486223812,
      "loss": 2.4467,
      "step": 42010
    },
    {
      "epoch": 0.8644334098781369,
      "grad_norm": 0.37469425797462463,
      "learning_rate": 0.00016704059746268108,
      "loss": 2.4497,
      "step": 42020
    },
    {
      "epoch": 0.8646391293950046,
      "grad_norm": 0.43491077423095703,
      "learning_rate": 0.0001670241167563767,
      "loss": 2.483,
      "step": 42030
    },
    {
      "epoch": 0.8648448489118723,
      "grad_norm": 0.43739721179008484,
      "learning_rate": 0.00016700763274413784,
      "loss": 2.4322,
      "step": 42040
    },
    {
      "epoch": 0.8650505684287401,
      "grad_norm": 0.378280907869339,
      "learning_rate": 0.00016699114542677762,
      "loss": 2.3985,
      "step": 42050
    },
    {
      "epoch": 0.8652562879456077,
      "grad_norm": 0.3997690677642822,
      "learning_rate": 0.0001669746548051093,
      "loss": 2.4474,
      "step": 42060
    },
    {
      "epoch": 0.8654620074624755,
      "grad_norm": 0.4247955083847046,
      "learning_rate": 0.00016695816087994617,
      "loss": 2.4242,
      "step": 42070
    },
    {
      "epoch": 0.8656677269793432,
      "grad_norm": 0.410145103931427,
      "learning_rate": 0.00016694166365210187,
      "loss": 2.3981,
      "step": 42080
    },
    {
      "epoch": 0.8658734464962109,
      "grad_norm": 0.38053712248802185,
      "learning_rate": 0.00016692516312239009,
      "loss": 2.4218,
      "step": 42090
    },
    {
      "epoch": 0.8660791660130787,
      "grad_norm": 0.44282394647598267,
      "learning_rate": 0.0001669086592916247,
      "loss": 2.3876,
      "step": 42100
    },
    {
      "epoch": 0.8662848855299463,
      "grad_norm": 0.42255479097366333,
      "learning_rate": 0.00016689215216061974,
      "loss": 2.4096,
      "step": 42110
    },
    {
      "epoch": 0.8664906050468141,
      "grad_norm": 0.39320194721221924,
      "learning_rate": 0.00016687564173018945,
      "loss": 2.4081,
      "step": 42120
    },
    {
      "epoch": 0.8666963245636817,
      "grad_norm": 0.3988926410675049,
      "learning_rate": 0.0001668591280011482,
      "loss": 2.4448,
      "step": 42130
    },
    {
      "epoch": 0.8669020440805495,
      "grad_norm": 0.4360792636871338,
      "learning_rate": 0.0001668426109743105,
      "loss": 2.4451,
      "step": 42140
    },
    {
      "epoch": 0.8671077635974171,
      "grad_norm": 0.3980504870414734,
      "learning_rate": 0.00016682609065049104,
      "loss": 2.4311,
      "step": 42150
    },
    {
      "epoch": 0.8673134831142849,
      "grad_norm": 0.39916813373565674,
      "learning_rate": 0.00016680956703050468,
      "loss": 2.4232,
      "step": 42160
    },
    {
      "epoch": 0.8675192026311527,
      "grad_norm": 0.39810284972190857,
      "learning_rate": 0.00016679304011516647,
      "loss": 2.4404,
      "step": 42170
    },
    {
      "epoch": 0.8677249221480203,
      "grad_norm": 0.43603259325027466,
      "learning_rate": 0.0001667765099052915,
      "loss": 2.4292,
      "step": 42180
    },
    {
      "epoch": 0.8679306416648881,
      "grad_norm": 0.4424685537815094,
      "learning_rate": 0.00016675997640169525,
      "loss": 2.3895,
      "step": 42190
    },
    {
      "epoch": 0.8681363611817557,
      "grad_norm": 0.4725337326526642,
      "learning_rate": 0.00016674343960519313,
      "loss": 2.4079,
      "step": 42200
    },
    {
      "epoch": 0.8683420806986235,
      "grad_norm": 0.4018429219722748,
      "learning_rate": 0.00016672689951660087,
      "loss": 2.4236,
      "step": 42210
    },
    {
      "epoch": 0.8685478002154912,
      "grad_norm": 0.4091150760650635,
      "learning_rate": 0.0001667103561367343,
      "loss": 2.4,
      "step": 42220
    },
    {
      "epoch": 0.8687535197323589,
      "grad_norm": 0.4050382077693939,
      "learning_rate": 0.00016669380946640933,
      "loss": 2.4238,
      "step": 42230
    },
    {
      "epoch": 0.8689592392492266,
      "grad_norm": 0.395934522151947,
      "learning_rate": 0.0001666772595064422,
      "loss": 2.4373,
      "step": 42240
    },
    {
      "epoch": 0.8691649587660943,
      "grad_norm": 0.42918896675109863,
      "learning_rate": 0.0001666607062576492,
      "loss": 2.3902,
      "step": 42250
    },
    {
      "epoch": 0.8693706782829621,
      "grad_norm": 0.5366836190223694,
      "learning_rate": 0.00016664414972084683,
      "loss": 2.4691,
      "step": 42260
    },
    {
      "epoch": 0.8695763977998298,
      "grad_norm": 0.38334691524505615,
      "learning_rate": 0.00016662758989685172,
      "loss": 2.4405,
      "step": 42270
    },
    {
      "epoch": 0.8697821173166975,
      "grad_norm": 0.4019439220428467,
      "learning_rate": 0.00016661102678648066,
      "loss": 2.4254,
      "step": 42280
    },
    {
      "epoch": 0.8699878368335652,
      "grad_norm": 0.3932039141654968,
      "learning_rate": 0.00016659446039055066,
      "loss": 2.4019,
      "step": 42290
    },
    {
      "epoch": 0.8701935563504329,
      "grad_norm": 0.43229207396507263,
      "learning_rate": 0.00016657789070987882,
      "loss": 2.4623,
      "step": 42300
    },
    {
      "epoch": 0.8703992758673006,
      "grad_norm": 0.42117956280708313,
      "learning_rate": 0.00016656131774528242,
      "loss": 2.4578,
      "step": 42310
    },
    {
      "epoch": 0.8706049953841684,
      "grad_norm": 0.3729720115661621,
      "learning_rate": 0.00016654474149757894,
      "loss": 2.4137,
      "step": 42320
    },
    {
      "epoch": 0.870810714901036,
      "grad_norm": 0.38320234417915344,
      "learning_rate": 0.000166528161967586,
      "loss": 2.444,
      "step": 42330
    },
    {
      "epoch": 0.8710164344179038,
      "grad_norm": 0.4526250660419464,
      "learning_rate": 0.00016651157915612135,
      "loss": 2.4244,
      "step": 42340
    },
    {
      "epoch": 0.8712221539347715,
      "grad_norm": 0.37800225615501404,
      "learning_rate": 0.00016649499306400296,
      "loss": 2.4153,
      "step": 42350
    },
    {
      "epoch": 0.8714278734516392,
      "grad_norm": 0.44069889187812805,
      "learning_rate": 0.0001664784036920489,
      "loss": 2.4217,
      "step": 42360
    },
    {
      "epoch": 0.871633592968507,
      "grad_norm": 0.402362197637558,
      "learning_rate": 0.00016646181104107744,
      "loss": 2.4522,
      "step": 42370
    },
    {
      "epoch": 0.8718393124853746,
      "grad_norm": 0.3782970607280731,
      "learning_rate": 0.00016644521511190706,
      "loss": 2.4314,
      "step": 42380
    },
    {
      "epoch": 0.8720450320022424,
      "grad_norm": 0.3954191207885742,
      "learning_rate": 0.00016642861590535624,
      "loss": 2.3696,
      "step": 42390
    },
    {
      "epoch": 0.87225075151911,
      "grad_norm": 0.38862866163253784,
      "learning_rate": 0.00016641201342224384,
      "loss": 2.4216,
      "step": 42400
    },
    {
      "epoch": 0.8724564710359778,
      "grad_norm": 0.42484813928604126,
      "learning_rate": 0.0001663954076633887,
      "loss": 2.4368,
      "step": 42410
    },
    {
      "epoch": 0.8726621905528454,
      "grad_norm": 0.4403657615184784,
      "learning_rate": 0.0001663787986296099,
      "loss": 2.379,
      "step": 42420
    },
    {
      "epoch": 0.8728679100697132,
      "grad_norm": 0.40785640478134155,
      "learning_rate": 0.00016636218632172671,
      "loss": 2.4857,
      "step": 42430
    },
    {
      "epoch": 0.873073629586581,
      "grad_norm": 0.4204210042953491,
      "learning_rate": 0.00016634557074055847,
      "loss": 2.4578,
      "step": 42440
    },
    {
      "epoch": 0.8732793491034486,
      "grad_norm": 0.4625241160392761,
      "learning_rate": 0.0001663289518869248,
      "loss": 2.425,
      "step": 42450
    },
    {
      "epoch": 0.8734850686203164,
      "grad_norm": 0.43981873989105225,
      "learning_rate": 0.0001663123297616454,
      "loss": 2.3209,
      "step": 42460
    },
    {
      "epoch": 0.873690788137184,
      "grad_norm": 0.4491250216960907,
      "learning_rate": 0.00016629570436554008,
      "loss": 2.4273,
      "step": 42470
    },
    {
      "epoch": 0.8738965076540518,
      "grad_norm": 0.3897930085659027,
      "learning_rate": 0.00016627907569942893,
      "loss": 2.4111,
      "step": 42480
    },
    {
      "epoch": 0.8741022271709195,
      "grad_norm": 0.4256199896335602,
      "learning_rate": 0.00016626244376413219,
      "loss": 2.4122,
      "step": 42490
    },
    {
      "epoch": 0.8743079466877872,
      "grad_norm": 0.44598227739334106,
      "learning_rate": 0.00016624580856047016,
      "loss": 2.4756,
      "step": 42500
    },
    {
      "epoch": 0.8745136662046549,
      "grad_norm": 0.4011949896812439,
      "learning_rate": 0.0001662291700892634,
      "loss": 2.4097,
      "step": 42510
    },
    {
      "epoch": 0.8747193857215226,
      "grad_norm": 0.3938823342323303,
      "learning_rate": 0.00016621252835133257,
      "loss": 2.3504,
      "step": 42520
    },
    {
      "epoch": 0.8749251052383904,
      "grad_norm": 0.3701915442943573,
      "learning_rate": 0.00016619588334749856,
      "loss": 2.4602,
      "step": 42530
    },
    {
      "epoch": 0.875130824755258,
      "grad_norm": 0.3961189091205597,
      "learning_rate": 0.00016617923507858233,
      "loss": 2.4362,
      "step": 42540
    },
    {
      "epoch": 0.8753365442721258,
      "grad_norm": 0.37972143292427063,
      "learning_rate": 0.00016616258354540508,
      "loss": 2.3888,
      "step": 42550
    },
    {
      "epoch": 0.8755422637889935,
      "grad_norm": 0.37895435094833374,
      "learning_rate": 0.00016614592874878808,
      "loss": 2.433,
      "step": 42560
    },
    {
      "epoch": 0.8757479833058612,
      "grad_norm": 0.4021894633769989,
      "learning_rate": 0.00016612927068955293,
      "loss": 2.3949,
      "step": 42570
    },
    {
      "epoch": 0.8759537028227289,
      "grad_norm": 0.40056270360946655,
      "learning_rate": 0.0001661126093685212,
      "loss": 2.4041,
      "step": 42580
    },
    {
      "epoch": 0.8761594223395966,
      "grad_norm": 0.37867283821105957,
      "learning_rate": 0.00016609594478651473,
      "loss": 2.4385,
      "step": 42590
    },
    {
      "epoch": 0.8763651418564643,
      "grad_norm": 0.5687005519866943,
      "learning_rate": 0.00016607927694435546,
      "loss": 2.3957,
      "step": 42600
    },
    {
      "epoch": 0.8765708613733321,
      "grad_norm": 0.4449623227119446,
      "learning_rate": 0.00016606260584286554,
      "loss": 2.4301,
      "step": 42610
    },
    {
      "epoch": 0.8767765808901998,
      "grad_norm": 0.40880510210990906,
      "learning_rate": 0.0001660459314828673,
      "loss": 2.4645,
      "step": 42620
    },
    {
      "epoch": 0.8769823004070675,
      "grad_norm": 0.408184289932251,
      "learning_rate": 0.00016602925386518313,
      "loss": 2.4227,
      "step": 42630
    },
    {
      "epoch": 0.8771880199239352,
      "grad_norm": 0.40850135684013367,
      "learning_rate": 0.0001660125729906357,
      "loss": 2.4235,
      "step": 42640
    },
    {
      "epoch": 0.8773937394408029,
      "grad_norm": 0.4705144762992859,
      "learning_rate": 0.0001659958888600478,
      "loss": 2.4054,
      "step": 42650
    },
    {
      "epoch": 0.8775994589576707,
      "grad_norm": 0.4210032820701599,
      "learning_rate": 0.0001659792014742423,
      "loss": 2.4553,
      "step": 42660
    },
    {
      "epoch": 0.8778051784745383,
      "grad_norm": 0.38824567198753357,
      "learning_rate": 0.00016596251083404235,
      "loss": 2.3822,
      "step": 42670
    },
    {
      "epoch": 0.8780108979914061,
      "grad_norm": 0.40167760848999023,
      "learning_rate": 0.00016594581694027118,
      "loss": 2.4125,
      "step": 42680
    },
    {
      "epoch": 0.8782166175082737,
      "grad_norm": 0.3817899227142334,
      "learning_rate": 0.00016592911979375225,
      "loss": 2.3799,
      "step": 42690
    },
    {
      "epoch": 0.8784223370251415,
      "grad_norm": 0.3851856291294098,
      "learning_rate": 0.00016591241939530908,
      "loss": 2.4412,
      "step": 42700
    },
    {
      "epoch": 0.8786280565420093,
      "grad_norm": 0.43014761805534363,
      "learning_rate": 0.00016589571574576547,
      "loss": 2.4243,
      "step": 42710
    },
    {
      "epoch": 0.8788337760588769,
      "grad_norm": 0.5904054641723633,
      "learning_rate": 0.00016587900884594526,
      "loss": 2.4739,
      "step": 42720
    },
    {
      "epoch": 0.8790394955757447,
      "grad_norm": 0.4004771113395691,
      "learning_rate": 0.00016586229869667256,
      "loss": 2.4259,
      "step": 42730
    },
    {
      "epoch": 0.8792452150926123,
      "grad_norm": 0.40767011046409607,
      "learning_rate": 0.00016584558529877158,
      "loss": 2.4512,
      "step": 42740
    },
    {
      "epoch": 0.8794509346094801,
      "grad_norm": 0.42858171463012695,
      "learning_rate": 0.00016582886865306668,
      "loss": 2.4535,
      "step": 42750
    },
    {
      "epoch": 0.8796566541263477,
      "grad_norm": 0.39445728063583374,
      "learning_rate": 0.00016581214876038242,
      "loss": 2.4351,
      "step": 42760
    },
    {
      "epoch": 0.8798623736432155,
      "grad_norm": 0.41572070121765137,
      "learning_rate": 0.00016579542562154348,
      "loss": 2.4421,
      "step": 42770
    },
    {
      "epoch": 0.8800680931600832,
      "grad_norm": 0.41337278485298157,
      "learning_rate": 0.00016577869923737476,
      "loss": 2.4671,
      "step": 42780
    },
    {
      "epoch": 0.8802738126769509,
      "grad_norm": 0.3971045911312103,
      "learning_rate": 0.00016576196960870125,
      "loss": 2.4462,
      "step": 42790
    },
    {
      "epoch": 0.8804795321938187,
      "grad_norm": 0.4527758061885834,
      "learning_rate": 0.00016574523673634813,
      "loss": 2.4898,
      "step": 42800
    },
    {
      "epoch": 0.8806852517106863,
      "grad_norm": 0.4048376977443695,
      "learning_rate": 0.00016572850062114073,
      "loss": 2.4402,
      "step": 42810
    },
    {
      "epoch": 0.8808909712275541,
      "grad_norm": 0.38889509439468384,
      "learning_rate": 0.0001657117612639046,
      "loss": 2.408,
      "step": 42820
    },
    {
      "epoch": 0.8810966907444218,
      "grad_norm": 0.39248812198638916,
      "learning_rate": 0.00016569501866546538,
      "loss": 2.3882,
      "step": 42830
    },
    {
      "epoch": 0.8813024102612895,
      "grad_norm": 0.44217631220817566,
      "learning_rate": 0.00016567827282664888,
      "loss": 2.4031,
      "step": 42840
    },
    {
      "epoch": 0.8815081297781572,
      "grad_norm": 0.4339802861213684,
      "learning_rate": 0.00016566152374828104,
      "loss": 2.3872,
      "step": 42850
    },
    {
      "epoch": 0.8817138492950249,
      "grad_norm": 0.44589197635650635,
      "learning_rate": 0.0001656447714311881,
      "loss": 2.5123,
      "step": 42860
    },
    {
      "epoch": 0.8819195688118926,
      "grad_norm": 0.3851049542427063,
      "learning_rate": 0.0001656280158761963,
      "loss": 2.4204,
      "step": 42870
    },
    {
      "epoch": 0.8821252883287604,
      "grad_norm": 0.4228336811065674,
      "learning_rate": 0.0001656112570841321,
      "loss": 2.4231,
      "step": 42880
    },
    {
      "epoch": 0.8823310078456281,
      "grad_norm": 0.40880152583122253,
      "learning_rate": 0.0001655944950558221,
      "loss": 2.454,
      "step": 42890
    },
    {
      "epoch": 0.8825367273624958,
      "grad_norm": 0.38891544938087463,
      "learning_rate": 0.00016557772979209313,
      "loss": 2.4607,
      "step": 42900
    },
    {
      "epoch": 0.8827424468793635,
      "grad_norm": 0.4238886535167694,
      "learning_rate": 0.00016556096129377213,
      "loss": 2.4123,
      "step": 42910
    },
    {
      "epoch": 0.8829481663962312,
      "grad_norm": 0.4489150941371918,
      "learning_rate": 0.00016554418956168614,
      "loss": 2.4647,
      "step": 42920
    },
    {
      "epoch": 0.883153885913099,
      "grad_norm": 0.37640106678009033,
      "learning_rate": 0.00016552741459666245,
      "loss": 2.4316,
      "step": 42930
    },
    {
      "epoch": 0.8833596054299666,
      "grad_norm": 0.42846566438674927,
      "learning_rate": 0.00016551063639952849,
      "loss": 2.4026,
      "step": 42940
    },
    {
      "epoch": 0.8835653249468344,
      "grad_norm": 0.4929647445678711,
      "learning_rate": 0.0001654938549711118,
      "loss": 2.4189,
      "step": 42950
    },
    {
      "epoch": 0.883771044463702,
      "grad_norm": 0.39294782280921936,
      "learning_rate": 0.0001654770703122402,
      "loss": 2.4344,
      "step": 42960
    },
    {
      "epoch": 0.8839767639805698,
      "grad_norm": 0.39443427324295044,
      "learning_rate": 0.0001654602824237415,
      "loss": 2.4309,
      "step": 42970
    },
    {
      "epoch": 0.8841824834974376,
      "grad_norm": 0.4267798662185669,
      "learning_rate": 0.00016544349130644378,
      "loss": 2.413,
      "step": 42980
    },
    {
      "epoch": 0.8843882030143052,
      "grad_norm": 0.410134881734848,
      "learning_rate": 0.00016542669696117525,
      "loss": 2.3988,
      "step": 42990
    },
    {
      "epoch": 0.884593922531173,
      "grad_norm": 0.41156530380249023,
      "learning_rate": 0.0001654098993887643,
      "loss": 2.4621,
      "step": 43000
    },
    {
      "epoch": 0.8847996420480406,
      "grad_norm": 0.4168036878108978,
      "learning_rate": 0.00016539309859003943,
      "loss": 2.4509,
      "step": 43010
    },
    {
      "epoch": 0.8850053615649084,
      "grad_norm": 0.4116109609603882,
      "learning_rate": 0.0001653762945658294,
      "loss": 2.3469,
      "step": 43020
    },
    {
      "epoch": 0.885211081081776,
      "grad_norm": 0.3753818869590759,
      "learning_rate": 0.000165359487316963,
      "loss": 2.4753,
      "step": 43030
    },
    {
      "epoch": 0.8854168005986438,
      "grad_norm": 0.3929738700389862,
      "learning_rate": 0.00016534267684426926,
      "loss": 2.4714,
      "step": 43040
    },
    {
      "epoch": 0.8856225201155115,
      "grad_norm": 0.3620801866054535,
      "learning_rate": 0.00016532586314857732,
      "loss": 2.4239,
      "step": 43050
    },
    {
      "epoch": 0.8858282396323792,
      "grad_norm": 0.40466591715812683,
      "learning_rate": 0.00016530904623071656,
      "loss": 2.4723,
      "step": 43060
    },
    {
      "epoch": 0.886033959149247,
      "grad_norm": 0.44749438762664795,
      "learning_rate": 0.00016529222609151646,
      "loss": 2.4056,
      "step": 43070
    },
    {
      "epoch": 0.8862396786661146,
      "grad_norm": 0.41423213481903076,
      "learning_rate": 0.0001652754027318066,
      "loss": 2.3715,
      "step": 43080
    },
    {
      "epoch": 0.8864453981829824,
      "grad_norm": 0.3826676607131958,
      "learning_rate": 0.00016525857615241687,
      "loss": 2.3933,
      "step": 43090
    },
    {
      "epoch": 0.88665111769985,
      "grad_norm": 0.38839098811149597,
      "learning_rate": 0.00016524174635417716,
      "loss": 2.4428,
      "step": 43100
    },
    {
      "epoch": 0.8868568372167178,
      "grad_norm": 0.39003080129623413,
      "learning_rate": 0.00016522491333791764,
      "loss": 2.3818,
      "step": 43110
    },
    {
      "epoch": 0.8870625567335855,
      "grad_norm": 0.44221287965774536,
      "learning_rate": 0.0001652080771044686,
      "loss": 2.4375,
      "step": 43120
    },
    {
      "epoch": 0.8872682762504532,
      "grad_norm": 0.43284735083580017,
      "learning_rate": 0.00016519123765466043,
      "loss": 2.4085,
      "step": 43130
    },
    {
      "epoch": 0.8874739957673209,
      "grad_norm": 0.4387851059436798,
      "learning_rate": 0.0001651743949893238,
      "loss": 2.4116,
      "step": 43140
    },
    {
      "epoch": 0.8876797152841887,
      "grad_norm": 0.41326001286506653,
      "learning_rate": 0.00016515754910928937,
      "loss": 2.4039,
      "step": 43150
    },
    {
      "epoch": 0.8878854348010564,
      "grad_norm": 0.3748425245285034,
      "learning_rate": 0.00016514070001538814,
      "loss": 2.396,
      "step": 43160
    },
    {
      "epoch": 0.8880911543179241,
      "grad_norm": 0.40465453267097473,
      "learning_rate": 0.00016512384770845114,
      "loss": 2.4289,
      "step": 43170
    },
    {
      "epoch": 0.8882968738347918,
      "grad_norm": 0.43663960695266724,
      "learning_rate": 0.00016510699218930962,
      "loss": 2.4498,
      "step": 43180
    },
    {
      "epoch": 0.8885025933516595,
      "grad_norm": 0.493873655796051,
      "learning_rate": 0.00016509013345879497,
      "loss": 2.4339,
      "step": 43190
    },
    {
      "epoch": 0.8887083128685273,
      "grad_norm": 0.3938480317592621,
      "learning_rate": 0.00016507327151773872,
      "loss": 2.4313,
      "step": 43200
    },
    {
      "epoch": 0.8889140323853949,
      "grad_norm": 0.41941511631011963,
      "learning_rate": 0.0001650564063669726,
      "loss": 2.4793,
      "step": 43210
    },
    {
      "epoch": 0.8891197519022627,
      "grad_norm": 0.39829400181770325,
      "learning_rate": 0.00016503953800732848,
      "loss": 2.4095,
      "step": 43220
    },
    {
      "epoch": 0.8893254714191303,
      "grad_norm": 0.4124215543270111,
      "learning_rate": 0.00016502266643963839,
      "loss": 2.4225,
      "step": 43230
    },
    {
      "epoch": 0.8895311909359981,
      "grad_norm": 0.3878476023674011,
      "learning_rate": 0.00016500579166473447,
      "loss": 2.4131,
      "step": 43240
    },
    {
      "epoch": 0.8897369104528658,
      "grad_norm": 0.42004910111427307,
      "learning_rate": 0.00016498891368344905,
      "loss": 2.4709,
      "step": 43250
    },
    {
      "epoch": 0.8899426299697335,
      "grad_norm": 0.40402689576148987,
      "learning_rate": 0.00016497203249661474,
      "loss": 2.4645,
      "step": 43260
    },
    {
      "epoch": 0.8901483494866013,
      "grad_norm": 0.3896211087703705,
      "learning_rate": 0.00016495514810506404,
      "loss": 2.4053,
      "step": 43270
    },
    {
      "epoch": 0.8903540690034689,
      "grad_norm": 0.622675895690918,
      "learning_rate": 0.0001649382605096299,
      "loss": 2.4646,
      "step": 43280
    },
    {
      "epoch": 0.8905597885203367,
      "grad_norm": 0.3882512152194977,
      "learning_rate": 0.00016492136971114522,
      "loss": 2.442,
      "step": 43290
    },
    {
      "epoch": 0.8907655080372043,
      "grad_norm": 0.41373103857040405,
      "learning_rate": 0.00016490447571044316,
      "loss": 2.4079,
      "step": 43300
    },
    {
      "epoch": 0.8909712275540721,
      "grad_norm": 0.4103577136993408,
      "learning_rate": 0.000164887578508357,
      "loss": 2.3979,
      "step": 43310
    },
    {
      "epoch": 0.8911769470709398,
      "grad_norm": 0.4195140600204468,
      "learning_rate": 0.00016487067810572022,
      "loss": 2.4242,
      "step": 43320
    },
    {
      "epoch": 0.8913826665878075,
      "grad_norm": 0.44405344128608704,
      "learning_rate": 0.00016485377450336635,
      "loss": 2.4223,
      "step": 43330
    },
    {
      "epoch": 0.8915883861046753,
      "grad_norm": 0.5358189940452576,
      "learning_rate": 0.00016483686770212922,
      "loss": 2.4042,
      "step": 43340
    },
    {
      "epoch": 0.8917941056215429,
      "grad_norm": 0.4478603005409241,
      "learning_rate": 0.0001648199577028427,
      "loss": 2.4206,
      "step": 43350
    },
    {
      "epoch": 0.8919998251384107,
      "grad_norm": 0.4043124318122864,
      "learning_rate": 0.00016480304450634093,
      "loss": 2.4629,
      "step": 43360
    },
    {
      "epoch": 0.8922055446552783,
      "grad_norm": 0.38760802149772644,
      "learning_rate": 0.00016478612811345804,
      "loss": 2.4377,
      "step": 43370
    },
    {
      "epoch": 0.8924112641721461,
      "grad_norm": 0.4324934780597687,
      "learning_rate": 0.00016476920852502854,
      "loss": 2.3826,
      "step": 43380
    },
    {
      "epoch": 0.8926169836890138,
      "grad_norm": 0.39500677585601807,
      "learning_rate": 0.00016475228574188694,
      "loss": 2.4327,
      "step": 43390
    },
    {
      "epoch": 0.8928227032058815,
      "grad_norm": 0.3994894027709961,
      "learning_rate": 0.00016473535976486794,
      "loss": 2.4039,
      "step": 43400
    },
    {
      "epoch": 0.8930284227227492,
      "grad_norm": 0.3811803460121155,
      "learning_rate": 0.00016471843059480637,
      "loss": 2.3999,
      "step": 43410
    },
    {
      "epoch": 0.893234142239617,
      "grad_norm": 0.39719435572624207,
      "learning_rate": 0.00016470149823253732,
      "loss": 2.387,
      "step": 43420
    },
    {
      "epoch": 0.8934398617564847,
      "grad_norm": 0.3989470303058624,
      "learning_rate": 0.00016468456267889595,
      "loss": 2.3569,
      "step": 43430
    },
    {
      "epoch": 0.8936455812733524,
      "grad_norm": 0.3605969548225403,
      "learning_rate": 0.00016466762393471757,
      "loss": 2.4422,
      "step": 43440
    },
    {
      "epoch": 0.8938513007902201,
      "grad_norm": 0.4102126657962799,
      "learning_rate": 0.00016465068200083774,
      "loss": 2.4217,
      "step": 43450
    },
    {
      "epoch": 0.8940570203070878,
      "grad_norm": 0.4311443269252777,
      "learning_rate": 0.000164633736878092,
      "loss": 2.4301,
      "step": 43460
    },
    {
      "epoch": 0.8942627398239555,
      "grad_norm": 0.39840182662010193,
      "learning_rate": 0.00016461678856731631,
      "loss": 2.4679,
      "step": 43470
    },
    {
      "epoch": 0.8944684593408232,
      "grad_norm": 0.4339863061904907,
      "learning_rate": 0.0001645998370693465,
      "loss": 2.4365,
      "step": 43480
    },
    {
      "epoch": 0.894674178857691,
      "grad_norm": 0.4289488196372986,
      "learning_rate": 0.0001645828823850188,
      "loss": 2.4276,
      "step": 43490
    },
    {
      "epoch": 0.8948798983745586,
      "grad_norm": 0.3842531144618988,
      "learning_rate": 0.00016456592451516942,
      "loss": 2.4713,
      "step": 43500
    },
    {
      "epoch": 0.8950856178914264,
      "grad_norm": 0.3962309956550598,
      "learning_rate": 0.00016454896346063482,
      "loss": 2.4095,
      "step": 43510
    },
    {
      "epoch": 0.8952913374082941,
      "grad_norm": 0.4956485331058502,
      "learning_rate": 0.0001645319992222516,
      "loss": 2.4512,
      "step": 43520
    },
    {
      "epoch": 0.8954970569251618,
      "grad_norm": 0.5139660835266113,
      "learning_rate": 0.00016451503180085655,
      "loss": 2.4344,
      "step": 43530
    },
    {
      "epoch": 0.8957027764420296,
      "grad_norm": 0.38346999883651733,
      "learning_rate": 0.00016449806119728653,
      "loss": 2.4163,
      "step": 43540
    },
    {
      "epoch": 0.8959084959588972,
      "grad_norm": 0.39796707034111023,
      "learning_rate": 0.0001644810874123786,
      "loss": 2.4305,
      "step": 43550
    },
    {
      "epoch": 0.896114215475765,
      "grad_norm": 0.4211248457431793,
      "learning_rate": 0.00016446411044697003,
      "loss": 2.4123,
      "step": 43560
    },
    {
      "epoch": 0.8963199349926326,
      "grad_norm": 0.40999630093574524,
      "learning_rate": 0.00016444713030189817,
      "loss": 2.3851,
      "step": 43570
    },
    {
      "epoch": 0.8965256545095004,
      "grad_norm": 0.38635584712028503,
      "learning_rate": 0.00016443014697800057,
      "loss": 2.4261,
      "step": 43580
    },
    {
      "epoch": 0.896731374026368,
      "grad_norm": 0.3773595094680786,
      "learning_rate": 0.00016441316047611493,
      "loss": 2.4326,
      "step": 43590
    },
    {
      "epoch": 0.8969370935432358,
      "grad_norm": 0.407809317111969,
      "learning_rate": 0.0001643961707970791,
      "loss": 2.4528,
      "step": 43600
    },
    {
      "epoch": 0.8971428130601036,
      "grad_norm": 0.3865322172641754,
      "learning_rate": 0.00016437917794173104,
      "loss": 2.4208,
      "step": 43610
    },
    {
      "epoch": 0.8973485325769712,
      "grad_norm": 0.38812220096588135,
      "learning_rate": 0.000164362181910909,
      "loss": 2.406,
      "step": 43620
    },
    {
      "epoch": 0.897554252093839,
      "grad_norm": 0.40133771300315857,
      "learning_rate": 0.00016434518270545123,
      "loss": 2.4045,
      "step": 43630
    },
    {
      "epoch": 0.8977599716107066,
      "grad_norm": 0.366186261177063,
      "learning_rate": 0.00016432818032619627,
      "loss": 2.4279,
      "step": 43640
    },
    {
      "epoch": 0.8979656911275744,
      "grad_norm": 0.38801291584968567,
      "learning_rate": 0.0001643111747739827,
      "loss": 2.4772,
      "step": 43650
    },
    {
      "epoch": 0.8981714106444421,
      "grad_norm": 0.3831966817378998,
      "learning_rate": 0.00016429416604964931,
      "loss": 2.4327,
      "step": 43660
    },
    {
      "epoch": 0.8983771301613098,
      "grad_norm": 0.379069447517395,
      "learning_rate": 0.0001642771541540351,
      "loss": 2.4028,
      "step": 43670
    },
    {
      "epoch": 0.8985828496781775,
      "grad_norm": 0.5399598479270935,
      "learning_rate": 0.00016426013908797912,
      "loss": 2.4326,
      "step": 43680
    },
    {
      "epoch": 0.8987885691950452,
      "grad_norm": 0.39234164357185364,
      "learning_rate": 0.00016424312085232067,
      "loss": 2.4124,
      "step": 43690
    },
    {
      "epoch": 0.898994288711913,
      "grad_norm": 0.4051510691642761,
      "learning_rate": 0.00016422609944789914,
      "loss": 2.4226,
      "step": 43700
    },
    {
      "epoch": 0.8992000082287807,
      "grad_norm": 0.3913564682006836,
      "learning_rate": 0.00016420907487555415,
      "loss": 2.4232,
      "step": 43710
    },
    {
      "epoch": 0.8994057277456484,
      "grad_norm": 0.432186096906662,
      "learning_rate": 0.00016419204713612537,
      "loss": 2.4691,
      "step": 43720
    },
    {
      "epoch": 0.8996114472625161,
      "grad_norm": 0.3918248116970062,
      "learning_rate": 0.00016417501623045268,
      "loss": 2.406,
      "step": 43730
    },
    {
      "epoch": 0.8998171667793838,
      "grad_norm": 0.37398162484169006,
      "learning_rate": 0.00016415798215937622,
      "loss": 2.3938,
      "step": 43740
    },
    {
      "epoch": 0.9000228862962515,
      "grad_norm": 0.4271586835384369,
      "learning_rate": 0.00016414094492373606,
      "loss": 2.4303,
      "step": 43750
    },
    {
      "epoch": 0.9002286058131193,
      "grad_norm": 0.552924633026123,
      "learning_rate": 0.00016412390452437263,
      "loss": 2.4544,
      "step": 43760
    },
    {
      "epoch": 0.900434325329987,
      "grad_norm": 0.4030751883983612,
      "learning_rate": 0.00016410686096212645,
      "loss": 2.5401,
      "step": 43770
    },
    {
      "epoch": 0.9006400448468547,
      "grad_norm": 0.3989793658256531,
      "learning_rate": 0.00016408981423783812,
      "loss": 2.4647,
      "step": 43780
    },
    {
      "epoch": 0.9008457643637224,
      "grad_norm": 0.40146759152412415,
      "learning_rate": 0.00016407276435234853,
      "loss": 2.3721,
      "step": 43790
    },
    {
      "epoch": 0.9010514838805901,
      "grad_norm": 0.4308350384235382,
      "learning_rate": 0.00016405571130649862,
      "loss": 2.4263,
      "step": 43800
    },
    {
      "epoch": 0.9012572033974579,
      "grad_norm": 0.43132540583610535,
      "learning_rate": 0.0001640386551011295,
      "loss": 2.4159,
      "step": 43810
    },
    {
      "epoch": 0.9014629229143255,
      "grad_norm": 0.40300583839416504,
      "learning_rate": 0.00016402159573708253,
      "loss": 2.4305,
      "step": 43820
    },
    {
      "epoch": 0.9016686424311933,
      "grad_norm": 0.4737556576728821,
      "learning_rate": 0.0001640045332151991,
      "loss": 2.4453,
      "step": 43830
    },
    {
      "epoch": 0.9018743619480609,
      "grad_norm": 0.44896966218948364,
      "learning_rate": 0.00016398746753632084,
      "loss": 2.4453,
      "step": 43840
    },
    {
      "epoch": 0.9020800814649287,
      "grad_norm": 0.422816663980484,
      "learning_rate": 0.0001639703987012895,
      "loss": 2.4046,
      "step": 43850
    },
    {
      "epoch": 0.9022858009817964,
      "grad_norm": 0.3713762164115906,
      "learning_rate": 0.00016395332671094697,
      "loss": 2.4624,
      "step": 43860
    },
    {
      "epoch": 0.9024915204986641,
      "grad_norm": 0.3791710138320923,
      "learning_rate": 0.00016393625156613538,
      "loss": 2.4118,
      "step": 43870
    },
    {
      "epoch": 0.9026972400155319,
      "grad_norm": 0.3673021197319031,
      "learning_rate": 0.00016391917326769688,
      "loss": 2.4143,
      "step": 43880
    },
    {
      "epoch": 0.9029029595323995,
      "grad_norm": 0.45343056321144104,
      "learning_rate": 0.00016390209181647386,
      "loss": 2.425,
      "step": 43890
    },
    {
      "epoch": 0.9031086790492673,
      "grad_norm": 0.40608924627304077,
      "learning_rate": 0.0001638850072133089,
      "loss": 2.4235,
      "step": 43900
    },
    {
      "epoch": 0.9033143985661349,
      "grad_norm": 0.3783380091190338,
      "learning_rate": 0.00016386791945904468,
      "loss": 2.4139,
      "step": 43910
    },
    {
      "epoch": 0.9035201180830027,
      "grad_norm": 0.38828733563423157,
      "learning_rate": 0.00016385082855452403,
      "loss": 2.45,
      "step": 43920
    },
    {
      "epoch": 0.9037258375998704,
      "grad_norm": 0.4110782742500305,
      "learning_rate": 0.00016383373450058994,
      "loss": 2.4517,
      "step": 43930
    },
    {
      "epoch": 0.9039315571167381,
      "grad_norm": 0.38445204496383667,
      "learning_rate": 0.00016381663729808557,
      "loss": 2.3776,
      "step": 43940
    },
    {
      "epoch": 0.9041372766336059,
      "grad_norm": 0.40519222617149353,
      "learning_rate": 0.00016379953694785424,
      "loss": 2.4163,
      "step": 43950
    },
    {
      "epoch": 0.9043429961504735,
      "grad_norm": 0.4362190067768097,
      "learning_rate": 0.00016378243345073945,
      "loss": 2.4145,
      "step": 43960
    },
    {
      "epoch": 0.9045487156673413,
      "grad_norm": 0.4311775863170624,
      "learning_rate": 0.00016376532680758475,
      "loss": 2.4022,
      "step": 43970
    },
    {
      "epoch": 0.904754435184209,
      "grad_norm": 0.37866121530532837,
      "learning_rate": 0.00016374821701923398,
      "loss": 2.4588,
      "step": 43980
    },
    {
      "epoch": 0.9049601547010767,
      "grad_norm": 0.40924257040023804,
      "learning_rate": 0.00016373110408653103,
      "loss": 2.4262,
      "step": 43990
    },
    {
      "epoch": 0.9051658742179444,
      "grad_norm": 0.39211466908454895,
      "learning_rate": 0.00016371398801032002,
      "loss": 2.3715,
      "step": 44000
    },
    {
      "epoch": 0.9053715937348121,
      "grad_norm": 0.3801810145378113,
      "learning_rate": 0.0001636968687914452,
      "loss": 2.45,
      "step": 44010
    },
    {
      "epoch": 0.9055773132516798,
      "grad_norm": 0.431463360786438,
      "learning_rate": 0.0001636797464307509,
      "loss": 2.4625,
      "step": 44020
    },
    {
      "epoch": 0.9057830327685475,
      "grad_norm": 0.39570876955986023,
      "learning_rate": 0.00016366262092908177,
      "loss": 2.4675,
      "step": 44030
    },
    {
      "epoch": 0.9059887522854153,
      "grad_norm": 0.4008672833442688,
      "learning_rate": 0.00016364549228728244,
      "loss": 2.3861,
      "step": 44040
    },
    {
      "epoch": 0.906194471802283,
      "grad_norm": 0.41283851861953735,
      "learning_rate": 0.00016362836050619786,
      "loss": 2.3753,
      "step": 44050
    },
    {
      "epoch": 0.9064001913191507,
      "grad_norm": 0.3955974578857422,
      "learning_rate": 0.00016361122558667293,
      "loss": 2.5012,
      "step": 44060
    },
    {
      "epoch": 0.9066059108360184,
      "grad_norm": 0.36705222725868225,
      "learning_rate": 0.0001635940875295529,
      "loss": 2.408,
      "step": 44070
    },
    {
      "epoch": 0.9068116303528861,
      "grad_norm": 0.43276745080947876,
      "learning_rate": 0.00016357694633568307,
      "loss": 2.4029,
      "step": 44080
    },
    {
      "epoch": 0.9070173498697538,
      "grad_norm": 0.4480190575122833,
      "learning_rate": 0.00016355980200590895,
      "loss": 2.3925,
      "step": 44090
    },
    {
      "epoch": 0.9072230693866216,
      "grad_norm": 0.4237850308418274,
      "learning_rate": 0.00016354265454107613,
      "loss": 2.4321,
      "step": 44100
    },
    {
      "epoch": 0.9074287889034892,
      "grad_norm": 0.39986997842788696,
      "learning_rate": 0.00016352550394203044,
      "loss": 2.3816,
      "step": 44110
    },
    {
      "epoch": 0.907634508420357,
      "grad_norm": 0.403833270072937,
      "learning_rate": 0.00016350835020961782,
      "loss": 2.4296,
      "step": 44120
    },
    {
      "epoch": 0.9078402279372247,
      "grad_norm": 0.4479232430458069,
      "learning_rate": 0.00016349119334468435,
      "loss": 2.3775,
      "step": 44130
    },
    {
      "epoch": 0.9080459474540924,
      "grad_norm": 0.43116623163223267,
      "learning_rate": 0.0001634740333480763,
      "loss": 2.4233,
      "step": 44140
    },
    {
      "epoch": 0.9082516669709602,
      "grad_norm": 0.4141734838485718,
      "learning_rate": 0.0001634568702206401,
      "loss": 2.3839,
      "step": 44150
    },
    {
      "epoch": 0.9084573864878278,
      "grad_norm": 0.427816778421402,
      "learning_rate": 0.00016343970396322225,
      "loss": 2.4166,
      "step": 44160
    },
    {
      "epoch": 0.9086631060046956,
      "grad_norm": 0.3903332054615021,
      "learning_rate": 0.0001634225345766695,
      "loss": 2.4581,
      "step": 44170
    },
    {
      "epoch": 0.9088688255215632,
      "grad_norm": 0.4288788437843323,
      "learning_rate": 0.00016340536206182877,
      "loss": 2.3994,
      "step": 44180
    },
    {
      "epoch": 0.909074545038431,
      "grad_norm": 0.3855397403240204,
      "learning_rate": 0.00016338818641954705,
      "loss": 2.3906,
      "step": 44190
    },
    {
      "epoch": 0.9092802645552986,
      "grad_norm": 0.3910064399242401,
      "learning_rate": 0.0001633710076506715,
      "loss": 2.3782,
      "step": 44200
    },
    {
      "epoch": 0.9094859840721664,
      "grad_norm": 0.39401957392692566,
      "learning_rate": 0.00016335382575604948,
      "loss": 2.4626,
      "step": 44210
    },
    {
      "epoch": 0.9096917035890342,
      "grad_norm": 0.3938751518726349,
      "learning_rate": 0.0001633366407365285,
      "loss": 2.4131,
      "step": 44220
    },
    {
      "epoch": 0.9098974231059018,
      "grad_norm": 0.4199388325214386,
      "learning_rate": 0.00016331945259295613,
      "loss": 2.4228,
      "step": 44230
    },
    {
      "epoch": 0.9101031426227696,
      "grad_norm": 0.41914793848991394,
      "learning_rate": 0.00016330226132618022,
      "loss": 2.442,
      "step": 44240
    },
    {
      "epoch": 0.9103088621396372,
      "grad_norm": 0.3888237774372101,
      "learning_rate": 0.00016328506693704875,
      "loss": 2.4454,
      "step": 44250
    },
    {
      "epoch": 0.910514581656505,
      "grad_norm": 0.4000997543334961,
      "learning_rate": 0.00016326786942640973,
      "loss": 2.4137,
      "step": 44260
    },
    {
      "epoch": 0.9107203011733727,
      "grad_norm": 0.3893219828605652,
      "learning_rate": 0.0001632506687951115,
      "loss": 2.4348,
      "step": 44270
    },
    {
      "epoch": 0.9109260206902404,
      "grad_norm": 0.41475433111190796,
      "learning_rate": 0.0001632334650440025,
      "loss": 2.5038,
      "step": 44280
    },
    {
      "epoch": 0.9111317402071081,
      "grad_norm": 0.5047181844711304,
      "learning_rate": 0.0001632162581739312,
      "loss": 2.47,
      "step": 44290
    },
    {
      "epoch": 0.9113374597239758,
      "grad_norm": 0.4104677438735962,
      "learning_rate": 0.00016319904818574638,
      "loss": 2.4794,
      "step": 44300
    },
    {
      "epoch": 0.9115431792408436,
      "grad_norm": 0.4008010923862457,
      "learning_rate": 0.00016318183508029694,
      "loss": 2.4525,
      "step": 44310
    },
    {
      "epoch": 0.9117488987577113,
      "grad_norm": 0.4312652051448822,
      "learning_rate": 0.00016316461885843185,
      "loss": 2.4352,
      "step": 44320
    },
    {
      "epoch": 0.911954618274579,
      "grad_norm": 0.4326065182685852,
      "learning_rate": 0.0001631473995210003,
      "loss": 2.4391,
      "step": 44330
    },
    {
      "epoch": 0.9121603377914467,
      "grad_norm": 0.48159080743789673,
      "learning_rate": 0.00016313017706885168,
      "loss": 2.4224,
      "step": 44340
    },
    {
      "epoch": 0.9123660573083144,
      "grad_norm": 0.39835628867149353,
      "learning_rate": 0.00016311295150283543,
      "loss": 2.4754,
      "step": 44350
    },
    {
      "epoch": 0.9125717768251821,
      "grad_norm": 0.3872421085834503,
      "learning_rate": 0.0001630957228238012,
      "loss": 2.4564,
      "step": 44360
    },
    {
      "epoch": 0.9127774963420499,
      "grad_norm": 0.4097432792186737,
      "learning_rate": 0.0001630784910325988,
      "loss": 2.4292,
      "step": 44370
    },
    {
      "epoch": 0.9129832158589175,
      "grad_norm": 0.41469457745552063,
      "learning_rate": 0.00016306125613007815,
      "loss": 2.4492,
      "step": 44380
    },
    {
      "epoch": 0.9131889353757853,
      "grad_norm": 0.3954777717590332,
      "learning_rate": 0.0001630440181170894,
      "loss": 2.4699,
      "step": 44390
    },
    {
      "epoch": 0.913394654892653,
      "grad_norm": 0.3900609016418457,
      "learning_rate": 0.00016302677699448279,
      "loss": 2.418,
      "step": 44400
    },
    {
      "epoch": 0.9136003744095207,
      "grad_norm": 0.37133264541625977,
      "learning_rate": 0.00016300953276310873,
      "loss": 2.4153,
      "step": 44410
    },
    {
      "epoch": 0.9138060939263885,
      "grad_norm": 0.4237765371799469,
      "learning_rate": 0.00016299228542381776,
      "loss": 2.474,
      "step": 44420
    },
    {
      "epoch": 0.9140118134432561,
      "grad_norm": 0.43731480836868286,
      "learning_rate": 0.00016297503497746062,
      "loss": 2.3824,
      "step": 44430
    },
    {
      "epoch": 0.9142175329601239,
      "grad_norm": 0.43814051151275635,
      "learning_rate": 0.0001629577814248882,
      "loss": 2.4576,
      "step": 44440
    },
    {
      "epoch": 0.9144232524769915,
      "grad_norm": 0.4388258457183838,
      "learning_rate": 0.00016294052476695147,
      "loss": 2.3684,
      "step": 44450
    },
    {
      "epoch": 0.9146289719938593,
      "grad_norm": 0.40324899554252625,
      "learning_rate": 0.00016292326500450164,
      "loss": 2.4328,
      "step": 44460
    },
    {
      "epoch": 0.9148346915107269,
      "grad_norm": 0.4061967134475708,
      "learning_rate": 0.00016290600213839004,
      "loss": 2.4273,
      "step": 44470
    },
    {
      "epoch": 0.9150404110275947,
      "grad_norm": 0.46507078409194946,
      "learning_rate": 0.0001628887361694682,
      "loss": 2.4692,
      "step": 44480
    },
    {
      "epoch": 0.9152461305444625,
      "grad_norm": 0.5531451106071472,
      "learning_rate": 0.00016287146709858768,
      "loss": 2.414,
      "step": 44490
    },
    {
      "epoch": 0.9154518500613301,
      "grad_norm": 0.41444131731987,
      "learning_rate": 0.00016285419492660026,
      "loss": 2.4108,
      "step": 44500
    },
    {
      "epoch": 0.9156575695781979,
      "grad_norm": 0.3976403772830963,
      "learning_rate": 0.00016283691965435795,
      "loss": 2.3992,
      "step": 44510
    },
    {
      "epoch": 0.9158632890950655,
      "grad_norm": 0.39577463269233704,
      "learning_rate": 0.0001628196412827128,
      "loss": 2.3968,
      "step": 44520
    },
    {
      "epoch": 0.9160690086119333,
      "grad_norm": 0.47197386622428894,
      "learning_rate": 0.00016280235981251705,
      "loss": 2.3904,
      "step": 44530
    },
    {
      "epoch": 0.916274728128801,
      "grad_norm": 0.47670936584472656,
      "learning_rate": 0.00016278507524462318,
      "loss": 2.397,
      "step": 44540
    },
    {
      "epoch": 0.9164804476456687,
      "grad_norm": 0.41684064269065857,
      "learning_rate": 0.00016276778757988366,
      "loss": 2.4031,
      "step": 44550
    },
    {
      "epoch": 0.9166861671625364,
      "grad_norm": 0.395794540643692,
      "learning_rate": 0.0001627504968191512,
      "loss": 2.4891,
      "step": 44560
    },
    {
      "epoch": 0.9168918866794041,
      "grad_norm": 0.4084272086620331,
      "learning_rate": 0.0001627332029632787,
      "loss": 2.4534,
      "step": 44570
    },
    {
      "epoch": 0.9170976061962719,
      "grad_norm": 0.4318248927593231,
      "learning_rate": 0.00016271590601311915,
      "loss": 2.3874,
      "step": 44580
    },
    {
      "epoch": 0.9173033257131396,
      "grad_norm": 0.48660966753959656,
      "learning_rate": 0.00016269860596952572,
      "loss": 2.4069,
      "step": 44590
    },
    {
      "epoch": 0.9175090452300073,
      "grad_norm": 0.40580445528030396,
      "learning_rate": 0.00016268130283335173,
      "loss": 2.3882,
      "step": 44600
    },
    {
      "epoch": 0.917714764746875,
      "grad_norm": 0.4109639823436737,
      "learning_rate": 0.00016266399660545063,
      "loss": 2.377,
      "step": 44610
    },
    {
      "epoch": 0.9179204842637427,
      "grad_norm": 0.3770707845687866,
      "learning_rate": 0.00016264668728667604,
      "loss": 2.4258,
      "step": 44620
    },
    {
      "epoch": 0.9181262037806104,
      "grad_norm": 0.3722958564758301,
      "learning_rate": 0.0001626293748778818,
      "loss": 2.4749,
      "step": 44630
    },
    {
      "epoch": 0.9183319232974781,
      "grad_norm": 0.5172194838523865,
      "learning_rate": 0.00016261205937992176,
      "loss": 2.3912,
      "step": 44640
    },
    {
      "epoch": 0.9185376428143458,
      "grad_norm": 0.4040628969669342,
      "learning_rate": 0.00016259474079365004,
      "loss": 2.4057,
      "step": 44650
    },
    {
      "epoch": 0.9187433623312136,
      "grad_norm": 0.4008595645427704,
      "learning_rate": 0.0001625774191199208,
      "loss": 2.3879,
      "step": 44660
    },
    {
      "epoch": 0.9189490818480813,
      "grad_norm": 0.40021827816963196,
      "learning_rate": 0.00016256009435958854,
      "loss": 2.4274,
      "step": 44670
    },
    {
      "epoch": 0.919154801364949,
      "grad_norm": 0.5007359385490417,
      "learning_rate": 0.0001625427665135077,
      "loss": 2.437,
      "step": 44680
    },
    {
      "epoch": 0.9193605208818167,
      "grad_norm": 0.4489557147026062,
      "learning_rate": 0.00016252543558253305,
      "loss": 2.4033,
      "step": 44690
    },
    {
      "epoch": 0.9195662403986844,
      "grad_norm": 0.42240700125694275,
      "learning_rate": 0.00016250810156751935,
      "loss": 2.4417,
      "step": 44700
    },
    {
      "epoch": 0.9197719599155522,
      "grad_norm": 0.40151116251945496,
      "learning_rate": 0.00016249076446932164,
      "loss": 2.4075,
      "step": 44710
    },
    {
      "epoch": 0.9199776794324198,
      "grad_norm": 0.4259032905101776,
      "learning_rate": 0.00016247342428879504,
      "loss": 2.4465,
      "step": 44720
    },
    {
      "epoch": 0.9201833989492876,
      "grad_norm": 0.4073300361633301,
      "learning_rate": 0.00016245608102679487,
      "loss": 2.4684,
      "step": 44730
    },
    {
      "epoch": 0.9203891184661552,
      "grad_norm": 0.42174920439720154,
      "learning_rate": 0.00016243873468417655,
      "loss": 2.4188,
      "step": 44740
    },
    {
      "epoch": 0.920594837983023,
      "grad_norm": 0.4472711384296417,
      "learning_rate": 0.00016242138526179574,
      "loss": 2.4285,
      "step": 44750
    },
    {
      "epoch": 0.9208005574998908,
      "grad_norm": 0.3755660057067871,
      "learning_rate": 0.00016240403276050812,
      "loss": 2.3982,
      "step": 44760
    },
    {
      "epoch": 0.9210062770167584,
      "grad_norm": 0.3944527506828308,
      "learning_rate": 0.00016238667718116963,
      "loss": 2.4631,
      "step": 44770
    },
    {
      "epoch": 0.9212119965336262,
      "grad_norm": 0.39591121673583984,
      "learning_rate": 0.00016236931852463632,
      "loss": 2.4116,
      "step": 44780
    },
    {
      "epoch": 0.9214177160504938,
      "grad_norm": 0.40534350275993347,
      "learning_rate": 0.0001623519567917644,
      "loss": 2.438,
      "step": 44790
    },
    {
      "epoch": 0.9216234355673616,
      "grad_norm": 0.43807393312454224,
      "learning_rate": 0.00016233459198341024,
      "loss": 2.4041,
      "step": 44800
    },
    {
      "epoch": 0.9218291550842292,
      "grad_norm": 0.38392356038093567,
      "learning_rate": 0.0001623172241004303,
      "loss": 2.4569,
      "step": 44810
    },
    {
      "epoch": 0.922034874601097,
      "grad_norm": 0.4095306396484375,
      "learning_rate": 0.00016229985314368132,
      "loss": 2.412,
      "step": 44820
    },
    {
      "epoch": 0.9222405941179647,
      "grad_norm": 0.4000006914138794,
      "learning_rate": 0.00016228247911402008,
      "loss": 2.4056,
      "step": 44830
    },
    {
      "epoch": 0.9224463136348324,
      "grad_norm": 0.4479193091392517,
      "learning_rate": 0.00016226510201230353,
      "loss": 2.4311,
      "step": 44840
    },
    {
      "epoch": 0.9226520331517002,
      "grad_norm": 0.41132423281669617,
      "learning_rate": 0.00016224772183938882,
      "loss": 2.4559,
      "step": 44850
    },
    {
      "epoch": 0.9228577526685678,
      "grad_norm": 0.41148507595062256,
      "learning_rate": 0.00016223033859613317,
      "loss": 2.411,
      "step": 44860
    },
    {
      "epoch": 0.9230634721854356,
      "grad_norm": 0.40064191818237305,
      "learning_rate": 0.00016221295228339405,
      "loss": 2.4179,
      "step": 44870
    },
    {
      "epoch": 0.9232691917023033,
      "grad_norm": 0.4651326537132263,
      "learning_rate": 0.000162195562902029,
      "loss": 2.4213,
      "step": 44880
    },
    {
      "epoch": 0.923474911219171,
      "grad_norm": 0.40003907680511475,
      "learning_rate": 0.0001621781704528958,
      "loss": 2.3736,
      "step": 44890
    },
    {
      "epoch": 0.9236806307360387,
      "grad_norm": 0.4008997678756714,
      "learning_rate": 0.00016216077493685224,
      "loss": 2.4082,
      "step": 44900
    },
    {
      "epoch": 0.9238863502529064,
      "grad_norm": 0.3933095335960388,
      "learning_rate": 0.0001621433763547564,
      "loss": 2.3963,
      "step": 44910
    },
    {
      "epoch": 0.9240920697697741,
      "grad_norm": 0.4226877987384796,
      "learning_rate": 0.00016212597470746646,
      "loss": 2.3869,
      "step": 44920
    },
    {
      "epoch": 0.9242977892866419,
      "grad_norm": 0.42409780621528625,
      "learning_rate": 0.00016210856999584073,
      "loss": 2.4301,
      "step": 44930
    },
    {
      "epoch": 0.9245035088035096,
      "grad_norm": 0.4288674294948578,
      "learning_rate": 0.00016209116222073766,
      "loss": 2.3998,
      "step": 44940
    },
    {
      "epoch": 0.9247092283203773,
      "grad_norm": 0.4499092996120453,
      "learning_rate": 0.00016207375138301594,
      "loss": 2.382,
      "step": 44950
    },
    {
      "epoch": 0.924914947837245,
      "grad_norm": 0.383222371339798,
      "learning_rate": 0.00016205633748353433,
      "loss": 2.4444,
      "step": 44960
    },
    {
      "epoch": 0.9251206673541127,
      "grad_norm": 0.44989103078842163,
      "learning_rate": 0.00016203892052315175,
      "loss": 2.4123,
      "step": 44970
    },
    {
      "epoch": 0.9253263868709805,
      "grad_norm": 0.39378073811531067,
      "learning_rate": 0.00016202150050272728,
      "loss": 2.3949,
      "step": 44980
    },
    {
      "epoch": 0.9255321063878481,
      "grad_norm": 0.4027988910675049,
      "learning_rate": 0.00016200407742312018,
      "loss": 2.4349,
      "step": 44990
    },
    {
      "epoch": 0.9257378259047159,
      "grad_norm": 0.38686278462409973,
      "learning_rate": 0.00016198665128518985,
      "loss": 2.4477,
      "step": 45000
    },
    {
      "epoch": 0.9259435454215835,
      "grad_norm": 0.43214842677116394,
      "learning_rate": 0.00016196922208979575,
      "loss": 2.4081,
      "step": 45010
    },
    {
      "epoch": 0.9261492649384513,
      "grad_norm": 0.5950024127960205,
      "learning_rate": 0.00016195178983779762,
      "loss": 2.3805,
      "step": 45020
    },
    {
      "epoch": 0.926354984455319,
      "grad_norm": 0.39144039154052734,
      "learning_rate": 0.00016193435453005535,
      "loss": 2.4325,
      "step": 45030
    },
    {
      "epoch": 0.9265607039721867,
      "grad_norm": 0.38022926449775696,
      "learning_rate": 0.0001619169161674288,
      "loss": 2.4908,
      "step": 45040
    },
    {
      "epoch": 0.9267664234890545,
      "grad_norm": 0.4307352602481842,
      "learning_rate": 0.00016189947475077823,
      "loss": 2.4217,
      "step": 45050
    },
    {
      "epoch": 0.9269721430059221,
      "grad_norm": 0.3929906189441681,
      "learning_rate": 0.00016188203028096387,
      "loss": 2.3343,
      "step": 45060
    },
    {
      "epoch": 0.9271778625227899,
      "grad_norm": 0.4134463667869568,
      "learning_rate": 0.00016186458275884618,
      "loss": 2.4253,
      "step": 45070
    },
    {
      "epoch": 0.9273835820396575,
      "grad_norm": 0.41401106119155884,
      "learning_rate": 0.00016184713218528573,
      "loss": 2.3981,
      "step": 45080
    },
    {
      "epoch": 0.9275893015565253,
      "grad_norm": 0.4024979770183563,
      "learning_rate": 0.00016182967856114328,
      "loss": 2.4031,
      "step": 45090
    },
    {
      "epoch": 0.927795021073393,
      "grad_norm": 0.4346258342266083,
      "learning_rate": 0.00016181222188727974,
      "loss": 2.4975,
      "step": 45100
    },
    {
      "epoch": 0.9280007405902607,
      "grad_norm": 0.3875690698623657,
      "learning_rate": 0.0001617947621645561,
      "loss": 2.4459,
      "step": 45110
    },
    {
      "epoch": 0.9282064601071285,
      "grad_norm": 0.41603749990463257,
      "learning_rate": 0.00016177729939383356,
      "loss": 2.454,
      "step": 45120
    },
    {
      "epoch": 0.9284121796239961,
      "grad_norm": 0.38526004552841187,
      "learning_rate": 0.00016175983357597353,
      "loss": 2.4277,
      "step": 45130
    },
    {
      "epoch": 0.9286178991408639,
      "grad_norm": 0.36654070019721985,
      "learning_rate": 0.00016174236471183746,
      "loss": 2.4283,
      "step": 45140
    },
    {
      "epoch": 0.9288236186577316,
      "grad_norm": 0.4164451062679291,
      "learning_rate": 0.000161724892802287,
      "loss": 2.4139,
      "step": 45150
    },
    {
      "epoch": 0.9290293381745993,
      "grad_norm": 0.39600229263305664,
      "learning_rate": 0.0001617074178481839,
      "loss": 2.4304,
      "step": 45160
    },
    {
      "epoch": 0.929235057691467,
      "grad_norm": 0.4542195200920105,
      "learning_rate": 0.00016168993985039016,
      "loss": 2.3989,
      "step": 45170
    },
    {
      "epoch": 0.9294407772083347,
      "grad_norm": 0.38705623149871826,
      "learning_rate": 0.00016167245880976787,
      "loss": 2.4378,
      "step": 45180
    },
    {
      "epoch": 0.9296464967252024,
      "grad_norm": 0.6203809380531311,
      "learning_rate": 0.00016165497472717924,
      "loss": 2.4592,
      "step": 45190
    },
    {
      "epoch": 0.9298522162420702,
      "grad_norm": 0.4417521357536316,
      "learning_rate": 0.0001616374876034867,
      "loss": 2.4172,
      "step": 45200
    },
    {
      "epoch": 0.9300579357589379,
      "grad_norm": 0.363601416349411,
      "learning_rate": 0.0001616199974395528,
      "loss": 2.4091,
      "step": 45210
    },
    {
      "epoch": 0.9302636552758056,
      "grad_norm": 0.41051197052001953,
      "learning_rate": 0.0001616025042362402,
      "loss": 2.4577,
      "step": 45220
    },
    {
      "epoch": 0.9304693747926733,
      "grad_norm": 0.40143442153930664,
      "learning_rate": 0.00016158500799441175,
      "loss": 2.4562,
      "step": 45230
    },
    {
      "epoch": 0.930675094309541,
      "grad_norm": 0.3935127258300781,
      "learning_rate": 0.00016156750871493047,
      "loss": 2.4042,
      "step": 45240
    },
    {
      "epoch": 0.9308808138264087,
      "grad_norm": 0.38918864727020264,
      "learning_rate": 0.00016155000639865946,
      "loss": 2.4505,
      "step": 45250
    },
    {
      "epoch": 0.9310865333432764,
      "grad_norm": 0.45326024293899536,
      "learning_rate": 0.00016153250104646206,
      "loss": 2.4548,
      "step": 45260
    },
    {
      "epoch": 0.9312922528601442,
      "grad_norm": 0.4021712839603424,
      "learning_rate": 0.0001615149926592017,
      "loss": 2.4732,
      "step": 45270
    },
    {
      "epoch": 0.9314979723770118,
      "grad_norm": 0.46272942423820496,
      "learning_rate": 0.00016149748123774196,
      "loss": 2.4018,
      "step": 45280
    },
    {
      "epoch": 0.9317036918938796,
      "grad_norm": 0.38885149359703064,
      "learning_rate": 0.0001614799667829466,
      "loss": 2.4119,
      "step": 45290
    },
    {
      "epoch": 0.9319094114107473,
      "grad_norm": 0.4501633048057556,
      "learning_rate": 0.0001614624492956795,
      "loss": 2.4212,
      "step": 45300
    },
    {
      "epoch": 0.932115130927615,
      "grad_norm": 0.39743760228157043,
      "learning_rate": 0.0001614449287768047,
      "loss": 2.4611,
      "step": 45310
    },
    {
      "epoch": 0.9323208504444828,
      "grad_norm": 0.3965342044830322,
      "learning_rate": 0.0001614274052271864,
      "loss": 2.4602,
      "step": 45320
    },
    {
      "epoch": 0.9325265699613504,
      "grad_norm": 0.4545183777809143,
      "learning_rate": 0.00016140987864768892,
      "loss": 2.4147,
      "step": 45330
    },
    {
      "epoch": 0.9327322894782182,
      "grad_norm": 0.3968730866909027,
      "learning_rate": 0.00016139234903917682,
      "loss": 2.4346,
      "step": 45340
    },
    {
      "epoch": 0.9329380089950858,
      "grad_norm": 0.3896832764148712,
      "learning_rate": 0.00016137481640251467,
      "loss": 2.3832,
      "step": 45350
    },
    {
      "epoch": 0.9331437285119536,
      "grad_norm": 0.4321286976337433,
      "learning_rate": 0.00016135728073856726,
      "loss": 2.4353,
      "step": 45360
    },
    {
      "epoch": 0.9333494480288212,
      "grad_norm": 0.40752097964286804,
      "learning_rate": 0.00016133974204819956,
      "loss": 2.4037,
      "step": 45370
    },
    {
      "epoch": 0.933555167545689,
      "grad_norm": 0.4136520326137543,
      "learning_rate": 0.00016132220033227666,
      "loss": 2.4334,
      "step": 45380
    },
    {
      "epoch": 0.9337608870625568,
      "grad_norm": 0.3803779184818268,
      "learning_rate": 0.00016130465559166375,
      "loss": 2.4097,
      "step": 45390
    },
    {
      "epoch": 0.9339666065794244,
      "grad_norm": 0.43654850125312805,
      "learning_rate": 0.00016128710782722626,
      "loss": 2.4739,
      "step": 45400
    },
    {
      "epoch": 0.9341723260962922,
      "grad_norm": 0.3918778598308563,
      "learning_rate": 0.00016126955703982973,
      "loss": 2.4729,
      "step": 45410
    },
    {
      "epoch": 0.9343780456131598,
      "grad_norm": 0.42253369092941284,
      "learning_rate": 0.00016125200323033984,
      "loss": 2.4378,
      "step": 45420
    },
    {
      "epoch": 0.9345837651300276,
      "grad_norm": 0.4541076719760895,
      "learning_rate": 0.0001612344463996224,
      "loss": 2.4655,
      "step": 45430
    },
    {
      "epoch": 0.9347894846468953,
      "grad_norm": 0.40717238187789917,
      "learning_rate": 0.0001612168865485434,
      "loss": 2.4375,
      "step": 45440
    },
    {
      "epoch": 0.934995204163763,
      "grad_norm": 0.35671448707580566,
      "learning_rate": 0.00016119932367796898,
      "loss": 2.3971,
      "step": 45450
    },
    {
      "epoch": 0.9352009236806308,
      "grad_norm": 0.45061466097831726,
      "learning_rate": 0.00016118175778876543,
      "loss": 2.4329,
      "step": 45460
    },
    {
      "epoch": 0.9354066431974984,
      "grad_norm": 0.405234694480896,
      "learning_rate": 0.00016116418888179913,
      "loss": 2.4303,
      "step": 45470
    },
    {
      "epoch": 0.9356123627143662,
      "grad_norm": 0.38855165243148804,
      "learning_rate": 0.00016114661695793676,
      "loss": 2.4336,
      "step": 45480
    },
    {
      "epoch": 0.9358180822312339,
      "grad_norm": 0.40322601795196533,
      "learning_rate": 0.00016112904201804495,
      "loss": 2.4254,
      "step": 45490
    },
    {
      "epoch": 0.9360238017481016,
      "grad_norm": 0.39773228764533997,
      "learning_rate": 0.00016111146406299061,
      "loss": 2.4321,
      "step": 45500
    },
    {
      "epoch": 0.9362295212649693,
      "grad_norm": 0.44218361377716064,
      "learning_rate": 0.00016109388309364079,
      "loss": 2.4799,
      "step": 45510
    },
    {
      "epoch": 0.936435240781837,
      "grad_norm": 0.40257441997528076,
      "learning_rate": 0.0001610762991108626,
      "loss": 2.4551,
      "step": 45520
    },
    {
      "epoch": 0.9366409602987047,
      "grad_norm": 0.4183054566383362,
      "learning_rate": 0.00016105871211552342,
      "loss": 2.3703,
      "step": 45530
    },
    {
      "epoch": 0.9368466798155725,
      "grad_norm": 0.4139370024204254,
      "learning_rate": 0.00016104112210849072,
      "loss": 2.4887,
      "step": 45540
    },
    {
      "epoch": 0.9370523993324402,
      "grad_norm": 0.4048835337162018,
      "learning_rate": 0.00016102352909063209,
      "loss": 2.4443,
      "step": 45550
    },
    {
      "epoch": 0.9372581188493079,
      "grad_norm": 0.4383450746536255,
      "learning_rate": 0.00016100593306281528,
      "loss": 2.4391,
      "step": 45560
    },
    {
      "epoch": 0.9374638383661756,
      "grad_norm": 0.4685347378253937,
      "learning_rate": 0.00016098833402590826,
      "loss": 2.4725,
      "step": 45570
    },
    {
      "epoch": 0.9376695578830433,
      "grad_norm": 0.4460226893424988,
      "learning_rate": 0.00016097073198077908,
      "loss": 2.4066,
      "step": 45580
    },
    {
      "epoch": 0.9378752773999111,
      "grad_norm": 0.4231642186641693,
      "learning_rate": 0.00016095312692829592,
      "loss": 2.3998,
      "step": 45590
    },
    {
      "epoch": 0.9380809969167787,
      "grad_norm": 0.40225937962532043,
      "learning_rate": 0.00016093551886932716,
      "loss": 2.4698,
      "step": 45600
    },
    {
      "epoch": 0.9382867164336465,
      "grad_norm": 0.41124898195266724,
      "learning_rate": 0.00016091790780474132,
      "loss": 2.4028,
      "step": 45610
    },
    {
      "epoch": 0.9384924359505141,
      "grad_norm": 0.44942548871040344,
      "learning_rate": 0.00016090029373540707,
      "loss": 2.4461,
      "step": 45620
    },
    {
      "epoch": 0.9386981554673819,
      "grad_norm": 0.4468129575252533,
      "learning_rate": 0.00016088267666219315,
      "loss": 2.3885,
      "step": 45630
    },
    {
      "epoch": 0.9389038749842497,
      "grad_norm": 0.3913700580596924,
      "learning_rate": 0.00016086505658596863,
      "loss": 2.4256,
      "step": 45640
    },
    {
      "epoch": 0.9391095945011173,
      "grad_norm": 0.4111441969871521,
      "learning_rate": 0.00016084743350760247,
      "loss": 2.4247,
      "step": 45650
    },
    {
      "epoch": 0.9393153140179851,
      "grad_norm": 0.42444664239883423,
      "learning_rate": 0.00016082980742796404,
      "loss": 2.427,
      "step": 45660
    },
    {
      "epoch": 0.9395210335348527,
      "grad_norm": 0.4353587329387665,
      "learning_rate": 0.00016081217834792267,
      "loss": 2.3921,
      "step": 45670
    },
    {
      "epoch": 0.9397267530517205,
      "grad_norm": 0.45517095923423767,
      "learning_rate": 0.00016079454626834796,
      "loss": 2.4494,
      "step": 45680
    },
    {
      "epoch": 0.9399324725685881,
      "grad_norm": 0.38007447123527527,
      "learning_rate": 0.00016077691119010954,
      "loss": 2.3728,
      "step": 45690
    },
    {
      "epoch": 0.9401381920854559,
      "grad_norm": 0.42284759879112244,
      "learning_rate": 0.0001607592731140773,
      "loss": 2.4536,
      "step": 45700
    },
    {
      "epoch": 0.9403439116023236,
      "grad_norm": 0.43641266226768494,
      "learning_rate": 0.0001607416320411212,
      "loss": 2.4302,
      "step": 45710
    },
    {
      "epoch": 0.9405496311191913,
      "grad_norm": 0.37246885895729065,
      "learning_rate": 0.00016072398797211142,
      "loss": 2.4578,
      "step": 45720
    },
    {
      "epoch": 0.9407553506360591,
      "grad_norm": 0.36844924092292786,
      "learning_rate": 0.0001607063409079182,
      "loss": 2.4078,
      "step": 45730
    },
    {
      "epoch": 0.9409610701529267,
      "grad_norm": 0.43191927671432495,
      "learning_rate": 0.00016068869084941202,
      "loss": 2.4357,
      "step": 45740
    },
    {
      "epoch": 0.9411667896697945,
      "grad_norm": 0.4818752408027649,
      "learning_rate": 0.0001606710377974634,
      "loss": 2.4597,
      "step": 45750
    },
    {
      "epoch": 0.9413725091866622,
      "grad_norm": 0.40043407678604126,
      "learning_rate": 0.0001606533817529431,
      "loss": 2.4447,
      "step": 45760
    },
    {
      "epoch": 0.9415782287035299,
      "grad_norm": 0.3743778467178345,
      "learning_rate": 0.00016063572271672204,
      "loss": 2.3722,
      "step": 45770
    },
    {
      "epoch": 0.9417839482203976,
      "grad_norm": 0.406446248292923,
      "learning_rate": 0.00016061806068967118,
      "loss": 2.3932,
      "step": 45780
    },
    {
      "epoch": 0.9419896677372653,
      "grad_norm": 0.4033811688423157,
      "learning_rate": 0.0001606003956726617,
      "loss": 2.4212,
      "step": 45790
    },
    {
      "epoch": 0.942195387254133,
      "grad_norm": 0.4641503393650055,
      "learning_rate": 0.00016058272766656496,
      "loss": 2.4315,
      "step": 45800
    },
    {
      "epoch": 0.9424011067710008,
      "grad_norm": 0.36780503392219543,
      "learning_rate": 0.00016056505667225239,
      "loss": 2.4512,
      "step": 45810
    },
    {
      "epoch": 0.9426068262878685,
      "grad_norm": 0.4126056730747223,
      "learning_rate": 0.0001605473826905956,
      "loss": 2.3657,
      "step": 45820
    },
    {
      "epoch": 0.9428125458047362,
      "grad_norm": 0.41673216223716736,
      "learning_rate": 0.0001605297057224664,
      "loss": 2.418,
      "step": 45830
    },
    {
      "epoch": 0.9430182653216039,
      "grad_norm": 0.45855912566185,
      "learning_rate": 0.00016051202576873662,
      "loss": 2.4185,
      "step": 45840
    },
    {
      "epoch": 0.9432239848384716,
      "grad_norm": 0.40323618054389954,
      "learning_rate": 0.0001604943428302784,
      "loss": 2.4548,
      "step": 45850
    },
    {
      "epoch": 0.9434297043553393,
      "grad_norm": 0.40240198373794556,
      "learning_rate": 0.00016047665690796387,
      "loss": 2.3704,
      "step": 45860
    },
    {
      "epoch": 0.943635423872207,
      "grad_norm": 0.3978036642074585,
      "learning_rate": 0.00016045896800266543,
      "loss": 2.465,
      "step": 45870
    },
    {
      "epoch": 0.9438411433890748,
      "grad_norm": 0.4373057186603546,
      "learning_rate": 0.00016044127611525557,
      "loss": 2.3819,
      "step": 45880
    },
    {
      "epoch": 0.9440468629059424,
      "grad_norm": 0.40318289399147034,
      "learning_rate": 0.0001604235812466069,
      "loss": 2.4332,
      "step": 45890
    },
    {
      "epoch": 0.9442525824228102,
      "grad_norm": 0.4214692711830139,
      "learning_rate": 0.00016040588339759227,
      "loss": 2.487,
      "step": 45900
    },
    {
      "epoch": 0.944458301939678,
      "grad_norm": 0.4459076523780823,
      "learning_rate": 0.00016038818256908455,
      "loss": 2.3983,
      "step": 45910
    },
    {
      "epoch": 0.9446640214565456,
      "grad_norm": 0.45043933391571045,
      "learning_rate": 0.00016037047876195688,
      "loss": 2.4169,
      "step": 45920
    },
    {
      "epoch": 0.9448697409734134,
      "grad_norm": 0.39523252844810486,
      "learning_rate": 0.00016035277197708244,
      "loss": 2.4586,
      "step": 45930
    },
    {
      "epoch": 0.945075460490281,
      "grad_norm": 0.4044773578643799,
      "learning_rate": 0.0001603350622153347,
      "loss": 2.4542,
      "step": 45940
    },
    {
      "epoch": 0.9452811800071488,
      "grad_norm": 0.47535210847854614,
      "learning_rate": 0.00016031734947758708,
      "loss": 2.3912,
      "step": 45950
    },
    {
      "epoch": 0.9454868995240164,
      "grad_norm": 0.444095641374588,
      "learning_rate": 0.00016029963376471332,
      "loss": 2.4721,
      "step": 45960
    },
    {
      "epoch": 0.9456926190408842,
      "grad_norm": 0.4229350984096527,
      "learning_rate": 0.0001602819150775872,
      "loss": 2.3574,
      "step": 45970
    },
    {
      "epoch": 0.9458983385577518,
      "grad_norm": 0.40640804171562195,
      "learning_rate": 0.00016026419341708275,
      "loss": 2.4285,
      "step": 45980
    },
    {
      "epoch": 0.9461040580746196,
      "grad_norm": 0.4334167242050171,
      "learning_rate": 0.000160246468784074,
      "loss": 2.4201,
      "step": 45990
    },
    {
      "epoch": 0.9463097775914874,
      "grad_norm": 0.3729149401187897,
      "learning_rate": 0.00016022874117943526,
      "loss": 2.4728,
      "step": 46000
    },
    {
      "epoch": 0.946515497108355,
      "grad_norm": 0.3974701464176178,
      "learning_rate": 0.00016021101060404092,
      "loss": 2.4466,
      "step": 46010
    },
    {
      "epoch": 0.9467212166252228,
      "grad_norm": 0.3905121684074402,
      "learning_rate": 0.00016019327705876554,
      "loss": 2.4085,
      "step": 46020
    },
    {
      "epoch": 0.9469269361420904,
      "grad_norm": 0.39922210574150085,
      "learning_rate": 0.0001601755405444838,
      "loss": 2.4515,
      "step": 46030
    },
    {
      "epoch": 0.9471326556589582,
      "grad_norm": 0.378390908241272,
      "learning_rate": 0.00016015780106207062,
      "loss": 2.4728,
      "step": 46040
    },
    {
      "epoch": 0.9473383751758259,
      "grad_norm": 0.4160323143005371,
      "learning_rate": 0.0001601400586124009,
      "loss": 2.4269,
      "step": 46050
    },
    {
      "epoch": 0.9475440946926936,
      "grad_norm": 0.4172133207321167,
      "learning_rate": 0.0001601223131963498,
      "loss": 2.4509,
      "step": 46060
    },
    {
      "epoch": 0.9477498142095613,
      "grad_norm": 0.41840988397598267,
      "learning_rate": 0.00016010456481479262,
      "loss": 2.4175,
      "step": 46070
    },
    {
      "epoch": 0.947955533726429,
      "grad_norm": 0.3863399028778076,
      "learning_rate": 0.0001600868134686048,
      "loss": 2.4641,
      "step": 46080
    },
    {
      "epoch": 0.9481612532432968,
      "grad_norm": 0.4131001830101013,
      "learning_rate": 0.0001600690591586619,
      "loss": 2.3919,
      "step": 46090
    },
    {
      "epoch": 0.9483669727601645,
      "grad_norm": 0.4208097457885742,
      "learning_rate": 0.00016005130188583965,
      "loss": 2.48,
      "step": 46100
    },
    {
      "epoch": 0.9485726922770322,
      "grad_norm": 0.4651497006416321,
      "learning_rate": 0.0001600335416510139,
      "loss": 2.4165,
      "step": 46110
    },
    {
      "epoch": 0.9487784117938999,
      "grad_norm": 0.416261225938797,
      "learning_rate": 0.0001600157784550607,
      "loss": 2.384,
      "step": 46120
    },
    {
      "epoch": 0.9489841313107676,
      "grad_norm": 0.3900994658470154,
      "learning_rate": 0.0001599980122988562,
      "loss": 2.4371,
      "step": 46130
    },
    {
      "epoch": 0.9491898508276353,
      "grad_norm": 0.4176484942436218,
      "learning_rate": 0.0001599802431832767,
      "loss": 2.3888,
      "step": 46140
    },
    {
      "epoch": 0.9493955703445031,
      "grad_norm": 0.37780022621154785,
      "learning_rate": 0.00015996247110919867,
      "loss": 2.4228,
      "step": 46150
    },
    {
      "epoch": 0.9496012898613707,
      "grad_norm": 0.4166997969150543,
      "learning_rate": 0.00015994469607749867,
      "loss": 2.3895,
      "step": 46160
    },
    {
      "epoch": 0.9498070093782385,
      "grad_norm": 0.5052530169487,
      "learning_rate": 0.0001599269180890535,
      "loss": 2.4076,
      "step": 46170
    },
    {
      "epoch": 0.9500127288951062,
      "grad_norm": 0.42788171768188477,
      "learning_rate": 0.00015990913714474,
      "loss": 2.3746,
      "step": 46180
    },
    {
      "epoch": 0.9502184484119739,
      "grad_norm": 0.4616537392139435,
      "learning_rate": 0.00015989135324543524,
      "loss": 2.4314,
      "step": 46190
    },
    {
      "epoch": 0.9504241679288417,
      "grad_norm": 0.452218621969223,
      "learning_rate": 0.0001598735663920164,
      "loss": 2.4008,
      "step": 46200
    },
    {
      "epoch": 0.9506298874457093,
      "grad_norm": 0.43602684140205383,
      "learning_rate": 0.00015985577658536078,
      "loss": 2.5125,
      "step": 46210
    },
    {
      "epoch": 0.9508356069625771,
      "grad_norm": 0.3942720592021942,
      "learning_rate": 0.0001598379838263459,
      "loss": 2.4325,
      "step": 46220
    },
    {
      "epoch": 0.9510413264794447,
      "grad_norm": 0.39205795526504517,
      "learning_rate": 0.00015982018811584934,
      "loss": 2.4064,
      "step": 46230
    },
    {
      "epoch": 0.9512470459963125,
      "grad_norm": 0.46749287843704224,
      "learning_rate": 0.00015980238945474884,
      "loss": 2.4154,
      "step": 46240
    },
    {
      "epoch": 0.9514527655131801,
      "grad_norm": 0.49049702286720276,
      "learning_rate": 0.00015978458784392244,
      "loss": 2.4103,
      "step": 46250
    },
    {
      "epoch": 0.9516584850300479,
      "grad_norm": 0.4266522526741028,
      "learning_rate": 0.00015976678328424808,
      "loss": 2.4246,
      "step": 46260
    },
    {
      "epoch": 0.9518642045469157,
      "grad_norm": 0.3767358958721161,
      "learning_rate": 0.00015974897577660394,
      "loss": 2.3827,
      "step": 46270
    },
    {
      "epoch": 0.9520699240637833,
      "grad_norm": 0.4132330119609833,
      "learning_rate": 0.00015973116532186848,
      "loss": 2.386,
      "step": 46280
    },
    {
      "epoch": 0.9522756435806511,
      "grad_norm": 0.4051300585269928,
      "learning_rate": 0.0001597133519209201,
      "loss": 2.4148,
      "step": 46290
    },
    {
      "epoch": 0.9524813630975187,
      "grad_norm": 0.3998493552207947,
      "learning_rate": 0.0001596955355746375,
      "loss": 2.4624,
      "step": 46300
    },
    {
      "epoch": 0.9526870826143865,
      "grad_norm": 0.43225210905075073,
      "learning_rate": 0.00015967771628389938,
      "loss": 2.4315,
      "step": 46310
    },
    {
      "epoch": 0.9528928021312542,
      "grad_norm": 0.4025077819824219,
      "learning_rate": 0.00015965989404958476,
      "loss": 2.405,
      "step": 46320
    },
    {
      "epoch": 0.9530985216481219,
      "grad_norm": 0.42374563217163086,
      "learning_rate": 0.00015964206887257264,
      "loss": 2.4344,
      "step": 46330
    },
    {
      "epoch": 0.9533042411649896,
      "grad_norm": 0.43893730640411377,
      "learning_rate": 0.00015962424075374233,
      "loss": 2.4365,
      "step": 46340
    },
    {
      "epoch": 0.9535099606818573,
      "grad_norm": 0.4363638162612915,
      "learning_rate": 0.00015960640969397307,
      "loss": 2.464,
      "step": 46350
    },
    {
      "epoch": 0.9537156801987251,
      "grad_norm": 0.4126207232475281,
      "learning_rate": 0.00015958857569414446,
      "loss": 2.4391,
      "step": 46360
    },
    {
      "epoch": 0.9539213997155928,
      "grad_norm": 0.43631771206855774,
      "learning_rate": 0.00015957073875513613,
      "loss": 2.4324,
      "step": 46370
    },
    {
      "epoch": 0.9541271192324605,
      "grad_norm": 0.41268250346183777,
      "learning_rate": 0.0001595528988778279,
      "loss": 2.3996,
      "step": 46380
    },
    {
      "epoch": 0.9543328387493282,
      "grad_norm": 0.46292105317115784,
      "learning_rate": 0.00015953505606309967,
      "loss": 2.4129,
      "step": 46390
    },
    {
      "epoch": 0.9545385582661959,
      "grad_norm": 0.4712483286857605,
      "learning_rate": 0.00015951721031183156,
      "loss": 2.4445,
      "step": 46400
    },
    {
      "epoch": 0.9547442777830636,
      "grad_norm": 0.40134182572364807,
      "learning_rate": 0.0001594993616249038,
      "loss": 2.4422,
      "step": 46410
    },
    {
      "epoch": 0.9549499972999314,
      "grad_norm": 0.40968409180641174,
      "learning_rate": 0.00015948151000319677,
      "loss": 2.4411,
      "step": 46420
    },
    {
      "epoch": 0.955155716816799,
      "grad_norm": 0.3878123164176941,
      "learning_rate": 0.00015946365544759097,
      "loss": 2.4362,
      "step": 46430
    },
    {
      "epoch": 0.9553614363336668,
      "grad_norm": 0.4207310676574707,
      "learning_rate": 0.00015944579795896705,
      "loss": 2.3745,
      "step": 46440
    },
    {
      "epoch": 0.9555671558505345,
      "grad_norm": 0.4441787600517273,
      "learning_rate": 0.00015942793753820591,
      "loss": 2.4437,
      "step": 46450
    },
    {
      "epoch": 0.9557728753674022,
      "grad_norm": 0.47313547134399414,
      "learning_rate": 0.00015941007418618846,
      "loss": 2.4138,
      "step": 46460
    },
    {
      "epoch": 0.95597859488427,
      "grad_norm": 0.3864361345767975,
      "learning_rate": 0.00015939220790379578,
      "loss": 2.3933,
      "step": 46470
    },
    {
      "epoch": 0.9561843144011376,
      "grad_norm": 0.43004950881004333,
      "learning_rate": 0.0001593743386919091,
      "loss": 2.4248,
      "step": 46480
    },
    {
      "epoch": 0.9563900339180054,
      "grad_norm": 0.4008776843547821,
      "learning_rate": 0.00015935646655140988,
      "loss": 2.4182,
      "step": 46490
    },
    {
      "epoch": 0.956595753434873,
      "grad_norm": 0.4148560166358948,
      "learning_rate": 0.00015933859148317963,
      "loss": 2.4468,
      "step": 46500
    },
    {
      "epoch": 0.9568014729517408,
      "grad_norm": 0.4037519693374634,
      "learning_rate": 0.0001593207134881,
      "loss": 2.3673,
      "step": 46510
    },
    {
      "epoch": 0.9570071924686084,
      "grad_norm": 0.42504462599754333,
      "learning_rate": 0.00015930283256705282,
      "loss": 2.4342,
      "step": 46520
    },
    {
      "epoch": 0.9572129119854762,
      "grad_norm": 0.42430293560028076,
      "learning_rate": 0.0001592849487209201,
      "loss": 2.449,
      "step": 46530
    },
    {
      "epoch": 0.957418631502344,
      "grad_norm": 0.39015108346939087,
      "learning_rate": 0.00015926706195058392,
      "loss": 2.4899,
      "step": 46540
    },
    {
      "epoch": 0.9576243510192116,
      "grad_norm": 0.4346599876880646,
      "learning_rate": 0.00015924917225692653,
      "loss": 2.4084,
      "step": 46550
    },
    {
      "epoch": 0.9578300705360794,
      "grad_norm": 0.3901709318161011,
      "learning_rate": 0.00015923127964083032,
      "loss": 2.4781,
      "step": 46560
    },
    {
      "epoch": 0.958035790052947,
      "grad_norm": 0.4443076252937317,
      "learning_rate": 0.0001592133841031779,
      "loss": 2.376,
      "step": 46570
    },
    {
      "epoch": 0.9582415095698148,
      "grad_norm": 0.42411983013153076,
      "learning_rate": 0.00015919548564485188,
      "loss": 2.3834,
      "step": 46580
    },
    {
      "epoch": 0.9584472290866825,
      "grad_norm": 0.4263920783996582,
      "learning_rate": 0.00015917758426673516,
      "loss": 2.4034,
      "step": 46590
    },
    {
      "epoch": 0.9586529486035502,
      "grad_norm": 0.3846514821052551,
      "learning_rate": 0.00015915967996971066,
      "loss": 2.4016,
      "step": 46600
    },
    {
      "epoch": 0.9588586681204179,
      "grad_norm": 0.3989250361919403,
      "learning_rate": 0.00015914177275466154,
      "loss": 2.4028,
      "step": 46610
    },
    {
      "epoch": 0.9590643876372856,
      "grad_norm": 0.40215808153152466,
      "learning_rate": 0.00015912386262247104,
      "loss": 2.4951,
      "step": 46620
    },
    {
      "epoch": 0.9592701071541534,
      "grad_norm": 0.38684770464897156,
      "learning_rate": 0.00015910594957402262,
      "loss": 2.4258,
      "step": 46630
    },
    {
      "epoch": 0.959475826671021,
      "grad_norm": 0.4923761188983917,
      "learning_rate": 0.00015908803361019977,
      "loss": 2.4267,
      "step": 46640
    },
    {
      "epoch": 0.9596815461878888,
      "grad_norm": 0.40551885962486267,
      "learning_rate": 0.00015907011473188622,
      "loss": 2.4125,
      "step": 46650
    },
    {
      "epoch": 0.9598872657047565,
      "grad_norm": 0.4269237220287323,
      "learning_rate": 0.0001590521929399658,
      "loss": 2.3944,
      "step": 46660
    },
    {
      "epoch": 0.9600929852216242,
      "grad_norm": 0.3880617618560791,
      "learning_rate": 0.0001590342682353225,
      "loss": 2.402,
      "step": 46670
    },
    {
      "epoch": 0.9602987047384919,
      "grad_norm": 0.4720204770565033,
      "learning_rate": 0.00015901634061884042,
      "loss": 2.4226,
      "step": 46680
    },
    {
      "epoch": 0.9605044242553596,
      "grad_norm": 0.39290741086006165,
      "learning_rate": 0.0001589984100914039,
      "loss": 2.4294,
      "step": 46690
    },
    {
      "epoch": 0.9607101437722273,
      "grad_norm": 0.3867391347885132,
      "learning_rate": 0.00015898047665389731,
      "loss": 2.4505,
      "step": 46700
    },
    {
      "epoch": 0.9609158632890951,
      "grad_norm": 0.41064760088920593,
      "learning_rate": 0.00015896254030720521,
      "loss": 2.4456,
      "step": 46710
    },
    {
      "epoch": 0.9611215828059628,
      "grad_norm": 0.447167307138443,
      "learning_rate": 0.00015894460105221226,
      "loss": 2.3895,
      "step": 46720
    },
    {
      "epoch": 0.9613273023228305,
      "grad_norm": 0.38860243558883667,
      "learning_rate": 0.0001589266588898034,
      "loss": 2.4458,
      "step": 46730
    },
    {
      "epoch": 0.9615330218396982,
      "grad_norm": 0.4164426922798157,
      "learning_rate": 0.00015890871382086353,
      "loss": 2.4126,
      "step": 46740
    },
    {
      "epoch": 0.9617387413565659,
      "grad_norm": 0.392238587141037,
      "learning_rate": 0.00015889076584627788,
      "loss": 2.4393,
      "step": 46750
    },
    {
      "epoch": 0.9619444608734337,
      "grad_norm": 0.4372043013572693,
      "learning_rate": 0.0001588728149669316,
      "loss": 2.3907,
      "step": 46760
    },
    {
      "epoch": 0.9621501803903013,
      "grad_norm": 0.40201878547668457,
      "learning_rate": 0.00015885486118371022,
      "loss": 2.4445,
      "step": 46770
    },
    {
      "epoch": 0.9623558999071691,
      "grad_norm": 0.4173728823661804,
      "learning_rate": 0.00015883690449749923,
      "loss": 2.4368,
      "step": 46780
    },
    {
      "epoch": 0.9625616194240367,
      "grad_norm": 0.3798469603061676,
      "learning_rate": 0.0001588189449091844,
      "loss": 2.472,
      "step": 46790
    },
    {
      "epoch": 0.9627673389409045,
      "grad_norm": 0.4256149232387543,
      "learning_rate": 0.0001588009824196515,
      "loss": 2.4377,
      "step": 46800
    },
    {
      "epoch": 0.9629730584577723,
      "grad_norm": 0.39139172434806824,
      "learning_rate": 0.00015878301702978659,
      "loss": 2.4148,
      "step": 46810
    },
    {
      "epoch": 0.9631787779746399,
      "grad_norm": 0.41526171565055847,
      "learning_rate": 0.00015876504874047577,
      "loss": 2.3911,
      "step": 46820
    },
    {
      "epoch": 0.9633844974915077,
      "grad_norm": 0.42728570103645325,
      "learning_rate": 0.00015874707755260533,
      "loss": 2.4546,
      "step": 46830
    },
    {
      "epoch": 0.9635902170083753,
      "grad_norm": 0.4105924069881439,
      "learning_rate": 0.0001587291034670617,
      "loss": 2.4464,
      "step": 46840
    },
    {
      "epoch": 0.9637959365252431,
      "grad_norm": 0.40088725090026855,
      "learning_rate": 0.0001587111264847314,
      "loss": 2.4551,
      "step": 46850
    },
    {
      "epoch": 0.9640016560421107,
      "grad_norm": 0.4229055345058441,
      "learning_rate": 0.0001586931466065012,
      "loss": 2.4816,
      "step": 46860
    },
    {
      "epoch": 0.9642073755589785,
      "grad_norm": 0.4245736598968506,
      "learning_rate": 0.00015867516383325788,
      "loss": 2.4938,
      "step": 46870
    },
    {
      "epoch": 0.9644130950758462,
      "grad_norm": 0.4089774489402771,
      "learning_rate": 0.0001586571781658885,
      "loss": 2.4864,
      "step": 46880
    },
    {
      "epoch": 0.9646188145927139,
      "grad_norm": 0.44832471013069153,
      "learning_rate": 0.00015863918960528018,
      "loss": 2.3924,
      "step": 46890
    },
    {
      "epoch": 0.9648245341095817,
      "grad_norm": 0.4819999039173126,
      "learning_rate": 0.00015862119815232014,
      "loss": 2.4149,
      "step": 46900
    },
    {
      "epoch": 0.9650302536264493,
      "grad_norm": 0.3988876938819885,
      "learning_rate": 0.00015860320380789585,
      "loss": 2.3982,
      "step": 46910
    },
    {
      "epoch": 0.9652359731433171,
      "grad_norm": 0.40608930587768555,
      "learning_rate": 0.0001585852065728949,
      "loss": 2.4747,
      "step": 46920
    },
    {
      "epoch": 0.9654416926601848,
      "grad_norm": 0.4241526126861572,
      "learning_rate": 0.00015856720644820494,
      "loss": 2.4507,
      "step": 46930
    },
    {
      "epoch": 0.9656474121770525,
      "grad_norm": 0.4103597402572632,
      "learning_rate": 0.00015854920343471383,
      "loss": 2.4203,
      "step": 46940
    },
    {
      "epoch": 0.9658531316939202,
      "grad_norm": 0.40694788098335266,
      "learning_rate": 0.0001585311975333096,
      "loss": 2.4353,
      "step": 46950
    },
    {
      "epoch": 0.9660588512107879,
      "grad_norm": 0.41848376393318176,
      "learning_rate": 0.0001585131887448803,
      "loss": 2.4859,
      "step": 46960
    },
    {
      "epoch": 0.9662645707276556,
      "grad_norm": 0.39738690853118896,
      "learning_rate": 0.00015849517707031428,
      "loss": 2.3976,
      "step": 46970
    },
    {
      "epoch": 0.9664702902445234,
      "grad_norm": 0.40542444586753845,
      "learning_rate": 0.00015847716251049995,
      "loss": 2.3894,
      "step": 46980
    },
    {
      "epoch": 0.9666760097613911,
      "grad_norm": 0.4417112171649933,
      "learning_rate": 0.00015845914506632585,
      "loss": 2.4012,
      "step": 46990
    },
    {
      "epoch": 0.9668817292782588,
      "grad_norm": 0.38669490814208984,
      "learning_rate": 0.0001584411247386807,
      "loss": 2.4632,
      "step": 47000
    },
    {
      "epoch": 0.9670874487951265,
      "grad_norm": 0.4272690415382385,
      "learning_rate": 0.0001584231015284533,
      "loss": 2.3954,
      "step": 47010
    },
    {
      "epoch": 0.9672931683119942,
      "grad_norm": 0.44233325123786926,
      "learning_rate": 0.0001584050754365327,
      "loss": 2.4714,
      "step": 47020
    },
    {
      "epoch": 0.967498887828862,
      "grad_norm": 0.44021040201187134,
      "learning_rate": 0.00015838704646380797,
      "loss": 2.4522,
      "step": 47030
    },
    {
      "epoch": 0.9677046073457296,
      "grad_norm": 0.47780466079711914,
      "learning_rate": 0.00015836901461116842,
      "loss": 2.3695,
      "step": 47040
    },
    {
      "epoch": 0.9679103268625974,
      "grad_norm": 0.6560479998588562,
      "learning_rate": 0.00015835097987950347,
      "loss": 2.3933,
      "step": 47050
    },
    {
      "epoch": 0.968116046379465,
      "grad_norm": 0.45059195160865784,
      "learning_rate": 0.00015833294226970263,
      "loss": 2.3763,
      "step": 47060
    },
    {
      "epoch": 0.9683217658963328,
      "grad_norm": 0.39357343316078186,
      "learning_rate": 0.00015831490178265563,
      "loss": 2.3939,
      "step": 47070
    },
    {
      "epoch": 0.9685274854132006,
      "grad_norm": 0.41019734740257263,
      "learning_rate": 0.00015829685841925234,
      "loss": 2.4649,
      "step": 47080
    },
    {
      "epoch": 0.9687332049300682,
      "grad_norm": 0.3972383439540863,
      "learning_rate": 0.00015827881218038266,
      "loss": 2.4398,
      "step": 47090
    },
    {
      "epoch": 0.968938924446936,
      "grad_norm": 0.36317765712738037,
      "learning_rate": 0.00015826076306693678,
      "loss": 2.3786,
      "step": 47100
    },
    {
      "epoch": 0.9691446439638036,
      "grad_norm": 0.3952277898788452,
      "learning_rate": 0.00015824271107980496,
      "loss": 2.4291,
      "step": 47110
    },
    {
      "epoch": 0.9693503634806714,
      "grad_norm": 0.376640647649765,
      "learning_rate": 0.00015822465621987753,
      "loss": 2.4355,
      "step": 47120
    },
    {
      "epoch": 0.969556082997539,
      "grad_norm": 0.4181044101715088,
      "learning_rate": 0.00015820659848804513,
      "loss": 2.4736,
      "step": 47130
    },
    {
      "epoch": 0.9697618025144068,
      "grad_norm": 0.4816547930240631,
      "learning_rate": 0.0001581885378851984,
      "loss": 2.4587,
      "step": 47140
    },
    {
      "epoch": 0.9699675220312746,
      "grad_norm": 0.4269205331802368,
      "learning_rate": 0.0001581704744122282,
      "loss": 2.4459,
      "step": 47150
    },
    {
      "epoch": 0.9701732415481422,
      "grad_norm": 0.4247255325317383,
      "learning_rate": 0.0001581524080700255,
      "loss": 2.4513,
      "step": 47160
    },
    {
      "epoch": 0.97037896106501,
      "grad_norm": 0.3941880464553833,
      "learning_rate": 0.00015813433885948137,
      "loss": 2.4701,
      "step": 47170
    },
    {
      "epoch": 0.9705846805818776,
      "grad_norm": 0.37867069244384766,
      "learning_rate": 0.00015811626678148712,
      "loss": 2.4165,
      "step": 47180
    },
    {
      "epoch": 0.9707904000987454,
      "grad_norm": 0.45438119769096375,
      "learning_rate": 0.00015809819183693413,
      "loss": 2.4175,
      "step": 47190
    },
    {
      "epoch": 0.970996119615613,
      "grad_norm": 0.42313331365585327,
      "learning_rate": 0.0001580801140267139,
      "loss": 2.427,
      "step": 47200
    },
    {
      "epoch": 0.9712018391324808,
      "grad_norm": 0.41186457872390747,
      "learning_rate": 0.0001580620333517182,
      "loss": 2.4011,
      "step": 47210
    },
    {
      "epoch": 0.9714075586493485,
      "grad_norm": 0.40528443455696106,
      "learning_rate": 0.00015804394981283875,
      "loss": 2.4023,
      "step": 47220
    },
    {
      "epoch": 0.9716132781662162,
      "grad_norm": 0.39784690737724304,
      "learning_rate": 0.00015802586341096757,
      "loss": 2.4247,
      "step": 47230
    },
    {
      "epoch": 0.971818997683084,
      "grad_norm": 0.4171393811702728,
      "learning_rate": 0.00015800777414699674,
      "loss": 2.453,
      "step": 47240
    },
    {
      "epoch": 0.9720247171999516,
      "grad_norm": 0.40215742588043213,
      "learning_rate": 0.00015798968202181853,
      "loss": 2.4565,
      "step": 47250
    },
    {
      "epoch": 0.9722304367168194,
      "grad_norm": 0.39022672176361084,
      "learning_rate": 0.0001579715870363253,
      "loss": 2.4556,
      "step": 47260
    },
    {
      "epoch": 0.9724361562336871,
      "grad_norm": 0.4169977903366089,
      "learning_rate": 0.0001579534891914096,
      "loss": 2.4143,
      "step": 47270
    },
    {
      "epoch": 0.9726418757505548,
      "grad_norm": 0.40719959139823914,
      "learning_rate": 0.0001579353884879641,
      "loss": 2.4106,
      "step": 47280
    },
    {
      "epoch": 0.9728475952674225,
      "grad_norm": 0.4013669192790985,
      "learning_rate": 0.00015791728492688155,
      "loss": 2.408,
      "step": 47290
    },
    {
      "epoch": 0.9730533147842902,
      "grad_norm": 0.40062215924263,
      "learning_rate": 0.00015789917850905498,
      "loss": 2.4163,
      "step": 47300
    },
    {
      "epoch": 0.9732590343011579,
      "grad_norm": 0.40071967244148254,
      "learning_rate": 0.00015788106923537746,
      "loss": 2.3853,
      "step": 47310
    },
    {
      "epoch": 0.9734647538180257,
      "grad_norm": 0.3806479871273041,
      "learning_rate": 0.00015786295710674218,
      "loss": 2.3915,
      "step": 47320
    },
    {
      "epoch": 0.9736704733348934,
      "grad_norm": 0.3830593526363373,
      "learning_rate": 0.00015784484212404257,
      "loss": 2.3797,
      "step": 47330
    },
    {
      "epoch": 0.9738761928517611,
      "grad_norm": 0.3930051028728485,
      "learning_rate": 0.0001578267242881721,
      "loss": 2.434,
      "step": 47340
    },
    {
      "epoch": 0.9740819123686288,
      "grad_norm": 0.46552175283432007,
      "learning_rate": 0.00015780860360002443,
      "loss": 2.4149,
      "step": 47350
    },
    {
      "epoch": 0.9742876318854965,
      "grad_norm": 0.4567583501338959,
      "learning_rate": 0.00015779048006049336,
      "loss": 2.4061,
      "step": 47360
    },
    {
      "epoch": 0.9744933514023643,
      "grad_norm": 0.3730459213256836,
      "learning_rate": 0.00015777235367047285,
      "loss": 2.3998,
      "step": 47370
    },
    {
      "epoch": 0.9746990709192319,
      "grad_norm": 0.4345560073852539,
      "learning_rate": 0.00015775422443085692,
      "loss": 2.4198,
      "step": 47380
    },
    {
      "epoch": 0.9749047904360997,
      "grad_norm": 0.4110722541809082,
      "learning_rate": 0.00015773609234253985,
      "loss": 2.4332,
      "step": 47390
    },
    {
      "epoch": 0.9751105099529673,
      "grad_norm": 0.376854807138443,
      "learning_rate": 0.00015771795740641598,
      "loss": 2.4333,
      "step": 47400
    },
    {
      "epoch": 0.9753162294698351,
      "grad_norm": 0.40942445397377014,
      "learning_rate": 0.00015769981962337977,
      "loss": 2.3889,
      "step": 47410
    },
    {
      "epoch": 0.9755219489867029,
      "grad_norm": 0.4272245764732361,
      "learning_rate": 0.0001576816789943259,
      "loss": 2.4717,
      "step": 47420
    },
    {
      "epoch": 0.9757276685035705,
      "grad_norm": 0.40553420782089233,
      "learning_rate": 0.00015766353552014914,
      "loss": 2.4341,
      "step": 47430
    },
    {
      "epoch": 0.9759333880204383,
      "grad_norm": 0.4326865077018738,
      "learning_rate": 0.00015764538920174436,
      "loss": 2.3917,
      "step": 47440
    },
    {
      "epoch": 0.9761391075373059,
      "grad_norm": 0.4197924733161926,
      "learning_rate": 0.0001576272400400067,
      "loss": 2.3909,
      "step": 47450
    },
    {
      "epoch": 0.9763448270541737,
      "grad_norm": 0.40986332297325134,
      "learning_rate": 0.00015760908803583133,
      "loss": 2.4243,
      "step": 47460
    },
    {
      "epoch": 0.9765505465710413,
      "grad_norm": 0.3958427309989929,
      "learning_rate": 0.00015759093319011352,
      "loss": 2.3847,
      "step": 47470
    },
    {
      "epoch": 0.9767562660879091,
      "grad_norm": 0.3824557662010193,
      "learning_rate": 0.00015757277550374887,
      "loss": 2.4193,
      "step": 47480
    },
    {
      "epoch": 0.9769619856047768,
      "grad_norm": 0.41356131434440613,
      "learning_rate": 0.00015755461497763288,
      "loss": 2.4624,
      "step": 47490
    },
    {
      "epoch": 0.9771677051216445,
      "grad_norm": 0.6407925486564636,
      "learning_rate": 0.00015753645161266144,
      "loss": 2.4439,
      "step": 47500
    },
    {
      "epoch": 0.9773734246385123,
      "grad_norm": 0.4019554555416107,
      "learning_rate": 0.00015751828540973032,
      "loss": 2.4178,
      "step": 47510
    },
    {
      "epoch": 0.9775791441553799,
      "grad_norm": 0.36874571442604065,
      "learning_rate": 0.00015750011636973566,
      "loss": 2.44,
      "step": 47520
    },
    {
      "epoch": 0.9777848636722477,
      "grad_norm": 0.46128198504447937,
      "learning_rate": 0.00015748194449357355,
      "loss": 2.4504,
      "step": 47530
    },
    {
      "epoch": 0.9779905831891154,
      "grad_norm": 0.40870246291160583,
      "learning_rate": 0.0001574637697821404,
      "loss": 2.3944,
      "step": 47540
    },
    {
      "epoch": 0.9781963027059831,
      "grad_norm": 0.41118699312210083,
      "learning_rate": 0.00015744559223633263,
      "loss": 2.3494,
      "step": 47550
    },
    {
      "epoch": 0.9784020222228508,
      "grad_norm": 0.4135887324810028,
      "learning_rate": 0.00015742741185704678,
      "loss": 2.3779,
      "step": 47560
    },
    {
      "epoch": 0.9786077417397185,
      "grad_norm": 0.368801087141037,
      "learning_rate": 0.00015740922864517968,
      "loss": 2.4134,
      "step": 47570
    },
    {
      "epoch": 0.9788134612565862,
      "grad_norm": 0.4491810202598572,
      "learning_rate": 0.0001573910426016282,
      "loss": 2.4185,
      "step": 47580
    },
    {
      "epoch": 0.979019180773454,
      "grad_norm": 0.3755457401275635,
      "learning_rate": 0.0001573728537272893,
      "loss": 2.4098,
      "step": 47590
    },
    {
      "epoch": 0.9792249002903217,
      "grad_norm": 0.4028891324996948,
      "learning_rate": 0.00015735466202306016,
      "loss": 2.4411,
      "step": 47600
    },
    {
      "epoch": 0.9794306198071894,
      "grad_norm": 0.39823755621910095,
      "learning_rate": 0.0001573364674898381,
      "loss": 2.4168,
      "step": 47610
    },
    {
      "epoch": 0.9796363393240571,
      "grad_norm": 0.43291300535202026,
      "learning_rate": 0.00015731827012852056,
      "loss": 2.3885,
      "step": 47620
    },
    {
      "epoch": 0.9798420588409248,
      "grad_norm": 0.45512059330940247,
      "learning_rate": 0.0001573000699400051,
      "loss": 2.3707,
      "step": 47630
    },
    {
      "epoch": 0.9800477783577926,
      "grad_norm": 0.4200173020362854,
      "learning_rate": 0.00015728186692518943,
      "loss": 2.4278,
      "step": 47640
    },
    {
      "epoch": 0.9802534978746602,
      "grad_norm": 0.38637784123420715,
      "learning_rate": 0.00015726366108497142,
      "loss": 2.3922,
      "step": 47650
    },
    {
      "epoch": 0.980459217391528,
      "grad_norm": 0.42845672369003296,
      "learning_rate": 0.00015724545242024903,
      "loss": 2.4342,
      "step": 47660
    },
    {
      "epoch": 0.9806649369083956,
      "grad_norm": 0.4049283266067505,
      "learning_rate": 0.00015722724093192046,
      "loss": 2.4329,
      "step": 47670
    },
    {
      "epoch": 0.9808706564252634,
      "grad_norm": 0.4231415092945099,
      "learning_rate": 0.00015720902662088392,
      "loss": 2.4042,
      "step": 47680
    },
    {
      "epoch": 0.9810763759421312,
      "grad_norm": 0.3879249393939972,
      "learning_rate": 0.00015719080948803783,
      "loss": 2.4475,
      "step": 47690
    },
    {
      "epoch": 0.9812820954589988,
      "grad_norm": 0.37972787022590637,
      "learning_rate": 0.00015717258953428081,
      "loss": 2.3951,
      "step": 47700
    },
    {
      "epoch": 0.9814878149758666,
      "grad_norm": 0.4790825843811035,
      "learning_rate": 0.0001571543667605115,
      "loss": 2.4552,
      "step": 47710
    },
    {
      "epoch": 0.9816935344927342,
      "grad_norm": 0.4164523780345917,
      "learning_rate": 0.0001571361411676287,
      "loss": 2.3352,
      "step": 47720
    },
    {
      "epoch": 0.981899254009602,
      "grad_norm": 0.4228799045085907,
      "learning_rate": 0.0001571179127565314,
      "loss": 2.402,
      "step": 47730
    },
    {
      "epoch": 0.9821049735264696,
      "grad_norm": 0.3900204598903656,
      "learning_rate": 0.00015709968152811875,
      "loss": 2.4526,
      "step": 47740
    },
    {
      "epoch": 0.9823106930433374,
      "grad_norm": 0.42220088839530945,
      "learning_rate": 0.00015708144748328997,
      "loss": 2.4002,
      "step": 47750
    },
    {
      "epoch": 0.982516412560205,
      "grad_norm": 0.4393618106842041,
      "learning_rate": 0.00015706321062294442,
      "loss": 2.4404,
      "step": 47760
    },
    {
      "epoch": 0.9827221320770728,
      "grad_norm": 0.4060407876968384,
      "learning_rate": 0.00015704497094798164,
      "loss": 2.4662,
      "step": 47770
    },
    {
      "epoch": 0.9829278515939406,
      "grad_norm": 0.3992854058742523,
      "learning_rate": 0.00015702672845930134,
      "loss": 2.4308,
      "step": 47780
    },
    {
      "epoch": 0.9831335711108082,
      "grad_norm": 0.4201027750968933,
      "learning_rate": 0.00015700848315780325,
      "loss": 2.4585,
      "step": 47790
    },
    {
      "epoch": 0.983339290627676,
      "grad_norm": 0.4140413999557495,
      "learning_rate": 0.00015699023504438738,
      "loss": 2.4156,
      "step": 47800
    },
    {
      "epoch": 0.9835450101445437,
      "grad_norm": 0.40179699659347534,
      "learning_rate": 0.00015697198411995374,
      "loss": 2.4511,
      "step": 47810
    },
    {
      "epoch": 0.9837507296614114,
      "grad_norm": 0.43247678875923157,
      "learning_rate": 0.0001569537303854026,
      "loss": 2.4455,
      "step": 47820
    },
    {
      "epoch": 0.9839564491782791,
      "grad_norm": 0.47335085272789,
      "learning_rate": 0.0001569354738416343,
      "loss": 2.4511,
      "step": 47830
    },
    {
      "epoch": 0.9841621686951468,
      "grad_norm": 0.379351407289505,
      "learning_rate": 0.0001569172144895493,
      "loss": 2.3838,
      "step": 47840
    },
    {
      "epoch": 0.9843678882120145,
      "grad_norm": 0.5078396797180176,
      "learning_rate": 0.00015689895233004833,
      "loss": 2.4817,
      "step": 47850
    },
    {
      "epoch": 0.9845736077288822,
      "grad_norm": 0.40986189246177673,
      "learning_rate": 0.00015688068736403204,
      "loss": 2.4312,
      "step": 47860
    },
    {
      "epoch": 0.98477932724575,
      "grad_norm": 0.4188336730003357,
      "learning_rate": 0.00015686241959240148,
      "loss": 2.4749,
      "step": 47870
    },
    {
      "epoch": 0.9849850467626177,
      "grad_norm": 0.3759417235851288,
      "learning_rate": 0.00015684414901605758,
      "loss": 2.3872,
      "step": 47880
    },
    {
      "epoch": 0.9851907662794854,
      "grad_norm": 0.452181339263916,
      "learning_rate": 0.00015682587563590157,
      "loss": 2.4245,
      "step": 47890
    },
    {
      "epoch": 0.9853964857963531,
      "grad_norm": 0.4346250295639038,
      "learning_rate": 0.00015680759945283477,
      "loss": 2.4421,
      "step": 47900
    },
    {
      "epoch": 0.9856022053132208,
      "grad_norm": 0.40076979994773865,
      "learning_rate": 0.00015678932046775867,
      "loss": 2.4084,
      "step": 47910
    },
    {
      "epoch": 0.9858079248300885,
      "grad_norm": 0.4046723544597626,
      "learning_rate": 0.0001567710386815749,
      "loss": 2.3977,
      "step": 47920
    },
    {
      "epoch": 0.9860136443469563,
      "grad_norm": 0.4031040072441101,
      "learning_rate": 0.00015675275409518505,
      "loss": 2.4388,
      "step": 47930
    },
    {
      "epoch": 0.9862193638638239,
      "grad_norm": 0.3835487961769104,
      "learning_rate": 0.00015673446670949118,
      "loss": 2.4914,
      "step": 47940
    },
    {
      "epoch": 0.9864250833806917,
      "grad_norm": 0.39216935634613037,
      "learning_rate": 0.00015671617652539523,
      "loss": 2.3717,
      "step": 47950
    },
    {
      "epoch": 0.9866308028975594,
      "grad_norm": 0.39843180775642395,
      "learning_rate": 0.00015669788354379937,
      "loss": 2.4004,
      "step": 47960
    },
    {
      "epoch": 0.9868365224144271,
      "grad_norm": 0.4551487863063812,
      "learning_rate": 0.00015667958776560586,
      "loss": 2.392,
      "step": 47970
    },
    {
      "epoch": 0.9870422419312949,
      "grad_norm": 0.4465467035770416,
      "learning_rate": 0.00015666128919171715,
      "loss": 2.463,
      "step": 47980
    },
    {
      "epoch": 0.9872479614481625,
      "grad_norm": 0.4047348201274872,
      "learning_rate": 0.00015664298782303582,
      "loss": 2.4681,
      "step": 47990
    },
    {
      "epoch": 0.9874536809650303,
      "grad_norm": 0.46569257974624634,
      "learning_rate": 0.00015662468366046457,
      "loss": 2.448,
      "step": 48000
    },
    {
      "epoch": 0.9876594004818979,
      "grad_norm": 0.4835076630115509,
      "learning_rate": 0.0001566063767049062,
      "loss": 2.4199,
      "step": 48010
    },
    {
      "epoch": 0.9878651199987657,
      "grad_norm": 0.43585649132728577,
      "learning_rate": 0.00015658806695726377,
      "loss": 2.3975,
      "step": 48020
    },
    {
      "epoch": 0.9880708395156333,
      "grad_norm": 0.4075622856616974,
      "learning_rate": 0.00015656975441844038,
      "loss": 2.4169,
      "step": 48030
    },
    {
      "epoch": 0.9882765590325011,
      "grad_norm": 0.4013272523880005,
      "learning_rate": 0.00015655143908933924,
      "loss": 2.4139,
      "step": 48040
    },
    {
      "epoch": 0.9884822785493689,
      "grad_norm": 0.44013386964797974,
      "learning_rate": 0.00015653312097086382,
      "loss": 2.4548,
      "step": 48050
    },
    {
      "epoch": 0.9886879980662365,
      "grad_norm": 0.42771801352500916,
      "learning_rate": 0.00015651480006391755,
      "loss": 2.4255,
      "step": 48060
    },
    {
      "epoch": 0.9888937175831043,
      "grad_norm": 0.46333983540534973,
      "learning_rate": 0.0001564964763694042,
      "loss": 2.4733,
      "step": 48070
    },
    {
      "epoch": 0.989099437099972,
      "grad_norm": 0.4169553220272064,
      "learning_rate": 0.00015647814988822755,
      "loss": 2.4583,
      "step": 48080
    },
    {
      "epoch": 0.9893051566168397,
      "grad_norm": 0.4156515896320343,
      "learning_rate": 0.00015645982062129153,
      "loss": 2.4461,
      "step": 48090
    },
    {
      "epoch": 0.9895108761337074,
      "grad_norm": 0.3930850625038147,
      "learning_rate": 0.0001564414885695002,
      "loss": 2.4045,
      "step": 48100
    },
    {
      "epoch": 0.9897165956505751,
      "grad_norm": 0.37250033020973206,
      "learning_rate": 0.00015642315373375784,
      "loss": 2.436,
      "step": 48110
    },
    {
      "epoch": 0.9899223151674428,
      "grad_norm": 0.7268489003181458,
      "learning_rate": 0.00015640481611496878,
      "loss": 2.3898,
      "step": 48120
    },
    {
      "epoch": 0.9901280346843105,
      "grad_norm": 0.39072951674461365,
      "learning_rate": 0.00015638647571403747,
      "loss": 2.4365,
      "step": 48130
    },
    {
      "epoch": 0.9903337542011783,
      "grad_norm": 0.39264655113220215,
      "learning_rate": 0.0001563681325318686,
      "loss": 2.4607,
      "step": 48140
    },
    {
      "epoch": 0.990539473718046,
      "grad_norm": 0.3855084180831909,
      "learning_rate": 0.00015634978656936698,
      "loss": 2.424,
      "step": 48150
    },
    {
      "epoch": 0.9907451932349137,
      "grad_norm": 0.3541298508644104,
      "learning_rate": 0.00015633143782743737,
      "loss": 2.3324,
      "step": 48160
    },
    {
      "epoch": 0.9909509127517814,
      "grad_norm": 0.4106385409832001,
      "learning_rate": 0.00015631308630698496,
      "loss": 2.4227,
      "step": 48170
    },
    {
      "epoch": 0.9911566322686491,
      "grad_norm": 0.3708224594593048,
      "learning_rate": 0.00015629473200891485,
      "loss": 2.3951,
      "step": 48180
    },
    {
      "epoch": 0.9913623517855168,
      "grad_norm": 0.41218578815460205,
      "learning_rate": 0.00015627637493413236,
      "loss": 2.448,
      "step": 48190
    },
    {
      "epoch": 0.9915680713023846,
      "grad_norm": 0.446262001991272,
      "learning_rate": 0.000156258015083543,
      "loss": 2.3796,
      "step": 48200
    },
    {
      "epoch": 0.9917737908192522,
      "grad_norm": 0.41539672017097473,
      "learning_rate": 0.0001562396524580523,
      "loss": 2.4297,
      "step": 48210
    },
    {
      "epoch": 0.99197951033612,
      "grad_norm": 0.4183237850666046,
      "learning_rate": 0.00015622128705856603,
      "loss": 2.3719,
      "step": 48220
    },
    {
      "epoch": 0.9921852298529877,
      "grad_norm": 0.40949422121047974,
      "learning_rate": 0.00015620291888599,
      "loss": 2.4847,
      "step": 48230
    },
    {
      "epoch": 0.9923909493698554,
      "grad_norm": 0.4143429696559906,
      "learning_rate": 0.0001561845479412303,
      "loss": 2.3915,
      "step": 48240
    },
    {
      "epoch": 0.9925966688867232,
      "grad_norm": 0.4995993375778198,
      "learning_rate": 0.000156166174225193,
      "loss": 2.4648,
      "step": 48250
    },
    {
      "epoch": 0.9928023884035908,
      "grad_norm": 0.472994327545166,
      "learning_rate": 0.0001561477977387844,
      "loss": 2.4928,
      "step": 48260
    },
    {
      "epoch": 0.9930081079204586,
      "grad_norm": 0.4076519310474396,
      "learning_rate": 0.00015612941848291092,
      "loss": 2.3597,
      "step": 48270
    },
    {
      "epoch": 0.9932138274373262,
      "grad_norm": 0.42183321714401245,
      "learning_rate": 0.00015611103645847907,
      "loss": 2.4305,
      "step": 48280
    },
    {
      "epoch": 0.993419546954194,
      "grad_norm": 0.47388756275177,
      "learning_rate": 0.00015609265166639555,
      "loss": 2.4338,
      "step": 48290
    },
    {
      "epoch": 0.9936252664710616,
      "grad_norm": 0.4221048355102539,
      "learning_rate": 0.0001560742641075672,
      "loss": 2.4287,
      "step": 48300
    },
    {
      "epoch": 0.9938309859879294,
      "grad_norm": 0.5020349621772766,
      "learning_rate": 0.00015605587378290098,
      "loss": 2.4655,
      "step": 48310
    },
    {
      "epoch": 0.9940367055047972,
      "grad_norm": 0.40288761258125305,
      "learning_rate": 0.000156037480693304,
      "loss": 2.4495,
      "step": 48320
    },
    {
      "epoch": 0.9942424250216648,
      "grad_norm": 0.42275765538215637,
      "learning_rate": 0.00015601908483968343,
      "loss": 2.4932,
      "step": 48330
    },
    {
      "epoch": 0.9944481445385326,
      "grad_norm": 0.40259066224098206,
      "learning_rate": 0.00015600068622294665,
      "loss": 2.4494,
      "step": 48340
    },
    {
      "epoch": 0.9946538640554002,
      "grad_norm": 0.48100316524505615,
      "learning_rate": 0.00015598228484400124,
      "loss": 2.4197,
      "step": 48350
    },
    {
      "epoch": 0.994859583572268,
      "grad_norm": 0.3822062313556671,
      "learning_rate": 0.00015596388070375475,
      "loss": 2.3959,
      "step": 48360
    },
    {
      "epoch": 0.9950653030891357,
      "grad_norm": 0.4292793869972229,
      "learning_rate": 0.000155945473803115,
      "loss": 2.3835,
      "step": 48370
    },
    {
      "epoch": 0.9952710226060034,
      "grad_norm": 0.4075075387954712,
      "learning_rate": 0.00015592706414298992,
      "loss": 2.4181,
      "step": 48380
    },
    {
      "epoch": 0.9954767421228711,
      "grad_norm": 0.38410958647727966,
      "learning_rate": 0.0001559086517242875,
      "loss": 2.4149,
      "step": 48390
    },
    {
      "epoch": 0.9956824616397388,
      "grad_norm": 0.444475382566452,
      "learning_rate": 0.00015589023654791597,
      "loss": 2.4286,
      "step": 48400
    },
    {
      "epoch": 0.9958881811566066,
      "grad_norm": 0.42802590131759644,
      "learning_rate": 0.00015587181861478362,
      "loss": 2.3961,
      "step": 48410
    },
    {
      "epoch": 0.9960939006734743,
      "grad_norm": 0.4056068956851959,
      "learning_rate": 0.0001558533979257989,
      "loss": 2.3935,
      "step": 48420
    },
    {
      "epoch": 0.996299620190342,
      "grad_norm": 0.47333985567092896,
      "learning_rate": 0.00015583497448187048,
      "loss": 2.4463,
      "step": 48430
    },
    {
      "epoch": 0.9965053397072097,
      "grad_norm": 0.393205463886261,
      "learning_rate": 0.00015581654828390702,
      "loss": 2.4102,
      "step": 48440
    },
    {
      "epoch": 0.9967110592240774,
      "grad_norm": 0.4302012026309967,
      "learning_rate": 0.00015579811933281738,
      "loss": 2.4348,
      "step": 48450
    },
    {
      "epoch": 0.9969167787409451,
      "grad_norm": 0.4342646598815918,
      "learning_rate": 0.00015577968762951058,
      "loss": 2.4351,
      "step": 48460
    },
    {
      "epoch": 0.9971224982578129,
      "grad_norm": 0.4396665096282959,
      "learning_rate": 0.00015576125317489576,
      "loss": 2.4695,
      "step": 48470
    },
    {
      "epoch": 0.9973282177746805,
      "grad_norm": 0.40958675742149353,
      "learning_rate": 0.00015574281596988218,
      "loss": 2.4163,
      "step": 48480
    },
    {
      "epoch": 0.9975339372915483,
      "grad_norm": 0.40462154150009155,
      "learning_rate": 0.00015572437601537925,
      "loss": 2.403,
      "step": 48490
    },
    {
      "epoch": 0.997739656808416,
      "grad_norm": 0.43783730268478394,
      "learning_rate": 0.0001557059333122965,
      "loss": 2.4014,
      "step": 48500
    },
    {
      "epoch": 0.9979453763252837,
      "grad_norm": 0.4193902313709259,
      "learning_rate": 0.00015568748786154365,
      "loss": 2.4101,
      "step": 48510
    },
    {
      "epoch": 0.9981510958421514,
      "grad_norm": 0.41907763481140137,
      "learning_rate": 0.00015566903966403049,
      "loss": 2.4282,
      "step": 48520
    },
    {
      "epoch": 0.9983568153590191,
      "grad_norm": 0.4087301194667816,
      "learning_rate": 0.0001556505887206669,
      "loss": 2.439,
      "step": 48530
    },
    {
      "epoch": 0.9985625348758869,
      "grad_norm": 0.3951023519039154,
      "learning_rate": 0.00015563213503236306,
      "loss": 2.404,
      "step": 48540
    },
    {
      "epoch": 0.9987682543927545,
      "grad_norm": 0.3978869616985321,
      "learning_rate": 0.00015561367860002917,
      "loss": 2.4215,
      "step": 48550
    },
    {
      "epoch": 0.9989739739096223,
      "grad_norm": 0.4676457345485687,
      "learning_rate": 0.0001555952194245756,
      "loss": 2.4028,
      "step": 48560
    },
    {
      "epoch": 0.9991796934264899,
      "grad_norm": 0.41104817390441895,
      "learning_rate": 0.00015557675750691276,
      "loss": 2.4565,
      "step": 48570
    },
    {
      "epoch": 0.9993854129433577,
      "grad_norm": 0.4070654809474945,
      "learning_rate": 0.00015555829284795135,
      "loss": 2.4343,
      "step": 48580
    },
    {
      "epoch": 0.9995911324602255,
      "grad_norm": 0.4014953374862671,
      "learning_rate": 0.0001555398254486021,
      "loss": 2.4269,
      "step": 48590
    },
    {
      "epoch": 0.9997968519770931,
      "grad_norm": 0.48480191826820374,
      "learning_rate": 0.00015552135530977595,
      "loss": 2.3825,
      "step": 48600
    },
    {
      "epoch": 1.0000205719516868,
      "grad_norm": 0.5921289920806885,
      "learning_rate": 0.00015550288243238385,
      "loss": 2.704,
      "step": 48610
    },
    {
      "epoch": 1.0002262914685545,
      "grad_norm": 0.4261581003665924,
      "learning_rate": 0.00015548440681733706,
      "loss": 2.3485,
      "step": 48620
    },
    {
      "epoch": 1.0004320109854221,
      "grad_norm": 0.4438159465789795,
      "learning_rate": 0.00015546592846554683,
      "loss": 2.3883,
      "step": 48630
    },
    {
      "epoch": 1.00063773050229,
      "grad_norm": 0.4164780080318451,
      "learning_rate": 0.00015544744737792457,
      "loss": 2.4033,
      "step": 48640
    },
    {
      "epoch": 1.0008434500191576,
      "grad_norm": 0.40342938899993896,
      "learning_rate": 0.00015542896355538193,
      "loss": 2.3966,
      "step": 48650
    },
    {
      "epoch": 1.0010491695360253,
      "grad_norm": 0.4254855215549469,
      "learning_rate": 0.0001554104769988305,
      "loss": 2.4246,
      "step": 48660
    },
    {
      "epoch": 1.001254889052893,
      "grad_norm": 0.397938996553421,
      "learning_rate": 0.00015539198770918224,
      "loss": 2.3733,
      "step": 48670
    },
    {
      "epoch": 1.0014606085697608,
      "grad_norm": 0.4272472858428955,
      "learning_rate": 0.00015537349568734907,
      "loss": 2.4006,
      "step": 48680
    },
    {
      "epoch": 1.0016663280866285,
      "grad_norm": 0.42747798562049866,
      "learning_rate": 0.00015535500093424309,
      "loss": 2.3821,
      "step": 48690
    },
    {
      "epoch": 1.0018720476034961,
      "grad_norm": 0.43557438254356384,
      "learning_rate": 0.0001553365034507766,
      "loss": 2.4341,
      "step": 48700
    },
    {
      "epoch": 1.002077767120364,
      "grad_norm": 0.3844374418258667,
      "learning_rate": 0.00015531800323786193,
      "loss": 2.4003,
      "step": 48710
    },
    {
      "epoch": 1.0022834866372317,
      "grad_norm": 0.397924542427063,
      "learning_rate": 0.00015529950029641158,
      "loss": 2.4247,
      "step": 48720
    },
    {
      "epoch": 1.0024892061540993,
      "grad_norm": 0.41945916414260864,
      "learning_rate": 0.00015528099462733822,
      "loss": 2.4492,
      "step": 48730
    },
    {
      "epoch": 1.002694925670967,
      "grad_norm": 0.4219111204147339,
      "learning_rate": 0.00015526248623155467,
      "loss": 2.38,
      "step": 48740
    },
    {
      "epoch": 1.0029006451878348,
      "grad_norm": 0.4225728511810303,
      "learning_rate": 0.0001552439751099738,
      "loss": 2.3749,
      "step": 48750
    },
    {
      "epoch": 1.0031063647047025,
      "grad_norm": 0.415576696395874,
      "learning_rate": 0.00015522546126350872,
      "loss": 2.3939,
      "step": 48760
    },
    {
      "epoch": 1.0033120842215701,
      "grad_norm": 0.40198031067848206,
      "learning_rate": 0.00015520694469307254,
      "loss": 2.36,
      "step": 48770
    },
    {
      "epoch": 1.003517803738438,
      "grad_norm": 0.4168078601360321,
      "learning_rate": 0.0001551884253995786,
      "loss": 2.3979,
      "step": 48780
    },
    {
      "epoch": 1.0037235232553057,
      "grad_norm": 0.43036139011383057,
      "learning_rate": 0.00015516990338394044,
      "loss": 2.3915,
      "step": 48790
    },
    {
      "epoch": 1.0039292427721733,
      "grad_norm": 0.38647907972335815,
      "learning_rate": 0.00015515137864707153,
      "loss": 2.3846,
      "step": 48800
    },
    {
      "epoch": 1.004134962289041,
      "grad_norm": 0.4062706530094147,
      "learning_rate": 0.00015513285118988568,
      "loss": 2.3811,
      "step": 48810
    },
    {
      "epoch": 1.0043406818059089,
      "grad_norm": 0.4240049123764038,
      "learning_rate": 0.0001551143210132967,
      "loss": 2.3944,
      "step": 48820
    },
    {
      "epoch": 1.0045464013227765,
      "grad_norm": 0.43529370427131653,
      "learning_rate": 0.00015509578811821864,
      "loss": 2.3712,
      "step": 48830
    },
    {
      "epoch": 1.0047521208396442,
      "grad_norm": 0.3981599509716034,
      "learning_rate": 0.00015507725250556558,
      "loss": 2.399,
      "step": 48840
    },
    {
      "epoch": 1.004957840356512,
      "grad_norm": 0.4193026125431061,
      "learning_rate": 0.00015505871417625174,
      "loss": 2.4359,
      "step": 48850
    },
    {
      "epoch": 1.0051635598733797,
      "grad_norm": 0.3873610496520996,
      "learning_rate": 0.0001550401731311916,
      "loss": 2.4114,
      "step": 48860
    },
    {
      "epoch": 1.0053692793902473,
      "grad_norm": 0.3615463078022003,
      "learning_rate": 0.00015502162937129966,
      "loss": 2.3867,
      "step": 48870
    },
    {
      "epoch": 1.005574998907115,
      "grad_norm": 0.4025648534297943,
      "learning_rate": 0.00015500308289749054,
      "loss": 2.3723,
      "step": 48880
    },
    {
      "epoch": 1.0057807184239829,
      "grad_norm": 0.43474462628364563,
      "learning_rate": 0.0001549845337106791,
      "loss": 2.456,
      "step": 48890
    },
    {
      "epoch": 1.0059864379408505,
      "grad_norm": 0.3809204697608948,
      "learning_rate": 0.00015496598181178022,
      "loss": 2.3843,
      "step": 48900
    },
    {
      "epoch": 1.0061921574577182,
      "grad_norm": 0.4801614582538605,
      "learning_rate": 0.00015494742720170902,
      "loss": 2.4546,
      "step": 48910
    },
    {
      "epoch": 1.0063978769745858,
      "grad_norm": 0.4475014805793762,
      "learning_rate": 0.00015492886988138064,
      "loss": 2.3446,
      "step": 48920
    },
    {
      "epoch": 1.0066035964914537,
      "grad_norm": 0.388069212436676,
      "learning_rate": 0.00015491030985171044,
      "loss": 2.4427,
      "step": 48930
    },
    {
      "epoch": 1.0068093160083214,
      "grad_norm": 0.4193936884403229,
      "learning_rate": 0.00015489174711361386,
      "loss": 2.3393,
      "step": 48940
    },
    {
      "epoch": 1.007015035525189,
      "grad_norm": 0.40725842118263245,
      "learning_rate": 0.00015487318166800654,
      "loss": 2.3838,
      "step": 48950
    },
    {
      "epoch": 1.0072207550420569,
      "grad_norm": 0.42522141337394714,
      "learning_rate": 0.00015485461351580422,
      "loss": 2.3506,
      "step": 48960
    },
    {
      "epoch": 1.0074264745589245,
      "grad_norm": 0.426299124956131,
      "learning_rate": 0.0001548360426579227,
      "loss": 2.4009,
      "step": 48970
    },
    {
      "epoch": 1.0076321940757922,
      "grad_norm": 0.4205209016799927,
      "learning_rate": 0.00015481746909527802,
      "loss": 2.4064,
      "step": 48980
    },
    {
      "epoch": 1.0078379135926598,
      "grad_norm": 0.4130765497684479,
      "learning_rate": 0.00015479889282878632,
      "loss": 2.4149,
      "step": 48990
    },
    {
      "epoch": 1.0080436331095277,
      "grad_norm": 0.4220925271511078,
      "learning_rate": 0.00015478031385936385,
      "loss": 2.3724,
      "step": 49000
    },
    {
      "epoch": 1.0082493526263954,
      "grad_norm": 0.4157769978046417,
      "learning_rate": 0.000154761732187927,
      "loss": 2.411,
      "step": 49010
    },
    {
      "epoch": 1.008455072143263,
      "grad_norm": 0.4305283725261688,
      "learning_rate": 0.00015474314781539232,
      "loss": 2.3927,
      "step": 49020
    },
    {
      "epoch": 1.008660791660131,
      "grad_norm": 0.41066300868988037,
      "learning_rate": 0.00015472456074267649,
      "loss": 2.3863,
      "step": 49030
    },
    {
      "epoch": 1.0088665111769985,
      "grad_norm": 0.38770854473114014,
      "learning_rate": 0.00015470597097069628,
      "loss": 2.4053,
      "step": 49040
    },
    {
      "epoch": 1.0090722306938662,
      "grad_norm": 0.44937863945961,
      "learning_rate": 0.00015468737850036864,
      "loss": 2.3877,
      "step": 49050
    },
    {
      "epoch": 1.0092779502107339,
      "grad_norm": 0.3909083306789398,
      "learning_rate": 0.00015466878333261064,
      "loss": 2.4514,
      "step": 49060
    },
    {
      "epoch": 1.0094836697276017,
      "grad_norm": 0.40818893909454346,
      "learning_rate": 0.00015465018546833944,
      "loss": 2.4358,
      "step": 49070
    },
    {
      "epoch": 1.0096893892444694,
      "grad_norm": 0.4227050840854645,
      "learning_rate": 0.00015463158490847242,
      "loss": 2.4462,
      "step": 49080
    },
    {
      "epoch": 1.009895108761337,
      "grad_norm": 0.3833294212818146,
      "learning_rate": 0.00015461298165392703,
      "loss": 2.4264,
      "step": 49090
    },
    {
      "epoch": 1.0101008282782047,
      "grad_norm": 0.401010125875473,
      "learning_rate": 0.0001545943757056208,
      "loss": 2.4185,
      "step": 49100
    },
    {
      "epoch": 1.0103065477950726,
      "grad_norm": 0.4114075005054474,
      "learning_rate": 0.0001545757670644716,
      "loss": 2.3761,
      "step": 49110
    },
    {
      "epoch": 1.0105122673119402,
      "grad_norm": 0.41569700837135315,
      "learning_rate": 0.00015455715573139717,
      "loss": 2.3976,
      "step": 49120
    },
    {
      "epoch": 1.0107179868288079,
      "grad_norm": 0.4163394272327423,
      "learning_rate": 0.00015453854170731558,
      "loss": 2.4272,
      "step": 49130
    },
    {
      "epoch": 1.0109237063456757,
      "grad_norm": 0.4393799304962158,
      "learning_rate": 0.00015451992499314487,
      "loss": 2.3955,
      "step": 49140
    },
    {
      "epoch": 1.0111294258625434,
      "grad_norm": 0.48676764965057373,
      "learning_rate": 0.00015450130558980338,
      "loss": 2.3528,
      "step": 49150
    },
    {
      "epoch": 1.011335145379411,
      "grad_norm": 0.5451964139938354,
      "learning_rate": 0.0001544826834982095,
      "loss": 2.4588,
      "step": 49160
    },
    {
      "epoch": 1.0115408648962787,
      "grad_norm": 0.4000711143016815,
      "learning_rate": 0.00015446405871928173,
      "loss": 2.4082,
      "step": 49170
    },
    {
      "epoch": 1.0117465844131466,
      "grad_norm": 0.40903201699256897,
      "learning_rate": 0.00015444543125393874,
      "loss": 2.3703,
      "step": 49180
    },
    {
      "epoch": 1.0119523039300142,
      "grad_norm": 0.41410714387893677,
      "learning_rate": 0.00015442680110309932,
      "loss": 2.3452,
      "step": 49190
    },
    {
      "epoch": 1.0121580234468819,
      "grad_norm": 0.42062845826148987,
      "learning_rate": 0.0001544081682676824,
      "loss": 2.3955,
      "step": 49200
    },
    {
      "epoch": 1.0123637429637498,
      "grad_norm": 0.4546811282634735,
      "learning_rate": 0.000154389532748607,
      "loss": 2.3699,
      "step": 49210
    },
    {
      "epoch": 1.0125694624806174,
      "grad_norm": 0.5549023747444153,
      "learning_rate": 0.00015437089454679235,
      "loss": 2.4119,
      "step": 49220
    },
    {
      "epoch": 1.012775181997485,
      "grad_norm": 0.46711838245391846,
      "learning_rate": 0.00015435225366315777,
      "loss": 2.3754,
      "step": 49230
    },
    {
      "epoch": 1.0129809015143527,
      "grad_norm": 0.4235474765300751,
      "learning_rate": 0.0001543336100986227,
      "loss": 2.3564,
      "step": 49240
    },
    {
      "epoch": 1.0131866210312206,
      "grad_norm": 0.42250314354896545,
      "learning_rate": 0.00015431496385410673,
      "loss": 2.4072,
      "step": 49250
    },
    {
      "epoch": 1.0133923405480882,
      "grad_norm": 0.4514583647251129,
      "learning_rate": 0.00015429631493052955,
      "loss": 2.413,
      "step": 49260
    },
    {
      "epoch": 1.013598060064956,
      "grad_norm": 0.40631723403930664,
      "learning_rate": 0.00015427766332881104,
      "loss": 2.3545,
      "step": 49270
    },
    {
      "epoch": 1.0138037795818235,
      "grad_norm": 0.4307166635990143,
      "learning_rate": 0.0001542590090498712,
      "loss": 2.4123,
      "step": 49280
    },
    {
      "epoch": 1.0140094990986914,
      "grad_norm": 0.4290085434913635,
      "learning_rate": 0.0001542403520946301,
      "loss": 2.446,
      "step": 49290
    },
    {
      "epoch": 1.014215218615559,
      "grad_norm": 0.4373873472213745,
      "learning_rate": 0.000154221692464008,
      "loss": 2.3718,
      "step": 49300
    },
    {
      "epoch": 1.0144209381324267,
      "grad_norm": 0.42481449246406555,
      "learning_rate": 0.00015420303015892527,
      "loss": 2.405,
      "step": 49310
    },
    {
      "epoch": 1.0146266576492946,
      "grad_norm": 0.41297289729118347,
      "learning_rate": 0.00015418436518030247,
      "loss": 2.3479,
      "step": 49320
    },
    {
      "epoch": 1.0148323771661623,
      "grad_norm": 0.43549105525016785,
      "learning_rate": 0.00015416569752906017,
      "loss": 2.4008,
      "step": 49330
    },
    {
      "epoch": 1.01503809668303,
      "grad_norm": 0.3959137201309204,
      "learning_rate": 0.00015414702720611923,
      "loss": 2.4084,
      "step": 49340
    },
    {
      "epoch": 1.0152438161998976,
      "grad_norm": 0.40441352128982544,
      "learning_rate": 0.00015412835421240047,
      "loss": 2.4,
      "step": 49350
    },
    {
      "epoch": 1.0154495357167654,
      "grad_norm": 0.45014381408691406,
      "learning_rate": 0.00015410967854882497,
      "loss": 2.4067,
      "step": 49360
    },
    {
      "epoch": 1.015655255233633,
      "grad_norm": 0.4340270757675171,
      "learning_rate": 0.0001540910002163139,
      "loss": 2.3964,
      "step": 49370
    },
    {
      "epoch": 1.0158609747505007,
      "grad_norm": 0.4377054274082184,
      "learning_rate": 0.0001540723192157885,
      "loss": 2.3961,
      "step": 49380
    },
    {
      "epoch": 1.0160666942673686,
      "grad_norm": 0.38745394349098206,
      "learning_rate": 0.00015405363554817027,
      "loss": 2.4588,
      "step": 49390
    },
    {
      "epoch": 1.0162724137842363,
      "grad_norm": 0.4027441740036011,
      "learning_rate": 0.00015403494921438079,
      "loss": 2.3731,
      "step": 49400
    },
    {
      "epoch": 1.016478133301104,
      "grad_norm": 0.41381242871284485,
      "learning_rate": 0.0001540162602153417,
      "loss": 2.4032,
      "step": 49410
    },
    {
      "epoch": 1.0166838528179716,
      "grad_norm": 0.4255903363227844,
      "learning_rate": 0.00015399756855197485,
      "loss": 2.42,
      "step": 49420
    },
    {
      "epoch": 1.0168895723348395,
      "grad_norm": 0.4546128809452057,
      "learning_rate": 0.0001539788742252022,
      "loss": 2.3935,
      "step": 49430
    },
    {
      "epoch": 1.017095291851707,
      "grad_norm": 0.379593163728714,
      "learning_rate": 0.0001539601772359458,
      "loss": 2.4085,
      "step": 49440
    },
    {
      "epoch": 1.0173010113685748,
      "grad_norm": 0.5124489665031433,
      "learning_rate": 0.00015394147758512796,
      "loss": 2.3673,
      "step": 49450
    },
    {
      "epoch": 1.0175067308854424,
      "grad_norm": 0.40481114387512207,
      "learning_rate": 0.00015392277527367094,
      "loss": 2.414,
      "step": 49460
    },
    {
      "epoch": 1.0177124504023103,
      "grad_norm": 0.4460028111934662,
      "learning_rate": 0.00015390407030249726,
      "loss": 2.4065,
      "step": 49470
    },
    {
      "epoch": 1.017918169919178,
      "grad_norm": 0.4513063132762909,
      "learning_rate": 0.00015388536267252952,
      "loss": 2.4484,
      "step": 49480
    },
    {
      "epoch": 1.0181238894360456,
      "grad_norm": 0.45418888330459595,
      "learning_rate": 0.00015386665238469055,
      "loss": 2.4184,
      "step": 49490
    },
    {
      "epoch": 1.0183296089529135,
      "grad_norm": 0.3916398882865906,
      "learning_rate": 0.00015384793943990305,
      "loss": 2.3822,
      "step": 49500
    },
    {
      "epoch": 1.0185353284697811,
      "grad_norm": 0.45988181233406067,
      "learning_rate": 0.0001538292238390902,
      "loss": 2.4222,
      "step": 49510
    },
    {
      "epoch": 1.0187410479866488,
      "grad_norm": 0.39085835218429565,
      "learning_rate": 0.0001538105055831751,
      "loss": 2.3975,
      "step": 49520
    },
    {
      "epoch": 1.0189467675035164,
      "grad_norm": 0.40556564927101135,
      "learning_rate": 0.00015379178467308092,
      "loss": 2.4021,
      "step": 49530
    },
    {
      "epoch": 1.0191524870203843,
      "grad_norm": 0.4320817291736603,
      "learning_rate": 0.0001537730611097312,
      "loss": 2.4142,
      "step": 49540
    },
    {
      "epoch": 1.019358206537252,
      "grad_norm": 0.40461674332618713,
      "learning_rate": 0.00015375433489404935,
      "loss": 2.391,
      "step": 49550
    },
    {
      "epoch": 1.0195639260541196,
      "grad_norm": 0.37904953956604004,
      "learning_rate": 0.00015373560602695912,
      "loss": 2.4549,
      "step": 49560
    },
    {
      "epoch": 1.0197696455709875,
      "grad_norm": 0.4256097078323364,
      "learning_rate": 0.00015371687450938424,
      "loss": 2.396,
      "step": 49570
    },
    {
      "epoch": 1.0199753650878551,
      "grad_norm": 0.39160627126693726,
      "learning_rate": 0.00015369814034224866,
      "loss": 2.3982,
      "step": 49580
    },
    {
      "epoch": 1.0201810846047228,
      "grad_norm": 0.3910102844238281,
      "learning_rate": 0.00015367940352647644,
      "loss": 2.3993,
      "step": 49590
    },
    {
      "epoch": 1.0203868041215904,
      "grad_norm": 0.4408862590789795,
      "learning_rate": 0.00015366066406299179,
      "loss": 2.3731,
      "step": 49600
    },
    {
      "epoch": 1.0205925236384583,
      "grad_norm": 0.407259464263916,
      "learning_rate": 0.000153641921952719,
      "loss": 2.4642,
      "step": 49610
    },
    {
      "epoch": 1.020798243155326,
      "grad_norm": 0.39599475264549255,
      "learning_rate": 0.00015362317719658247,
      "loss": 2.3981,
      "step": 49620
    },
    {
      "epoch": 1.0210039626721936,
      "grad_norm": 0.4035496711730957,
      "learning_rate": 0.00015360442979550686,
      "loss": 2.4193,
      "step": 49630
    },
    {
      "epoch": 1.0212096821890613,
      "grad_norm": 0.46307194232940674,
      "learning_rate": 0.00015358567975041682,
      "loss": 2.3934,
      "step": 49640
    },
    {
      "epoch": 1.0214154017059291,
      "grad_norm": 0.3983724117279053,
      "learning_rate": 0.0001535669270622372,
      "loss": 2.3779,
      "step": 49650
    },
    {
      "epoch": 1.0216211212227968,
      "grad_norm": 0.4352116584777832,
      "learning_rate": 0.000153548171731893,
      "loss": 2.4775,
      "step": 49660
    },
    {
      "epoch": 1.0218268407396645,
      "grad_norm": 0.39586177468299866,
      "learning_rate": 0.00015352941376030926,
      "loss": 2.3863,
      "step": 49670
    },
    {
      "epoch": 1.0220325602565323,
      "grad_norm": 0.45010557770729065,
      "learning_rate": 0.00015351065314841126,
      "loss": 2.4089,
      "step": 49680
    },
    {
      "epoch": 1.0222382797734,
      "grad_norm": 0.4315042495727539,
      "learning_rate": 0.00015349188989712432,
      "loss": 2.4043,
      "step": 49690
    },
    {
      "epoch": 1.0224439992902676,
      "grad_norm": 0.4514741003513336,
      "learning_rate": 0.00015347312400737395,
      "loss": 2.4285,
      "step": 49700
    },
    {
      "epoch": 1.0226497188071353,
      "grad_norm": 0.4085547924041748,
      "learning_rate": 0.0001534543554800858,
      "loss": 2.3634,
      "step": 49710
    },
    {
      "epoch": 1.0228554383240032,
      "grad_norm": 0.41834795475006104,
      "learning_rate": 0.00015343558431618555,
      "loss": 2.3816,
      "step": 49720
    },
    {
      "epoch": 1.0230611578408708,
      "grad_norm": 0.39640894532203674,
      "learning_rate": 0.00015341681051659912,
      "loss": 2.3769,
      "step": 49730
    },
    {
      "epoch": 1.0232668773577385,
      "grad_norm": 0.3967615067958832,
      "learning_rate": 0.0001533980340822525,
      "loss": 2.347,
      "step": 49740
    },
    {
      "epoch": 1.0234725968746063,
      "grad_norm": 0.4310699701309204,
      "learning_rate": 0.00015337925501407185,
      "loss": 2.3509,
      "step": 49750
    },
    {
      "epoch": 1.023678316391474,
      "grad_norm": 0.42651644349098206,
      "learning_rate": 0.00015336047331298346,
      "loss": 2.416,
      "step": 49760
    },
    {
      "epoch": 1.0238840359083416,
      "grad_norm": 0.38891860842704773,
      "learning_rate": 0.00015334168897991368,
      "loss": 2.4045,
      "step": 49770
    },
    {
      "epoch": 1.0240897554252093,
      "grad_norm": 0.4417003095149994,
      "learning_rate": 0.00015332290201578905,
      "loss": 2.3752,
      "step": 49780
    },
    {
      "epoch": 1.0242954749420772,
      "grad_norm": 0.40681931376457214,
      "learning_rate": 0.00015330411242153627,
      "loss": 2.4515,
      "step": 49790
    },
    {
      "epoch": 1.0245011944589448,
      "grad_norm": 0.46309593319892883,
      "learning_rate": 0.00015328532019808207,
      "loss": 2.4263,
      "step": 49800
    },
    {
      "epoch": 1.0247069139758125,
      "grad_norm": 0.43037450313568115,
      "learning_rate": 0.0001532665253463534,
      "loss": 2.3651,
      "step": 49810
    },
    {
      "epoch": 1.0249126334926801,
      "grad_norm": 0.43061041831970215,
      "learning_rate": 0.0001532477278672773,
      "loss": 2.3853,
      "step": 49820
    },
    {
      "epoch": 1.025118353009548,
      "grad_norm": 0.4072180986404419,
      "learning_rate": 0.00015322892776178096,
      "loss": 2.4472,
      "step": 49830
    },
    {
      "epoch": 1.0253240725264157,
      "grad_norm": 0.45984840393066406,
      "learning_rate": 0.00015321012503079171,
      "loss": 2.386,
      "step": 49840
    },
    {
      "epoch": 1.0255297920432833,
      "grad_norm": 0.4127037525177002,
      "learning_rate": 0.0001531913196752369,
      "loss": 2.3699,
      "step": 49850
    },
    {
      "epoch": 1.0257355115601512,
      "grad_norm": 0.4072965681552887,
      "learning_rate": 0.00015317251169604416,
      "loss": 2.4158,
      "step": 49860
    },
    {
      "epoch": 1.0259412310770188,
      "grad_norm": 0.42316097021102905,
      "learning_rate": 0.00015315370109414116,
      "loss": 2.4084,
      "step": 49870
    },
    {
      "epoch": 1.0261469505938865,
      "grad_norm": 0.46141737699508667,
      "learning_rate": 0.00015313488787045576,
      "loss": 2.3833,
      "step": 49880
    },
    {
      "epoch": 1.0263526701107542,
      "grad_norm": 0.3676004409790039,
      "learning_rate": 0.0001531160720259159,
      "loss": 2.4506,
      "step": 49890
    },
    {
      "epoch": 1.026558389627622,
      "grad_norm": 0.4611794352531433,
      "learning_rate": 0.00015309725356144963,
      "loss": 2.3889,
      "step": 49900
    },
    {
      "epoch": 1.0267641091444897,
      "grad_norm": 0.43131494522094727,
      "learning_rate": 0.0001530784324779852,
      "loss": 2.4199,
      "step": 49910
    },
    {
      "epoch": 1.0269698286613573,
      "grad_norm": 0.4739114046096802,
      "learning_rate": 0.00015305960877645092,
      "loss": 2.4199,
      "step": 49920
    },
    {
      "epoch": 1.0271755481782252,
      "grad_norm": 0.45556730031967163,
      "learning_rate": 0.00015304078245777532,
      "loss": 2.4268,
      "step": 49930
    },
    {
      "epoch": 1.0273812676950929,
      "grad_norm": 0.4504871964454651,
      "learning_rate": 0.00015302195352288696,
      "loss": 2.4277,
      "step": 49940
    },
    {
      "epoch": 1.0275869872119605,
      "grad_norm": 0.43723803758621216,
      "learning_rate": 0.0001530031219727145,
      "loss": 2.4759,
      "step": 49950
    },
    {
      "epoch": 1.0277927067288282,
      "grad_norm": 0.4177836775779724,
      "learning_rate": 0.0001529842878081869,
      "loss": 2.3785,
      "step": 49960
    },
    {
      "epoch": 1.027998426245696,
      "grad_norm": 0.4093233346939087,
      "learning_rate": 0.00015296545103023315,
      "loss": 2.4108,
      "step": 49970
    },
    {
      "epoch": 1.0282041457625637,
      "grad_norm": 0.4101930856704712,
      "learning_rate": 0.00015294661163978227,
      "loss": 2.4024,
      "step": 49980
    },
    {
      "epoch": 1.0284098652794313,
      "grad_norm": 0.44782137870788574,
      "learning_rate": 0.0001529277696377636,
      "loss": 2.4411,
      "step": 49990
    },
    {
      "epoch": 1.028615584796299,
      "grad_norm": 0.40667790174484253,
      "learning_rate": 0.00015290892502510647,
      "loss": 2.3913,
      "step": 50000
    },
    {
      "epoch": 1.0288213043131669,
      "grad_norm": 0.4032019078731537,
      "learning_rate": 0.0001528900778027404,
      "loss": 2.4306,
      "step": 50010
    },
    {
      "epoch": 1.0290270238300345,
      "grad_norm": 0.4276130795478821,
      "learning_rate": 0.00015287122797159498,
      "loss": 2.3771,
      "step": 50020
    },
    {
      "epoch": 1.0292327433469022,
      "grad_norm": 0.4034406244754791,
      "learning_rate": 0.00015285237553259998,
      "loss": 2.2941,
      "step": 50030
    },
    {
      "epoch": 1.02943846286377,
      "grad_norm": 0.4460429847240448,
      "learning_rate": 0.00015283352048668536,
      "loss": 2.4031,
      "step": 50040
    },
    {
      "epoch": 1.0296441823806377,
      "grad_norm": 0.42499497532844543,
      "learning_rate": 0.00015281466283478104,
      "loss": 2.4183,
      "step": 50050
    },
    {
      "epoch": 1.0298499018975054,
      "grad_norm": 0.3819199204444885,
      "learning_rate": 0.00015279580257781725,
      "loss": 2.3984,
      "step": 50060
    },
    {
      "epoch": 1.030055621414373,
      "grad_norm": 0.3955994248390198,
      "learning_rate": 0.00015277693971672417,
      "loss": 2.3548,
      "step": 50070
    },
    {
      "epoch": 1.0302613409312409,
      "grad_norm": 0.43712517619132996,
      "learning_rate": 0.0001527580742524323,
      "loss": 2.4432,
      "step": 50080
    },
    {
      "epoch": 1.0304670604481085,
      "grad_norm": 0.4463714063167572,
      "learning_rate": 0.0001527392061858721,
      "loss": 2.4506,
      "step": 50090
    },
    {
      "epoch": 1.0306727799649762,
      "grad_norm": 0.4111897945404053,
      "learning_rate": 0.00015272033551797426,
      "loss": 2.4214,
      "step": 50100
    },
    {
      "epoch": 1.030878499481844,
      "grad_norm": 0.46302053332328796,
      "learning_rate": 0.00015270146224966955,
      "loss": 2.4616,
      "step": 50110
    },
    {
      "epoch": 1.0310842189987117,
      "grad_norm": 0.4122423529624939,
      "learning_rate": 0.0001526825863818889,
      "loss": 2.3942,
      "step": 50120
    },
    {
      "epoch": 1.0312899385155794,
      "grad_norm": 0.39460626244544983,
      "learning_rate": 0.00015266370791556337,
      "loss": 2.3951,
      "step": 50130
    },
    {
      "epoch": 1.031495658032447,
      "grad_norm": 0.421147882938385,
      "learning_rate": 0.0001526448268516241,
      "loss": 2.4404,
      "step": 50140
    },
    {
      "epoch": 1.031701377549315,
      "grad_norm": 0.4392460882663727,
      "learning_rate": 0.0001526259431910024,
      "loss": 2.4044,
      "step": 50150
    },
    {
      "epoch": 1.0319070970661826,
      "grad_norm": 0.4151359796524048,
      "learning_rate": 0.0001526070569346297,
      "loss": 2.4245,
      "step": 50160
    },
    {
      "epoch": 1.0321128165830502,
      "grad_norm": 0.43241754174232483,
      "learning_rate": 0.00015258816808343752,
      "loss": 2.3727,
      "step": 50170
    },
    {
      "epoch": 1.0323185360999179,
      "grad_norm": 0.41011759638786316,
      "learning_rate": 0.00015256927663835762,
      "loss": 2.408,
      "step": 50180
    },
    {
      "epoch": 1.0325242556167857,
      "grad_norm": 0.4150097966194153,
      "learning_rate": 0.00015255038260032175,
      "loss": 2.3791,
      "step": 50190
    },
    {
      "epoch": 1.0327299751336534,
      "grad_norm": 0.4430963397026062,
      "learning_rate": 0.0001525314859702619,
      "loss": 2.3895,
      "step": 50200
    },
    {
      "epoch": 1.032935694650521,
      "grad_norm": 0.449190616607666,
      "learning_rate": 0.0001525125867491101,
      "loss": 2.471,
      "step": 50210
    },
    {
      "epoch": 1.033141414167389,
      "grad_norm": 0.4944794476032257,
      "learning_rate": 0.00015249368493779853,
      "loss": 2.4912,
      "step": 50220
    },
    {
      "epoch": 1.0333471336842566,
      "grad_norm": 0.40520721673965454,
      "learning_rate": 0.00015247478053725955,
      "loss": 2.4204,
      "step": 50230
    },
    {
      "epoch": 1.0335528532011242,
      "grad_norm": 0.39631056785583496,
      "learning_rate": 0.00015245587354842561,
      "loss": 2.4414,
      "step": 50240
    },
    {
      "epoch": 1.0337585727179919,
      "grad_norm": 0.40232741832733154,
      "learning_rate": 0.00015243696397222926,
      "loss": 2.3693,
      "step": 50250
    },
    {
      "epoch": 1.0339642922348597,
      "grad_norm": 0.4849339723587036,
      "learning_rate": 0.00015241805180960327,
      "loss": 2.4583,
      "step": 50260
    },
    {
      "epoch": 1.0341700117517274,
      "grad_norm": 0.4316275715827942,
      "learning_rate": 0.00015239913706148039,
      "loss": 2.3676,
      "step": 50270
    },
    {
      "epoch": 1.034375731268595,
      "grad_norm": 0.45159387588500977,
      "learning_rate": 0.00015238021972879362,
      "loss": 2.4046,
      "step": 50280
    },
    {
      "epoch": 1.034581450785463,
      "grad_norm": 0.4174150824546814,
      "learning_rate": 0.00015236129981247606,
      "loss": 2.4122,
      "step": 50290
    },
    {
      "epoch": 1.0347871703023306,
      "grad_norm": 0.41949060559272766,
      "learning_rate": 0.0001523423773134609,
      "loss": 2.338,
      "step": 50300
    },
    {
      "epoch": 1.0349928898191982,
      "grad_norm": 0.44650065898895264,
      "learning_rate": 0.00015232345223268153,
      "loss": 2.4755,
      "step": 50310
    },
    {
      "epoch": 1.035198609336066,
      "grad_norm": 0.4148569405078888,
      "learning_rate": 0.00015230452457107137,
      "loss": 2.3752,
      "step": 50320
    },
    {
      "epoch": 1.0354043288529338,
      "grad_norm": 0.3991631269454956,
      "learning_rate": 0.00015228559432956402,
      "loss": 2.3837,
      "step": 50330
    },
    {
      "epoch": 1.0356100483698014,
      "grad_norm": 0.41063573956489563,
      "learning_rate": 0.00015226666150909327,
      "loss": 2.3777,
      "step": 50340
    },
    {
      "epoch": 1.035815767886669,
      "grad_norm": 0.42928168177604675,
      "learning_rate": 0.00015224772611059287,
      "loss": 2.4131,
      "step": 50350
    },
    {
      "epoch": 1.036021487403537,
      "grad_norm": 0.40459418296813965,
      "learning_rate": 0.00015222878813499687,
      "loss": 2.4054,
      "step": 50360
    },
    {
      "epoch": 1.0362272069204046,
      "grad_norm": 0.4180908501148224,
      "learning_rate": 0.0001522098475832394,
      "loss": 2.4019,
      "step": 50370
    },
    {
      "epoch": 1.0364329264372723,
      "grad_norm": 0.4249191880226135,
      "learning_rate": 0.00015219090445625465,
      "loss": 2.4567,
      "step": 50380
    },
    {
      "epoch": 1.03663864595414,
      "grad_norm": 0.4047751724720001,
      "learning_rate": 0.00015217195875497693,
      "loss": 2.41,
      "step": 50390
    },
    {
      "epoch": 1.0368443654710078,
      "grad_norm": 0.4393456280231476,
      "learning_rate": 0.00015215301048034086,
      "loss": 2.3453,
      "step": 50400
    },
    {
      "epoch": 1.0370500849878754,
      "grad_norm": 0.4444811940193176,
      "learning_rate": 0.00015213405963328096,
      "loss": 2.3928,
      "step": 50410
    },
    {
      "epoch": 1.037255804504743,
      "grad_norm": 0.4106164574623108,
      "learning_rate": 0.000152115106214732,
      "loss": 2.4088,
      "step": 50420
    },
    {
      "epoch": 1.0374615240216107,
      "grad_norm": 0.4427502453327179,
      "learning_rate": 0.00015209615022562881,
      "loss": 2.3521,
      "step": 50430
    },
    {
      "epoch": 1.0376672435384786,
      "grad_norm": 0.44476866722106934,
      "learning_rate": 0.00015207719166690647,
      "loss": 2.4449,
      "step": 50440
    },
    {
      "epoch": 1.0378729630553463,
      "grad_norm": 0.44491276144981384,
      "learning_rate": 0.00015205823053950003,
      "loss": 2.4105,
      "step": 50450
    },
    {
      "epoch": 1.038078682572214,
      "grad_norm": 0.4168094992637634,
      "learning_rate": 0.00015203926684434474,
      "loss": 2.4848,
      "step": 50460
    },
    {
      "epoch": 1.0382844020890818,
      "grad_norm": 0.41887590289115906,
      "learning_rate": 0.00015202030058237602,
      "loss": 2.4241,
      "step": 50470
    },
    {
      "epoch": 1.0384901216059494,
      "grad_norm": 0.40735673904418945,
      "learning_rate": 0.00015200133175452936,
      "loss": 2.4272,
      "step": 50480
    },
    {
      "epoch": 1.038695841122817,
      "grad_norm": 0.4405399262905121,
      "learning_rate": 0.00015198236036174034,
      "loss": 2.4156,
      "step": 50490
    },
    {
      "epoch": 1.0389015606396848,
      "grad_norm": 0.4392273426055908,
      "learning_rate": 0.0001519633864049448,
      "loss": 2.3657,
      "step": 50500
    },
    {
      "epoch": 1.0391072801565526,
      "grad_norm": 0.4166141748428345,
      "learning_rate": 0.00015194440988507854,
      "loss": 2.3782,
      "step": 50510
    },
    {
      "epoch": 1.0393129996734203,
      "grad_norm": 0.4087090790271759,
      "learning_rate": 0.0001519254308030776,
      "loss": 2.4018,
      "step": 50520
    },
    {
      "epoch": 1.039518719190288,
      "grad_norm": 0.429459810256958,
      "learning_rate": 0.00015190644915987815,
      "loss": 2.3604,
      "step": 50530
    },
    {
      "epoch": 1.0397244387071556,
      "grad_norm": 0.41213637590408325,
      "learning_rate": 0.00015188746495641638,
      "loss": 2.412,
      "step": 50540
    },
    {
      "epoch": 1.0399301582240235,
      "grad_norm": 0.4531717598438263,
      "learning_rate": 0.00015186847819362873,
      "loss": 2.42,
      "step": 50550
    },
    {
      "epoch": 1.0401358777408911,
      "grad_norm": 0.4355602562427521,
      "learning_rate": 0.0001518494888724517,
      "loss": 2.3924,
      "step": 50560
    },
    {
      "epoch": 1.0403415972577588,
      "grad_norm": 0.4339573085308075,
      "learning_rate": 0.00015183049699382195,
      "loss": 2.3997,
      "step": 50570
    },
    {
      "epoch": 1.0405473167746266,
      "grad_norm": 0.4112797677516937,
      "learning_rate": 0.00015181150255867625,
      "loss": 2.3724,
      "step": 50580
    },
    {
      "epoch": 1.0407530362914943,
      "grad_norm": 0.40404266119003296,
      "learning_rate": 0.0001517925055679514,
      "loss": 2.3959,
      "step": 50590
    },
    {
      "epoch": 1.040958755808362,
      "grad_norm": 0.4457431435585022,
      "learning_rate": 0.0001517735060225845,
      "loss": 2.4257,
      "step": 50600
    },
    {
      "epoch": 1.0411644753252296,
      "grad_norm": 0.40843939781188965,
      "learning_rate": 0.0001517545039235127,
      "loss": 2.431,
      "step": 50610
    },
    {
      "epoch": 1.0413701948420975,
      "grad_norm": 0.4574012756347656,
      "learning_rate": 0.00015173549927167325,
      "loss": 2.3626,
      "step": 50620
    },
    {
      "epoch": 1.0415759143589651,
      "grad_norm": 0.43049129843711853,
      "learning_rate": 0.00015171649206800354,
      "loss": 2.4698,
      "step": 50630
    },
    {
      "epoch": 1.0417816338758328,
      "grad_norm": 0.4144086539745331,
      "learning_rate": 0.00015169748231344112,
      "loss": 2.4018,
      "step": 50640
    },
    {
      "epoch": 1.0419873533927007,
      "grad_norm": 0.4176633059978485,
      "learning_rate": 0.00015167847000892358,
      "loss": 2.4296,
      "step": 50650
    },
    {
      "epoch": 1.0421930729095683,
      "grad_norm": 0.41139090061187744,
      "learning_rate": 0.00015165945515538875,
      "loss": 2.4052,
      "step": 50660
    },
    {
      "epoch": 1.042398792426436,
      "grad_norm": 0.4220792353153229,
      "learning_rate": 0.00015164043775377448,
      "loss": 2.3802,
      "step": 50670
    },
    {
      "epoch": 1.0426045119433036,
      "grad_norm": 0.40402358770370483,
      "learning_rate": 0.00015162141780501886,
      "loss": 2.4062,
      "step": 50680
    },
    {
      "epoch": 1.0428102314601715,
      "grad_norm": 0.43228456377983093,
      "learning_rate": 0.00015160239531005994,
      "loss": 2.3876,
      "step": 50690
    },
    {
      "epoch": 1.0430159509770391,
      "grad_norm": 0.4179137945175171,
      "learning_rate": 0.00015158337026983614,
      "loss": 2.4375,
      "step": 50700
    },
    {
      "epoch": 1.0432216704939068,
      "grad_norm": 0.43765267729759216,
      "learning_rate": 0.00015156434268528575,
      "loss": 2.3929,
      "step": 50710
    },
    {
      "epoch": 1.0434273900107747,
      "grad_norm": 0.41821426153182983,
      "learning_rate": 0.0001515453125573473,
      "loss": 2.375,
      "step": 50720
    },
    {
      "epoch": 1.0436331095276423,
      "grad_norm": 0.4839707016944885,
      "learning_rate": 0.0001515262798869595,
      "loss": 2.4414,
      "step": 50730
    },
    {
      "epoch": 1.04383882904451,
      "grad_norm": 0.4157828688621521,
      "learning_rate": 0.00015150724467506108,
      "loss": 2.4272,
      "step": 50740
    },
    {
      "epoch": 1.0440445485613776,
      "grad_norm": 0.4227120876312256,
      "learning_rate": 0.00015148820692259096,
      "loss": 2.3707,
      "step": 50750
    },
    {
      "epoch": 1.0442502680782455,
      "grad_norm": 0.40899208188056946,
      "learning_rate": 0.00015146916663048819,
      "loss": 2.4572,
      "step": 50760
    },
    {
      "epoch": 1.0444559875951132,
      "grad_norm": 0.436514288187027,
      "learning_rate": 0.0001514501237996919,
      "loss": 2.4073,
      "step": 50770
    },
    {
      "epoch": 1.0446617071119808,
      "grad_norm": 0.4089309573173523,
      "learning_rate": 0.00015143107843114138,
      "loss": 2.4342,
      "step": 50780
    },
    {
      "epoch": 1.0448674266288485,
      "grad_norm": 0.38731616735458374,
      "learning_rate": 0.00015141203052577596,
      "loss": 2.4077,
      "step": 50790
    },
    {
      "epoch": 1.0450731461457163,
      "grad_norm": 0.4195520877838135,
      "learning_rate": 0.00015139298008453534,
      "loss": 2.3875,
      "step": 50800
    },
    {
      "epoch": 1.045278865662584,
      "grad_norm": 0.40013039112091064,
      "learning_rate": 0.000151373927108359,
      "loss": 2.4316,
      "step": 50810
    },
    {
      "epoch": 1.0454845851794516,
      "grad_norm": 0.399587482213974,
      "learning_rate": 0.0001513548715981868,
      "loss": 2.3459,
      "step": 50820
    },
    {
      "epoch": 1.0456903046963195,
      "grad_norm": 0.41323786973953247,
      "learning_rate": 0.00015133581355495864,
      "loss": 2.3408,
      "step": 50830
    },
    {
      "epoch": 1.0458960242131872,
      "grad_norm": 0.4129198491573334,
      "learning_rate": 0.00015131675297961453,
      "loss": 2.3497,
      "step": 50840
    },
    {
      "epoch": 1.0461017437300548,
      "grad_norm": 0.4488673508167267,
      "learning_rate": 0.00015129768987309468,
      "loss": 2.3985,
      "step": 50850
    },
    {
      "epoch": 1.0463074632469225,
      "grad_norm": 0.3963506817817688,
      "learning_rate": 0.00015127862423633927,
      "loss": 2.4066,
      "step": 50860
    },
    {
      "epoch": 1.0465131827637904,
      "grad_norm": 0.43394654989242554,
      "learning_rate": 0.0001512595560702888,
      "loss": 2.4122,
      "step": 50870
    },
    {
      "epoch": 1.046718902280658,
      "grad_norm": 0.41417062282562256,
      "learning_rate": 0.00015124048537588377,
      "loss": 2.3932,
      "step": 50880
    },
    {
      "epoch": 1.0469246217975257,
      "grad_norm": 0.4367990493774414,
      "learning_rate": 0.0001512214121540648,
      "loss": 2.3672,
      "step": 50890
    },
    {
      "epoch": 1.0471303413143933,
      "grad_norm": 0.41505303978919983,
      "learning_rate": 0.0001512023364057727,
      "loss": 2.4018,
      "step": 50900
    },
    {
      "epoch": 1.0473360608312612,
      "grad_norm": 0.42059987783432007,
      "learning_rate": 0.00015118325813194834,
      "loss": 2.4328,
      "step": 50910
    },
    {
      "epoch": 1.0475417803481288,
      "grad_norm": 0.41564691066741943,
      "learning_rate": 0.0001511641773335328,
      "loss": 2.5041,
      "step": 50920
    },
    {
      "epoch": 1.0477474998649965,
      "grad_norm": 0.40706804394721985,
      "learning_rate": 0.00015114509401146725,
      "loss": 2.4634,
      "step": 50930
    },
    {
      "epoch": 1.0479532193818644,
      "grad_norm": 0.4291899800300598,
      "learning_rate": 0.00015112600816669284,
      "loss": 2.4676,
      "step": 50940
    },
    {
      "epoch": 1.048158938898732,
      "grad_norm": 0.4477938115596771,
      "learning_rate": 0.00015110691980015106,
      "loss": 2.443,
      "step": 50950
    },
    {
      "epoch": 1.0483646584155997,
      "grad_norm": 0.4288772642612457,
      "learning_rate": 0.00015108782891278346,
      "loss": 2.3643,
      "step": 50960
    },
    {
      "epoch": 1.0485703779324673,
      "grad_norm": 0.4314512610435486,
      "learning_rate": 0.0001510687355055316,
      "loss": 2.4303,
      "step": 50970
    },
    {
      "epoch": 1.0487760974493352,
      "grad_norm": 0.4439695179462433,
      "learning_rate": 0.00015104963957933738,
      "loss": 2.4207,
      "step": 50980
    },
    {
      "epoch": 1.0489818169662029,
      "grad_norm": 0.41565433144569397,
      "learning_rate": 0.00015103054113514258,
      "loss": 2.4106,
      "step": 50990
    },
    {
      "epoch": 1.0491875364830705,
      "grad_norm": 0.4132968485355377,
      "learning_rate": 0.0001510114401738893,
      "loss": 2.4636,
      "step": 51000
    },
    {
      "epoch": 1.0493932559999384,
      "grad_norm": 0.38889384269714355,
      "learning_rate": 0.00015099233669651962,
      "loss": 2.3709,
      "step": 51010
    },
    {
      "epoch": 1.049598975516806,
      "grad_norm": 0.39115431904792786,
      "learning_rate": 0.00015097323070397585,
      "loss": 2.3836,
      "step": 51020
    },
    {
      "epoch": 1.0498046950336737,
      "grad_norm": 0.4071335792541504,
      "learning_rate": 0.00015095412219720043,
      "loss": 2.429,
      "step": 51030
    },
    {
      "epoch": 1.0500104145505413,
      "grad_norm": 0.43326881527900696,
      "learning_rate": 0.00015093501117713579,
      "loss": 2.3671,
      "step": 51040
    },
    {
      "epoch": 1.0502161340674092,
      "grad_norm": 0.44845396280288696,
      "learning_rate": 0.0001509158976447246,
      "loss": 2.4186,
      "step": 51050
    },
    {
      "epoch": 1.0504218535842769,
      "grad_norm": 0.4181499481201172,
      "learning_rate": 0.00015089678160090967,
      "loss": 2.3667,
      "step": 51060
    },
    {
      "epoch": 1.0506275731011445,
      "grad_norm": 0.4058658480644226,
      "learning_rate": 0.00015087766304663385,
      "loss": 2.3962,
      "step": 51070
    },
    {
      "epoch": 1.0508332926180124,
      "grad_norm": 0.4091082215309143,
      "learning_rate": 0.00015085854198284012,
      "loss": 2.4143,
      "step": 51080
    },
    {
      "epoch": 1.05103901213488,
      "grad_norm": 0.43868115544319153,
      "learning_rate": 0.0001508394184104717,
      "loss": 2.4037,
      "step": 51090
    },
    {
      "epoch": 1.0512447316517477,
      "grad_norm": 0.41575887799263,
      "learning_rate": 0.0001508202923304718,
      "loss": 2.3686,
      "step": 51100
    },
    {
      "epoch": 1.0514504511686154,
      "grad_norm": 0.4224376082420349,
      "learning_rate": 0.00015080116374378383,
      "loss": 2.3677,
      "step": 51110
    },
    {
      "epoch": 1.0516561706854832,
      "grad_norm": 0.4457456171512604,
      "learning_rate": 0.00015078203265135126,
      "loss": 2.4317,
      "step": 51120
    },
    {
      "epoch": 1.0518618902023509,
      "grad_norm": 0.421220988035202,
      "learning_rate": 0.00015076289905411778,
      "loss": 2.4426,
      "step": 51130
    },
    {
      "epoch": 1.0520676097192185,
      "grad_norm": 0.4178074598312378,
      "learning_rate": 0.0001507437629530271,
      "loss": 2.3971,
      "step": 51140
    },
    {
      "epoch": 1.0522733292360862,
      "grad_norm": 0.3902132511138916,
      "learning_rate": 0.0001507246243490231,
      "loss": 2.3318,
      "step": 51150
    },
    {
      "epoch": 1.052479048752954,
      "grad_norm": 0.5202465057373047,
      "learning_rate": 0.0001507054832430498,
      "loss": 2.441,
      "step": 51160
    },
    {
      "epoch": 1.0526847682698217,
      "grad_norm": 0.44148558378219604,
      "learning_rate": 0.00015068633963605135,
      "loss": 2.4319,
      "step": 51170
    },
    {
      "epoch": 1.0528904877866894,
      "grad_norm": 0.3999145030975342,
      "learning_rate": 0.00015066719352897193,
      "loss": 2.3694,
      "step": 51180
    },
    {
      "epoch": 1.0530962073035572,
      "grad_norm": 0.40861931443214417,
      "learning_rate": 0.00015064804492275602,
      "loss": 2.3814,
      "step": 51190
    },
    {
      "epoch": 1.053301926820425,
      "grad_norm": 0.49774444103240967,
      "learning_rate": 0.00015062889381834798,
      "loss": 2.4851,
      "step": 51200
    },
    {
      "epoch": 1.0535076463372925,
      "grad_norm": 0.4345712959766388,
      "learning_rate": 0.00015060974021669253,
      "loss": 2.388,
      "step": 51210
    },
    {
      "epoch": 1.0537133658541602,
      "grad_norm": 0.4198484420776367,
      "learning_rate": 0.0001505905841187344,
      "loss": 2.4258,
      "step": 51220
    },
    {
      "epoch": 1.053919085371028,
      "grad_norm": 0.4691254794597626,
      "learning_rate": 0.00015057142552541846,
      "loss": 2.4307,
      "step": 51230
    },
    {
      "epoch": 1.0541248048878957,
      "grad_norm": 0.4521004855632782,
      "learning_rate": 0.0001505522644376896,
      "loss": 2.4004,
      "step": 51240
    },
    {
      "epoch": 1.0543305244047634,
      "grad_norm": 0.42679810523986816,
      "learning_rate": 0.0001505331008564931,
      "loss": 2.4318,
      "step": 51250
    },
    {
      "epoch": 1.0545362439216313,
      "grad_norm": 0.40426555275917053,
      "learning_rate": 0.00015051393478277407,
      "loss": 2.4406,
      "step": 51260
    },
    {
      "epoch": 1.054741963438499,
      "grad_norm": 0.43126168847084045,
      "learning_rate": 0.0001504947662174779,
      "loss": 2.3992,
      "step": 51270
    },
    {
      "epoch": 1.0549476829553666,
      "grad_norm": 0.38449037075042725,
      "learning_rate": 0.0001504755951615501,
      "loss": 2.3869,
      "step": 51280
    },
    {
      "epoch": 1.0551534024722342,
      "grad_norm": 0.48545899987220764,
      "learning_rate": 0.00015045642161593624,
      "loss": 2.428,
      "step": 51290
    },
    {
      "epoch": 1.055359121989102,
      "grad_norm": 0.4444867968559265,
      "learning_rate": 0.00015043724558158204,
      "loss": 2.4225,
      "step": 51300
    },
    {
      "epoch": 1.0555648415059697,
      "grad_norm": 0.46161341667175293,
      "learning_rate": 0.00015041806705943337,
      "loss": 2.3971,
      "step": 51310
    },
    {
      "epoch": 1.0557705610228374,
      "grad_norm": 0.4040471017360687,
      "learning_rate": 0.0001503988860504362,
      "loss": 2.3772,
      "step": 51320
    },
    {
      "epoch": 1.055976280539705,
      "grad_norm": 0.4191186726093292,
      "learning_rate": 0.0001503797025555366,
      "loss": 2.3693,
      "step": 51330
    },
    {
      "epoch": 1.056182000056573,
      "grad_norm": 0.45076099038124084,
      "learning_rate": 0.00015036051657568085,
      "loss": 2.3904,
      "step": 51340
    },
    {
      "epoch": 1.0563877195734406,
      "grad_norm": 0.4272932708263397,
      "learning_rate": 0.00015034132811181524,
      "loss": 2.4371,
      "step": 51350
    },
    {
      "epoch": 1.0565934390903082,
      "grad_norm": 0.39973267912864685,
      "learning_rate": 0.0001503221371648862,
      "loss": 2.4108,
      "step": 51360
    },
    {
      "epoch": 1.056799158607176,
      "grad_norm": 0.5149929523468018,
      "learning_rate": 0.0001503029437358404,
      "loss": 2.4442,
      "step": 51370
    },
    {
      "epoch": 1.0570048781240438,
      "grad_norm": 0.42496126890182495,
      "learning_rate": 0.00015028374782562453,
      "loss": 2.3998,
      "step": 51380
    },
    {
      "epoch": 1.0572105976409114,
      "grad_norm": 0.4482967257499695,
      "learning_rate": 0.00015026454943518536,
      "loss": 2.3898,
      "step": 51390
    },
    {
      "epoch": 1.057416317157779,
      "grad_norm": 0.4411485195159912,
      "learning_rate": 0.00015024534856546983,
      "loss": 2.344,
      "step": 51400
    },
    {
      "epoch": 1.057622036674647,
      "grad_norm": 0.447113573551178,
      "learning_rate": 0.00015022614521742513,
      "loss": 2.374,
      "step": 51410
    },
    {
      "epoch": 1.0578277561915146,
      "grad_norm": 0.429050087928772,
      "learning_rate": 0.00015020693939199836,
      "loss": 2.4054,
      "step": 51420
    },
    {
      "epoch": 1.0580334757083822,
      "grad_norm": 0.45453354716300964,
      "learning_rate": 0.00015018773109013687,
      "loss": 2.4249,
      "step": 51430
    },
    {
      "epoch": 1.0582391952252501,
      "grad_norm": 0.4004703164100647,
      "learning_rate": 0.00015016852031278808,
      "loss": 2.4027,
      "step": 51440
    },
    {
      "epoch": 1.0584449147421178,
      "grad_norm": 0.4002636969089508,
      "learning_rate": 0.00015014930706089955,
      "loss": 2.4137,
      "step": 51450
    },
    {
      "epoch": 1.0586506342589854,
      "grad_norm": 0.535732626914978,
      "learning_rate": 0.00015013009133541902,
      "loss": 2.3754,
      "step": 51460
    },
    {
      "epoch": 1.058856353775853,
      "grad_norm": 0.4065394103527069,
      "learning_rate": 0.00015011087313729425,
      "loss": 2.3801,
      "step": 51470
    },
    {
      "epoch": 1.059062073292721,
      "grad_norm": 0.422893226146698,
      "learning_rate": 0.00015009165246747318,
      "loss": 2.4287,
      "step": 51480
    },
    {
      "epoch": 1.0592677928095886,
      "grad_norm": 0.4215599000453949,
      "learning_rate": 0.00015007242932690387,
      "loss": 2.434,
      "step": 51490
    },
    {
      "epoch": 1.0594735123264563,
      "grad_norm": 0.4167447090148926,
      "learning_rate": 0.00015005320371653448,
      "loss": 2.4074,
      "step": 51500
    },
    {
      "epoch": 1.059679231843324,
      "grad_norm": 0.41234228014945984,
      "learning_rate": 0.0001500339756373133,
      "loss": 2.4151,
      "step": 51510
    },
    {
      "epoch": 1.0598849513601918,
      "grad_norm": 0.4251292943954468,
      "learning_rate": 0.00015001474509018876,
      "loss": 2.393,
      "step": 51520
    },
    {
      "epoch": 1.0600906708770594,
      "grad_norm": 0.4342307150363922,
      "learning_rate": 0.00014999551207610937,
      "loss": 2.3686,
      "step": 51530
    },
    {
      "epoch": 1.060296390393927,
      "grad_norm": 0.43527722358703613,
      "learning_rate": 0.00014997627659602385,
      "loss": 2.4235,
      "step": 51540
    },
    {
      "epoch": 1.060502109910795,
      "grad_norm": 0.3902512788772583,
      "learning_rate": 0.00014995703865088093,
      "loss": 2.3447,
      "step": 51550
    },
    {
      "epoch": 1.0607078294276626,
      "grad_norm": 0.4933490455150604,
      "learning_rate": 0.0001499377982416295,
      "loss": 2.4151,
      "step": 51560
    },
    {
      "epoch": 1.0609135489445303,
      "grad_norm": 0.41323503851890564,
      "learning_rate": 0.00014991855536921865,
      "loss": 2.3959,
      "step": 51570
    },
    {
      "epoch": 1.061119268461398,
      "grad_norm": 0.42315244674682617,
      "learning_rate": 0.00014989931003459746,
      "loss": 2.4169,
      "step": 51580
    },
    {
      "epoch": 1.0613249879782658,
      "grad_norm": 0.39707279205322266,
      "learning_rate": 0.00014988006223871523,
      "loss": 2.3964,
      "step": 51590
    },
    {
      "epoch": 1.0615307074951335,
      "grad_norm": 0.4017598628997803,
      "learning_rate": 0.00014986081198252137,
      "loss": 2.4244,
      "step": 51600
    },
    {
      "epoch": 1.061736427012001,
      "grad_norm": 0.41947227716445923,
      "learning_rate": 0.00014984155926696532,
      "loss": 2.4086,
      "step": 51610
    },
    {
      "epoch": 1.061942146528869,
      "grad_norm": 0.44918179512023926,
      "learning_rate": 0.00014982230409299677,
      "loss": 2.4779,
      "step": 51620
    },
    {
      "epoch": 1.0621478660457366,
      "grad_norm": 0.4971177875995636,
      "learning_rate": 0.0001498030464615655,
      "loss": 2.3835,
      "step": 51630
    },
    {
      "epoch": 1.0623535855626043,
      "grad_norm": 0.43231213092803955,
      "learning_rate": 0.0001497837863736213,
      "loss": 2.3744,
      "step": 51640
    },
    {
      "epoch": 1.062559305079472,
      "grad_norm": 0.4428907036781311,
      "learning_rate": 0.00014976452383011423,
      "loss": 2.4795,
      "step": 51650
    },
    {
      "epoch": 1.0627650245963398,
      "grad_norm": 0.5288029313087463,
      "learning_rate": 0.00014974525883199434,
      "loss": 2.4097,
      "step": 51660
    },
    {
      "epoch": 1.0629707441132075,
      "grad_norm": 0.43720725178718567,
      "learning_rate": 0.00014972599138021194,
      "loss": 2.3987,
      "step": 51670
    },
    {
      "epoch": 1.0631764636300751,
      "grad_norm": 0.4160318076610565,
      "learning_rate": 0.00014970672147571735,
      "loss": 2.3732,
      "step": 51680
    },
    {
      "epoch": 1.0633821831469428,
      "grad_norm": 0.40306374430656433,
      "learning_rate": 0.00014968744911946107,
      "loss": 2.4825,
      "step": 51690
    },
    {
      "epoch": 1.0635879026638106,
      "grad_norm": 0.43502846360206604,
      "learning_rate": 0.0001496681743123937,
      "loss": 2.4231,
      "step": 51700
    },
    {
      "epoch": 1.0637936221806783,
      "grad_norm": 0.408210813999176,
      "learning_rate": 0.00014964889705546593,
      "loss": 2.3583,
      "step": 51710
    },
    {
      "epoch": 1.063999341697546,
      "grad_norm": 0.4726521074771881,
      "learning_rate": 0.0001496296173496286,
      "loss": 2.416,
      "step": 51720
    },
    {
      "epoch": 1.0642050612144138,
      "grad_norm": 0.3935950696468353,
      "learning_rate": 0.00014961033519583276,
      "loss": 2.4154,
      "step": 51730
    },
    {
      "epoch": 1.0644107807312815,
      "grad_norm": 0.42868685722351074,
      "learning_rate": 0.0001495910505950294,
      "loss": 2.3751,
      "step": 51740
    },
    {
      "epoch": 1.0646165002481491,
      "grad_norm": 0.40897077322006226,
      "learning_rate": 0.00014957176354816974,
      "loss": 2.3854,
      "step": 51750
    },
    {
      "epoch": 1.0648222197650168,
      "grad_norm": 0.42684653401374817,
      "learning_rate": 0.0001495524740562051,
      "loss": 2.431,
      "step": 51760
    },
    {
      "epoch": 1.0650279392818847,
      "grad_norm": 0.4196072816848755,
      "learning_rate": 0.00014953318212008698,
      "loss": 2.4096,
      "step": 51770
    },
    {
      "epoch": 1.0652336587987523,
      "grad_norm": 0.3946353793144226,
      "learning_rate": 0.0001495138877407669,
      "loss": 2.3772,
      "step": 51780
    },
    {
      "epoch": 1.06543937831562,
      "grad_norm": 0.3977743685245514,
      "learning_rate": 0.00014949459091919657,
      "loss": 2.4157,
      "step": 51790
    },
    {
      "epoch": 1.0656450978324878,
      "grad_norm": 0.42096155881881714,
      "learning_rate": 0.00014947529165632774,
      "loss": 2.4106,
      "step": 51800
    },
    {
      "epoch": 1.0658508173493555,
      "grad_norm": 0.43715548515319824,
      "learning_rate": 0.0001494559899531124,
      "loss": 2.4062,
      "step": 51810
    },
    {
      "epoch": 1.0660565368662231,
      "grad_norm": 0.4521862268447876,
      "learning_rate": 0.0001494366858105026,
      "loss": 2.447,
      "step": 51820
    },
    {
      "epoch": 1.0662622563830908,
      "grad_norm": 0.40112927556037903,
      "learning_rate": 0.0001494173792294505,
      "loss": 2.4077,
      "step": 51830
    },
    {
      "epoch": 1.0664679758999587,
      "grad_norm": 0.42270928621292114,
      "learning_rate": 0.0001493980702109083,
      "loss": 2.4005,
      "step": 51840
    },
    {
      "epoch": 1.0666736954168263,
      "grad_norm": 0.44486892223358154,
      "learning_rate": 0.00014937875875582854,
      "loss": 2.379,
      "step": 51850
    },
    {
      "epoch": 1.066879414933694,
      "grad_norm": 0.39589425921440125,
      "learning_rate": 0.0001493594448651637,
      "loss": 2.4041,
      "step": 51860
    },
    {
      "epoch": 1.0670851344505619,
      "grad_norm": 0.4133852422237396,
      "learning_rate": 0.00014934012853986642,
      "loss": 2.3641,
      "step": 51870
    },
    {
      "epoch": 1.0672908539674295,
      "grad_norm": 0.44897252321243286,
      "learning_rate": 0.00014932080978088944,
      "loss": 2.4271,
      "step": 51880
    },
    {
      "epoch": 1.0674965734842972,
      "grad_norm": 0.4231034815311432,
      "learning_rate": 0.0001493014885891857,
      "loss": 2.4157,
      "step": 51890
    },
    {
      "epoch": 1.0677022930011648,
      "grad_norm": 0.41772082448005676,
      "learning_rate": 0.00014928216496570818,
      "loss": 2.403,
      "step": 51900
    },
    {
      "epoch": 1.0679080125180327,
      "grad_norm": 0.3864991366863251,
      "learning_rate": 0.00014926283891141,
      "loss": 2.4561,
      "step": 51910
    },
    {
      "epoch": 1.0681137320349003,
      "grad_norm": 0.38412269949913025,
      "learning_rate": 0.00014924351042724444,
      "loss": 2.3824,
      "step": 51920
    },
    {
      "epoch": 1.068319451551768,
      "grad_norm": 0.41648873686790466,
      "learning_rate": 0.0001492241795141649,
      "loss": 2.3959,
      "step": 51930
    },
    {
      "epoch": 1.0685251710686356,
      "grad_norm": 0.4153674840927124,
      "learning_rate": 0.00014920484617312476,
      "loss": 2.4124,
      "step": 51940
    },
    {
      "epoch": 1.0687308905855035,
      "grad_norm": 0.3936762511730194,
      "learning_rate": 0.00014918551040507774,
      "loss": 2.4509,
      "step": 51950
    },
    {
      "epoch": 1.0689366101023712,
      "grad_norm": 0.4435018002986908,
      "learning_rate": 0.0001491661722109775,
      "loss": 2.4047,
      "step": 51960
    },
    {
      "epoch": 1.0691423296192388,
      "grad_norm": 0.4111737012863159,
      "learning_rate": 0.0001491468315917779,
      "loss": 2.3556,
      "step": 51970
    },
    {
      "epoch": 1.0693480491361067,
      "grad_norm": 0.39218780398368835,
      "learning_rate": 0.00014912748854843292,
      "loss": 2.4118,
      "step": 51980
    },
    {
      "epoch": 1.0695537686529744,
      "grad_norm": 0.4630088210105896,
      "learning_rate": 0.00014910814308189666,
      "loss": 2.3568,
      "step": 51990
    },
    {
      "epoch": 1.069759488169842,
      "grad_norm": 0.4274585247039795,
      "learning_rate": 0.00014908879519312328,
      "loss": 2.4524,
      "step": 52000
    },
    {
      "epoch": 1.0699652076867097,
      "grad_norm": 0.43680599331855774,
      "learning_rate": 0.00014906944488306714,
      "loss": 2.4089,
      "step": 52010
    },
    {
      "epoch": 1.0701709272035775,
      "grad_norm": 0.4115709960460663,
      "learning_rate": 0.00014905009215268272,
      "loss": 2.4257,
      "step": 52020
    },
    {
      "epoch": 1.0703766467204452,
      "grad_norm": 0.42130205035209656,
      "learning_rate": 0.0001490307370029245,
      "loss": 2.4253,
      "step": 52030
    },
    {
      "epoch": 1.0705823662373128,
      "grad_norm": 0.43304571509361267,
      "learning_rate": 0.00014901137943474724,
      "loss": 2.3862,
      "step": 52040
    },
    {
      "epoch": 1.0707880857541805,
      "grad_norm": 0.425008624792099,
      "learning_rate": 0.0001489920194491057,
      "loss": 2.4288,
      "step": 52050
    },
    {
      "epoch": 1.0709938052710484,
      "grad_norm": 0.4085164964199066,
      "learning_rate": 0.00014897265704695483,
      "loss": 2.3659,
      "step": 52060
    },
    {
      "epoch": 1.071199524787916,
      "grad_norm": 0.4462035298347473,
      "learning_rate": 0.00014895329222924962,
      "loss": 2.37,
      "step": 52070
    },
    {
      "epoch": 1.0714052443047837,
      "grad_norm": 0.4225359857082367,
      "learning_rate": 0.0001489339249969453,
      "loss": 2.4099,
      "step": 52080
    },
    {
      "epoch": 1.0716109638216516,
      "grad_norm": 0.4242302477359772,
      "learning_rate": 0.00014891455535099709,
      "loss": 2.4253,
      "step": 52090
    },
    {
      "epoch": 1.0718166833385192,
      "grad_norm": 0.4316127598285675,
      "learning_rate": 0.00014889518329236042,
      "loss": 2.4326,
      "step": 52100
    },
    {
      "epoch": 1.0720224028553869,
      "grad_norm": 0.44207262992858887,
      "learning_rate": 0.00014887580882199083,
      "loss": 2.495,
      "step": 52110
    },
    {
      "epoch": 1.0722281223722545,
      "grad_norm": 0.44301074743270874,
      "learning_rate": 0.0001488564319408439,
      "loss": 2.3757,
      "step": 52120
    },
    {
      "epoch": 1.0724338418891224,
      "grad_norm": 0.43318474292755127,
      "learning_rate": 0.00014883705264987545,
      "loss": 2.4569,
      "step": 52130
    },
    {
      "epoch": 1.07263956140599,
      "grad_norm": 0.4251428246498108,
      "learning_rate": 0.0001488176709500413,
      "loss": 2.3587,
      "step": 52140
    },
    {
      "epoch": 1.0728452809228577,
      "grad_norm": 0.4286699593067169,
      "learning_rate": 0.0001487982868422975,
      "loss": 2.4063,
      "step": 52150
    },
    {
      "epoch": 1.0730510004397256,
      "grad_norm": 0.42470353841781616,
      "learning_rate": 0.0001487789003276001,
      "loss": 2.3925,
      "step": 52160
    },
    {
      "epoch": 1.0732567199565932,
      "grad_norm": 0.4104568064212799,
      "learning_rate": 0.00014875951140690537,
      "loss": 2.4295,
      "step": 52170
    },
    {
      "epoch": 1.0734624394734609,
      "grad_norm": 0.4243650734424591,
      "learning_rate": 0.00014874012008116963,
      "loss": 2.3924,
      "step": 52180
    },
    {
      "epoch": 1.0736681589903285,
      "grad_norm": 0.4870106875896454,
      "learning_rate": 0.0001487207263513494,
      "loss": 2.3864,
      "step": 52190
    },
    {
      "epoch": 1.0738738785071964,
      "grad_norm": 0.41370218992233276,
      "learning_rate": 0.00014870133021840123,
      "loss": 2.3756,
      "step": 52200
    },
    {
      "epoch": 1.074079598024064,
      "grad_norm": 0.4195683002471924,
      "learning_rate": 0.00014868193168328183,
      "loss": 2.3902,
      "step": 52210
    },
    {
      "epoch": 1.0742853175409317,
      "grad_norm": 0.4101768136024475,
      "learning_rate": 0.000148662530746948,
      "loss": 2.3514,
      "step": 52220
    },
    {
      "epoch": 1.0744910370577996,
      "grad_norm": 0.45651522278785706,
      "learning_rate": 0.0001486431274103567,
      "loss": 2.4283,
      "step": 52230
    },
    {
      "epoch": 1.0746967565746672,
      "grad_norm": 0.46575602889060974,
      "learning_rate": 0.00014862372167446502,
      "loss": 2.4595,
      "step": 52240
    },
    {
      "epoch": 1.0749024760915349,
      "grad_norm": 0.40727272629737854,
      "learning_rate": 0.00014860431354023014,
      "loss": 2.3997,
      "step": 52250
    },
    {
      "epoch": 1.0751081956084025,
      "grad_norm": 0.4042564034461975,
      "learning_rate": 0.0001485849030086093,
      "loss": 2.4025,
      "step": 52260
    },
    {
      "epoch": 1.0753139151252704,
      "grad_norm": 0.5244693756103516,
      "learning_rate": 0.00014856549008055998,
      "loss": 2.4012,
      "step": 52270
    },
    {
      "epoch": 1.075519634642138,
      "grad_norm": 0.4472123682498932,
      "learning_rate": 0.00014854607475703967,
      "loss": 2.4095,
      "step": 52280
    },
    {
      "epoch": 1.0757253541590057,
      "grad_norm": 0.4553723931312561,
      "learning_rate": 0.000148526657039006,
      "loss": 2.3576,
      "step": 52290
    },
    {
      "epoch": 1.0759310736758734,
      "grad_norm": 0.40732061862945557,
      "learning_rate": 0.00014850723692741682,
      "loss": 2.4121,
      "step": 52300
    },
    {
      "epoch": 1.0761367931927412,
      "grad_norm": 0.4113919734954834,
      "learning_rate": 0.00014848781442323,
      "loss": 2.4452,
      "step": 52310
    },
    {
      "epoch": 1.076342512709609,
      "grad_norm": 0.8432266116142273,
      "learning_rate": 0.00014846838952740349,
      "loss": 2.3907,
      "step": 52320
    },
    {
      "epoch": 1.0765482322264766,
      "grad_norm": 0.3937157690525055,
      "learning_rate": 0.0001484489622408954,
      "loss": 2.4568,
      "step": 52330
    },
    {
      "epoch": 1.0767539517433444,
      "grad_norm": 0.4046473503112793,
      "learning_rate": 0.00014842953256466408,
      "loss": 2.4384,
      "step": 52340
    },
    {
      "epoch": 1.076959671260212,
      "grad_norm": 0.4402866065502167,
      "learning_rate": 0.0001484101004996678,
      "loss": 2.3995,
      "step": 52350
    },
    {
      "epoch": 1.0771653907770797,
      "grad_norm": 0.4634626805782318,
      "learning_rate": 0.0001483906660468651,
      "loss": 2.4399,
      "step": 52360
    },
    {
      "epoch": 1.0773711102939474,
      "grad_norm": 0.4456382393836975,
      "learning_rate": 0.0001483712292072145,
      "loss": 2.3986,
      "step": 52370
    },
    {
      "epoch": 1.0775768298108153,
      "grad_norm": 0.39813393354415894,
      "learning_rate": 0.00014835178998167476,
      "loss": 2.4388,
      "step": 52380
    },
    {
      "epoch": 1.077782549327683,
      "grad_norm": 0.4387318193912506,
      "learning_rate": 0.00014833234837120473,
      "loss": 2.4245,
      "step": 52390
    },
    {
      "epoch": 1.0779882688445506,
      "grad_norm": 0.39774999022483826,
      "learning_rate": 0.0001483129043767633,
      "loss": 2.4177,
      "step": 52400
    },
    {
      "epoch": 1.0781939883614182,
      "grad_norm": 0.4207608103752136,
      "learning_rate": 0.0001482934579993096,
      "loss": 2.4282,
      "step": 52410
    },
    {
      "epoch": 1.078399707878286,
      "grad_norm": 0.4223477840423584,
      "learning_rate": 0.00014827400923980277,
      "loss": 2.4353,
      "step": 52420
    },
    {
      "epoch": 1.0786054273951537,
      "grad_norm": 0.43467578291893005,
      "learning_rate": 0.00014825455809920211,
      "loss": 2.4136,
      "step": 52430
    },
    {
      "epoch": 1.0788111469120214,
      "grad_norm": 0.4519253075122833,
      "learning_rate": 0.00014823510457846712,
      "loss": 2.4266,
      "step": 52440
    },
    {
      "epoch": 1.0790168664288893,
      "grad_norm": 0.4592055678367615,
      "learning_rate": 0.00014821564867855723,
      "loss": 2.4656,
      "step": 52450
    },
    {
      "epoch": 1.079222585945757,
      "grad_norm": 0.4285256266593933,
      "learning_rate": 0.00014819619040043212,
      "loss": 2.3952,
      "step": 52460
    },
    {
      "epoch": 1.0794283054626246,
      "grad_norm": 0.42906269431114197,
      "learning_rate": 0.0001481767297450516,
      "loss": 2.3805,
      "step": 52470
    },
    {
      "epoch": 1.0796340249794922,
      "grad_norm": 0.40353333950042725,
      "learning_rate": 0.0001481572667133755,
      "loss": 2.3667,
      "step": 52480
    },
    {
      "epoch": 1.07983974449636,
      "grad_norm": 0.5372778177261353,
      "learning_rate": 0.00014813780130636388,
      "loss": 2.4327,
      "step": 52490
    },
    {
      "epoch": 1.0800454640132278,
      "grad_norm": 0.44278180599212646,
      "learning_rate": 0.00014811833352497685,
      "loss": 2.4511,
      "step": 52500
    },
    {
      "epoch": 1.0802511835300954,
      "grad_norm": 0.4185883700847626,
      "learning_rate": 0.00014809886337017463,
      "loss": 2.3759,
      "step": 52510
    },
    {
      "epoch": 1.0804569030469633,
      "grad_norm": 0.41842395067214966,
      "learning_rate": 0.0001480793908429176,
      "loss": 2.3658,
      "step": 52520
    },
    {
      "epoch": 1.080662622563831,
      "grad_norm": 0.42119091749191284,
      "learning_rate": 0.0001480599159441662,
      "loss": 2.4723,
      "step": 52530
    },
    {
      "epoch": 1.0808683420806986,
      "grad_norm": 0.4229893982410431,
      "learning_rate": 0.00014804043867488103,
      "loss": 2.5015,
      "step": 52540
    },
    {
      "epoch": 1.0810740615975662,
      "grad_norm": 0.4671395421028137,
      "learning_rate": 0.00014802095903602283,
      "loss": 2.3928,
      "step": 52550
    },
    {
      "epoch": 1.0812797811144341,
      "grad_norm": 0.40669751167297363,
      "learning_rate": 0.00014800147702855244,
      "loss": 2.4609,
      "step": 52560
    },
    {
      "epoch": 1.0814855006313018,
      "grad_norm": 0.4672379195690155,
      "learning_rate": 0.00014798199265343072,
      "loss": 2.4215,
      "step": 52570
    },
    {
      "epoch": 1.0816912201481694,
      "grad_norm": 0.3903380334377289,
      "learning_rate": 0.00014796250591161877,
      "loss": 2.402,
      "step": 52580
    },
    {
      "epoch": 1.0818969396650373,
      "grad_norm": 0.4119887053966522,
      "learning_rate": 0.00014794301680407779,
      "loss": 2.3929,
      "step": 52590
    },
    {
      "epoch": 1.082102659181905,
      "grad_norm": 0.44279369711875916,
      "learning_rate": 0.00014792352533176905,
      "loss": 2.3561,
      "step": 52600
    },
    {
      "epoch": 1.0823083786987726,
      "grad_norm": 0.42619195580482483,
      "learning_rate": 0.00014790403149565393,
      "loss": 2.3876,
      "step": 52610
    },
    {
      "epoch": 1.0825140982156403,
      "grad_norm": 0.4579702317714691,
      "learning_rate": 0.000147884535296694,
      "loss": 2.4112,
      "step": 52620
    },
    {
      "epoch": 1.0827198177325081,
      "grad_norm": 0.489894837141037,
      "learning_rate": 0.00014786503673585091,
      "loss": 2.4537,
      "step": 52630
    },
    {
      "epoch": 1.0829255372493758,
      "grad_norm": 0.38479936122894287,
      "learning_rate": 0.00014784553581408634,
      "loss": 2.4173,
      "step": 52640
    },
    {
      "epoch": 1.0831312567662434,
      "grad_norm": 0.4613379240036011,
      "learning_rate": 0.0001478260325323622,
      "loss": 2.4285,
      "step": 52650
    },
    {
      "epoch": 1.083336976283111,
      "grad_norm": 0.4344017505645752,
      "learning_rate": 0.00014780652689164056,
      "loss": 2.4183,
      "step": 52660
    },
    {
      "epoch": 1.083542695799979,
      "grad_norm": 0.45150816440582275,
      "learning_rate": 0.00014778701889288343,
      "loss": 2.3819,
      "step": 52670
    },
    {
      "epoch": 1.0837484153168466,
      "grad_norm": 0.41326451301574707,
      "learning_rate": 0.00014776750853705308,
      "loss": 2.4123,
      "step": 52680
    },
    {
      "epoch": 1.0839541348337143,
      "grad_norm": 0.40912699699401855,
      "learning_rate": 0.00014774799582511182,
      "loss": 2.3918,
      "step": 52690
    },
    {
      "epoch": 1.0841598543505822,
      "grad_norm": 0.4205480217933655,
      "learning_rate": 0.00014772848075802214,
      "loss": 2.3891,
      "step": 52700
    },
    {
      "epoch": 1.0843655738674498,
      "grad_norm": 0.4679830074310303,
      "learning_rate": 0.00014770896333674658,
      "loss": 2.4147,
      "step": 52710
    },
    {
      "epoch": 1.0845712933843175,
      "grad_norm": 0.4027094542980194,
      "learning_rate": 0.00014768944356224786,
      "loss": 2.3725,
      "step": 52720
    },
    {
      "epoch": 1.0847770129011851,
      "grad_norm": 0.4069872498512268,
      "learning_rate": 0.0001476699214354887,
      "loss": 2.3713,
      "step": 52730
    },
    {
      "epoch": 1.084982732418053,
      "grad_norm": 0.47945496439933777,
      "learning_rate": 0.00014765039695743216,
      "loss": 2.4036,
      "step": 52740
    },
    {
      "epoch": 1.0851884519349206,
      "grad_norm": 0.485006719827652,
      "learning_rate": 0.0001476308701290412,
      "loss": 2.3952,
      "step": 52750
    },
    {
      "epoch": 1.0853941714517883,
      "grad_norm": 0.4154641032218933,
      "learning_rate": 0.00014761134095127895,
      "loss": 2.4218,
      "step": 52760
    },
    {
      "epoch": 1.085599890968656,
      "grad_norm": 0.42726707458496094,
      "learning_rate": 0.00014759180942510868,
      "loss": 2.3749,
      "step": 52770
    },
    {
      "epoch": 1.0858056104855238,
      "grad_norm": 0.43000832200050354,
      "learning_rate": 0.00014757227555149382,
      "loss": 2.3983,
      "step": 52780
    },
    {
      "epoch": 1.0860113300023915,
      "grad_norm": 0.5024123787879944,
      "learning_rate": 0.00014755273933139785,
      "loss": 2.4017,
      "step": 52790
    },
    {
      "epoch": 1.0862170495192591,
      "grad_norm": 0.3960753381252289,
      "learning_rate": 0.00014753320076578435,
      "loss": 2.4637,
      "step": 52800
    },
    {
      "epoch": 1.086422769036127,
      "grad_norm": 0.42171603441238403,
      "learning_rate": 0.0001475136598556171,
      "loss": 2.4224,
      "step": 52810
    },
    {
      "epoch": 1.0866284885529947,
      "grad_norm": 0.4406597912311554,
      "learning_rate": 0.00014749411660185997,
      "loss": 2.4315,
      "step": 52820
    },
    {
      "epoch": 1.0868342080698623,
      "grad_norm": 0.40588951110839844,
      "learning_rate": 0.00014747457100547685,
      "loss": 2.3972,
      "step": 52830
    },
    {
      "epoch": 1.08703992758673,
      "grad_norm": 0.4268730580806732,
      "learning_rate": 0.0001474550230674318,
      "loss": 2.3953,
      "step": 52840
    },
    {
      "epoch": 1.0872456471035978,
      "grad_norm": 0.4277016222476959,
      "learning_rate": 0.00014743547278868913,
      "loss": 2.4396,
      "step": 52850
    },
    {
      "epoch": 1.0874513666204655,
      "grad_norm": 0.44552767276763916,
      "learning_rate": 0.00014741592017021303,
      "loss": 2.4494,
      "step": 52860
    },
    {
      "epoch": 1.0876570861373331,
      "grad_norm": 0.4168246388435364,
      "learning_rate": 0.000147396365212968,
      "loss": 2.4219,
      "step": 52870
    },
    {
      "epoch": 1.087862805654201,
      "grad_norm": 0.4345172345638275,
      "learning_rate": 0.00014737680791791853,
      "loss": 2.3834,
      "step": 52880
    },
    {
      "epoch": 1.0880685251710687,
      "grad_norm": 0.41480395197868347,
      "learning_rate": 0.0001473572482860293,
      "loss": 2.3071,
      "step": 52890
    },
    {
      "epoch": 1.0882742446879363,
      "grad_norm": 0.4330279231071472,
      "learning_rate": 0.0001473376863182651,
      "loss": 2.4355,
      "step": 52900
    },
    {
      "epoch": 1.088479964204804,
      "grad_norm": 0.429376482963562,
      "learning_rate": 0.0001473181220155908,
      "loss": 2.4096,
      "step": 52910
    },
    {
      "epoch": 1.0886856837216718,
      "grad_norm": 0.44008854031562805,
      "learning_rate": 0.00014729855537897138,
      "loss": 2.3722,
      "step": 52920
    },
    {
      "epoch": 1.0888914032385395,
      "grad_norm": 0.450540155172348,
      "learning_rate": 0.00014727898640937194,
      "loss": 2.4099,
      "step": 52930
    },
    {
      "epoch": 1.0890971227554072,
      "grad_norm": 0.4215453267097473,
      "learning_rate": 0.00014725941510775777,
      "loss": 2.3956,
      "step": 52940
    },
    {
      "epoch": 1.089302842272275,
      "grad_norm": 0.4256453812122345,
      "learning_rate": 0.0001472398414750942,
      "loss": 2.3809,
      "step": 52950
    },
    {
      "epoch": 1.0895085617891427,
      "grad_norm": 0.41394299268722534,
      "learning_rate": 0.00014722026551234666,
      "loss": 2.398,
      "step": 52960
    },
    {
      "epoch": 1.0897142813060103,
      "grad_norm": 0.42144498229026794,
      "learning_rate": 0.00014720068722048074,
      "loss": 2.4035,
      "step": 52970
    },
    {
      "epoch": 1.089920000822878,
      "grad_norm": 0.415425181388855,
      "learning_rate": 0.00014718110660046215,
      "loss": 2.3536,
      "step": 52980
    },
    {
      "epoch": 1.0901257203397459,
      "grad_norm": 0.41210633516311646,
      "learning_rate": 0.00014716152365325667,
      "loss": 2.3491,
      "step": 52990
    },
    {
      "epoch": 1.0903314398566135,
      "grad_norm": 0.4748896062374115,
      "learning_rate": 0.0001471419383798302,
      "loss": 2.3567,
      "step": 53000
    },
    {
      "epoch": 1.0905371593734812,
      "grad_norm": 0.46986135840415955,
      "learning_rate": 0.00014712235078114883,
      "loss": 2.3791,
      "step": 53010
    },
    {
      "epoch": 1.090742878890349,
      "grad_norm": 0.40639057755470276,
      "learning_rate": 0.00014710276085817866,
      "loss": 2.4131,
      "step": 53020
    },
    {
      "epoch": 1.0909485984072167,
      "grad_norm": 0.4061913788318634,
      "learning_rate": 0.000147083168611886,
      "loss": 2.3889,
      "step": 53030
    },
    {
      "epoch": 1.0911543179240843,
      "grad_norm": 0.4694267809391022,
      "learning_rate": 0.00014706357404323722,
      "loss": 2.4388,
      "step": 53040
    },
    {
      "epoch": 1.091360037440952,
      "grad_norm": 0.40486690402030945,
      "learning_rate": 0.00014704397715319876,
      "loss": 2.3599,
      "step": 53050
    },
    {
      "epoch": 1.0915657569578199,
      "grad_norm": 0.41208934783935547,
      "learning_rate": 0.00014702437794273727,
      "loss": 2.4376,
      "step": 53060
    },
    {
      "epoch": 1.0917714764746875,
      "grad_norm": 0.4315822720527649,
      "learning_rate": 0.00014700477641281946,
      "loss": 2.3572,
      "step": 53070
    },
    {
      "epoch": 1.0919771959915552,
      "grad_norm": 0.4680173397064209,
      "learning_rate": 0.00014698517256441223,
      "loss": 2.3838,
      "step": 53080
    },
    {
      "epoch": 1.0921829155084228,
      "grad_norm": 0.41017860174179077,
      "learning_rate": 0.0001469655663984824,
      "loss": 2.374,
      "step": 53090
    },
    {
      "epoch": 1.0923886350252907,
      "grad_norm": 0.439981073141098,
      "learning_rate": 0.00014694595791599717,
      "loss": 2.4223,
      "step": 53100
    },
    {
      "epoch": 1.0925943545421584,
      "grad_norm": 0.40991610288619995,
      "learning_rate": 0.00014692634711792365,
      "loss": 2.3556,
      "step": 53110
    },
    {
      "epoch": 1.092800074059026,
      "grad_norm": 0.43338873982429504,
      "learning_rate": 0.0001469067340052291,
      "loss": 2.3949,
      "step": 53120
    },
    {
      "epoch": 1.0930057935758937,
      "grad_norm": 0.42259272933006287,
      "learning_rate": 0.000146887118578881,
      "loss": 2.3797,
      "step": 53130
    },
    {
      "epoch": 1.0932115130927615,
      "grad_norm": 0.41643384099006653,
      "learning_rate": 0.00014686750083984688,
      "loss": 2.3712,
      "step": 53140
    },
    {
      "epoch": 1.0934172326096292,
      "grad_norm": 0.4552798271179199,
      "learning_rate": 0.00014684788078909433,
      "loss": 2.4339,
      "step": 53150
    },
    {
      "epoch": 1.0936229521264969,
      "grad_norm": 0.4331582486629486,
      "learning_rate": 0.0001468282584275911,
      "loss": 2.4338,
      "step": 53160
    },
    {
      "epoch": 1.0938286716433647,
      "grad_norm": 0.37732943892478943,
      "learning_rate": 0.00014680863375630504,
      "loss": 2.3672,
      "step": 53170
    },
    {
      "epoch": 1.0940343911602324,
      "grad_norm": 0.4493192136287689,
      "learning_rate": 0.00014678900677620417,
      "loss": 2.4279,
      "step": 53180
    },
    {
      "epoch": 1.0942401106771,
      "grad_norm": 0.3981453776359558,
      "learning_rate": 0.00014676937748825657,
      "loss": 2.3567,
      "step": 53190
    },
    {
      "epoch": 1.0944458301939677,
      "grad_norm": 0.4490521252155304,
      "learning_rate": 0.00014674974589343044,
      "loss": 2.4168,
      "step": 53200
    },
    {
      "epoch": 1.0946515497108356,
      "grad_norm": 0.4129481017589569,
      "learning_rate": 0.0001467301119926941,
      "loss": 2.483,
      "step": 53210
    },
    {
      "epoch": 1.0948572692277032,
      "grad_norm": 0.4364009499549866,
      "learning_rate": 0.00014671047578701597,
      "loss": 2.4431,
      "step": 53220
    },
    {
      "epoch": 1.0950629887445709,
      "grad_norm": 0.4395448863506317,
      "learning_rate": 0.00014669083727736462,
      "loss": 2.4307,
      "step": 53230
    },
    {
      "epoch": 1.0952687082614387,
      "grad_norm": 0.40834149718284607,
      "learning_rate": 0.00014667119646470872,
      "loss": 2.3529,
      "step": 53240
    },
    {
      "epoch": 1.0954744277783064,
      "grad_norm": 0.4297262132167816,
      "learning_rate": 0.00014665155335001701,
      "loss": 2.434,
      "step": 53250
    },
    {
      "epoch": 1.095680147295174,
      "grad_norm": 0.43462347984313965,
      "learning_rate": 0.0001466319079342584,
      "loss": 2.4005,
      "step": 53260
    },
    {
      "epoch": 1.0958858668120417,
      "grad_norm": 0.46029579639434814,
      "learning_rate": 0.00014661226021840188,
      "loss": 2.3301,
      "step": 53270
    },
    {
      "epoch": 1.0960915863289096,
      "grad_norm": 0.419368177652359,
      "learning_rate": 0.00014659261020341657,
      "loss": 2.4158,
      "step": 53280
    },
    {
      "epoch": 1.0962973058457772,
      "grad_norm": 0.4166259467601776,
      "learning_rate": 0.0001465729578902717,
      "loss": 2.4099,
      "step": 53290
    },
    {
      "epoch": 1.0965030253626449,
      "grad_norm": 0.4092617332935333,
      "learning_rate": 0.00014655330327993662,
      "loss": 2.3851,
      "step": 53300
    },
    {
      "epoch": 1.0967087448795128,
      "grad_norm": 0.4225976765155792,
      "learning_rate": 0.00014653364637338075,
      "loss": 2.4153,
      "step": 53310
    },
    {
      "epoch": 1.0969144643963804,
      "grad_norm": 0.43281689286231995,
      "learning_rate": 0.0001465139871715737,
      "loss": 2.4119,
      "step": 53320
    },
    {
      "epoch": 1.097120183913248,
      "grad_norm": 0.42041122913360596,
      "learning_rate": 0.0001464943256754851,
      "loss": 2.4282,
      "step": 53330
    },
    {
      "epoch": 1.0973259034301157,
      "grad_norm": 0.46099457144737244,
      "learning_rate": 0.00014647466188608482,
      "loss": 2.4105,
      "step": 53340
    },
    {
      "epoch": 1.0975316229469836,
      "grad_norm": 0.4039875864982605,
      "learning_rate": 0.00014645499580434273,
      "loss": 2.4133,
      "step": 53350
    },
    {
      "epoch": 1.0977373424638512,
      "grad_norm": 0.4316827952861786,
      "learning_rate": 0.00014643532743122882,
      "loss": 2.5012,
      "step": 53360
    },
    {
      "epoch": 1.097943061980719,
      "grad_norm": 0.42470699548721313,
      "learning_rate": 0.00014641565676771324,
      "loss": 2.3749,
      "step": 53370
    },
    {
      "epoch": 1.0981487814975868,
      "grad_norm": 0.4459165334701538,
      "learning_rate": 0.0001463959838147663,
      "loss": 2.3572,
      "step": 53380
    },
    {
      "epoch": 1.0983545010144544,
      "grad_norm": 0.4285952150821686,
      "learning_rate": 0.00014637630857335825,
      "loss": 2.4779,
      "step": 53390
    },
    {
      "epoch": 1.098560220531322,
      "grad_norm": 0.43083456158638,
      "learning_rate": 0.00014635663104445965,
      "loss": 2.4433,
      "step": 53400
    },
    {
      "epoch": 1.0987659400481897,
      "grad_norm": 0.4053090214729309,
      "learning_rate": 0.00014633695122904104,
      "loss": 2.4098,
      "step": 53410
    },
    {
      "epoch": 1.0989716595650576,
      "grad_norm": 0.4593876302242279,
      "learning_rate": 0.00014631726912807313,
      "loss": 2.4012,
      "step": 53420
    },
    {
      "epoch": 1.0991773790819253,
      "grad_norm": 0.44275936484336853,
      "learning_rate": 0.00014629758474252674,
      "loss": 2.3786,
      "step": 53430
    },
    {
      "epoch": 1.099383098598793,
      "grad_norm": 0.4284738302230835,
      "learning_rate": 0.0001462778980733728,
      "loss": 2.4357,
      "step": 53440
    },
    {
      "epoch": 1.0995888181156606,
      "grad_norm": 0.4372345507144928,
      "learning_rate": 0.0001462582091215823,
      "loss": 2.4155,
      "step": 53450
    },
    {
      "epoch": 1.0997945376325284,
      "grad_norm": 0.4435071051120758,
      "learning_rate": 0.00014623851788812643,
      "loss": 2.354,
      "step": 53460
    },
    {
      "epoch": 1.100000257149396,
      "grad_norm": 0.41102081537246704,
      "learning_rate": 0.00014621882437397646,
      "loss": 2.3993,
      "step": 53470
    },
    {
      "epoch": 1.1002059766662637,
      "grad_norm": 0.40282776951789856,
      "learning_rate": 0.00014619912858010375,
      "loss": 2.4278,
      "step": 53480
    },
    {
      "epoch": 1.1004116961831316,
      "grad_norm": 0.41911745071411133,
      "learning_rate": 0.0001461794305074798,
      "loss": 2.3989,
      "step": 53490
    },
    {
      "epoch": 1.1006174156999993,
      "grad_norm": 0.471041738986969,
      "learning_rate": 0.0001461597301570761,
      "loss": 2.4295,
      "step": 53500
    },
    {
      "epoch": 1.100823135216867,
      "grad_norm": 0.419885516166687,
      "learning_rate": 0.00014614002752986454,
      "loss": 2.414,
      "step": 53510
    },
    {
      "epoch": 1.1010288547337346,
      "grad_norm": 0.43599987030029297,
      "learning_rate": 0.00014612032262681683,
      "loss": 2.3758,
      "step": 53520
    },
    {
      "epoch": 1.1012345742506024,
      "grad_norm": 0.3967612087726593,
      "learning_rate": 0.00014610061544890492,
      "loss": 2.3472,
      "step": 53530
    },
    {
      "epoch": 1.10144029376747,
      "grad_norm": 0.5112113356590271,
      "learning_rate": 0.00014608090599710084,
      "loss": 2.4534,
      "step": 53540
    },
    {
      "epoch": 1.1016460132843378,
      "grad_norm": 0.4408872127532959,
      "learning_rate": 0.00014606119427237687,
      "loss": 2.4381,
      "step": 53550
    },
    {
      "epoch": 1.1018517328012054,
      "grad_norm": 0.45835840702056885,
      "learning_rate": 0.00014604148027570513,
      "loss": 2.4152,
      "step": 53560
    },
    {
      "epoch": 1.1020574523180733,
      "grad_norm": 0.4072355031967163,
      "learning_rate": 0.00014602176400805805,
      "loss": 2.4362,
      "step": 53570
    },
    {
      "epoch": 1.102263171834941,
      "grad_norm": 0.4090149998664856,
      "learning_rate": 0.00014600204547040812,
      "loss": 2.4273,
      "step": 53580
    },
    {
      "epoch": 1.1024688913518086,
      "grad_norm": 0.4087699353694916,
      "learning_rate": 0.00014598232466372804,
      "loss": 2.3686,
      "step": 53590
    },
    {
      "epoch": 1.1026746108686765,
      "grad_norm": 0.41348356008529663,
      "learning_rate": 0.00014596260158899042,
      "loss": 2.3461,
      "step": 53600
    },
    {
      "epoch": 1.1028803303855441,
      "grad_norm": 0.40501096844673157,
      "learning_rate": 0.00014594287624716815,
      "loss": 2.3985,
      "step": 53610
    },
    {
      "epoch": 1.1030860499024118,
      "grad_norm": 0.42254313826560974,
      "learning_rate": 0.00014592314863923412,
      "loss": 2.3625,
      "step": 53620
    },
    {
      "epoch": 1.1032917694192794,
      "grad_norm": 0.41798466444015503,
      "learning_rate": 0.00014590341876616144,
      "loss": 2.3778,
      "step": 53630
    },
    {
      "epoch": 1.1034974889361473,
      "grad_norm": 0.3951932489871979,
      "learning_rate": 0.00014588368662892323,
      "loss": 2.4213,
      "step": 53640
    },
    {
      "epoch": 1.103703208453015,
      "grad_norm": 0.4475236237049103,
      "learning_rate": 0.00014586395222849284,
      "loss": 2.4539,
      "step": 53650
    },
    {
      "epoch": 1.1039089279698826,
      "grad_norm": 0.4499334692955017,
      "learning_rate": 0.00014584421556584359,
      "loss": 2.447,
      "step": 53660
    },
    {
      "epoch": 1.1041146474867505,
      "grad_norm": 0.5483506321907043,
      "learning_rate": 0.000145824476641949,
      "loss": 2.4251,
      "step": 53670
    },
    {
      "epoch": 1.1043203670036181,
      "grad_norm": 0.4472408890724182,
      "learning_rate": 0.0001458047354577827,
      "loss": 2.3681,
      "step": 53680
    },
    {
      "epoch": 1.1045260865204858,
      "grad_norm": 0.4216628968715668,
      "learning_rate": 0.0001457849920143184,
      "loss": 2.3936,
      "step": 53690
    },
    {
      "epoch": 1.1047318060373534,
      "grad_norm": 0.42976194620132446,
      "learning_rate": 0.00014576524631252993,
      "loss": 2.4089,
      "step": 53700
    },
    {
      "epoch": 1.1049375255542213,
      "grad_norm": 0.4263087511062622,
      "learning_rate": 0.0001457454983533913,
      "loss": 2.4142,
      "step": 53710
    },
    {
      "epoch": 1.105143245071089,
      "grad_norm": 0.43217727541923523,
      "learning_rate": 0.00014572574813787647,
      "loss": 2.304,
      "step": 53720
    },
    {
      "epoch": 1.1053489645879566,
      "grad_norm": 0.44255706667900085,
      "learning_rate": 0.00014570599566695965,
      "loss": 2.3937,
      "step": 53730
    },
    {
      "epoch": 1.1055546841048245,
      "grad_norm": 0.46627867221832275,
      "learning_rate": 0.00014568624094161517,
      "loss": 2.4317,
      "step": 53740
    },
    {
      "epoch": 1.1057604036216921,
      "grad_norm": 0.4531958997249603,
      "learning_rate": 0.00014566648396281736,
      "loss": 2.3879,
      "step": 53750
    },
    {
      "epoch": 1.1059661231385598,
      "grad_norm": 0.5129088163375854,
      "learning_rate": 0.00014564672473154077,
      "loss": 2.4426,
      "step": 53760
    },
    {
      "epoch": 1.1061718426554275,
      "grad_norm": 0.385010689496994,
      "learning_rate": 0.00014562696324876,
      "loss": 2.3775,
      "step": 53770
    },
    {
      "epoch": 1.1063775621722953,
      "grad_norm": 0.4024609625339508,
      "learning_rate": 0.00014560719951544975,
      "loss": 2.4,
      "step": 53780
    },
    {
      "epoch": 1.106583281689163,
      "grad_norm": 0.43158090114593506,
      "learning_rate": 0.00014558743353258486,
      "loss": 2.4198,
      "step": 53790
    },
    {
      "epoch": 1.1067890012060306,
      "grad_norm": 0.4188199043273926,
      "learning_rate": 0.00014556766530114033,
      "loss": 2.4245,
      "step": 53800
    },
    {
      "epoch": 1.1069947207228983,
      "grad_norm": 0.4447111487388611,
      "learning_rate": 0.0001455478948220912,
      "loss": 2.4042,
      "step": 53810
    },
    {
      "epoch": 1.1072004402397662,
      "grad_norm": 0.4456194043159485,
      "learning_rate": 0.00014552812209641256,
      "loss": 2.422,
      "step": 53820
    },
    {
      "epoch": 1.1074061597566338,
      "grad_norm": 0.5176692008972168,
      "learning_rate": 0.0001455083471250798,
      "loss": 2.3887,
      "step": 53830
    },
    {
      "epoch": 1.1076118792735015,
      "grad_norm": 0.4120701849460602,
      "learning_rate": 0.00014548856990906827,
      "loss": 2.4292,
      "step": 53840
    },
    {
      "epoch": 1.1078175987903693,
      "grad_norm": 0.44978630542755127,
      "learning_rate": 0.00014546879044935346,
      "loss": 2.3769,
      "step": 53850
    },
    {
      "epoch": 1.108023318307237,
      "grad_norm": 0.40745311975479126,
      "learning_rate": 0.00014544900874691096,
      "loss": 2.4598,
      "step": 53860
    },
    {
      "epoch": 1.1082290378241046,
      "grad_norm": 0.4278717637062073,
      "learning_rate": 0.00014542922480271657,
      "loss": 2.369,
      "step": 53870
    },
    {
      "epoch": 1.1084347573409723,
      "grad_norm": 0.4115549623966217,
      "learning_rate": 0.00014540943861774606,
      "loss": 2.4319,
      "step": 53880
    },
    {
      "epoch": 1.1086404768578402,
      "grad_norm": 0.4386846721172333,
      "learning_rate": 0.00014538965019297541,
      "loss": 2.4618,
      "step": 53890
    },
    {
      "epoch": 1.1088461963747078,
      "grad_norm": 0.4089476764202118,
      "learning_rate": 0.00014536985952938065,
      "loss": 2.4289,
      "step": 53900
    },
    {
      "epoch": 1.1090519158915755,
      "grad_norm": 0.44693276286125183,
      "learning_rate": 0.00014535006662793796,
      "loss": 2.4155,
      "step": 53910
    },
    {
      "epoch": 1.1092576354084431,
      "grad_norm": 0.4188670814037323,
      "learning_rate": 0.00014533027148962358,
      "loss": 2.3973,
      "step": 53920
    },
    {
      "epoch": 1.109463354925311,
      "grad_norm": 0.43595394492149353,
      "learning_rate": 0.00014531047411541398,
      "loss": 2.4258,
      "step": 53930
    },
    {
      "epoch": 1.1096690744421787,
      "grad_norm": 0.42586252093315125,
      "learning_rate": 0.00014529067450628558,
      "loss": 2.409,
      "step": 53940
    },
    {
      "epoch": 1.1098747939590463,
      "grad_norm": 0.44201400876045227,
      "learning_rate": 0.000145270872663215,
      "loss": 2.459,
      "step": 53950
    },
    {
      "epoch": 1.1100805134759142,
      "grad_norm": 0.4015015661716461,
      "learning_rate": 0.00014525106858717898,
      "loss": 2.3854,
      "step": 53960
    },
    {
      "epoch": 1.1102862329927818,
      "grad_norm": 0.4362947642803192,
      "learning_rate": 0.00014523126227915435,
      "loss": 2.3674,
      "step": 53970
    },
    {
      "epoch": 1.1104919525096495,
      "grad_norm": 0.43388089537620544,
      "learning_rate": 0.000145211453740118,
      "loss": 2.3504,
      "step": 53980
    },
    {
      "epoch": 1.1106976720265171,
      "grad_norm": 0.3904128074645996,
      "learning_rate": 0.00014519164297104704,
      "loss": 2.394,
      "step": 53990
    },
    {
      "epoch": 1.110903391543385,
      "grad_norm": 0.4229183495044708,
      "learning_rate": 0.0001451718299729186,
      "loss": 2.4574,
      "step": 54000
    },
    {
      "epoch": 1.1111091110602527,
      "grad_norm": 0.4589713215827942,
      "learning_rate": 0.0001451520147467099,
      "loss": 2.42,
      "step": 54010
    },
    {
      "epoch": 1.1113148305771203,
      "grad_norm": 0.42603981494903564,
      "learning_rate": 0.0001451321972933984,
      "loss": 2.4117,
      "step": 54020
    },
    {
      "epoch": 1.1115205500939882,
      "grad_norm": 0.41165000200271606,
      "learning_rate": 0.00014511237761396156,
      "loss": 2.3984,
      "step": 54030
    },
    {
      "epoch": 1.1117262696108559,
      "grad_norm": 0.4053005576133728,
      "learning_rate": 0.0001450925557093769,
      "loss": 2.3249,
      "step": 54040
    },
    {
      "epoch": 1.1119319891277235,
      "grad_norm": 0.48358985781669617,
      "learning_rate": 0.00014507273158062228,
      "loss": 2.3934,
      "step": 54050
    },
    {
      "epoch": 1.1121377086445912,
      "grad_norm": 0.4390851557254791,
      "learning_rate": 0.00014505290522867536,
      "loss": 2.3668,
      "step": 54060
    },
    {
      "epoch": 1.112343428161459,
      "grad_norm": 0.41147541999816895,
      "learning_rate": 0.00014503307665451418,
      "loss": 2.4499,
      "step": 54070
    },
    {
      "epoch": 1.1125491476783267,
      "grad_norm": 0.5216407179832458,
      "learning_rate": 0.0001450132458591167,
      "loss": 2.447,
      "step": 54080
    },
    {
      "epoch": 1.1127548671951943,
      "grad_norm": 0.41345736384391785,
      "learning_rate": 0.00014499341284346108,
      "loss": 2.4021,
      "step": 54090
    },
    {
      "epoch": 1.1129605867120622,
      "grad_norm": 0.47353997826576233,
      "learning_rate": 0.0001449735776085256,
      "loss": 2.4139,
      "step": 54100
    },
    {
      "epoch": 1.1131663062289299,
      "grad_norm": 0.4141007661819458,
      "learning_rate": 0.00014495374015528864,
      "loss": 2.3907,
      "step": 54110
    },
    {
      "epoch": 1.1133720257457975,
      "grad_norm": 0.4104970693588257,
      "learning_rate": 0.00014493390048472864,
      "loss": 2.4487,
      "step": 54120
    },
    {
      "epoch": 1.1135777452626652,
      "grad_norm": 0.4494886100292206,
      "learning_rate": 0.00014491405859782419,
      "loss": 2.4101,
      "step": 54130
    },
    {
      "epoch": 1.113783464779533,
      "grad_norm": 0.4426702857017517,
      "learning_rate": 0.00014489421449555395,
      "loss": 2.3904,
      "step": 54140
    },
    {
      "epoch": 1.1139891842964007,
      "grad_norm": 0.4534662067890167,
      "learning_rate": 0.0001448743681788968,
      "loss": 2.4294,
      "step": 54150
    },
    {
      "epoch": 1.1141949038132684,
      "grad_norm": 0.45654550194740295,
      "learning_rate": 0.00014485451964883157,
      "loss": 2.3769,
      "step": 54160
    },
    {
      "epoch": 1.114400623330136,
      "grad_norm": 0.43741747736930847,
      "learning_rate": 0.00014483466890633732,
      "loss": 2.346,
      "step": 54170
    },
    {
      "epoch": 1.1146063428470039,
      "grad_norm": 0.41245800256729126,
      "learning_rate": 0.00014481481595239316,
      "loss": 2.3606,
      "step": 54180
    },
    {
      "epoch": 1.1148120623638715,
      "grad_norm": 0.40273505449295044,
      "learning_rate": 0.00014479496078797838,
      "loss": 2.4647,
      "step": 54190
    },
    {
      "epoch": 1.1150177818807392,
      "grad_norm": 0.41828691959381104,
      "learning_rate": 0.00014477510341407226,
      "loss": 2.3489,
      "step": 54200
    },
    {
      "epoch": 1.115223501397607,
      "grad_norm": 0.4076041579246521,
      "learning_rate": 0.0001447552438316543,
      "loss": 2.4191,
      "step": 54210
    },
    {
      "epoch": 1.1154292209144747,
      "grad_norm": 0.4521813690662384,
      "learning_rate": 0.00014473538204170402,
      "loss": 2.4126,
      "step": 54220
    },
    {
      "epoch": 1.1156349404313424,
      "grad_norm": 0.4055631458759308,
      "learning_rate": 0.00014471551804520115,
      "loss": 2.4167,
      "step": 54230
    },
    {
      "epoch": 1.11584065994821,
      "grad_norm": 0.4136882424354553,
      "learning_rate": 0.00014469565184312543,
      "loss": 2.3973,
      "step": 54240
    },
    {
      "epoch": 1.116046379465078,
      "grad_norm": 0.44675564765930176,
      "learning_rate": 0.00014467578343645675,
      "loss": 2.3932,
      "step": 54250
    },
    {
      "epoch": 1.1162520989819456,
      "grad_norm": 0.41981229186058044,
      "learning_rate": 0.00014465591282617515,
      "loss": 2.3752,
      "step": 54260
    },
    {
      "epoch": 1.1164578184988132,
      "grad_norm": 0.4077763855457306,
      "learning_rate": 0.0001446360400132607,
      "loss": 2.4201,
      "step": 54270
    },
    {
      "epoch": 1.1166635380156809,
      "grad_norm": 0.43532899022102356,
      "learning_rate": 0.00014461616499869364,
      "loss": 2.4278,
      "step": 54280
    },
    {
      "epoch": 1.1168692575325487,
      "grad_norm": 0.43636539578437805,
      "learning_rate": 0.00014459628778345433,
      "loss": 2.3766,
      "step": 54290
    },
    {
      "epoch": 1.1170749770494164,
      "grad_norm": 0.4159994125366211,
      "learning_rate": 0.0001445764083685231,
      "loss": 2.3942,
      "step": 54300
    },
    {
      "epoch": 1.117280696566284,
      "grad_norm": 0.412068635225296,
      "learning_rate": 0.00014455652675488058,
      "loss": 2.463,
      "step": 54310
    },
    {
      "epoch": 1.117486416083152,
      "grad_norm": 0.4122473895549774,
      "learning_rate": 0.00014453664294350745,
      "loss": 2.4473,
      "step": 54320
    },
    {
      "epoch": 1.1176921356000196,
      "grad_norm": 0.4248279333114624,
      "learning_rate": 0.00014451675693538435,
      "loss": 2.4234,
      "step": 54330
    },
    {
      "epoch": 1.1178978551168872,
      "grad_norm": 0.41176894307136536,
      "learning_rate": 0.0001444968687314922,
      "loss": 2.3862,
      "step": 54340
    },
    {
      "epoch": 1.1181035746337549,
      "grad_norm": 0.40472713112831116,
      "learning_rate": 0.00014447697833281208,
      "loss": 2.3636,
      "step": 54350
    },
    {
      "epoch": 1.1183092941506227,
      "grad_norm": 0.40652135014533997,
      "learning_rate": 0.00014445708574032496,
      "loss": 2.4022,
      "step": 54360
    },
    {
      "epoch": 1.1185150136674904,
      "grad_norm": 0.4086135923862457,
      "learning_rate": 0.00014443719095501206,
      "loss": 2.4047,
      "step": 54370
    },
    {
      "epoch": 1.118720733184358,
      "grad_norm": 0.4063267707824707,
      "learning_rate": 0.00014441729397785466,
      "loss": 2.3543,
      "step": 54380
    },
    {
      "epoch": 1.118926452701226,
      "grad_norm": 0.43560588359832764,
      "learning_rate": 0.00014439739480983418,
      "loss": 2.3861,
      "step": 54390
    },
    {
      "epoch": 1.1191321722180936,
      "grad_norm": 0.5325965285301208,
      "learning_rate": 0.00014437749345193218,
      "loss": 2.376,
      "step": 54400
    },
    {
      "epoch": 1.1193378917349612,
      "grad_norm": 0.45623019337654114,
      "learning_rate": 0.00014435758990513026,
      "loss": 2.4148,
      "step": 54410
    },
    {
      "epoch": 1.1195436112518289,
      "grad_norm": 0.43054822087287903,
      "learning_rate": 0.00014433768417041013,
      "loss": 2.4109,
      "step": 54420
    },
    {
      "epoch": 1.1197493307686968,
      "grad_norm": 0.40052109956741333,
      "learning_rate": 0.00014431777624875368,
      "loss": 2.3724,
      "step": 54430
    },
    {
      "epoch": 1.1199550502855644,
      "grad_norm": 0.4218246042728424,
      "learning_rate": 0.0001442978661411428,
      "loss": 2.3824,
      "step": 54440
    },
    {
      "epoch": 1.120160769802432,
      "grad_norm": 0.40776583552360535,
      "learning_rate": 0.00014427795384855963,
      "loss": 2.4023,
      "step": 54450
    },
    {
      "epoch": 1.1203664893193,
      "grad_norm": 0.4615797698497772,
      "learning_rate": 0.00014425803937198622,
      "loss": 2.4117,
      "step": 54460
    },
    {
      "epoch": 1.1205722088361676,
      "grad_norm": 0.40770235657691956,
      "learning_rate": 0.00014423812271240495,
      "loss": 2.425,
      "step": 54470
    },
    {
      "epoch": 1.1207779283530352,
      "grad_norm": 0.45642659068107605,
      "learning_rate": 0.0001442182038707981,
      "loss": 2.3847,
      "step": 54480
    },
    {
      "epoch": 1.120983647869903,
      "grad_norm": 0.4274861812591553,
      "learning_rate": 0.00014419828284814828,
      "loss": 2.38,
      "step": 54490
    },
    {
      "epoch": 1.1211893673867708,
      "grad_norm": 0.4627732038497925,
      "learning_rate": 0.00014417835964543798,
      "loss": 2.4551,
      "step": 54500
    },
    {
      "epoch": 1.1213950869036384,
      "grad_norm": 0.4105342924594879,
      "learning_rate": 0.00014415843426364994,
      "loss": 2.4395,
      "step": 54510
    },
    {
      "epoch": 1.121600806420506,
      "grad_norm": 0.3881133794784546,
      "learning_rate": 0.000144138506703767,
      "loss": 2.4475,
      "step": 54520
    },
    {
      "epoch": 1.1218065259373737,
      "grad_norm": 0.41046246886253357,
      "learning_rate": 0.000144118576966772,
      "loss": 2.4094,
      "step": 54530
    },
    {
      "epoch": 1.1220122454542416,
      "grad_norm": 0.6153079271316528,
      "learning_rate": 0.00014409864505364808,
      "loss": 2.4108,
      "step": 54540
    },
    {
      "epoch": 1.1222179649711093,
      "grad_norm": 0.4030677378177643,
      "learning_rate": 0.00014407871096537827,
      "loss": 2.3253,
      "step": 54550
    },
    {
      "epoch": 1.122423684487977,
      "grad_norm": 0.4269111156463623,
      "learning_rate": 0.00014405877470294586,
      "loss": 2.4608,
      "step": 54560
    },
    {
      "epoch": 1.1226294040048448,
      "grad_norm": 0.5093051195144653,
      "learning_rate": 0.00014403883626733417,
      "loss": 2.4496,
      "step": 54570
    },
    {
      "epoch": 1.1228351235217124,
      "grad_norm": 0.4303312301635742,
      "learning_rate": 0.0001440188956595267,
      "loss": 2.407,
      "step": 54580
    },
    {
      "epoch": 1.12304084303858,
      "grad_norm": 0.42357057332992554,
      "learning_rate": 0.00014399895288050692,
      "loss": 2.426,
      "step": 54590
    },
    {
      "epoch": 1.1232465625554477,
      "grad_norm": 0.4075775742530823,
      "learning_rate": 0.00014397900793125862,
      "loss": 2.4365,
      "step": 54600
    },
    {
      "epoch": 1.1234522820723156,
      "grad_norm": 0.43938615918159485,
      "learning_rate": 0.0001439590608127655,
      "loss": 2.4788,
      "step": 54610
    },
    {
      "epoch": 1.1236580015891833,
      "grad_norm": 0.42878738045692444,
      "learning_rate": 0.00014393911152601148,
      "loss": 2.4033,
      "step": 54620
    },
    {
      "epoch": 1.123863721106051,
      "grad_norm": 0.4375840425491333,
      "learning_rate": 0.00014391916007198047,
      "loss": 2.4116,
      "step": 54630
    },
    {
      "epoch": 1.1240694406229186,
      "grad_norm": 0.4272397458553314,
      "learning_rate": 0.00014389920645165667,
      "loss": 2.4575,
      "step": 54640
    },
    {
      "epoch": 1.1242751601397865,
      "grad_norm": 0.41680020093917847,
      "learning_rate": 0.00014387925066602423,
      "loss": 2.4066,
      "step": 54650
    },
    {
      "epoch": 1.124480879656654,
      "grad_norm": 0.407766729593277,
      "learning_rate": 0.00014385929271606746,
      "loss": 2.4136,
      "step": 54660
    },
    {
      "epoch": 1.1246865991735218,
      "grad_norm": 0.39231207966804504,
      "learning_rate": 0.00014383933260277078,
      "loss": 2.3795,
      "step": 54670
    },
    {
      "epoch": 1.1248923186903896,
      "grad_norm": 0.422164648771286,
      "learning_rate": 0.00014381937032711874,
      "loss": 2.4613,
      "step": 54680
    },
    {
      "epoch": 1.1250980382072573,
      "grad_norm": 0.41897284984588623,
      "learning_rate": 0.00014379940589009593,
      "loss": 2.3946,
      "step": 54690
    },
    {
      "epoch": 1.125303757724125,
      "grad_norm": 0.42029058933258057,
      "learning_rate": 0.0001437794392926871,
      "loss": 2.404,
      "step": 54700
    },
    {
      "epoch": 1.1255094772409926,
      "grad_norm": 0.41545867919921875,
      "learning_rate": 0.00014375947053587714,
      "loss": 2.4345,
      "step": 54710
    },
    {
      "epoch": 1.1257151967578605,
      "grad_norm": 0.42580878734588623,
      "learning_rate": 0.0001437394996206509,
      "loss": 2.4371,
      "step": 54720
    },
    {
      "epoch": 1.1259209162747281,
      "grad_norm": 0.3950398862361908,
      "learning_rate": 0.00014371952654799352,
      "loss": 2.3781,
      "step": 54730
    },
    {
      "epoch": 1.1261266357915958,
      "grad_norm": 0.39474356174468994,
      "learning_rate": 0.00014369955131889018,
      "loss": 2.3805,
      "step": 54740
    },
    {
      "epoch": 1.1263323553084637,
      "grad_norm": 0.4128539264202118,
      "learning_rate": 0.00014367957393432603,
      "loss": 2.4176,
      "step": 54750
    },
    {
      "epoch": 1.1265380748253313,
      "grad_norm": 0.416254460811615,
      "learning_rate": 0.00014365959439528657,
      "loss": 2.4301,
      "step": 54760
    },
    {
      "epoch": 1.126743794342199,
      "grad_norm": 0.41511207818984985,
      "learning_rate": 0.00014363961270275722,
      "loss": 2.4005,
      "step": 54770
    },
    {
      "epoch": 1.1269495138590666,
      "grad_norm": 0.5137596726417542,
      "learning_rate": 0.00014361962885772362,
      "loss": 2.4256,
      "step": 54780
    },
    {
      "epoch": 1.1271552333759345,
      "grad_norm": 0.4149496257305145,
      "learning_rate": 0.00014359964286117137,
      "loss": 2.3655,
      "step": 54790
    },
    {
      "epoch": 1.1273609528928021,
      "grad_norm": 0.48708194494247437,
      "learning_rate": 0.00014357965471408637,
      "loss": 2.4775,
      "step": 54800
    },
    {
      "epoch": 1.1275666724096698,
      "grad_norm": 0.42143940925598145,
      "learning_rate": 0.00014355966441745448,
      "loss": 2.4347,
      "step": 54810
    },
    {
      "epoch": 1.1277723919265377,
      "grad_norm": 0.4435315430164337,
      "learning_rate": 0.0001435396719722617,
      "loss": 2.3883,
      "step": 54820
    },
    {
      "epoch": 1.1279781114434053,
      "grad_norm": 0.42584311962127686,
      "learning_rate": 0.00014351967737949415,
      "loss": 2.4843,
      "step": 54830
    },
    {
      "epoch": 1.128183830960273,
      "grad_norm": 0.3861250579357147,
      "learning_rate": 0.00014349968064013809,
      "loss": 2.4775,
      "step": 54840
    },
    {
      "epoch": 1.1283895504771406,
      "grad_norm": 0.4010423719882965,
      "learning_rate": 0.00014347968175517983,
      "loss": 2.4584,
      "step": 54850
    },
    {
      "epoch": 1.1285952699940085,
      "grad_norm": 0.3810669779777527,
      "learning_rate": 0.00014345968072560582,
      "loss": 2.4361,
      "step": 54860
    },
    {
      "epoch": 1.1288009895108762,
      "grad_norm": 0.4569080173969269,
      "learning_rate": 0.00014343967755240256,
      "loss": 2.4201,
      "step": 54870
    },
    {
      "epoch": 1.1290067090277438,
      "grad_norm": 0.42455172538757324,
      "learning_rate": 0.0001434196722365567,
      "loss": 2.3916,
      "step": 54880
    },
    {
      "epoch": 1.1292124285446117,
      "grad_norm": 0.4424413740634918,
      "learning_rate": 0.00014339966477905507,
      "loss": 2.4171,
      "step": 54890
    },
    {
      "epoch": 1.1294181480614793,
      "grad_norm": 0.5751585960388184,
      "learning_rate": 0.00014337965518088446,
      "loss": 2.4637,
      "step": 54900
    },
    {
      "epoch": 1.129623867578347,
      "grad_norm": 0.42739930748939514,
      "learning_rate": 0.00014335964344303184,
      "loss": 2.364,
      "step": 54910
    },
    {
      "epoch": 1.1298295870952146,
      "grad_norm": 0.42077064514160156,
      "learning_rate": 0.0001433396295664843,
      "loss": 2.4457,
      "step": 54920
    },
    {
      "epoch": 1.1300353066120825,
      "grad_norm": 0.4299302101135254,
      "learning_rate": 0.000143319613552229,
      "loss": 2.3969,
      "step": 54930
    },
    {
      "epoch": 1.1302410261289502,
      "grad_norm": 0.44772911071777344,
      "learning_rate": 0.00014329959540125323,
      "loss": 2.4425,
      "step": 54940
    },
    {
      "epoch": 1.1304467456458178,
      "grad_norm": 0.40294593572616577,
      "learning_rate": 0.00014327957511454435,
      "loss": 2.398,
      "step": 54950
    },
    {
      "epoch": 1.1306524651626855,
      "grad_norm": 0.45521530508995056,
      "learning_rate": 0.00014325955269308994,
      "loss": 2.4806,
      "step": 54960
    },
    {
      "epoch": 1.1308581846795533,
      "grad_norm": 0.42343246936798096,
      "learning_rate": 0.00014323952813787745,
      "loss": 2.424,
      "step": 54970
    },
    {
      "epoch": 1.131063904196421,
      "grad_norm": 0.402645081281662,
      "learning_rate": 0.00014321950144989472,
      "loss": 2.3897,
      "step": 54980
    },
    {
      "epoch": 1.1312696237132887,
      "grad_norm": 0.47767531871795654,
      "learning_rate": 0.00014319947263012947,
      "loss": 2.404,
      "step": 54990
    },
    {
      "epoch": 1.1314753432301563,
      "grad_norm": 0.44949835538864136,
      "learning_rate": 0.00014317944167956966,
      "loss": 2.3609,
      "step": 55000
    },
    {
      "epoch": 1.1316810627470242,
      "grad_norm": 0.419689416885376,
      "learning_rate": 0.00014315940859920332,
      "loss": 2.4196,
      "step": 55010
    },
    {
      "epoch": 1.1318867822638918,
      "grad_norm": 0.4162551462650299,
      "learning_rate": 0.0001431393733900185,
      "loss": 2.4545,
      "step": 55020
    },
    {
      "epoch": 1.1320925017807595,
      "grad_norm": 0.4085865616798401,
      "learning_rate": 0.00014311933605300347,
      "loss": 2.4078,
      "step": 55030
    },
    {
      "epoch": 1.1322982212976274,
      "grad_norm": 0.40949735045433044,
      "learning_rate": 0.0001430992965891466,
      "loss": 2.4157,
      "step": 55040
    },
    {
      "epoch": 1.132503940814495,
      "grad_norm": 0.4031698703765869,
      "learning_rate": 0.00014307925499943626,
      "loss": 2.3931,
      "step": 55050
    },
    {
      "epoch": 1.1327096603313627,
      "grad_norm": 0.4462254047393799,
      "learning_rate": 0.00014305921128486107,
      "loss": 2.3724,
      "step": 55060
    },
    {
      "epoch": 1.1329153798482303,
      "grad_norm": 0.41118890047073364,
      "learning_rate": 0.00014303916544640958,
      "loss": 2.3994,
      "step": 55070
    },
    {
      "epoch": 1.1331210993650982,
      "grad_norm": 0.44678157567977905,
      "learning_rate": 0.00014301911748507062,
      "loss": 2.4521,
      "step": 55080
    },
    {
      "epoch": 1.1333268188819658,
      "grad_norm": 0.42171525955200195,
      "learning_rate": 0.00014299906740183302,
      "loss": 2.3999,
      "step": 55090
    },
    {
      "epoch": 1.1335325383988335,
      "grad_norm": 0.4257585108280182,
      "learning_rate": 0.00014297901519768575,
      "loss": 2.3967,
      "step": 55100
    },
    {
      "epoch": 1.1337382579157014,
      "grad_norm": 0.44942933320999146,
      "learning_rate": 0.0001429589608736178,
      "loss": 2.4406,
      "step": 55110
    },
    {
      "epoch": 1.133943977432569,
      "grad_norm": 0.41050949692726135,
      "learning_rate": 0.0001429389044306185,
      "loss": 2.3613,
      "step": 55120
    },
    {
      "epoch": 1.1341496969494367,
      "grad_norm": 0.4603174030780792,
      "learning_rate": 0.000142918845869677,
      "loss": 2.4344,
      "step": 55130
    },
    {
      "epoch": 1.1343554164663043,
      "grad_norm": 0.40461018681526184,
      "learning_rate": 0.0001428987851917827,
      "loss": 2.4032,
      "step": 55140
    },
    {
      "epoch": 1.1345611359831722,
      "grad_norm": 0.44680845737457275,
      "learning_rate": 0.00014287872239792513,
      "loss": 2.4311,
      "step": 55150
    },
    {
      "epoch": 1.1347668555000399,
      "grad_norm": 0.4046897292137146,
      "learning_rate": 0.00014285865748909382,
      "loss": 2.366,
      "step": 55160
    },
    {
      "epoch": 1.1349725750169075,
      "grad_norm": 0.41041937470436096,
      "learning_rate": 0.00014283859046627847,
      "loss": 2.4115,
      "step": 55170
    },
    {
      "epoch": 1.1351782945337754,
      "grad_norm": 0.4219658672809601,
      "learning_rate": 0.0001428185213304689,
      "loss": 2.4495,
      "step": 55180
    },
    {
      "epoch": 1.135384014050643,
      "grad_norm": 0.442384272813797,
      "learning_rate": 0.00014279845008265504,
      "loss": 2.4623,
      "step": 55190
    },
    {
      "epoch": 1.1355897335675107,
      "grad_norm": 0.44000402092933655,
      "learning_rate": 0.00014277837672382684,
      "loss": 2.3938,
      "step": 55200
    },
    {
      "epoch": 1.1357954530843783,
      "grad_norm": 0.4395250082015991,
      "learning_rate": 0.00014275830125497443,
      "loss": 2.358,
      "step": 55210
    },
    {
      "epoch": 1.1360011726012462,
      "grad_norm": 0.4233960211277008,
      "learning_rate": 0.00014273822367708806,
      "loss": 2.4148,
      "step": 55220
    },
    {
      "epoch": 1.1362068921181139,
      "grad_norm": 0.549728274345398,
      "learning_rate": 0.00014271814399115793,
      "loss": 2.4118,
      "step": 55230
    },
    {
      "epoch": 1.1364126116349815,
      "grad_norm": 0.4178822338581085,
      "learning_rate": 0.0001426980621981746,
      "loss": 2.3611,
      "step": 55240
    },
    {
      "epoch": 1.1366183311518494,
      "grad_norm": 0.42597144842147827,
      "learning_rate": 0.00014267797829912855,
      "loss": 2.3439,
      "step": 55250
    },
    {
      "epoch": 1.136824050668717,
      "grad_norm": 0.49101734161376953,
      "learning_rate": 0.00014265789229501037,
      "loss": 2.3807,
      "step": 55260
    },
    {
      "epoch": 1.1370297701855847,
      "grad_norm": 0.5137002468109131,
      "learning_rate": 0.0001426378041868108,
      "loss": 2.4237,
      "step": 55270
    },
    {
      "epoch": 1.1372354897024524,
      "grad_norm": 0.4461151957511902,
      "learning_rate": 0.00014261771397552076,
      "loss": 2.3946,
      "step": 55280
    },
    {
      "epoch": 1.1374412092193202,
      "grad_norm": 0.4032224714756012,
      "learning_rate": 0.00014259762166213106,
      "loss": 2.3977,
      "step": 55290
    },
    {
      "epoch": 1.137646928736188,
      "grad_norm": 0.43274450302124023,
      "learning_rate": 0.00014257752724763288,
      "loss": 2.429,
      "step": 55300
    },
    {
      "epoch": 1.1378526482530555,
      "grad_norm": 0.422097384929657,
      "learning_rate": 0.00014255743073301723,
      "loss": 2.4265,
      "step": 55310
    },
    {
      "epoch": 1.1380583677699232,
      "grad_norm": 0.4145790636539459,
      "learning_rate": 0.00014253733211927548,
      "loss": 2.3872,
      "step": 55320
    },
    {
      "epoch": 1.138264087286791,
      "grad_norm": 0.444140762090683,
      "learning_rate": 0.00014251723140739893,
      "loss": 2.452,
      "step": 55330
    },
    {
      "epoch": 1.1384698068036587,
      "grad_norm": 0.4703926742076874,
      "learning_rate": 0.00014249712859837902,
      "loss": 2.4419,
      "step": 55340
    },
    {
      "epoch": 1.1386755263205264,
      "grad_norm": 0.41541874408721924,
      "learning_rate": 0.00014247702369320737,
      "loss": 2.4516,
      "step": 55350
    },
    {
      "epoch": 1.138881245837394,
      "grad_norm": 0.39105990529060364,
      "learning_rate": 0.00014245691669287559,
      "loss": 2.3971,
      "step": 55360
    },
    {
      "epoch": 1.139086965354262,
      "grad_norm": 0.46324101090431213,
      "learning_rate": 0.0001424368075983755,
      "loss": 2.4218,
      "step": 55370
    },
    {
      "epoch": 1.1392926848711296,
      "grad_norm": 0.4016708731651306,
      "learning_rate": 0.00014241669641069894,
      "loss": 2.4489,
      "step": 55380
    },
    {
      "epoch": 1.1394984043879972,
      "grad_norm": 0.42411839962005615,
      "learning_rate": 0.00014239658313083787,
      "loss": 2.396,
      "step": 55390
    },
    {
      "epoch": 1.139704123904865,
      "grad_norm": 0.41417983174324036,
      "learning_rate": 0.0001423764677597844,
      "loss": 2.3649,
      "step": 55400
    },
    {
      "epoch": 1.1399098434217327,
      "grad_norm": 0.4666159451007843,
      "learning_rate": 0.0001423563502985307,
      "loss": 2.3614,
      "step": 55410
    },
    {
      "epoch": 1.1401155629386004,
      "grad_norm": 0.5141103863716125,
      "learning_rate": 0.00014233623074806902,
      "loss": 2.3712,
      "step": 55420
    },
    {
      "epoch": 1.140321282455468,
      "grad_norm": 0.450491726398468,
      "learning_rate": 0.00014231610910939182,
      "loss": 2.4311,
      "step": 55430
    },
    {
      "epoch": 1.140527001972336,
      "grad_norm": 0.40169796347618103,
      "learning_rate": 0.00014229598538349156,
      "loss": 2.4115,
      "step": 55440
    },
    {
      "epoch": 1.1407327214892036,
      "grad_norm": 0.4035622179508209,
      "learning_rate": 0.00014227585957136081,
      "loss": 2.4301,
      "step": 55450
    },
    {
      "epoch": 1.1409384410060712,
      "grad_norm": 0.4190278649330139,
      "learning_rate": 0.00014225573167399227,
      "loss": 2.4166,
      "step": 55460
    },
    {
      "epoch": 1.141144160522939,
      "grad_norm": 0.425203412771225,
      "learning_rate": 0.00014223560169237878,
      "loss": 2.4097,
      "step": 55470
    },
    {
      "epoch": 1.1413498800398068,
      "grad_norm": 0.4328905940055847,
      "learning_rate": 0.00014221546962751318,
      "loss": 2.4177,
      "step": 55480
    },
    {
      "epoch": 1.1415555995566744,
      "grad_norm": 0.41394853591918945,
      "learning_rate": 0.00014219533548038852,
      "loss": 2.4326,
      "step": 55490
    },
    {
      "epoch": 1.141761319073542,
      "grad_norm": 0.44234439730644226,
      "learning_rate": 0.0001421751992519979,
      "loss": 2.4343,
      "step": 55500
    },
    {
      "epoch": 1.14196703859041,
      "grad_norm": 0.42253512144088745,
      "learning_rate": 0.00014215506094333456,
      "loss": 2.415,
      "step": 55510
    },
    {
      "epoch": 1.1421727581072776,
      "grad_norm": 0.3919607102870941,
      "learning_rate": 0.00014213492055539174,
      "loss": 2.4394,
      "step": 55520
    },
    {
      "epoch": 1.1423784776241452,
      "grad_norm": 0.4055464267730713,
      "learning_rate": 0.0001421147780891629,
      "loss": 2.4054,
      "step": 55530
    },
    {
      "epoch": 1.1425841971410131,
      "grad_norm": 0.43985646963119507,
      "learning_rate": 0.00014209463354564162,
      "loss": 2.3963,
      "step": 55540
    },
    {
      "epoch": 1.1427899166578808,
      "grad_norm": 0.45242586731910706,
      "learning_rate": 0.00014207448692582137,
      "loss": 2.4408,
      "step": 55550
    },
    {
      "epoch": 1.1429956361747484,
      "grad_norm": 0.38216057419776917,
      "learning_rate": 0.000142054338230696,
      "loss": 2.3926,
      "step": 55560
    },
    {
      "epoch": 1.143201355691616,
      "grad_norm": 0.44002848863601685,
      "learning_rate": 0.00014203418746125933,
      "loss": 2.3577,
      "step": 55570
    },
    {
      "epoch": 1.143407075208484,
      "grad_norm": 0.39529451727867126,
      "learning_rate": 0.00014201403461850518,
      "loss": 2.4042,
      "step": 55580
    },
    {
      "epoch": 1.1436127947253516,
      "grad_norm": 0.46869710087776184,
      "learning_rate": 0.00014199387970342765,
      "loss": 2.3929,
      "step": 55590
    },
    {
      "epoch": 1.1438185142422193,
      "grad_norm": 0.45083484053611755,
      "learning_rate": 0.00014197372271702093,
      "loss": 2.3764,
      "step": 55600
    },
    {
      "epoch": 1.1440242337590871,
      "grad_norm": 0.41347670555114746,
      "learning_rate": 0.00014195356366027918,
      "loss": 2.4332,
      "step": 55610
    },
    {
      "epoch": 1.1442299532759548,
      "grad_norm": 0.46145161986351013,
      "learning_rate": 0.00014193340253419674,
      "loss": 2.3694,
      "step": 55620
    },
    {
      "epoch": 1.1444356727928224,
      "grad_norm": 0.4295431673526764,
      "learning_rate": 0.00014191323933976805,
      "loss": 2.3898,
      "step": 55630
    },
    {
      "epoch": 1.14464139230969,
      "grad_norm": 0.39654740691185,
      "learning_rate": 0.0001418930740779877,
      "loss": 2.3617,
      "step": 55640
    },
    {
      "epoch": 1.144847111826558,
      "grad_norm": 0.4389348328113556,
      "learning_rate": 0.00014187290674985028,
      "loss": 2.3918,
      "step": 55650
    },
    {
      "epoch": 1.1450528313434256,
      "grad_norm": 0.47993937134742737,
      "learning_rate": 0.00014185273735635052,
      "loss": 2.3948,
      "step": 55660
    },
    {
      "epoch": 1.1452585508602933,
      "grad_norm": 0.39263707399368286,
      "learning_rate": 0.00014183256589848334,
      "loss": 2.3839,
      "step": 55670
    },
    {
      "epoch": 1.1454642703771611,
      "grad_norm": 0.4910462498664856,
      "learning_rate": 0.00014181239237724363,
      "loss": 2.3857,
      "step": 55680
    },
    {
      "epoch": 1.1456699898940288,
      "grad_norm": 0.4143996834754944,
      "learning_rate": 0.00014179221679362644,
      "loss": 2.4123,
      "step": 55690
    },
    {
      "epoch": 1.1458757094108964,
      "grad_norm": 0.4439791142940521,
      "learning_rate": 0.00014177203914862696,
      "loss": 2.4463,
      "step": 55700
    },
    {
      "epoch": 1.146081428927764,
      "grad_norm": 0.44328832626342773,
      "learning_rate": 0.0001417518594432404,
      "loss": 2.3512,
      "step": 55710
    },
    {
      "epoch": 1.1462871484446318,
      "grad_norm": 0.4648325443267822,
      "learning_rate": 0.00014173167767846212,
      "loss": 2.4237,
      "step": 55720
    },
    {
      "epoch": 1.1464928679614996,
      "grad_norm": 0.46873703598976135,
      "learning_rate": 0.00014171149385528762,
      "loss": 2.3686,
      "step": 55730
    },
    {
      "epoch": 1.1466985874783673,
      "grad_norm": 0.4432934820652008,
      "learning_rate": 0.00014169130797471243,
      "loss": 2.4035,
      "step": 55740
    },
    {
      "epoch": 1.146904306995235,
      "grad_norm": 0.42491891980171204,
      "learning_rate": 0.0001416711200377322,
      "loss": 2.3648,
      "step": 55750
    },
    {
      "epoch": 1.1471100265121028,
      "grad_norm": 0.42654457688331604,
      "learning_rate": 0.0001416509300453427,
      "loss": 2.4225,
      "step": 55760
    },
    {
      "epoch": 1.1473157460289705,
      "grad_norm": 0.4254855513572693,
      "learning_rate": 0.00014163073799853976,
      "loss": 2.4085,
      "step": 55770
    },
    {
      "epoch": 1.1475214655458381,
      "grad_norm": 0.4321717619895935,
      "learning_rate": 0.00014161054389831942,
      "loss": 2.4286,
      "step": 55780
    },
    {
      "epoch": 1.1477271850627058,
      "grad_norm": 0.4701155722141266,
      "learning_rate": 0.00014159034774567766,
      "loss": 2.3848,
      "step": 55790
    },
    {
      "epoch": 1.1479329045795736,
      "grad_norm": 0.4217800498008728,
      "learning_rate": 0.00014157014954161072,
      "loss": 2.4136,
      "step": 55800
    },
    {
      "epoch": 1.1481386240964413,
      "grad_norm": 0.44549840688705444,
      "learning_rate": 0.00014154994928711483,
      "loss": 2.4062,
      "step": 55810
    },
    {
      "epoch": 1.148344343613309,
      "grad_norm": 0.43974462151527405,
      "learning_rate": 0.00014152974698318634,
      "loss": 2.4359,
      "step": 55820
    },
    {
      "epoch": 1.1485500631301768,
      "grad_norm": 0.4458647668361664,
      "learning_rate": 0.00014150954263082176,
      "loss": 2.4254,
      "step": 55830
    },
    {
      "epoch": 1.1487557826470445,
      "grad_norm": 0.4006269872188568,
      "learning_rate": 0.00014148933623101762,
      "loss": 2.4099,
      "step": 55840
    },
    {
      "epoch": 1.1489615021639121,
      "grad_norm": 0.4189605116844177,
      "learning_rate": 0.00014146912778477064,
      "loss": 2.3481,
      "step": 55850
    },
    {
      "epoch": 1.1491672216807798,
      "grad_norm": 0.45115384459495544,
      "learning_rate": 0.00014144891729307756,
      "loss": 2.4162,
      "step": 55860
    },
    {
      "epoch": 1.1493729411976477,
      "grad_norm": 0.41320088505744934,
      "learning_rate": 0.0001414287047569353,
      "loss": 2.4186,
      "step": 55870
    },
    {
      "epoch": 1.1495786607145153,
      "grad_norm": 0.39204415678977966,
      "learning_rate": 0.0001414084901773407,
      "loss": 2.4181,
      "step": 55880
    },
    {
      "epoch": 1.149784380231383,
      "grad_norm": 0.4137560725212097,
      "learning_rate": 0.000141388273555291,
      "loss": 2.329,
      "step": 55890
    },
    {
      "epoch": 1.1499900997482508,
      "grad_norm": 0.43304455280303955,
      "learning_rate": 0.0001413680548917833,
      "loss": 2.3924,
      "step": 55900
    },
    {
      "epoch": 1.1501958192651185,
      "grad_norm": 0.46705877780914307,
      "learning_rate": 0.00014134783418781483,
      "loss": 2.394,
      "step": 55910
    },
    {
      "epoch": 1.1504015387819861,
      "grad_norm": 0.4114449620246887,
      "learning_rate": 0.00014132761144438306,
      "loss": 2.451,
      "step": 55920
    },
    {
      "epoch": 1.1506072582988538,
      "grad_norm": 0.3968847095966339,
      "learning_rate": 0.0001413073866624854,
      "loss": 2.4799,
      "step": 55930
    },
    {
      "epoch": 1.1508129778157217,
      "grad_norm": 0.42700788378715515,
      "learning_rate": 0.00014128715984311948,
      "loss": 2.4038,
      "step": 55940
    },
    {
      "epoch": 1.1510186973325893,
      "grad_norm": 0.45636242628097534,
      "learning_rate": 0.00014126693098728295,
      "loss": 2.4521,
      "step": 55950
    },
    {
      "epoch": 1.151224416849457,
      "grad_norm": 0.47920939326286316,
      "learning_rate": 0.00014124670009597358,
      "loss": 2.445,
      "step": 55960
    },
    {
      "epoch": 1.1514301363663249,
      "grad_norm": 0.4233396351337433,
      "learning_rate": 0.00014122646717018927,
      "loss": 2.4028,
      "step": 55970
    },
    {
      "epoch": 1.1516358558831925,
      "grad_norm": 0.39799195528030396,
      "learning_rate": 0.00014120623221092798,
      "loss": 2.3927,
      "step": 55980
    },
    {
      "epoch": 1.1518415754000602,
      "grad_norm": 0.42279568314552307,
      "learning_rate": 0.00014118599521918782,
      "loss": 2.438,
      "step": 55990
    },
    {
      "epoch": 1.1520472949169278,
      "grad_norm": 0.43054699897766113,
      "learning_rate": 0.00014116575619596694,
      "loss": 2.434,
      "step": 56000
    },
    {
      "epoch": 1.1522530144337957,
      "grad_norm": 0.423684686422348,
      "learning_rate": 0.00014114551514226364,
      "loss": 2.4368,
      "step": 56010
    },
    {
      "epoch": 1.1524587339506633,
      "grad_norm": 0.4396611154079437,
      "learning_rate": 0.0001411252720590763,
      "loss": 2.379,
      "step": 56020
    },
    {
      "epoch": 1.152664453467531,
      "grad_norm": 0.43401750922203064,
      "learning_rate": 0.00014110502694740339,
      "loss": 2.3579,
      "step": 56030
    },
    {
      "epoch": 1.1528701729843989,
      "grad_norm": 0.47348180413246155,
      "learning_rate": 0.00014108477980824348,
      "loss": 2.4074,
      "step": 56040
    },
    {
      "epoch": 1.1530758925012665,
      "grad_norm": 0.40021899342536926,
      "learning_rate": 0.00014106453064259528,
      "loss": 2.4165,
      "step": 56050
    },
    {
      "epoch": 1.1532816120181342,
      "grad_norm": 0.4513547122478485,
      "learning_rate": 0.00014104427945145758,
      "loss": 2.3788,
      "step": 56060
    },
    {
      "epoch": 1.1534873315350018,
      "grad_norm": 0.43590694665908813,
      "learning_rate": 0.0001410240262358292,
      "loss": 2.3777,
      "step": 56070
    },
    {
      "epoch": 1.1536930510518697,
      "grad_norm": 0.4199402928352356,
      "learning_rate": 0.0001410037709967092,
      "loss": 2.4462,
      "step": 56080
    },
    {
      "epoch": 1.1538987705687374,
      "grad_norm": 0.43340423703193665,
      "learning_rate": 0.0001409835137350966,
      "loss": 2.4248,
      "step": 56090
    },
    {
      "epoch": 1.154104490085605,
      "grad_norm": 0.4125595986843109,
      "learning_rate": 0.0001409632544519906,
      "loss": 2.406,
      "step": 56100
    },
    {
      "epoch": 1.1543102096024727,
      "grad_norm": 0.4404347538948059,
      "learning_rate": 0.00014094299314839045,
      "loss": 2.4047,
      "step": 56110
    },
    {
      "epoch": 1.1545159291193405,
      "grad_norm": 0.42717552185058594,
      "learning_rate": 0.0001409227298252956,
      "loss": 2.4506,
      "step": 56120
    },
    {
      "epoch": 1.1547216486362082,
      "grad_norm": 0.4175814986228943,
      "learning_rate": 0.00014090246448370547,
      "loss": 2.3939,
      "step": 56130
    },
    {
      "epoch": 1.1549273681530758,
      "grad_norm": 0.4541178345680237,
      "learning_rate": 0.00014088219712461968,
      "loss": 2.4213,
      "step": 56140
    },
    {
      "epoch": 1.1551330876699435,
      "grad_norm": 0.4431092143058777,
      "learning_rate": 0.0001408619277490379,
      "loss": 2.4368,
      "step": 56150
    },
    {
      "epoch": 1.1553388071868114,
      "grad_norm": 0.40117987990379333,
      "learning_rate": 0.00014084165635795985,
      "loss": 2.4269,
      "step": 56160
    },
    {
      "epoch": 1.155544526703679,
      "grad_norm": 0.44109904766082764,
      "learning_rate": 0.00014082138295238547,
      "loss": 2.4,
      "step": 56170
    },
    {
      "epoch": 1.1557502462205467,
      "grad_norm": 0.431456595659256,
      "learning_rate": 0.00014080110753331474,
      "loss": 2.4451,
      "step": 56180
    },
    {
      "epoch": 1.1559559657374145,
      "grad_norm": 0.42080676555633545,
      "learning_rate": 0.0001407808301017477,
      "loss": 2.4335,
      "step": 56190
    },
    {
      "epoch": 1.1561616852542822,
      "grad_norm": 0.4754880368709564,
      "learning_rate": 0.00014076055065868453,
      "loss": 2.4235,
      "step": 56200
    },
    {
      "epoch": 1.1563674047711499,
      "grad_norm": 0.4183773696422577,
      "learning_rate": 0.00014074026920512554,
      "loss": 2.3597,
      "step": 56210
    },
    {
      "epoch": 1.1565731242880175,
      "grad_norm": 0.4204297959804535,
      "learning_rate": 0.00014071998574207104,
      "loss": 2.4064,
      "step": 56220
    },
    {
      "epoch": 1.1567788438048854,
      "grad_norm": 0.43888169527053833,
      "learning_rate": 0.00014069970027052154,
      "loss": 2.4298,
      "step": 56230
    },
    {
      "epoch": 1.156984563321753,
      "grad_norm": 0.4726904332637787,
      "learning_rate": 0.00014067941279147765,
      "loss": 2.3757,
      "step": 56240
    },
    {
      "epoch": 1.1571902828386207,
      "grad_norm": 0.43065229058265686,
      "learning_rate": 0.00014065912330593999,
      "loss": 2.4021,
      "step": 56250
    },
    {
      "epoch": 1.1573960023554886,
      "grad_norm": 0.4118926227092743,
      "learning_rate": 0.00014063883181490933,
      "loss": 2.3779,
      "step": 56260
    },
    {
      "epoch": 1.1576017218723562,
      "grad_norm": 0.4062788784503937,
      "learning_rate": 0.00014061853831938656,
      "loss": 2.4216,
      "step": 56270
    },
    {
      "epoch": 1.1578074413892239,
      "grad_norm": 0.4385016858577728,
      "learning_rate": 0.00014059824282037264,
      "loss": 2.3921,
      "step": 56280
    },
    {
      "epoch": 1.1580131609060915,
      "grad_norm": 0.4328394830226898,
      "learning_rate": 0.00014057794531886864,
      "loss": 2.4252,
      "step": 56290
    },
    {
      "epoch": 1.1582188804229594,
      "grad_norm": 0.4531835913658142,
      "learning_rate": 0.00014055764581587572,
      "loss": 2.3933,
      "step": 56300
    },
    {
      "epoch": 1.158424599939827,
      "grad_norm": 0.4126129746437073,
      "learning_rate": 0.00014053734431239512,
      "loss": 2.4042,
      "step": 56310
    },
    {
      "epoch": 1.1586303194566947,
      "grad_norm": 0.40201303362846375,
      "learning_rate": 0.00014051704080942823,
      "loss": 2.4366,
      "step": 56320
    },
    {
      "epoch": 1.1588360389735626,
      "grad_norm": 0.43325385451316833,
      "learning_rate": 0.00014049673530797655,
      "loss": 2.3836,
      "step": 56330
    },
    {
      "epoch": 1.1590417584904302,
      "grad_norm": 0.45103591680526733,
      "learning_rate": 0.00014047642780904158,
      "loss": 2.4058,
      "step": 56340
    },
    {
      "epoch": 1.1592474780072979,
      "grad_norm": 0.4936228096485138,
      "learning_rate": 0.000140456118313625,
      "loss": 2.3416,
      "step": 56350
    },
    {
      "epoch": 1.1594531975241655,
      "grad_norm": 0.4296051561832428,
      "learning_rate": 0.00014043580682272853,
      "loss": 2.3732,
      "step": 56360
    },
    {
      "epoch": 1.1596589170410334,
      "grad_norm": 0.4483116567134857,
      "learning_rate": 0.00014041549333735409,
      "loss": 2.3927,
      "step": 56370
    },
    {
      "epoch": 1.159864636557901,
      "grad_norm": 0.44668862223625183,
      "learning_rate": 0.00014039517785850361,
      "loss": 2.4088,
      "step": 56380
    },
    {
      "epoch": 1.1600703560747687,
      "grad_norm": 0.4426628649234772,
      "learning_rate": 0.0001403748603871791,
      "loss": 2.4284,
      "step": 56390
    },
    {
      "epoch": 1.1602760755916366,
      "grad_norm": 0.4017592966556549,
      "learning_rate": 0.00014035454092438276,
      "loss": 2.3803,
      "step": 56400
    },
    {
      "epoch": 1.1604817951085042,
      "grad_norm": 0.4265606701374054,
      "learning_rate": 0.00014033421947111683,
      "loss": 2.4324,
      "step": 56410
    },
    {
      "epoch": 1.160687514625372,
      "grad_norm": 0.4422931671142578,
      "learning_rate": 0.00014031389602838363,
      "loss": 2.4113,
      "step": 56420
    },
    {
      "epoch": 1.1608932341422395,
      "grad_norm": 0.42017048597335815,
      "learning_rate": 0.00014029357059718565,
      "loss": 2.4157,
      "step": 56430
    },
    {
      "epoch": 1.1610989536591074,
      "grad_norm": 0.3947219252586365,
      "learning_rate": 0.00014027324317852536,
      "loss": 2.3894,
      "step": 56440
    },
    {
      "epoch": 1.161304673175975,
      "grad_norm": 0.3985052704811096,
      "learning_rate": 0.0001402529137734055,
      "loss": 2.3554,
      "step": 56450
    },
    {
      "epoch": 1.1615103926928427,
      "grad_norm": 0.5213798880577087,
      "learning_rate": 0.00014023258238282874,
      "loss": 2.4288,
      "step": 56460
    },
    {
      "epoch": 1.1617161122097104,
      "grad_norm": 0.4306434988975525,
      "learning_rate": 0.00014021224900779793,
      "loss": 2.4261,
      "step": 56470
    },
    {
      "epoch": 1.1619218317265783,
      "grad_norm": 0.4378605782985687,
      "learning_rate": 0.000140191913649316,
      "loss": 2.3833,
      "step": 56480
    },
    {
      "epoch": 1.162127551243446,
      "grad_norm": 0.40836623311042786,
      "learning_rate": 0.00014017157630838597,
      "loss": 2.4225,
      "step": 56490
    },
    {
      "epoch": 1.1623332707603136,
      "grad_norm": 0.4420659840106964,
      "learning_rate": 0.00014015123698601104,
      "loss": 2.4691,
      "step": 56500
    },
    {
      "epoch": 1.1625389902771812,
      "grad_norm": 0.3863637149333954,
      "learning_rate": 0.00014013089568319436,
      "loss": 2.374,
      "step": 56510
    },
    {
      "epoch": 1.162744709794049,
      "grad_norm": 0.4082736074924469,
      "learning_rate": 0.00014011055240093924,
      "loss": 2.3733,
      "step": 56520
    },
    {
      "epoch": 1.1629504293109167,
      "grad_norm": 0.4465745687484741,
      "learning_rate": 0.0001400902071402492,
      "loss": 2.4196,
      "step": 56530
    },
    {
      "epoch": 1.1631561488277844,
      "grad_norm": 0.4544455409049988,
      "learning_rate": 0.0001400698599021277,
      "loss": 2.3593,
      "step": 56540
    },
    {
      "epoch": 1.1633618683446523,
      "grad_norm": 0.4532378017902374,
      "learning_rate": 0.00014004951068757838,
      "loss": 2.3896,
      "step": 56550
    },
    {
      "epoch": 1.16356758786152,
      "grad_norm": 0.46421223878860474,
      "learning_rate": 0.0001400291594976049,
      "loss": 2.3968,
      "step": 56560
    },
    {
      "epoch": 1.1637733073783876,
      "grad_norm": 0.3960363268852234,
      "learning_rate": 0.00014000880633321114,
      "loss": 2.4183,
      "step": 56570
    },
    {
      "epoch": 1.1639790268952552,
      "grad_norm": 0.42405569553375244,
      "learning_rate": 0.000139988451195401,
      "loss": 2.4322,
      "step": 56580
    },
    {
      "epoch": 1.164184746412123,
      "grad_norm": 0.4831300675868988,
      "learning_rate": 0.00013996809408517845,
      "loss": 2.3696,
      "step": 56590
    },
    {
      "epoch": 1.1643904659289908,
      "grad_norm": 0.4110110402107239,
      "learning_rate": 0.0001399477350035476,
      "loss": 2.4337,
      "step": 56600
    },
    {
      "epoch": 1.1645961854458584,
      "grad_norm": 0.4689091742038727,
      "learning_rate": 0.0001399273739515127,
      "loss": 2.37,
      "step": 56610
    },
    {
      "epoch": 1.1648019049627263,
      "grad_norm": 0.41905611753463745,
      "learning_rate": 0.00013990701093007804,
      "loss": 2.4422,
      "step": 56620
    },
    {
      "epoch": 1.165007624479594,
      "grad_norm": 0.4241427481174469,
      "learning_rate": 0.000139886645940248,
      "loss": 2.3984,
      "step": 56630
    },
    {
      "epoch": 1.1652133439964616,
      "grad_norm": 0.4626876711845398,
      "learning_rate": 0.000139866278983027,
      "loss": 2.434,
      "step": 56640
    },
    {
      "epoch": 1.1654190635133292,
      "grad_norm": 0.4340544044971466,
      "learning_rate": 0.00013984591005941974,
      "loss": 2.3757,
      "step": 56650
    },
    {
      "epoch": 1.1656247830301971,
      "grad_norm": 0.4354488253593445,
      "learning_rate": 0.0001398255391704309,
      "loss": 2.3946,
      "step": 56660
    },
    {
      "epoch": 1.1658305025470648,
      "grad_norm": 0.45304614305496216,
      "learning_rate": 0.00013980516631706524,
      "loss": 2.4196,
      "step": 56670
    },
    {
      "epoch": 1.1660362220639324,
      "grad_norm": 0.4203000068664551,
      "learning_rate": 0.00013978479150032758,
      "loss": 2.4123,
      "step": 56680
    },
    {
      "epoch": 1.1662419415808003,
      "grad_norm": 0.45170366764068604,
      "learning_rate": 0.000139764414721223,
      "loss": 2.3941,
      "step": 56690
    },
    {
      "epoch": 1.166447661097668,
      "grad_norm": 0.4227188229560852,
      "learning_rate": 0.00013974403598075654,
      "loss": 2.3657,
      "step": 56700
    },
    {
      "epoch": 1.1666533806145356,
      "grad_norm": 0.46277961134910583,
      "learning_rate": 0.00013972365527993333,
      "loss": 2.443,
      "step": 56710
    },
    {
      "epoch": 1.1668591001314033,
      "grad_norm": 0.4091207683086395,
      "learning_rate": 0.0001397032726197587,
      "loss": 2.3747,
      "step": 56720
    },
    {
      "epoch": 1.1670648196482711,
      "grad_norm": 0.4571800231933594,
      "learning_rate": 0.00013968288800123797,
      "loss": 2.4351,
      "step": 56730
    },
    {
      "epoch": 1.1672705391651388,
      "grad_norm": 0.4327608048915863,
      "learning_rate": 0.00013966250142537662,
      "loss": 2.4548,
      "step": 56740
    },
    {
      "epoch": 1.1674762586820064,
      "grad_norm": 0.4449971318244934,
      "learning_rate": 0.00013964211289318023,
      "loss": 2.4714,
      "step": 56750
    },
    {
      "epoch": 1.1676819781988743,
      "grad_norm": 0.3939802646636963,
      "learning_rate": 0.0001396217224056544,
      "loss": 2.3831,
      "step": 56760
    },
    {
      "epoch": 1.167887697715742,
      "grad_norm": 0.4444056451320648,
      "learning_rate": 0.00013960132996380494,
      "loss": 2.4896,
      "step": 56770
    },
    {
      "epoch": 1.1680934172326096,
      "grad_norm": 0.4353215992450714,
      "learning_rate": 0.00013958093556863769,
      "loss": 2.3498,
      "step": 56780
    },
    {
      "epoch": 1.1682991367494773,
      "grad_norm": 0.4607085585594177,
      "learning_rate": 0.00013956053922115858,
      "loss": 2.4436,
      "step": 56790
    },
    {
      "epoch": 1.1685048562663451,
      "grad_norm": 0.46054506301879883,
      "learning_rate": 0.0001395401409223736,
      "loss": 2.3932,
      "step": 56800
    },
    {
      "epoch": 1.1687105757832128,
      "grad_norm": 0.3948620557785034,
      "learning_rate": 0.00013951974067328898,
      "loss": 2.3698,
      "step": 56810
    },
    {
      "epoch": 1.1689162953000805,
      "grad_norm": 0.4460168182849884,
      "learning_rate": 0.0001394993384749109,
      "loss": 2.4123,
      "step": 56820
    },
    {
      "epoch": 1.169122014816948,
      "grad_norm": 0.43991976976394653,
      "learning_rate": 0.0001394789343282457,
      "loss": 2.3477,
      "step": 56830
    },
    {
      "epoch": 1.169327734333816,
      "grad_norm": 0.43043088912963867,
      "learning_rate": 0.00013945852823429977,
      "loss": 2.4567,
      "step": 56840
    },
    {
      "epoch": 1.1695334538506836,
      "grad_norm": 0.40004926919937134,
      "learning_rate": 0.0001394381201940797,
      "loss": 2.3625,
      "step": 56850
    },
    {
      "epoch": 1.1697391733675513,
      "grad_norm": 0.401394248008728,
      "learning_rate": 0.0001394177102085921,
      "loss": 2.4741,
      "step": 56860
    },
    {
      "epoch": 1.169944892884419,
      "grad_norm": 0.43187999725341797,
      "learning_rate": 0.00013939729827884363,
      "loss": 2.4162,
      "step": 56870
    },
    {
      "epoch": 1.1701506124012868,
      "grad_norm": 0.4256247282028198,
      "learning_rate": 0.0001393768844058411,
      "loss": 2.3311,
      "step": 56880
    },
    {
      "epoch": 1.1703563319181545,
      "grad_norm": 0.42859506607055664,
      "learning_rate": 0.00013935646859059147,
      "loss": 2.368,
      "step": 56890
    },
    {
      "epoch": 1.1705620514350221,
      "grad_norm": 0.4003622531890869,
      "learning_rate": 0.00013933605083410168,
      "loss": 2.4182,
      "step": 56900
    },
    {
      "epoch": 1.17076777095189,
      "grad_norm": 0.4351826310157776,
      "learning_rate": 0.0001393156311373789,
      "loss": 2.3607,
      "step": 56910
    },
    {
      "epoch": 1.1709734904687576,
      "grad_norm": 0.4409160315990448,
      "learning_rate": 0.00013929520950143026,
      "loss": 2.4032,
      "step": 56920
    },
    {
      "epoch": 1.1711792099856253,
      "grad_norm": 0.41573596000671387,
      "learning_rate": 0.00013927478592726304,
      "loss": 2.388,
      "step": 56930
    },
    {
      "epoch": 1.171384929502493,
      "grad_norm": 0.43518128991127014,
      "learning_rate": 0.0001392543604158847,
      "loss": 2.4004,
      "step": 56940
    },
    {
      "epoch": 1.1715906490193608,
      "grad_norm": 0.4197786748409271,
      "learning_rate": 0.00013923393296830268,
      "loss": 2.4366,
      "step": 56950
    },
    {
      "epoch": 1.1717963685362285,
      "grad_norm": 0.4215196967124939,
      "learning_rate": 0.0001392135035855245,
      "loss": 2.4304,
      "step": 56960
    },
    {
      "epoch": 1.1720020880530961,
      "grad_norm": 0.4015788733959198,
      "learning_rate": 0.00013919307226855786,
      "loss": 2.4243,
      "step": 56970
    },
    {
      "epoch": 1.172207807569964,
      "grad_norm": 0.4033617377281189,
      "learning_rate": 0.0001391726390184106,
      "loss": 2.3975,
      "step": 56980
    },
    {
      "epoch": 1.1724135270868317,
      "grad_norm": 0.4545380771160126,
      "learning_rate": 0.0001391522038360905,
      "loss": 2.4362,
      "step": 56990
    },
    {
      "epoch": 1.1726192466036993,
      "grad_norm": 0.4426898956298828,
      "learning_rate": 0.00013913176672260554,
      "loss": 2.3632,
      "step": 57000
    },
    {
      "epoch": 1.172824966120567,
      "grad_norm": 0.4191928803920746,
      "learning_rate": 0.00013911132767896377,
      "loss": 2.4212,
      "step": 57010
    },
    {
      "epoch": 1.1730306856374348,
      "grad_norm": 0.42898038029670715,
      "learning_rate": 0.00013909088670617333,
      "loss": 2.3887,
      "step": 57020
    },
    {
      "epoch": 1.1732364051543025,
      "grad_norm": 0.3999112546443939,
      "learning_rate": 0.00013907044380524246,
      "loss": 2.4247,
      "step": 57030
    },
    {
      "epoch": 1.1734421246711702,
      "grad_norm": 0.3849518299102783,
      "learning_rate": 0.00013904999897717952,
      "loss": 2.406,
      "step": 57040
    },
    {
      "epoch": 1.173647844188038,
      "grad_norm": 0.530903160572052,
      "learning_rate": 0.00013902955222299293,
      "loss": 2.4195,
      "step": 57050
    },
    {
      "epoch": 1.1738535637049057,
      "grad_norm": 0.48556044697761536,
      "learning_rate": 0.00013900910354369122,
      "loss": 2.402,
      "step": 57060
    },
    {
      "epoch": 1.1740592832217733,
      "grad_norm": 0.41912877559661865,
      "learning_rate": 0.00013898865294028297,
      "loss": 2.414,
      "step": 57070
    },
    {
      "epoch": 1.174265002738641,
      "grad_norm": 0.48765236139297485,
      "learning_rate": 0.000138968200413777,
      "loss": 2.3796,
      "step": 57080
    },
    {
      "epoch": 1.1744707222555089,
      "grad_norm": 0.43905118107795715,
      "learning_rate": 0.00013894774596518198,
      "loss": 2.3878,
      "step": 57090
    },
    {
      "epoch": 1.1746764417723765,
      "grad_norm": 0.4331335723400116,
      "learning_rate": 0.00013892728959550695,
      "loss": 2.4404,
      "step": 57100
    },
    {
      "epoch": 1.1748821612892442,
      "grad_norm": 0.4954529404640198,
      "learning_rate": 0.00013890683130576083,
      "loss": 2.4018,
      "step": 57110
    },
    {
      "epoch": 1.175087880806112,
      "grad_norm": 0.4340052008628845,
      "learning_rate": 0.00013888637109695274,
      "loss": 2.4046,
      "step": 57120
    },
    {
      "epoch": 1.1752936003229797,
      "grad_norm": 0.45542457699775696,
      "learning_rate": 0.00013886590897009183,
      "loss": 2.3968,
      "step": 57130
    },
    {
      "epoch": 1.1754993198398473,
      "grad_norm": 0.41216757893562317,
      "learning_rate": 0.00013884544492618746,
      "loss": 2.4326,
      "step": 57140
    },
    {
      "epoch": 1.175705039356715,
      "grad_norm": 0.4298321306705475,
      "learning_rate": 0.000138824978966249,
      "loss": 2.3635,
      "step": 57150
    },
    {
      "epoch": 1.1759107588735829,
      "grad_norm": 0.4844430983066559,
      "learning_rate": 0.00013880451109128585,
      "loss": 2.4283,
      "step": 57160
    },
    {
      "epoch": 1.1761164783904505,
      "grad_norm": 0.4072985053062439,
      "learning_rate": 0.00013878404130230766,
      "loss": 2.4613,
      "step": 57170
    },
    {
      "epoch": 1.1763221979073182,
      "grad_norm": 0.4125087261199951,
      "learning_rate": 0.00013876356960032405,
      "loss": 2.4252,
      "step": 57180
    },
    {
      "epoch": 1.176527917424186,
      "grad_norm": 0.49727943539619446,
      "learning_rate": 0.00013874309598634478,
      "loss": 2.4154,
      "step": 57190
    },
    {
      "epoch": 1.1767336369410537,
      "grad_norm": 0.4293023943901062,
      "learning_rate": 0.0001387226204613797,
      "loss": 2.4116,
      "step": 57200
    },
    {
      "epoch": 1.1769393564579214,
      "grad_norm": 0.4202162027359009,
      "learning_rate": 0.0001387021430264388,
      "loss": 2.363,
      "step": 57210
    },
    {
      "epoch": 1.177145075974789,
      "grad_norm": 0.3986722528934479,
      "learning_rate": 0.00013868166368253206,
      "loss": 2.4001,
      "step": 57220
    },
    {
      "epoch": 1.1773507954916567,
      "grad_norm": 0.4190526008605957,
      "learning_rate": 0.00013866118243066966,
      "loss": 2.4174,
      "step": 57230
    },
    {
      "epoch": 1.1775565150085245,
      "grad_norm": 0.42505019903182983,
      "learning_rate": 0.0001386406992718618,
      "loss": 2.3841,
      "step": 57240
    },
    {
      "epoch": 1.1777622345253922,
      "grad_norm": 0.4458499550819397,
      "learning_rate": 0.00013862021420711878,
      "loss": 2.4505,
      "step": 57250
    },
    {
      "epoch": 1.1779679540422598,
      "grad_norm": 0.4226420223712921,
      "learning_rate": 0.0001385997272374511,
      "loss": 2.441,
      "step": 57260
    },
    {
      "epoch": 1.1781736735591277,
      "grad_norm": 0.4443265199661255,
      "learning_rate": 0.0001385792383638692,
      "loss": 2.4271,
      "step": 57270
    },
    {
      "epoch": 1.1783793930759954,
      "grad_norm": 0.4212915003299713,
      "learning_rate": 0.0001385587475873837,
      "loss": 2.352,
      "step": 57280
    },
    {
      "epoch": 1.178585112592863,
      "grad_norm": 0.4259984493255615,
      "learning_rate": 0.0001385382549090053,
      "loss": 2.3852,
      "step": 57290
    },
    {
      "epoch": 1.1787908321097307,
      "grad_norm": 0.46441003680229187,
      "learning_rate": 0.0001385177603297448,
      "loss": 2.3904,
      "step": 57300
    },
    {
      "epoch": 1.1789965516265986,
      "grad_norm": 0.39245492219924927,
      "learning_rate": 0.00013849726385061306,
      "loss": 2.3677,
      "step": 57310
    },
    {
      "epoch": 1.1792022711434662,
      "grad_norm": 0.44646283984184265,
      "learning_rate": 0.0001384767654726211,
      "loss": 2.4233,
      "step": 57320
    },
    {
      "epoch": 1.1794079906603339,
      "grad_norm": 0.4159093499183655,
      "learning_rate": 0.00013845626519677997,
      "loss": 2.3896,
      "step": 57330
    },
    {
      "epoch": 1.1796137101772017,
      "grad_norm": 0.38824620842933655,
      "learning_rate": 0.0001384357630241008,
      "loss": 2.4314,
      "step": 57340
    },
    {
      "epoch": 1.1798194296940694,
      "grad_norm": 0.404323935508728,
      "learning_rate": 0.00013841525895559495,
      "loss": 2.3819,
      "step": 57350
    },
    {
      "epoch": 1.180025149210937,
      "grad_norm": 0.40991026163101196,
      "learning_rate": 0.0001383947529922737,
      "loss": 2.3824,
      "step": 57360
    },
    {
      "epoch": 1.1802308687278047,
      "grad_norm": 0.40878528356552124,
      "learning_rate": 0.00013837424513514848,
      "loss": 2.3853,
      "step": 57370
    },
    {
      "epoch": 1.1804365882446726,
      "grad_norm": 0.5164943337440491,
      "learning_rate": 0.0001383537353852309,
      "loss": 2.4062,
      "step": 57380
    },
    {
      "epoch": 1.1806423077615402,
      "grad_norm": 0.43800321221351624,
      "learning_rate": 0.0001383332237435325,
      "loss": 2.3893,
      "step": 57390
    },
    {
      "epoch": 1.1808480272784079,
      "grad_norm": 0.4573386013507843,
      "learning_rate": 0.00013831271021106514,
      "loss": 2.4658,
      "step": 57400
    },
    {
      "epoch": 1.1810537467952757,
      "grad_norm": 0.431536465883255,
      "learning_rate": 0.0001382921947888405,
      "loss": 2.4643,
      "step": 57410
    },
    {
      "epoch": 1.1812594663121434,
      "grad_norm": 0.42992210388183594,
      "learning_rate": 0.00013827167747787057,
      "loss": 2.3782,
      "step": 57420
    },
    {
      "epoch": 1.181465185829011,
      "grad_norm": 0.42780283093452454,
      "learning_rate": 0.0001382511582791674,
      "loss": 2.4398,
      "step": 57430
    },
    {
      "epoch": 1.1816709053458787,
      "grad_norm": 0.42814314365386963,
      "learning_rate": 0.00013823063719374298,
      "loss": 2.3421,
      "step": 57440
    },
    {
      "epoch": 1.1818766248627466,
      "grad_norm": 0.41893646121025085,
      "learning_rate": 0.00013821011422260955,
      "loss": 2.4328,
      "step": 57450
    },
    {
      "epoch": 1.1820823443796142,
      "grad_norm": 0.6647921204566956,
      "learning_rate": 0.00013818958936677946,
      "loss": 2.3643,
      "step": 57460
    },
    {
      "epoch": 1.1822880638964819,
      "grad_norm": 0.48340514302253723,
      "learning_rate": 0.000138169062627265,
      "loss": 2.434,
      "step": 57470
    },
    {
      "epoch": 1.1824937834133498,
      "grad_norm": 0.4241376519203186,
      "learning_rate": 0.00013814853400507867,
      "loss": 2.4378,
      "step": 57480
    },
    {
      "epoch": 1.1826995029302174,
      "grad_norm": 0.4665413498878479,
      "learning_rate": 0.00013812800350123308,
      "loss": 2.4467,
      "step": 57490
    },
    {
      "epoch": 1.182905222447085,
      "grad_norm": 0.41864925622940063,
      "learning_rate": 0.0001381074711167408,
      "loss": 2.4168,
      "step": 57500
    },
    {
      "epoch": 1.1831109419639527,
      "grad_norm": 0.39738914370536804,
      "learning_rate": 0.00013808693685261466,
      "loss": 2.4177,
      "step": 57510
    },
    {
      "epoch": 1.1833166614808206,
      "grad_norm": 0.436943382024765,
      "learning_rate": 0.00013806640070986747,
      "loss": 2.4361,
      "step": 57520
    },
    {
      "epoch": 1.1835223809976883,
      "grad_norm": 0.4358673691749573,
      "learning_rate": 0.00013804586268951216,
      "loss": 2.4237,
      "step": 57530
    },
    {
      "epoch": 1.183728100514556,
      "grad_norm": 0.42154043912887573,
      "learning_rate": 0.00013802532279256178,
      "loss": 2.3677,
      "step": 57540
    },
    {
      "epoch": 1.1839338200314238,
      "grad_norm": 0.4417233169078827,
      "learning_rate": 0.00013800478102002944,
      "loss": 2.3748,
      "step": 57550
    },
    {
      "epoch": 1.1841395395482914,
      "grad_norm": 0.4336179196834564,
      "learning_rate": 0.00013798423737292837,
      "loss": 2.3747,
      "step": 57560
    },
    {
      "epoch": 1.184345259065159,
      "grad_norm": 0.4162953197956085,
      "learning_rate": 0.00013796369185227182,
      "loss": 2.4133,
      "step": 57570
    },
    {
      "epoch": 1.1845509785820267,
      "grad_norm": 0.4142835736274719,
      "learning_rate": 0.00013794314445907325,
      "loss": 2.4202,
      "step": 57580
    },
    {
      "epoch": 1.1847566980988946,
      "grad_norm": 0.4479345977306366,
      "learning_rate": 0.00013792259519434613,
      "loss": 2.3857,
      "step": 57590
    },
    {
      "epoch": 1.1849624176157623,
      "grad_norm": 0.4234392046928406,
      "learning_rate": 0.00013790204405910404,
      "loss": 2.356,
      "step": 57600
    },
    {
      "epoch": 1.18516813713263,
      "grad_norm": 0.4212665855884552,
      "learning_rate": 0.00013788149105436066,
      "loss": 2.4276,
      "step": 57610
    },
    {
      "epoch": 1.1853738566494976,
      "grad_norm": 0.5293823480606079,
      "learning_rate": 0.00013786093618112975,
      "loss": 2.3923,
      "step": 57620
    },
    {
      "epoch": 1.1855795761663654,
      "grad_norm": 0.4299435615539551,
      "learning_rate": 0.00013784037944042517,
      "loss": 2.4149,
      "step": 57630
    },
    {
      "epoch": 1.185785295683233,
      "grad_norm": 0.42215466499328613,
      "learning_rate": 0.00013781982083326092,
      "loss": 2.4867,
      "step": 57640
    },
    {
      "epoch": 1.1859910152001008,
      "grad_norm": 0.4226495325565338,
      "learning_rate": 0.00013779926036065095,
      "loss": 2.4645,
      "step": 57650
    },
    {
      "epoch": 1.1861967347169684,
      "grad_norm": 0.4154425263404846,
      "learning_rate": 0.00013777869802360948,
      "loss": 2.3917,
      "step": 57660
    },
    {
      "epoch": 1.1864024542338363,
      "grad_norm": 0.4358964264392853,
      "learning_rate": 0.00013775813382315067,
      "loss": 2.3845,
      "step": 57670
    },
    {
      "epoch": 1.186608173750704,
      "grad_norm": 0.4375600814819336,
      "learning_rate": 0.00013773756776028892,
      "loss": 2.458,
      "step": 57680
    },
    {
      "epoch": 1.1868138932675716,
      "grad_norm": 0.4508390426635742,
      "learning_rate": 0.00013771699983603855,
      "loss": 2.3735,
      "step": 57690
    },
    {
      "epoch": 1.1870196127844395,
      "grad_norm": 0.43149086833000183,
      "learning_rate": 0.00013769643005141416,
      "loss": 2.4158,
      "step": 57700
    },
    {
      "epoch": 1.1872253323013071,
      "grad_norm": 0.4121832251548767,
      "learning_rate": 0.00013767585840743026,
      "loss": 2.3465,
      "step": 57710
    },
    {
      "epoch": 1.1874310518181748,
      "grad_norm": 0.45349931716918945,
      "learning_rate": 0.0001376552849051016,
      "loss": 2.4107,
      "step": 57720
    },
    {
      "epoch": 1.1876367713350424,
      "grad_norm": 0.42796793580055237,
      "learning_rate": 0.0001376347095454429,
      "loss": 2.4061,
      "step": 57730
    },
    {
      "epoch": 1.1878424908519103,
      "grad_norm": 0.43086379766464233,
      "learning_rate": 0.00013761413232946907,
      "loss": 2.3652,
      "step": 57740
    },
    {
      "epoch": 1.188048210368778,
      "grad_norm": 0.4026664197444916,
      "learning_rate": 0.00013759355325819513,
      "loss": 2.3651,
      "step": 57750
    },
    {
      "epoch": 1.1882539298856456,
      "grad_norm": 0.4186220169067383,
      "learning_rate": 0.000137572972332636,
      "loss": 2.4241,
      "step": 57760
    },
    {
      "epoch": 1.1884596494025135,
      "grad_norm": 0.4222855567932129,
      "learning_rate": 0.00013755238955380687,
      "loss": 2.4035,
      "step": 57770
    },
    {
      "epoch": 1.1886653689193811,
      "grad_norm": 0.4136469066143036,
      "learning_rate": 0.00013753180492272307,
      "loss": 2.3912,
      "step": 57780
    },
    {
      "epoch": 1.1888710884362488,
      "grad_norm": 0.43505099415779114,
      "learning_rate": 0.0001375112184403998,
      "loss": 2.4217,
      "step": 57790
    },
    {
      "epoch": 1.1890768079531164,
      "grad_norm": 0.4259325861930847,
      "learning_rate": 0.00013749063010785257,
      "loss": 2.3748,
      "step": 57800
    },
    {
      "epoch": 1.1892825274699843,
      "grad_norm": 0.4001666009426117,
      "learning_rate": 0.00013747003992609685,
      "loss": 2.3454,
      "step": 57810
    },
    {
      "epoch": 1.189488246986852,
      "grad_norm": 0.4257529079914093,
      "learning_rate": 0.00013744944789614825,
      "loss": 2.3771,
      "step": 57820
    },
    {
      "epoch": 1.1896939665037196,
      "grad_norm": 0.4654272794723511,
      "learning_rate": 0.00013742885401902242,
      "loss": 2.3862,
      "step": 57830
    },
    {
      "epoch": 1.1898996860205875,
      "grad_norm": 0.4462968707084656,
      "learning_rate": 0.00013740825829573522,
      "loss": 2.3822,
      "step": 57840
    },
    {
      "epoch": 1.1901054055374551,
      "grad_norm": 0.44506698846817017,
      "learning_rate": 0.00013738766072730248,
      "loss": 2.3934,
      "step": 57850
    },
    {
      "epoch": 1.1903111250543228,
      "grad_norm": 0.4311242699623108,
      "learning_rate": 0.0001373670613147402,
      "loss": 2.3808,
      "step": 57860
    },
    {
      "epoch": 1.1905168445711904,
      "grad_norm": 0.4197620153427124,
      "learning_rate": 0.0001373464600590644,
      "loss": 2.3288,
      "step": 57870
    },
    {
      "epoch": 1.1907225640880583,
      "grad_norm": 0.43219977617263794,
      "learning_rate": 0.00013732585696129123,
      "loss": 2.415,
      "step": 57880
    },
    {
      "epoch": 1.190928283604926,
      "grad_norm": 0.4482002556324005,
      "learning_rate": 0.00013730525202243694,
      "loss": 2.4771,
      "step": 57890
    },
    {
      "epoch": 1.1911340031217936,
      "grad_norm": 0.5056447982788086,
      "learning_rate": 0.00013728464524351782,
      "loss": 2.4206,
      "step": 57900
    },
    {
      "epoch": 1.1913397226386615,
      "grad_norm": 0.4340673089027405,
      "learning_rate": 0.00013726403662555042,
      "loss": 2.3462,
      "step": 57910
    },
    {
      "epoch": 1.1915454421555292,
      "grad_norm": 0.423520565032959,
      "learning_rate": 0.0001372434261695511,
      "loss": 2.4074,
      "step": 57920
    },
    {
      "epoch": 1.1917511616723968,
      "grad_norm": 0.42436060309410095,
      "learning_rate": 0.00013722281387653647,
      "loss": 2.3851,
      "step": 57930
    },
    {
      "epoch": 1.1919568811892645,
      "grad_norm": 0.43216025829315186,
      "learning_rate": 0.00013720219974752335,
      "loss": 2.3968,
      "step": 57940
    },
    {
      "epoch": 1.1921626007061323,
      "grad_norm": 0.48080840706825256,
      "learning_rate": 0.0001371815837835284,
      "loss": 2.4016,
      "step": 57950
    },
    {
      "epoch": 1.192368320223,
      "grad_norm": 0.426355242729187,
      "learning_rate": 0.00013716096598556858,
      "loss": 2.4301,
      "step": 57960
    },
    {
      "epoch": 1.1925740397398676,
      "grad_norm": 0.4675924777984619,
      "learning_rate": 0.00013714034635466078,
      "loss": 2.4018,
      "step": 57970
    },
    {
      "epoch": 1.1927797592567353,
      "grad_norm": 0.4133855104446411,
      "learning_rate": 0.00013711972489182208,
      "loss": 2.4073,
      "step": 57980
    },
    {
      "epoch": 1.1929854787736032,
      "grad_norm": 0.40725967288017273,
      "learning_rate": 0.00013709910159806965,
      "loss": 2.4003,
      "step": 57990
    },
    {
      "epoch": 1.1931911982904708,
      "grad_norm": 0.4879593253135681,
      "learning_rate": 0.00013707847647442071,
      "loss": 2.4299,
      "step": 58000
    },
    {
      "epoch": 1.1933969178073385,
      "grad_norm": 0.4433601200580597,
      "learning_rate": 0.00013705784952189253,
      "loss": 2.3921,
      "step": 58010
    },
    {
      "epoch": 1.1936026373242061,
      "grad_norm": 0.4188608229160309,
      "learning_rate": 0.00013703722074150261,
      "loss": 2.3653,
      "step": 58020
    },
    {
      "epoch": 1.193808356841074,
      "grad_norm": 0.4238938093185425,
      "learning_rate": 0.00013701659013426841,
      "loss": 2.3569,
      "step": 58030
    },
    {
      "epoch": 1.1940140763579417,
      "grad_norm": 0.4396616220474243,
      "learning_rate": 0.00013699595770120756,
      "loss": 2.4199,
      "step": 58040
    },
    {
      "epoch": 1.1942197958748093,
      "grad_norm": 0.42911940813064575,
      "learning_rate": 0.0001369753234433377,
      "loss": 2.4248,
      "step": 58050
    },
    {
      "epoch": 1.1944255153916772,
      "grad_norm": 0.40196776390075684,
      "learning_rate": 0.00013695468736167661,
      "loss": 2.4181,
      "step": 58060
    },
    {
      "epoch": 1.1946312349085448,
      "grad_norm": 0.46418410539627075,
      "learning_rate": 0.0001369340494572422,
      "loss": 2.4681,
      "step": 58070
    },
    {
      "epoch": 1.1948369544254125,
      "grad_norm": 0.4274253249168396,
      "learning_rate": 0.00013691340973105237,
      "loss": 2.4084,
      "step": 58080
    },
    {
      "epoch": 1.1950426739422801,
      "grad_norm": 0.3962094783782959,
      "learning_rate": 0.0001368927681841252,
      "loss": 2.395,
      "step": 58090
    },
    {
      "epoch": 1.195248393459148,
      "grad_norm": 0.4453972280025482,
      "learning_rate": 0.00013687212481747885,
      "loss": 2.4149,
      "step": 58100
    },
    {
      "epoch": 1.1954541129760157,
      "grad_norm": 0.44077807664871216,
      "learning_rate": 0.00013685147963213146,
      "loss": 2.4446,
      "step": 58110
    },
    {
      "epoch": 1.1956598324928833,
      "grad_norm": 0.42336803674697876,
      "learning_rate": 0.00013683083262910144,
      "loss": 2.3999,
      "step": 58120
    },
    {
      "epoch": 1.1958655520097512,
      "grad_norm": 0.4170703589916229,
      "learning_rate": 0.00013681018380940713,
      "loss": 2.4327,
      "step": 58130
    },
    {
      "epoch": 1.1960712715266189,
      "grad_norm": 0.4389790892601013,
      "learning_rate": 0.00013678953317406705,
      "loss": 2.4231,
      "step": 58140
    },
    {
      "epoch": 1.1962769910434865,
      "grad_norm": 0.430120050907135,
      "learning_rate": 0.00013676888072409975,
      "loss": 2.4653,
      "step": 58150
    },
    {
      "epoch": 1.1964827105603542,
      "grad_norm": 0.41845759749412537,
      "learning_rate": 0.00013674822646052398,
      "loss": 2.3764,
      "step": 58160
    },
    {
      "epoch": 1.196688430077222,
      "grad_norm": 0.40075966715812683,
      "learning_rate": 0.0001367275703843584,
      "loss": 2.4622,
      "step": 58170
    },
    {
      "epoch": 1.1968941495940897,
      "grad_norm": 0.41035711765289307,
      "learning_rate": 0.00013670691249662197,
      "loss": 2.3937,
      "step": 58180
    },
    {
      "epoch": 1.1970998691109573,
      "grad_norm": 0.4185965657234192,
      "learning_rate": 0.00013668625279833355,
      "loss": 2.4206,
      "step": 58190
    },
    {
      "epoch": 1.1973055886278252,
      "grad_norm": 0.45965760946273804,
      "learning_rate": 0.00013666559129051217,
      "loss": 2.4303,
      "step": 58200
    },
    {
      "epoch": 1.1975113081446929,
      "grad_norm": 0.4797777235507965,
      "learning_rate": 0.000136644927974177,
      "loss": 2.3498,
      "step": 58210
    },
    {
      "epoch": 1.1977170276615605,
      "grad_norm": 0.4285399317741394,
      "learning_rate": 0.00013662426285034719,
      "loss": 2.4152,
      "step": 58220
    },
    {
      "epoch": 1.1979227471784282,
      "grad_norm": 0.39802002906799316,
      "learning_rate": 0.00013660359592004213,
      "loss": 2.3986,
      "step": 58230
    },
    {
      "epoch": 1.198128466695296,
      "grad_norm": 0.40470823645591736,
      "learning_rate": 0.00013658292718428112,
      "loss": 2.4281,
      "step": 58240
    },
    {
      "epoch": 1.1983341862121637,
      "grad_norm": 0.407489538192749,
      "learning_rate": 0.00013656225664408365,
      "loss": 2.3505,
      "step": 58250
    },
    {
      "epoch": 1.1985399057290314,
      "grad_norm": 0.4187672436237335,
      "learning_rate": 0.00013654158430046932,
      "loss": 2.4323,
      "step": 58260
    },
    {
      "epoch": 1.1987456252458992,
      "grad_norm": 0.42633572220802307,
      "learning_rate": 0.00013652091015445777,
      "loss": 2.3783,
      "step": 58270
    },
    {
      "epoch": 1.1989513447627669,
      "grad_norm": 0.43881091475486755,
      "learning_rate": 0.00013650023420706873,
      "loss": 2.4701,
      "step": 58280
    },
    {
      "epoch": 1.1991570642796345,
      "grad_norm": 0.40447676181793213,
      "learning_rate": 0.00013647955645932204,
      "loss": 2.4003,
      "step": 58290
    },
    {
      "epoch": 1.1993627837965022,
      "grad_norm": 0.4069564640522003,
      "learning_rate": 0.0001364588769122376,
      "loss": 2.3842,
      "step": 58300
    },
    {
      "epoch": 1.19956850331337,
      "grad_norm": 0.41948550939559937,
      "learning_rate": 0.00013643819556683547,
      "loss": 2.4188,
      "step": 58310
    },
    {
      "epoch": 1.1997742228302377,
      "grad_norm": 0.4130954444408417,
      "learning_rate": 0.0001364175124241357,
      "loss": 2.4002,
      "step": 58320
    },
    {
      "epoch": 1.1999799423471054,
      "grad_norm": 0.4364904463291168,
      "learning_rate": 0.0001363968274851585,
      "loss": 2.4202,
      "step": 58330
    },
    {
      "epoch": 1.200185661863973,
      "grad_norm": 0.42809322476387024,
      "learning_rate": 0.00013637614075092414,
      "loss": 2.3991,
      "step": 58340
    },
    {
      "epoch": 1.200391381380841,
      "grad_norm": 0.39170733094215393,
      "learning_rate": 0.00013635545222245296,
      "loss": 2.4119,
      "step": 58350
    },
    {
      "epoch": 1.2005971008977085,
      "grad_norm": 0.41381052136421204,
      "learning_rate": 0.00013633476190076546,
      "loss": 2.3699,
      "step": 58360
    },
    {
      "epoch": 1.2008028204145762,
      "grad_norm": 0.4600119888782501,
      "learning_rate": 0.00013631406978688215,
      "loss": 2.4033,
      "step": 58370
    },
    {
      "epoch": 1.2010085399314439,
      "grad_norm": 0.403782457113266,
      "learning_rate": 0.00013629337588182366,
      "loss": 2.452,
      "step": 58380
    },
    {
      "epoch": 1.2012142594483117,
      "grad_norm": 0.4163946509361267,
      "learning_rate": 0.00013627268018661073,
      "loss": 2.4358,
      "step": 58390
    },
    {
      "epoch": 1.2014199789651794,
      "grad_norm": 0.40910565853118896,
      "learning_rate": 0.00013625198270226415,
      "loss": 2.3679,
      "step": 58400
    },
    {
      "epoch": 1.201625698482047,
      "grad_norm": 0.44749778509140015,
      "learning_rate": 0.00013623128342980478,
      "loss": 2.3397,
      "step": 58410
    },
    {
      "epoch": 1.201831417998915,
      "grad_norm": 0.407245010137558,
      "learning_rate": 0.00013621058237025366,
      "loss": 2.4329,
      "step": 58420
    },
    {
      "epoch": 1.2020371375157826,
      "grad_norm": 0.38033801317214966,
      "learning_rate": 0.00013618987952463183,
      "loss": 2.4601,
      "step": 58430
    },
    {
      "epoch": 1.2022428570326502,
      "grad_norm": 0.4192621111869812,
      "learning_rate": 0.00013616917489396043,
      "loss": 2.4139,
      "step": 58440
    },
    {
      "epoch": 1.2024485765495179,
      "grad_norm": 0.516571044921875,
      "learning_rate": 0.00013614846847926078,
      "loss": 2.3493,
      "step": 58450
    },
    {
      "epoch": 1.2026542960663857,
      "grad_norm": 0.44646283984184265,
      "learning_rate": 0.00013612776028155415,
      "loss": 2.4185,
      "step": 58460
    },
    {
      "epoch": 1.2028600155832534,
      "grad_norm": 0.4359157085418701,
      "learning_rate": 0.00013610705030186194,
      "loss": 2.3398,
      "step": 58470
    },
    {
      "epoch": 1.203065735100121,
      "grad_norm": 0.42369699478149414,
      "learning_rate": 0.00013608633854120575,
      "loss": 2.4513,
      "step": 58480
    },
    {
      "epoch": 1.203271454616989,
      "grad_norm": 0.4519021213054657,
      "learning_rate": 0.00013606562500060711,
      "loss": 2.3455,
      "step": 58490
    },
    {
      "epoch": 1.2034771741338566,
      "grad_norm": 0.41817739605903625,
      "learning_rate": 0.0001360449096810877,
      "loss": 2.4142,
      "step": 58500
    },
    {
      "epoch": 1.2036828936507242,
      "grad_norm": 0.46095016598701477,
      "learning_rate": 0.00013602419258366933,
      "loss": 2.41,
      "step": 58510
    },
    {
      "epoch": 1.2038886131675919,
      "grad_norm": 0.402915358543396,
      "learning_rate": 0.00013600347370937388,
      "loss": 2.4512,
      "step": 58520
    },
    {
      "epoch": 1.2040943326844598,
      "grad_norm": 0.45273348689079285,
      "learning_rate": 0.0001359827530592232,
      "loss": 2.4178,
      "step": 58530
    },
    {
      "epoch": 1.2043000522013274,
      "grad_norm": 0.42845094203948975,
      "learning_rate": 0.00013596203063423945,
      "loss": 2.4345,
      "step": 58540
    },
    {
      "epoch": 1.204505771718195,
      "grad_norm": 0.4321528673171997,
      "learning_rate": 0.00013594130643544466,
      "loss": 2.3705,
      "step": 58550
    },
    {
      "epoch": 1.204711491235063,
      "grad_norm": 0.43036797642707825,
      "learning_rate": 0.00013592058046386113,
      "loss": 2.4209,
      "step": 58560
    },
    {
      "epoch": 1.2049172107519306,
      "grad_norm": 0.43057864904403687,
      "learning_rate": 0.00013589985272051107,
      "loss": 2.4727,
      "step": 58570
    },
    {
      "epoch": 1.2051229302687982,
      "grad_norm": 0.44844886660575867,
      "learning_rate": 0.0001358791232064169,
      "loss": 2.4227,
      "step": 58580
    },
    {
      "epoch": 1.205328649785666,
      "grad_norm": 0.4244158864021301,
      "learning_rate": 0.00013585839192260113,
      "loss": 2.3553,
      "step": 58590
    },
    {
      "epoch": 1.2055343693025338,
      "grad_norm": 0.4260387122631073,
      "learning_rate": 0.0001358376588700863,
      "loss": 2.4137,
      "step": 58600
    },
    {
      "epoch": 1.2057400888194014,
      "grad_norm": 0.4608263075351715,
      "learning_rate": 0.00013581692404989503,
      "loss": 2.3802,
      "step": 58610
    },
    {
      "epoch": 1.205945808336269,
      "grad_norm": 0.42730557918548584,
      "learning_rate": 0.00013579618746305006,
      "loss": 2.4157,
      "step": 58620
    },
    {
      "epoch": 1.206151527853137,
      "grad_norm": 0.4652026295661926,
      "learning_rate": 0.00013577544911057423,
      "loss": 2.4009,
      "step": 58630
    },
    {
      "epoch": 1.2063572473700046,
      "grad_norm": 0.39449620246887207,
      "learning_rate": 0.00013575470899349046,
      "loss": 2.4175,
      "step": 58640
    },
    {
      "epoch": 1.2065629668868723,
      "grad_norm": 0.48960089683532715,
      "learning_rate": 0.00013573396711282176,
      "loss": 2.4312,
      "step": 58650
    },
    {
      "epoch": 1.20676868640374,
      "grad_norm": 0.4196504056453705,
      "learning_rate": 0.00013571322346959112,
      "loss": 2.4198,
      "step": 58660
    },
    {
      "epoch": 1.2069744059206078,
      "grad_norm": 0.4682256579399109,
      "learning_rate": 0.00013569247806482186,
      "loss": 2.3929,
      "step": 58670
    },
    {
      "epoch": 1.2071801254374754,
      "grad_norm": 0.43150800466537476,
      "learning_rate": 0.0001356717308995371,
      "loss": 2.3861,
      "step": 58680
    },
    {
      "epoch": 1.207385844954343,
      "grad_norm": 0.45439013838768005,
      "learning_rate": 0.00013565098197476029,
      "loss": 2.4458,
      "step": 58690
    },
    {
      "epoch": 1.2075915644712107,
      "grad_norm": 0.46347615122795105,
      "learning_rate": 0.00013563023129151478,
      "loss": 2.4363,
      "step": 58700
    },
    {
      "epoch": 1.2077972839880786,
      "grad_norm": 0.44625183939933777,
      "learning_rate": 0.00013560947885082411,
      "loss": 2.3968,
      "step": 58710
    },
    {
      "epoch": 1.2080030035049463,
      "grad_norm": 0.3795473575592041,
      "learning_rate": 0.00013558872465371192,
      "loss": 2.3699,
      "step": 58720
    },
    {
      "epoch": 1.208208723021814,
      "grad_norm": 0.4209994077682495,
      "learning_rate": 0.00013556796870120187,
      "loss": 2.4194,
      "step": 58730
    },
    {
      "epoch": 1.2084144425386816,
      "grad_norm": 0.4382881820201874,
      "learning_rate": 0.00013554721099431774,
      "loss": 2.4202,
      "step": 58740
    },
    {
      "epoch": 1.2086201620555495,
      "grad_norm": 0.39817795157432556,
      "learning_rate": 0.00013552645153408344,
      "loss": 2.3758,
      "step": 58750
    },
    {
      "epoch": 1.208825881572417,
      "grad_norm": 0.4410206377506256,
      "learning_rate": 0.00013550569032152286,
      "loss": 2.3795,
      "step": 58760
    },
    {
      "epoch": 1.2090316010892848,
      "grad_norm": 0.4475173354148865,
      "learning_rate": 0.00013548492735766004,
      "loss": 2.4568,
      "step": 58770
    },
    {
      "epoch": 1.2092373206061526,
      "grad_norm": 0.4261646270751953,
      "learning_rate": 0.00013546416264351914,
      "loss": 2.3854,
      "step": 58780
    },
    {
      "epoch": 1.2094430401230203,
      "grad_norm": 0.41765332221984863,
      "learning_rate": 0.00013544339618012434,
      "loss": 2.3864,
      "step": 58790
    },
    {
      "epoch": 1.209648759639888,
      "grad_norm": 0.46553686261177063,
      "learning_rate": 0.00013542262796849999,
      "loss": 2.3763,
      "step": 58800
    },
    {
      "epoch": 1.2098544791567556,
      "grad_norm": 0.4301518201828003,
      "learning_rate": 0.0001354018580096704,
      "loss": 2.4042,
      "step": 58810
    },
    {
      "epoch": 1.2100601986736235,
      "grad_norm": 0.41139325499534607,
      "learning_rate": 0.00013538108630466002,
      "loss": 2.4302,
      "step": 58820
    },
    {
      "epoch": 1.2102659181904911,
      "grad_norm": 0.41508716344833374,
      "learning_rate": 0.00013536031285449354,
      "loss": 2.3943,
      "step": 58830
    },
    {
      "epoch": 1.2104716377073588,
      "grad_norm": 0.4573284983634949,
      "learning_rate": 0.0001353395376601955,
      "loss": 2.3733,
      "step": 58840
    },
    {
      "epoch": 1.2106773572242266,
      "grad_norm": 0.4229241609573364,
      "learning_rate": 0.0001353187607227906,
      "loss": 2.4138,
      "step": 58850
    },
    {
      "epoch": 1.2108830767410943,
      "grad_norm": 0.42492908239364624,
      "learning_rate": 0.00013529798204330376,
      "loss": 2.4103,
      "step": 58860
    },
    {
      "epoch": 1.211088796257962,
      "grad_norm": 0.40039074420928955,
      "learning_rate": 0.00013527720162275977,
      "loss": 2.4243,
      "step": 58870
    },
    {
      "epoch": 1.2112945157748296,
      "grad_norm": 0.447127103805542,
      "learning_rate": 0.00013525641946218365,
      "loss": 2.382,
      "step": 58880
    },
    {
      "epoch": 1.2115002352916975,
      "grad_norm": 0.4264889359474182,
      "learning_rate": 0.00013523563556260053,
      "loss": 2.4479,
      "step": 58890
    },
    {
      "epoch": 1.2117059548085651,
      "grad_norm": 0.4590247571468353,
      "learning_rate": 0.0001352148499250355,
      "loss": 2.4425,
      "step": 58900
    },
    {
      "epoch": 1.2119116743254328,
      "grad_norm": 0.5851163864135742,
      "learning_rate": 0.0001351940625505138,
      "loss": 2.4106,
      "step": 58910
    },
    {
      "epoch": 1.2121173938423007,
      "grad_norm": 0.46473485231399536,
      "learning_rate": 0.0001351732734400608,
      "loss": 2.3868,
      "step": 58920
    },
    {
      "epoch": 1.2123231133591683,
      "grad_norm": 0.3990190029144287,
      "learning_rate": 0.00013515248259470192,
      "loss": 2.4031,
      "step": 58930
    },
    {
      "epoch": 1.212528832876036,
      "grad_norm": 0.446025013923645,
      "learning_rate": 0.00013513169001546258,
      "loss": 2.3933,
      "step": 58940
    },
    {
      "epoch": 1.2127345523929036,
      "grad_norm": 0.42660030722618103,
      "learning_rate": 0.00013511089570336844,
      "loss": 2.3961,
      "step": 58950
    },
    {
      "epoch": 1.2129402719097715,
      "grad_norm": 0.433715283870697,
      "learning_rate": 0.00013509009965944518,
      "loss": 2.4223,
      "step": 58960
    },
    {
      "epoch": 1.2131459914266391,
      "grad_norm": 0.4186384379863739,
      "learning_rate": 0.00013506930188471848,
      "loss": 2.4703,
      "step": 58970
    },
    {
      "epoch": 1.2133517109435068,
      "grad_norm": 0.43276387453079224,
      "learning_rate": 0.0001350485023802142,
      "loss": 2.4118,
      "step": 58980
    },
    {
      "epoch": 1.2135574304603747,
      "grad_norm": 0.43387001752853394,
      "learning_rate": 0.00013502770114695833,
      "loss": 2.4252,
      "step": 58990
    },
    {
      "epoch": 1.2137631499772423,
      "grad_norm": 0.4730061888694763,
      "learning_rate": 0.00013500689818597686,
      "loss": 2.4502,
      "step": 59000
    },
    {
      "epoch": 1.21396886949411,
      "grad_norm": 0.41322773694992065,
      "learning_rate": 0.0001349860934982958,
      "loss": 2.358,
      "step": 59010
    },
    {
      "epoch": 1.2141745890109776,
      "grad_norm": 0.4377094507217407,
      "learning_rate": 0.00013496528708494145,
      "loss": 2.3644,
      "step": 59020
    },
    {
      "epoch": 1.2143803085278455,
      "grad_norm": 0.40760451555252075,
      "learning_rate": 0.00013494447894694,
      "loss": 2.4113,
      "step": 59030
    },
    {
      "epoch": 1.2145860280447132,
      "grad_norm": 0.5230692028999329,
      "learning_rate": 0.00013492366908531784,
      "loss": 2.4081,
      "step": 59040
    },
    {
      "epoch": 1.2147917475615808,
      "grad_norm": 0.39266470074653625,
      "learning_rate": 0.0001349028575011014,
      "loss": 2.4081,
      "step": 59050
    },
    {
      "epoch": 1.2149974670784487,
      "grad_norm": 0.40151259303092957,
      "learning_rate": 0.00013488204419531718,
      "loss": 2.4411,
      "step": 59060
    },
    {
      "epoch": 1.2152031865953163,
      "grad_norm": 0.4250291883945465,
      "learning_rate": 0.00013486122916899182,
      "loss": 2.3557,
      "step": 59070
    },
    {
      "epoch": 1.215408906112184,
      "grad_norm": 0.43603041768074036,
      "learning_rate": 0.000134840412423152,
      "loss": 2.4108,
      "step": 59080
    },
    {
      "epoch": 1.2156146256290516,
      "grad_norm": 0.463690847158432,
      "learning_rate": 0.0001348195939588245,
      "loss": 2.3987,
      "step": 59090
    },
    {
      "epoch": 1.2158203451459193,
      "grad_norm": 0.4972306787967682,
      "learning_rate": 0.00013479877377703614,
      "loss": 2.3515,
      "step": 59100
    },
    {
      "epoch": 1.2160260646627872,
      "grad_norm": 0.4210202097892761,
      "learning_rate": 0.00013477795187881393,
      "loss": 2.3882,
      "step": 59110
    },
    {
      "epoch": 1.2162317841796548,
      "grad_norm": 0.3753770589828491,
      "learning_rate": 0.00013475712826518487,
      "loss": 2.3842,
      "step": 59120
    },
    {
      "epoch": 1.2164375036965225,
      "grad_norm": 0.4380203187465668,
      "learning_rate": 0.00013473630293717607,
      "loss": 2.3744,
      "step": 59130
    },
    {
      "epoch": 1.2166432232133904,
      "grad_norm": 0.45444872975349426,
      "learning_rate": 0.0001347154758958147,
      "loss": 2.3871,
      "step": 59140
    },
    {
      "epoch": 1.216848942730258,
      "grad_norm": 0.3949085474014282,
      "learning_rate": 0.00013469464714212814,
      "loss": 2.3632,
      "step": 59150
    },
    {
      "epoch": 1.2170546622471257,
      "grad_norm": 0.4334692358970642,
      "learning_rate": 0.00013467381667714367,
      "loss": 2.4147,
      "step": 59160
    },
    {
      "epoch": 1.2172603817639933,
      "grad_norm": 0.41429057717323303,
      "learning_rate": 0.00013465298450188876,
      "loss": 2.4774,
      "step": 59170
    },
    {
      "epoch": 1.2174661012808612,
      "grad_norm": 0.4587591290473938,
      "learning_rate": 0.000134632150617391,
      "loss": 2.3904,
      "step": 59180
    },
    {
      "epoch": 1.2176718207977288,
      "grad_norm": 0.41085541248321533,
      "learning_rate": 0.00013461131502467795,
      "loss": 2.3895,
      "step": 59190
    },
    {
      "epoch": 1.2178775403145965,
      "grad_norm": 0.44564393162727356,
      "learning_rate": 0.00013459047772477732,
      "loss": 2.3605,
      "step": 59200
    },
    {
      "epoch": 1.2180832598314644,
      "grad_norm": 0.44384893774986267,
      "learning_rate": 0.00013456963871871693,
      "loss": 2.3829,
      "step": 59210
    },
    {
      "epoch": 1.218288979348332,
      "grad_norm": 0.379656583070755,
      "learning_rate": 0.00013454879800752466,
      "loss": 2.3903,
      "step": 59220
    },
    {
      "epoch": 1.2184946988651997,
      "grad_norm": 0.4454666078090668,
      "learning_rate": 0.00013452795559222843,
      "loss": 2.4236,
      "step": 59230
    },
    {
      "epoch": 1.2187004183820673,
      "grad_norm": 0.39713677763938904,
      "learning_rate": 0.00013450711147385634,
      "loss": 2.4387,
      "step": 59240
    },
    {
      "epoch": 1.2189061378989352,
      "grad_norm": 0.4456329345703125,
      "learning_rate": 0.00013448626565343646,
      "loss": 2.4362,
      "step": 59250
    },
    {
      "epoch": 1.2191118574158029,
      "grad_norm": 0.4071741998195648,
      "learning_rate": 0.00013446541813199702,
      "loss": 2.3476,
      "step": 59260
    },
    {
      "epoch": 1.2193175769326705,
      "grad_norm": 0.4094280004501343,
      "learning_rate": 0.0001344445689105663,
      "loss": 2.4035,
      "step": 59270
    },
    {
      "epoch": 1.2195232964495384,
      "grad_norm": 0.4283280074596405,
      "learning_rate": 0.00013442371799017274,
      "loss": 2.3754,
      "step": 59280
    },
    {
      "epoch": 1.219729015966406,
      "grad_norm": 0.409420907497406,
      "learning_rate": 0.00013440286537184473,
      "loss": 2.423,
      "step": 59290
    },
    {
      "epoch": 1.2199347354832737,
      "grad_norm": 0.4345073103904724,
      "learning_rate": 0.00013438201105661087,
      "loss": 2.4228,
      "step": 59300
    },
    {
      "epoch": 1.2201404550001413,
      "grad_norm": 0.42487645149230957,
      "learning_rate": 0.00013436115504549974,
      "loss": 2.3907,
      "step": 59310
    },
    {
      "epoch": 1.2203461745170092,
      "grad_norm": 0.43728217482566833,
      "learning_rate": 0.0001343402973395401,
      "loss": 2.4383,
      "step": 59320
    },
    {
      "epoch": 1.2205518940338769,
      "grad_norm": 0.46243950724601746,
      "learning_rate": 0.0001343194379397607,
      "loss": 2.4311,
      "step": 59330
    },
    {
      "epoch": 1.2207576135507445,
      "grad_norm": 0.4385877251625061,
      "learning_rate": 0.00013429857684719044,
      "loss": 2.426,
      "step": 59340
    },
    {
      "epoch": 1.2209633330676124,
      "grad_norm": 0.44391825795173645,
      "learning_rate": 0.0001342777140628583,
      "loss": 2.4453,
      "step": 59350
    },
    {
      "epoch": 1.22116905258448,
      "grad_norm": 0.4101431369781494,
      "learning_rate": 0.00013425684958779332,
      "loss": 2.3653,
      "step": 59360
    },
    {
      "epoch": 1.2213747721013477,
      "grad_norm": 0.45091596245765686,
      "learning_rate": 0.00013423598342302462,
      "loss": 2.4524,
      "step": 59370
    },
    {
      "epoch": 1.2215804916182154,
      "grad_norm": 0.4238888621330261,
      "learning_rate": 0.00013421511556958144,
      "loss": 2.4051,
      "step": 59380
    },
    {
      "epoch": 1.2217862111350832,
      "grad_norm": 0.4292200207710266,
      "learning_rate": 0.000134194246028493,
      "loss": 2.4302,
      "step": 59390
    },
    {
      "epoch": 1.2219919306519509,
      "grad_norm": 0.39461955428123474,
      "learning_rate": 0.0001341733748007888,
      "loss": 2.3551,
      "step": 59400
    },
    {
      "epoch": 1.2221976501688185,
      "grad_norm": 0.40186989307403564,
      "learning_rate": 0.00013415250188749826,
      "loss": 2.4464,
      "step": 59410
    },
    {
      "epoch": 1.2224033696856864,
      "grad_norm": 0.4090019464492798,
      "learning_rate": 0.00013413162728965087,
      "loss": 2.4084,
      "step": 59420
    },
    {
      "epoch": 1.222609089202554,
      "grad_norm": 0.39313286542892456,
      "learning_rate": 0.0001341107510082763,
      "loss": 2.4088,
      "step": 59430
    },
    {
      "epoch": 1.2228148087194217,
      "grad_norm": 0.45055344700813293,
      "learning_rate": 0.0001340898730444043,
      "loss": 2.3509,
      "step": 59440
    },
    {
      "epoch": 1.2230205282362894,
      "grad_norm": 0.43190664052963257,
      "learning_rate": 0.0001340689933990646,
      "loss": 2.423,
      "step": 59450
    },
    {
      "epoch": 1.2232262477531572,
      "grad_norm": 0.4087514877319336,
      "learning_rate": 0.00013404811207328712,
      "loss": 2.3516,
      "step": 59460
    },
    {
      "epoch": 1.223431967270025,
      "grad_norm": 0.4443471133708954,
      "learning_rate": 0.00013402722906810178,
      "loss": 2.4398,
      "step": 59470
    },
    {
      "epoch": 1.2236376867868926,
      "grad_norm": 0.41272038221359253,
      "learning_rate": 0.0001340063443845387,
      "loss": 2.389,
      "step": 59480
    },
    {
      "epoch": 1.2238434063037602,
      "grad_norm": 0.4518701136112213,
      "learning_rate": 0.00013398545802362793,
      "loss": 2.4075,
      "step": 59490
    },
    {
      "epoch": 1.224049125820628,
      "grad_norm": 0.443751722574234,
      "learning_rate": 0.00013396456998639975,
      "loss": 2.3991,
      "step": 59500
    },
    {
      "epoch": 1.2242548453374957,
      "grad_norm": 0.4322722852230072,
      "learning_rate": 0.00013394368027388437,
      "loss": 2.3701,
      "step": 59510
    },
    {
      "epoch": 1.2244605648543634,
      "grad_norm": 0.427644819021225,
      "learning_rate": 0.00013392278888711226,
      "loss": 2.4153,
      "step": 59520
    },
    {
      "epoch": 1.224666284371231,
      "grad_norm": 0.397978812456131,
      "learning_rate": 0.0001339018958271138,
      "loss": 2.3932,
      "step": 59530
    },
    {
      "epoch": 1.224872003888099,
      "grad_norm": 0.4448750913143158,
      "learning_rate": 0.0001338810010949196,
      "loss": 2.4089,
      "step": 59540
    },
    {
      "epoch": 1.2250777234049666,
      "grad_norm": 0.42386212944984436,
      "learning_rate": 0.00013386010469156019,
      "loss": 2.3793,
      "step": 59550
    },
    {
      "epoch": 1.2252834429218342,
      "grad_norm": 0.41582611203193665,
      "learning_rate": 0.00013383920661806634,
      "loss": 2.4028,
      "step": 59560
    },
    {
      "epoch": 1.225489162438702,
      "grad_norm": 0.4075925350189209,
      "learning_rate": 0.00013381830687546887,
      "loss": 2.4246,
      "step": 59570
    },
    {
      "epoch": 1.2256948819555697,
      "grad_norm": 0.42385363578796387,
      "learning_rate": 0.0001337974054647986,
      "loss": 2.3861,
      "step": 59580
    },
    {
      "epoch": 1.2259006014724374,
      "grad_norm": 0.4469834268093109,
      "learning_rate": 0.00013377650238708647,
      "loss": 2.3734,
      "step": 59590
    },
    {
      "epoch": 1.226106320989305,
      "grad_norm": 0.4225524961948395,
      "learning_rate": 0.00013375559764336353,
      "loss": 2.353,
      "step": 59600
    },
    {
      "epoch": 1.226312040506173,
      "grad_norm": 0.40611377358436584,
      "learning_rate": 0.00013373469123466094,
      "loss": 2.4156,
      "step": 59610
    },
    {
      "epoch": 1.2265177600230406,
      "grad_norm": 0.43019673228263855,
      "learning_rate": 0.00013371378316200984,
      "loss": 2.4303,
      "step": 59620
    },
    {
      "epoch": 1.2267234795399082,
      "grad_norm": 0.4657006859779358,
      "learning_rate": 0.00013369287342644156,
      "loss": 2.4028,
      "step": 59630
    },
    {
      "epoch": 1.226929199056776,
      "grad_norm": 0.5409207940101624,
      "learning_rate": 0.0001336719620289874,
      "loss": 2.4354,
      "step": 59640
    },
    {
      "epoch": 1.2271349185736438,
      "grad_norm": 0.4799281656742096,
      "learning_rate": 0.00013365104897067887,
      "loss": 2.4205,
      "step": 59650
    },
    {
      "epoch": 1.2273406380905114,
      "grad_norm": 0.43941834568977356,
      "learning_rate": 0.00013363013425254745,
      "loss": 2.3781,
      "step": 59660
    },
    {
      "epoch": 1.227546357607379,
      "grad_norm": 0.41170111298561096,
      "learning_rate": 0.00013360921787562477,
      "loss": 2.4037,
      "step": 59670
    },
    {
      "epoch": 1.227752077124247,
      "grad_norm": 0.4538206458091736,
      "learning_rate": 0.00013358829984094254,
      "loss": 2.3748,
      "step": 59680
    },
    {
      "epoch": 1.2279577966411146,
      "grad_norm": 0.494983971118927,
      "learning_rate": 0.0001335673801495325,
      "loss": 2.3554,
      "step": 59690
    },
    {
      "epoch": 1.2281635161579822,
      "grad_norm": 0.4932754635810852,
      "learning_rate": 0.00013354645880242654,
      "loss": 2.3786,
      "step": 59700
    },
    {
      "epoch": 1.2283692356748501,
      "grad_norm": 0.4313593804836273,
      "learning_rate": 0.00013352553580065653,
      "loss": 2.4284,
      "step": 59710
    },
    {
      "epoch": 1.2285749551917178,
      "grad_norm": 0.41684234142303467,
      "learning_rate": 0.00013350461114525458,
      "loss": 2.4154,
      "step": 59720
    },
    {
      "epoch": 1.2287806747085854,
      "grad_norm": 0.428242951631546,
      "learning_rate": 0.0001334836848372527,
      "loss": 2.4193,
      "step": 59730
    },
    {
      "epoch": 1.228986394225453,
      "grad_norm": 0.4406963884830475,
      "learning_rate": 0.00013346275687768316,
      "loss": 2.3686,
      "step": 59740
    },
    {
      "epoch": 1.229192113742321,
      "grad_norm": 0.4186779260635376,
      "learning_rate": 0.0001334418272675781,
      "loss": 2.4235,
      "step": 59750
    },
    {
      "epoch": 1.2293978332591886,
      "grad_norm": 0.4503598213195801,
      "learning_rate": 0.00013342089600797,
      "loss": 2.3827,
      "step": 59760
    },
    {
      "epoch": 1.2296035527760563,
      "grad_norm": 0.4401802122592926,
      "learning_rate": 0.0001333999630998912,
      "loss": 2.32,
      "step": 59770
    },
    {
      "epoch": 1.2298092722929241,
      "grad_norm": 0.39754942059516907,
      "learning_rate": 0.00013337902854437423,
      "loss": 2.3992,
      "step": 59780
    },
    {
      "epoch": 1.2300149918097918,
      "grad_norm": 0.4280611276626587,
      "learning_rate": 0.00013335809234245167,
      "loss": 2.4686,
      "step": 59790
    },
    {
      "epoch": 1.2302207113266594,
      "grad_norm": 0.44431018829345703,
      "learning_rate": 0.0001333371544951562,
      "loss": 2.3976,
      "step": 59800
    },
    {
      "epoch": 1.230426430843527,
      "grad_norm": 0.5206215977668762,
      "learning_rate": 0.00013331621500352057,
      "loss": 2.4035,
      "step": 59810
    },
    {
      "epoch": 1.230632150360395,
      "grad_norm": 0.4175627827644348,
      "learning_rate": 0.00013329527386857757,
      "loss": 2.4197,
      "step": 59820
    },
    {
      "epoch": 1.2308378698772626,
      "grad_norm": 0.4382178485393524,
      "learning_rate": 0.0001332743310913602,
      "loss": 2.382,
      "step": 59830
    },
    {
      "epoch": 1.2310435893941303,
      "grad_norm": 0.44736117124557495,
      "learning_rate": 0.00013325338667290136,
      "loss": 2.3711,
      "step": 59840
    },
    {
      "epoch": 1.231249308910998,
      "grad_norm": 0.37764304876327515,
      "learning_rate": 0.0001332324406142342,
      "loss": 2.3629,
      "step": 59850
    },
    {
      "epoch": 1.2314550284278658,
      "grad_norm": 0.40967604517936707,
      "learning_rate": 0.00013321149291639184,
      "loss": 2.4297,
      "step": 59860
    },
    {
      "epoch": 1.2316607479447335,
      "grad_norm": 0.4263017773628235,
      "learning_rate": 0.00013319054358040746,
      "loss": 2.3368,
      "step": 59870
    },
    {
      "epoch": 1.2318664674616011,
      "grad_norm": 0.4737505614757538,
      "learning_rate": 0.00013316959260731447,
      "loss": 2.461,
      "step": 59880
    },
    {
      "epoch": 1.2320721869784688,
      "grad_norm": 0.39269110560417175,
      "learning_rate": 0.00013314863999814626,
      "loss": 2.4051,
      "step": 59890
    },
    {
      "epoch": 1.2322779064953366,
      "grad_norm": 0.4021637439727783,
      "learning_rate": 0.00013312768575393625,
      "loss": 2.3804,
      "step": 59900
    },
    {
      "epoch": 1.2324836260122043,
      "grad_norm": 0.44541868567466736,
      "learning_rate": 0.000133106729875718,
      "loss": 2.4293,
      "step": 59910
    },
    {
      "epoch": 1.232689345529072,
      "grad_norm": 0.4007214903831482,
      "learning_rate": 0.00013308577236452522,
      "loss": 2.3939,
      "step": 59920
    },
    {
      "epoch": 1.2328950650459398,
      "grad_norm": 0.4171394407749176,
      "learning_rate": 0.00013306481322139155,
      "loss": 2.3711,
      "step": 59930
    },
    {
      "epoch": 1.2331007845628075,
      "grad_norm": 0.436899870634079,
      "learning_rate": 0.00013304385244735083,
      "loss": 2.4119,
      "step": 59940
    },
    {
      "epoch": 1.2333065040796751,
      "grad_norm": 0.42982855439186096,
      "learning_rate": 0.00013302289004343693,
      "loss": 2.4477,
      "step": 59950
    },
    {
      "epoch": 1.2335122235965428,
      "grad_norm": 0.40838271379470825,
      "learning_rate": 0.00013300192601068385,
      "loss": 2.413,
      "step": 59960
    },
    {
      "epoch": 1.2337179431134107,
      "grad_norm": 0.479189395904541,
      "learning_rate": 0.00013298096035012559,
      "loss": 2.4004,
      "step": 59970
    },
    {
      "epoch": 1.2339236626302783,
      "grad_norm": 0.43373236060142517,
      "learning_rate": 0.00013295999306279626,
      "loss": 2.4107,
      "step": 59980
    },
    {
      "epoch": 1.234129382147146,
      "grad_norm": 0.41976261138916016,
      "learning_rate": 0.0001329390241497301,
      "loss": 2.3407,
      "step": 59990
    },
    {
      "epoch": 1.2343351016640138,
      "grad_norm": 0.4296691417694092,
      "learning_rate": 0.00013291805361196135,
      "loss": 2.4044,
      "step": 60000
    },
    {
      "epoch": 1.2345408211808815,
      "grad_norm": 0.43625491857528687,
      "learning_rate": 0.00013289708145052442,
      "loss": 2.3895,
      "step": 60010
    },
    {
      "epoch": 1.2347465406977491,
      "grad_norm": 0.412814736366272,
      "learning_rate": 0.00013287610766645372,
      "loss": 2.3181,
      "step": 60020
    },
    {
      "epoch": 1.2349522602146168,
      "grad_norm": 0.4529026746749878,
      "learning_rate": 0.0001328551322607838,
      "loss": 2.3997,
      "step": 60030
    },
    {
      "epoch": 1.2351579797314847,
      "grad_norm": 0.4518376886844635,
      "learning_rate": 0.0001328341552345492,
      "loss": 2.3561,
      "step": 60040
    },
    {
      "epoch": 1.2353636992483523,
      "grad_norm": 0.39201298356056213,
      "learning_rate": 0.0001328131765887847,
      "loss": 2.4173,
      "step": 60050
    },
    {
      "epoch": 1.23556941876522,
      "grad_norm": 0.42335113883018494,
      "learning_rate": 0.000132792196324525,
      "loss": 2.4024,
      "step": 60060
    },
    {
      "epoch": 1.2357751382820878,
      "grad_norm": 0.4023076295852661,
      "learning_rate": 0.0001327712144428049,
      "loss": 2.3615,
      "step": 60070
    },
    {
      "epoch": 1.2359808577989555,
      "grad_norm": 0.41598159074783325,
      "learning_rate": 0.00013275023094465945,
      "loss": 2.3674,
      "step": 60080
    },
    {
      "epoch": 1.2361865773158232,
      "grad_norm": 0.4474698305130005,
      "learning_rate": 0.00013272924583112352,
      "loss": 2.4284,
      "step": 60090
    },
    {
      "epoch": 1.2363922968326908,
      "grad_norm": 0.5003789663314819,
      "learning_rate": 0.00013270825910323227,
      "loss": 2.4313,
      "step": 60100
    },
    {
      "epoch": 1.2365980163495587,
      "grad_norm": 0.44707050919532776,
      "learning_rate": 0.00013268727076202084,
      "loss": 2.4001,
      "step": 60110
    },
    {
      "epoch": 1.2368037358664263,
      "grad_norm": 0.4255330562591553,
      "learning_rate": 0.00013266628080852444,
      "loss": 2.4446,
      "step": 60120
    },
    {
      "epoch": 1.237009455383294,
      "grad_norm": 0.40682584047317505,
      "learning_rate": 0.00013264528924377843,
      "loss": 2.3962,
      "step": 60130
    },
    {
      "epoch": 1.2372151749001619,
      "grad_norm": 0.4352306127548218,
      "learning_rate": 0.00013262429606881822,
      "loss": 2.3336,
      "step": 60140
    },
    {
      "epoch": 1.2374208944170295,
      "grad_norm": 0.4311786890029907,
      "learning_rate": 0.00013260330128467924,
      "loss": 2.3486,
      "step": 60150
    },
    {
      "epoch": 1.2376266139338972,
      "grad_norm": 0.4221861660480499,
      "learning_rate": 0.0001325823048923971,
      "loss": 2.4312,
      "step": 60160
    },
    {
      "epoch": 1.2378323334507648,
      "grad_norm": 0.43898850679397583,
      "learning_rate": 0.00013256130689300735,
      "loss": 2.3769,
      "step": 60170
    },
    {
      "epoch": 1.2380380529676327,
      "grad_norm": 0.4454375207424164,
      "learning_rate": 0.00013254030728754586,
      "loss": 2.3825,
      "step": 60180
    },
    {
      "epoch": 1.2382437724845003,
      "grad_norm": 0.42550164461135864,
      "learning_rate": 0.00013251930607704825,
      "loss": 2.3929,
      "step": 60190
    },
    {
      "epoch": 1.238449492001368,
      "grad_norm": 0.4343186616897583,
      "learning_rate": 0.00013249830326255053,
      "loss": 2.4073,
      "step": 60200
    },
    {
      "epoch": 1.2386552115182357,
      "grad_norm": 0.39960578083992004,
      "learning_rate": 0.0001324772988450886,
      "loss": 2.3814,
      "step": 60210
    },
    {
      "epoch": 1.2388609310351035,
      "grad_norm": 0.47240129113197327,
      "learning_rate": 0.00013245629282569848,
      "loss": 2.3535,
      "step": 60220
    },
    {
      "epoch": 1.2390666505519712,
      "grad_norm": 0.43950116634368896,
      "learning_rate": 0.0001324352852054163,
      "loss": 2.4202,
      "step": 60230
    },
    {
      "epoch": 1.2392723700688388,
      "grad_norm": 0.39623504877090454,
      "learning_rate": 0.0001324142759852783,
      "loss": 2.3653,
      "step": 60240
    },
    {
      "epoch": 1.2394780895857065,
      "grad_norm": 0.3979087471961975,
      "learning_rate": 0.00013239326516632067,
      "loss": 2.3454,
      "step": 60250
    },
    {
      "epoch": 1.2396838091025744,
      "grad_norm": 0.43511998653411865,
      "learning_rate": 0.0001323722527495798,
      "loss": 2.4149,
      "step": 60260
    },
    {
      "epoch": 1.239889528619442,
      "grad_norm": 0.4420076012611389,
      "learning_rate": 0.00013235123873609207,
      "loss": 2.3971,
      "step": 60270
    },
    {
      "epoch": 1.2400952481363097,
      "grad_norm": 0.4476384222507477,
      "learning_rate": 0.00013233022312689405,
      "loss": 2.3808,
      "step": 60280
    },
    {
      "epoch": 1.2403009676531775,
      "grad_norm": 0.43287137150764465,
      "learning_rate": 0.00013230920592302228,
      "loss": 2.3622,
      "step": 60290
    },
    {
      "epoch": 1.2405066871700452,
      "grad_norm": 0.44155067205429077,
      "learning_rate": 0.0001322881871255135,
      "loss": 2.4036,
      "step": 60300
    },
    {
      "epoch": 1.2407124066869128,
      "grad_norm": 0.43461525440216064,
      "learning_rate": 0.00013226716673540436,
      "loss": 2.3927,
      "step": 60310
    },
    {
      "epoch": 1.2409181262037805,
      "grad_norm": 0.44219499826431274,
      "learning_rate": 0.00013224614475373174,
      "loss": 2.4142,
      "step": 60320
    },
    {
      "epoch": 1.2411238457206484,
      "grad_norm": 0.4295717179775238,
      "learning_rate": 0.0001322251211815325,
      "loss": 2.4238,
      "step": 60330
    },
    {
      "epoch": 1.241329565237516,
      "grad_norm": 0.46387726068496704,
      "learning_rate": 0.00013220409601984368,
      "loss": 2.387,
      "step": 60340
    },
    {
      "epoch": 1.2415352847543837,
      "grad_norm": 0.40952467918395996,
      "learning_rate": 0.00013218306926970224,
      "loss": 2.3868,
      "step": 60350
    },
    {
      "epoch": 1.2417410042712516,
      "grad_norm": 0.45894691348075867,
      "learning_rate": 0.00013216204093214538,
      "loss": 2.4158,
      "step": 60360
    },
    {
      "epoch": 1.2419467237881192,
      "grad_norm": 0.4533841013908386,
      "learning_rate": 0.00013214101100821033,
      "loss": 2.3843,
      "step": 60370
    },
    {
      "epoch": 1.2421524433049869,
      "grad_norm": 0.41660022735595703,
      "learning_rate": 0.00013211997949893434,
      "loss": 2.368,
      "step": 60380
    },
    {
      "epoch": 1.2423581628218545,
      "grad_norm": 0.45127829909324646,
      "learning_rate": 0.00013209894640535475,
      "loss": 2.3864,
      "step": 60390
    },
    {
      "epoch": 1.2425638823387224,
      "grad_norm": 0.4463704824447632,
      "learning_rate": 0.00013207791172850915,
      "loss": 2.4231,
      "step": 60400
    },
    {
      "epoch": 1.24276960185559,
      "grad_norm": 0.4320020079612732,
      "learning_rate": 0.0001320568754694349,
      "loss": 2.4939,
      "step": 60410
    },
    {
      "epoch": 1.2429753213724577,
      "grad_norm": 0.4969484806060791,
      "learning_rate": 0.00013203583762916966,
      "loss": 2.3563,
      "step": 60420
    },
    {
      "epoch": 1.2431810408893256,
      "grad_norm": 0.41009944677352905,
      "learning_rate": 0.00013201479820875114,
      "loss": 2.3843,
      "step": 60430
    },
    {
      "epoch": 1.2433867604061932,
      "grad_norm": 0.4364258050918579,
      "learning_rate": 0.00013199375720921713,
      "loss": 2.3635,
      "step": 60440
    },
    {
      "epoch": 1.2435924799230609,
      "grad_norm": 0.4036719501018524,
      "learning_rate": 0.00013197271463160536,
      "loss": 2.4138,
      "step": 60450
    },
    {
      "epoch": 1.2437981994399285,
      "grad_norm": 0.42498981952667236,
      "learning_rate": 0.00013195167047695385,
      "loss": 2.353,
      "step": 60460
    },
    {
      "epoch": 1.2440039189567964,
      "grad_norm": 0.4378860592842102,
      "learning_rate": 0.00013193062474630053,
      "loss": 2.3852,
      "step": 60470
    },
    {
      "epoch": 1.244209638473664,
      "grad_norm": 0.3972528874874115,
      "learning_rate": 0.00013190957744068347,
      "loss": 2.4098,
      "step": 60480
    },
    {
      "epoch": 1.2444153579905317,
      "grad_norm": 0.4641270339488983,
      "learning_rate": 0.00013188852856114088,
      "loss": 2.4598,
      "step": 60490
    },
    {
      "epoch": 1.2446210775073996,
      "grad_norm": 0.4322975277900696,
      "learning_rate": 0.00013186747810871096,
      "loss": 2.4288,
      "step": 60500
    },
    {
      "epoch": 1.2448267970242672,
      "grad_norm": 0.42032134532928467,
      "learning_rate": 0.00013184642608443195,
      "loss": 2.4462,
      "step": 60510
    },
    {
      "epoch": 1.245032516541135,
      "grad_norm": 0.441719114780426,
      "learning_rate": 0.00013182537248934231,
      "loss": 2.4362,
      "step": 60520
    },
    {
      "epoch": 1.2452382360580025,
      "grad_norm": 0.44727981090545654,
      "learning_rate": 0.0001318043173244805,
      "loss": 2.3962,
      "step": 60530
    },
    {
      "epoch": 1.2454439555748704,
      "grad_norm": 0.420921266078949,
      "learning_rate": 0.000131783260590885,
      "loss": 2.4253,
      "step": 60540
    },
    {
      "epoch": 1.245649675091738,
      "grad_norm": 0.4295295178890228,
      "learning_rate": 0.00013176220228959445,
      "loss": 2.3821,
      "step": 60550
    },
    {
      "epoch": 1.2458553946086057,
      "grad_norm": 0.41576215624809265,
      "learning_rate": 0.00013174114242164755,
      "loss": 2.4024,
      "step": 60560
    },
    {
      "epoch": 1.2460611141254736,
      "grad_norm": 0.4339081346988678,
      "learning_rate": 0.00013172008098808307,
      "loss": 2.4128,
      "step": 60570
    },
    {
      "epoch": 1.2462668336423413,
      "grad_norm": 0.48634129762649536,
      "learning_rate": 0.00013169901798993982,
      "loss": 2.4096,
      "step": 60580
    },
    {
      "epoch": 1.246472553159209,
      "grad_norm": 0.43196773529052734,
      "learning_rate": 0.00013167795342825678,
      "loss": 2.4075,
      "step": 60590
    },
    {
      "epoch": 1.2466782726760766,
      "grad_norm": 0.46708256006240845,
      "learning_rate": 0.0001316568873040729,
      "loss": 2.4491,
      "step": 60600
    },
    {
      "epoch": 1.2468839921929442,
      "grad_norm": 0.4087798297405243,
      "learning_rate": 0.0001316358196184273,
      "loss": 2.4154,
      "step": 60610
    },
    {
      "epoch": 1.247089711709812,
      "grad_norm": 0.4077291488647461,
      "learning_rate": 0.00013161475037235911,
      "loss": 2.4192,
      "step": 60620
    },
    {
      "epoch": 1.2472954312266797,
      "grad_norm": 0.41619035601615906,
      "learning_rate": 0.00013159367956690757,
      "loss": 2.3912,
      "step": 60630
    },
    {
      "epoch": 1.2475011507435474,
      "grad_norm": 0.4130935072898865,
      "learning_rate": 0.00013157260720311195,
      "loss": 2.4385,
      "step": 60640
    },
    {
      "epoch": 1.2477068702604153,
      "grad_norm": 0.4142833650112152,
      "learning_rate": 0.0001315515332820117,
      "loss": 2.4011,
      "step": 60650
    },
    {
      "epoch": 1.247912589777283,
      "grad_norm": 0.4212685525417328,
      "learning_rate": 0.00013153045780464625,
      "loss": 2.4445,
      "step": 60660
    },
    {
      "epoch": 1.2481183092941506,
      "grad_norm": 0.46141213178634644,
      "learning_rate": 0.0001315093807720551,
      "loss": 2.3558,
      "step": 60670
    },
    {
      "epoch": 1.2483240288110182,
      "grad_norm": 0.4146651029586792,
      "learning_rate": 0.0001314883021852779,
      "loss": 2.4364,
      "step": 60680
    },
    {
      "epoch": 1.248529748327886,
      "grad_norm": 0.45039260387420654,
      "learning_rate": 0.00013146722204535443,
      "loss": 2.3926,
      "step": 60690
    },
    {
      "epoch": 1.2487354678447538,
      "grad_norm": 0.4431957006454468,
      "learning_rate": 0.0001314461403533243,
      "loss": 2.392,
      "step": 60700
    },
    {
      "epoch": 1.2489411873616214,
      "grad_norm": 0.3777964413166046,
      "learning_rate": 0.00013142505711022745,
      "loss": 2.3356,
      "step": 60710
    },
    {
      "epoch": 1.2491469068784893,
      "grad_norm": 0.46701204776763916,
      "learning_rate": 0.00013140397231710375,
      "loss": 2.4012,
      "step": 60720
    },
    {
      "epoch": 1.249352626395357,
      "grad_norm": 0.4748583137989044,
      "learning_rate": 0.00013138288597499325,
      "loss": 2.3968,
      "step": 60730
    },
    {
      "epoch": 1.2495583459122246,
      "grad_norm": 0.4103579819202423,
      "learning_rate": 0.00013136179808493598,
      "loss": 2.3613,
      "step": 60740
    },
    {
      "epoch": 1.2497640654290922,
      "grad_norm": 0.42892134189605713,
      "learning_rate": 0.00013134070864797212,
      "loss": 2.437,
      "step": 60750
    },
    {
      "epoch": 1.2499697849459601,
      "grad_norm": 0.4393906593322754,
      "learning_rate": 0.0001313196176651419,
      "loss": 2.3745,
      "step": 60760
    },
    {
      "epoch": 1.2501755044628278,
      "grad_norm": 0.44075143337249756,
      "learning_rate": 0.0001312985251374856,
      "loss": 2.3456,
      "step": 60770
    },
    {
      "epoch": 1.2503812239796954,
      "grad_norm": 0.40245917439460754,
      "learning_rate": 0.00013127743106604363,
      "loss": 2.3877,
      "step": 60780
    },
    {
      "epoch": 1.2505869434965633,
      "grad_norm": 0.43450310826301575,
      "learning_rate": 0.00013125633545185643,
      "loss": 2.3881,
      "step": 60790
    },
    {
      "epoch": 1.250792663013431,
      "grad_norm": 0.45944511890411377,
      "learning_rate": 0.0001312352382959645,
      "loss": 2.4261,
      "step": 60800
    },
    {
      "epoch": 1.2509983825302986,
      "grad_norm": 0.42588531970977783,
      "learning_rate": 0.00013121413959940847,
      "loss": 2.3768,
      "step": 60810
    },
    {
      "epoch": 1.2512041020471663,
      "grad_norm": 0.4356590509414673,
      "learning_rate": 0.00013119303936322908,
      "loss": 2.4265,
      "step": 60820
    },
    {
      "epoch": 1.2514098215640341,
      "grad_norm": 0.5182738900184631,
      "learning_rate": 0.000131171937588467,
      "loss": 2.4743,
      "step": 60830
    },
    {
      "epoch": 1.2516155410809018,
      "grad_norm": 0.4432328939437866,
      "learning_rate": 0.00013115083427616312,
      "loss": 2.4193,
      "step": 60840
    },
    {
      "epoch": 1.2518212605977694,
      "grad_norm": 0.44779977202415466,
      "learning_rate": 0.00013112972942735834,
      "loss": 2.3919,
      "step": 60850
    },
    {
      "epoch": 1.2520269801146373,
      "grad_norm": 0.3960517942905426,
      "learning_rate": 0.00013110862304309363,
      "loss": 2.3563,
      "step": 60860
    },
    {
      "epoch": 1.252232699631505,
      "grad_norm": 0.4167066216468811,
      "learning_rate": 0.0001310875151244101,
      "loss": 2.4335,
      "step": 60870
    },
    {
      "epoch": 1.2524384191483726,
      "grad_norm": 0.4126606583595276,
      "learning_rate": 0.00013106640567234882,
      "loss": 2.4108,
      "step": 60880
    },
    {
      "epoch": 1.2526441386652403,
      "grad_norm": 0.42140164971351624,
      "learning_rate": 0.00013104529468795103,
      "loss": 2.4278,
      "step": 60890
    },
    {
      "epoch": 1.252849858182108,
      "grad_norm": 0.3908074200153351,
      "learning_rate": 0.00013102418217225806,
      "loss": 2.3878,
      "step": 60900
    },
    {
      "epoch": 1.2530555776989758,
      "grad_norm": 0.43006113171577454,
      "learning_rate": 0.00013100306812631124,
      "loss": 2.4245,
      "step": 60910
    },
    {
      "epoch": 1.2532612972158435,
      "grad_norm": 0.4184061884880066,
      "learning_rate": 0.000130981952551152,
      "loss": 2.4371,
      "step": 60920
    },
    {
      "epoch": 1.2534670167327113,
      "grad_norm": 0.4254903197288513,
      "learning_rate": 0.00013096083544782188,
      "loss": 2.4006,
      "step": 60930
    },
    {
      "epoch": 1.253672736249579,
      "grad_norm": 0.4368396997451782,
      "learning_rate": 0.00013093971681736247,
      "loss": 2.3502,
      "step": 60940
    },
    {
      "epoch": 1.2538784557664466,
      "grad_norm": 0.43188732862472534,
      "learning_rate": 0.00013091859666081545,
      "loss": 2.4047,
      "step": 60950
    },
    {
      "epoch": 1.2540841752833143,
      "grad_norm": 0.38253629207611084,
      "learning_rate": 0.0001308974749792225,
      "loss": 2.3966,
      "step": 60960
    },
    {
      "epoch": 1.254289894800182,
      "grad_norm": 0.4228619337081909,
      "learning_rate": 0.00013087635177362549,
      "loss": 2.3875,
      "step": 60970
    },
    {
      "epoch": 1.2544956143170498,
      "grad_norm": 0.4071093499660492,
      "learning_rate": 0.00013085522704506635,
      "loss": 2.3801,
      "step": 60980
    },
    {
      "epoch": 1.2547013338339175,
      "grad_norm": 0.4456779360771179,
      "learning_rate": 0.00013083410079458695,
      "loss": 2.416,
      "step": 60990
    },
    {
      "epoch": 1.2549070533507853,
      "grad_norm": 0.46356290578842163,
      "learning_rate": 0.00013081297302322938,
      "loss": 2.3973,
      "step": 61000
    },
    {
      "epoch": 1.255112772867653,
      "grad_norm": 0.4675435423851013,
      "learning_rate": 0.0001307918437320358,
      "loss": 2.4302,
      "step": 61010
    },
    {
      "epoch": 1.2553184923845206,
      "grad_norm": 0.42590656876564026,
      "learning_rate": 0.00013077071292204835,
      "loss": 2.4177,
      "step": 61020
    },
    {
      "epoch": 1.2555242119013883,
      "grad_norm": 0.41944101452827454,
      "learning_rate": 0.00013074958059430927,
      "loss": 2.4234,
      "step": 61030
    },
    {
      "epoch": 1.255729931418256,
      "grad_norm": 0.4509047269821167,
      "learning_rate": 0.000130728446749861,
      "loss": 2.3806,
      "step": 61040
    },
    {
      "epoch": 1.2559356509351238,
      "grad_norm": 0.39255818724632263,
      "learning_rate": 0.0001307073113897459,
      "loss": 2.4033,
      "step": 61050
    },
    {
      "epoch": 1.2561413704519915,
      "grad_norm": 0.4662669599056244,
      "learning_rate": 0.00013068617451500642,
      "loss": 2.4315,
      "step": 61060
    },
    {
      "epoch": 1.2563470899688591,
      "grad_norm": 0.42317330837249756,
      "learning_rate": 0.0001306650361266852,
      "loss": 2.3892,
      "step": 61070
    },
    {
      "epoch": 1.256552809485727,
      "grad_norm": 0.40330198407173157,
      "learning_rate": 0.0001306438962258248,
      "loss": 2.4143,
      "step": 61080
    },
    {
      "epoch": 1.2567585290025947,
      "grad_norm": 0.43369996547698975,
      "learning_rate": 0.00013062275481346807,
      "loss": 2.4821,
      "step": 61090
    },
    {
      "epoch": 1.2569642485194623,
      "grad_norm": 0.3962782025337219,
      "learning_rate": 0.00013060161189065768,
      "loss": 2.37,
      "step": 61100
    },
    {
      "epoch": 1.25716996803633,
      "grad_norm": 0.44947290420532227,
      "learning_rate": 0.00013058046745843658,
      "loss": 2.3631,
      "step": 61110
    },
    {
      "epoch": 1.2573756875531978,
      "grad_norm": 0.4639286696910858,
      "learning_rate": 0.0001305593215178476,
      "loss": 2.3899,
      "step": 61120
    },
    {
      "epoch": 1.2575814070700655,
      "grad_norm": 0.4318888485431671,
      "learning_rate": 0.00013053817406993382,
      "loss": 2.3923,
      "step": 61130
    },
    {
      "epoch": 1.2577871265869331,
      "grad_norm": 0.43137797713279724,
      "learning_rate": 0.00013051702511573837,
      "loss": 2.3611,
      "step": 61140
    },
    {
      "epoch": 1.257992846103801,
      "grad_norm": 0.4454030394554138,
      "learning_rate": 0.00013049587465630436,
      "loss": 2.361,
      "step": 61150
    },
    {
      "epoch": 1.2581985656206687,
      "grad_norm": 0.5262303352355957,
      "learning_rate": 0.000130474722692675,
      "loss": 2.3685,
      "step": 61160
    },
    {
      "epoch": 1.2584042851375363,
      "grad_norm": 0.4932654798030853,
      "learning_rate": 0.00013045356922589368,
      "loss": 2.3111,
      "step": 61170
    },
    {
      "epoch": 1.258610004654404,
      "grad_norm": 0.40897125005722046,
      "learning_rate": 0.0001304324142570037,
      "loss": 2.4442,
      "step": 61180
    },
    {
      "epoch": 1.2588157241712719,
      "grad_norm": 0.42979714274406433,
      "learning_rate": 0.00013041125778704858,
      "loss": 2.375,
      "step": 61190
    },
    {
      "epoch": 1.2590214436881395,
      "grad_norm": 0.45809033513069153,
      "learning_rate": 0.00013039009981707185,
      "loss": 2.4413,
      "step": 61200
    },
    {
      "epoch": 1.2592271632050072,
      "grad_norm": 0.40939584374427795,
      "learning_rate": 0.0001303689403481171,
      "loss": 2.4264,
      "step": 61210
    },
    {
      "epoch": 1.259432882721875,
      "grad_norm": 0.44843193888664246,
      "learning_rate": 0.000130347779381228,
      "loss": 2.3599,
      "step": 61220
    },
    {
      "epoch": 1.2596386022387427,
      "grad_norm": 0.6597434878349304,
      "learning_rate": 0.0001303266169174483,
      "loss": 2.4558,
      "step": 61230
    },
    {
      "epoch": 1.2598443217556103,
      "grad_norm": 0.41558218002319336,
      "learning_rate": 0.00013030545295782185,
      "loss": 2.3719,
      "step": 61240
    },
    {
      "epoch": 1.260050041272478,
      "grad_norm": 0.4142625033855438,
      "learning_rate": 0.00013028428750339258,
      "loss": 2.4626,
      "step": 61250
    },
    {
      "epoch": 1.2602557607893459,
      "grad_norm": 0.44476017355918884,
      "learning_rate": 0.00013026312055520445,
      "loss": 2.342,
      "step": 61260
    },
    {
      "epoch": 1.2604614803062135,
      "grad_norm": 0.4193267226219177,
      "learning_rate": 0.00013024195211430148,
      "loss": 2.4002,
      "step": 61270
    },
    {
      "epoch": 1.2606671998230812,
      "grad_norm": 0.43045616149902344,
      "learning_rate": 0.0001302207821817278,
      "loss": 2.4165,
      "step": 61280
    },
    {
      "epoch": 1.260872919339949,
      "grad_norm": 0.47885239124298096,
      "learning_rate": 0.0001301996107585276,
      "loss": 2.4072,
      "step": 61290
    },
    {
      "epoch": 1.2610786388568167,
      "grad_norm": 0.42659658193588257,
      "learning_rate": 0.00013017843784574523,
      "loss": 2.3876,
      "step": 61300
    },
    {
      "epoch": 1.2612843583736844,
      "grad_norm": 0.44602176547050476,
      "learning_rate": 0.00013015726344442493,
      "loss": 2.3954,
      "step": 61310
    },
    {
      "epoch": 1.261490077890552,
      "grad_norm": 0.40859535336494446,
      "learning_rate": 0.00013013608755561116,
      "loss": 2.4141,
      "step": 61320
    },
    {
      "epoch": 1.2616957974074197,
      "grad_norm": 0.4425780475139618,
      "learning_rate": 0.00013011491018034843,
      "loss": 2.4548,
      "step": 61330
    },
    {
      "epoch": 1.2619015169242875,
      "grad_norm": 0.4795572757720947,
      "learning_rate": 0.0001300937313196813,
      "loss": 2.4328,
      "step": 61340
    },
    {
      "epoch": 1.2621072364411552,
      "grad_norm": 0.48715952038764954,
      "learning_rate": 0.00013007255097465434,
      "loss": 2.3409,
      "step": 61350
    },
    {
      "epoch": 1.262312955958023,
      "grad_norm": 0.44453486800193787,
      "learning_rate": 0.00013005136914631235,
      "loss": 2.3581,
      "step": 61360
    },
    {
      "epoch": 1.2625186754748907,
      "grad_norm": 0.42535069584846497,
      "learning_rate": 0.00013003018583570007,
      "loss": 2.368,
      "step": 61370
    },
    {
      "epoch": 1.2627243949917584,
      "grad_norm": 0.448678195476532,
      "learning_rate": 0.00013000900104386237,
      "loss": 2.427,
      "step": 61380
    },
    {
      "epoch": 1.262930114508626,
      "grad_norm": 0.45180636644363403,
      "learning_rate": 0.0001299878147718442,
      "loss": 2.4076,
      "step": 61390
    },
    {
      "epoch": 1.2631358340254937,
      "grad_norm": 0.41511860489845276,
      "learning_rate": 0.00012996662702069055,
      "loss": 2.3676,
      "step": 61400
    },
    {
      "epoch": 1.2633415535423616,
      "grad_norm": 0.40307971835136414,
      "learning_rate": 0.00012994543779144645,
      "loss": 2.3802,
      "step": 61410
    },
    {
      "epoch": 1.2635472730592292,
      "grad_norm": 0.5214289426803589,
      "learning_rate": 0.0001299242470851571,
      "loss": 2.3949,
      "step": 61420
    },
    {
      "epoch": 1.2637529925760969,
      "grad_norm": 0.4035434424877167,
      "learning_rate": 0.0001299030549028677,
      "loss": 2.4304,
      "step": 61430
    },
    {
      "epoch": 1.2639587120929647,
      "grad_norm": 0.48260265588760376,
      "learning_rate": 0.00012988186124562355,
      "loss": 2.4326,
      "step": 61440
    },
    {
      "epoch": 1.2641644316098324,
      "grad_norm": 0.4301333427429199,
      "learning_rate": 0.00012986066611447004,
      "loss": 2.4255,
      "step": 61450
    },
    {
      "epoch": 1.2643701511267,
      "grad_norm": 0.4615151584148407,
      "learning_rate": 0.0001298394695104526,
      "loss": 2.3329,
      "step": 61460
    },
    {
      "epoch": 1.2645758706435677,
      "grad_norm": 0.4188295304775238,
      "learning_rate": 0.00012981827143461672,
      "loss": 2.4109,
      "step": 61470
    },
    {
      "epoch": 1.2647815901604356,
      "grad_norm": 0.404002845287323,
      "learning_rate": 0.00012979707188800802,
      "loss": 2.3449,
      "step": 61480
    },
    {
      "epoch": 1.2649873096773032,
      "grad_norm": 0.46450185775756836,
      "learning_rate": 0.0001297758708716721,
      "loss": 2.4286,
      "step": 61490
    },
    {
      "epoch": 1.2651930291941709,
      "grad_norm": 0.4424736499786377,
      "learning_rate": 0.00012975466838665474,
      "loss": 2.4042,
      "step": 61500
    },
    {
      "epoch": 1.2653987487110387,
      "grad_norm": 0.4795340597629547,
      "learning_rate": 0.00012973346443400177,
      "loss": 2.3846,
      "step": 61510
    },
    {
      "epoch": 1.2656044682279064,
      "grad_norm": 0.47201353311538696,
      "learning_rate": 0.00012971225901475898,
      "loss": 2.4318,
      "step": 61520
    },
    {
      "epoch": 1.265810187744774,
      "grad_norm": 0.43495139479637146,
      "learning_rate": 0.00012969105212997238,
      "loss": 2.4121,
      "step": 61530
    },
    {
      "epoch": 1.2660159072616417,
      "grad_norm": 0.47899165749549866,
      "learning_rate": 0.00012966984378068802,
      "loss": 2.4472,
      "step": 61540
    },
    {
      "epoch": 1.2662216267785096,
      "grad_norm": 0.38461068272590637,
      "learning_rate": 0.0001296486339679519,
      "loss": 2.3855,
      "step": 61550
    },
    {
      "epoch": 1.2664273462953772,
      "grad_norm": 0.429553747177124,
      "learning_rate": 0.0001296274226928102,
      "loss": 2.4557,
      "step": 61560
    },
    {
      "epoch": 1.2666330658122449,
      "grad_norm": 0.46434882283210754,
      "learning_rate": 0.00012960620995630925,
      "loss": 2.3981,
      "step": 61570
    },
    {
      "epoch": 1.2668387853291128,
      "grad_norm": 0.39721032977104187,
      "learning_rate": 0.00012958499575949528,
      "loss": 2.4186,
      "step": 61580
    },
    {
      "epoch": 1.2670445048459804,
      "grad_norm": 0.43096354603767395,
      "learning_rate": 0.0001295637801034147,
      "loss": 2.4249,
      "step": 61590
    },
    {
      "epoch": 1.267250224362848,
      "grad_norm": 0.5740247368812561,
      "learning_rate": 0.00012954256298911392,
      "loss": 2.4394,
      "step": 61600
    },
    {
      "epoch": 1.2674559438797157,
      "grad_norm": 0.4364524781703949,
      "learning_rate": 0.0001295213444176395,
      "loss": 2.4064,
      "step": 61610
    },
    {
      "epoch": 1.2676616633965836,
      "grad_norm": 0.42749330401420593,
      "learning_rate": 0.00012950012439003804,
      "loss": 2.4103,
      "step": 61620
    },
    {
      "epoch": 1.2678673829134512,
      "grad_norm": 0.44333115220069885,
      "learning_rate": 0.0001294789029073562,
      "loss": 2.4337,
      "step": 61630
    },
    {
      "epoch": 1.268073102430319,
      "grad_norm": 0.45138174295425415,
      "learning_rate": 0.0001294576799706407,
      "loss": 2.3738,
      "step": 61640
    },
    {
      "epoch": 1.2682788219471868,
      "grad_norm": 0.4324849247932434,
      "learning_rate": 0.00012943645558093842,
      "loss": 2.3332,
      "step": 61650
    },
    {
      "epoch": 1.2684845414640544,
      "grad_norm": 0.4483864903450012,
      "learning_rate": 0.00012941522973929618,
      "loss": 2.3812,
      "step": 61660
    },
    {
      "epoch": 1.268690260980922,
      "grad_norm": 0.424373596906662,
      "learning_rate": 0.00012939400244676094,
      "loss": 2.3437,
      "step": 61670
    },
    {
      "epoch": 1.2688959804977897,
      "grad_norm": 0.4267510175704956,
      "learning_rate": 0.00012937277370437972,
      "loss": 2.4045,
      "step": 61680
    },
    {
      "epoch": 1.2691017000146574,
      "grad_norm": 0.47403964400291443,
      "learning_rate": 0.00012935154351319966,
      "loss": 2.4025,
      "step": 61690
    },
    {
      "epoch": 1.2693074195315253,
      "grad_norm": 0.4165914058685303,
      "learning_rate": 0.0001293303118742679,
      "loss": 2.3716,
      "step": 61700
    },
    {
      "epoch": 1.269513139048393,
      "grad_norm": 0.4973384439945221,
      "learning_rate": 0.0001293090787886317,
      "loss": 2.417,
      "step": 61710
    },
    {
      "epoch": 1.2697188585652608,
      "grad_norm": 0.43314117193222046,
      "learning_rate": 0.00012928784425733834,
      "loss": 2.4259,
      "step": 61720
    },
    {
      "epoch": 1.2699245780821284,
      "grad_norm": 0.4185028374195099,
      "learning_rate": 0.0001292666082814352,
      "loss": 2.4115,
      "step": 61730
    },
    {
      "epoch": 1.270130297598996,
      "grad_norm": 0.40722501277923584,
      "learning_rate": 0.0001292453708619698,
      "loss": 2.4208,
      "step": 61740
    },
    {
      "epoch": 1.2703360171158637,
      "grad_norm": 0.4344175159931183,
      "learning_rate": 0.00012922413199998963,
      "loss": 2.3955,
      "step": 61750
    },
    {
      "epoch": 1.2705417366327314,
      "grad_norm": 0.46390146017074585,
      "learning_rate": 0.00012920289169654226,
      "loss": 2.4185,
      "step": 61760
    },
    {
      "epoch": 1.2707474561495993,
      "grad_norm": 0.4557572603225708,
      "learning_rate": 0.00012918164995267533,
      "loss": 2.4379,
      "step": 61770
    },
    {
      "epoch": 1.270953175666467,
      "grad_norm": 0.4201681911945343,
      "learning_rate": 0.0001291604067694367,
      "loss": 2.3831,
      "step": 61780
    },
    {
      "epoch": 1.2711588951833346,
      "grad_norm": 0.46967992186546326,
      "learning_rate": 0.0001291391621478741,
      "loss": 2.3861,
      "step": 61790
    },
    {
      "epoch": 1.2713646147002025,
      "grad_norm": 0.4228135645389557,
      "learning_rate": 0.0001291179160890354,
      "loss": 2.4122,
      "step": 61800
    },
    {
      "epoch": 1.27157033421707,
      "grad_norm": 0.4016229808330536,
      "learning_rate": 0.00012909666859396856,
      "loss": 2.4173,
      "step": 61810
    },
    {
      "epoch": 1.2717760537339378,
      "grad_norm": 0.43151599168777466,
      "learning_rate": 0.00012907541966372164,
      "loss": 2.4337,
      "step": 61820
    },
    {
      "epoch": 1.2719817732508054,
      "grad_norm": 0.4208572208881378,
      "learning_rate": 0.00012905416929934272,
      "loss": 2.414,
      "step": 61830
    },
    {
      "epoch": 1.2721874927676733,
      "grad_norm": 0.40688374638557434,
      "learning_rate": 0.00012903291750187993,
      "loss": 2.404,
      "step": 61840
    },
    {
      "epoch": 1.272393212284541,
      "grad_norm": 0.4177713990211487,
      "learning_rate": 0.00012901166427238154,
      "loss": 2.4256,
      "step": 61850
    },
    {
      "epoch": 1.2725989318014086,
      "grad_norm": 0.423977255821228,
      "learning_rate": 0.00012899040961189583,
      "loss": 2.3824,
      "step": 61860
    },
    {
      "epoch": 1.2728046513182765,
      "grad_norm": 0.42048707604408264,
      "learning_rate": 0.00012896915352147122,
      "loss": 2.4472,
      "step": 61870
    },
    {
      "epoch": 1.2730103708351441,
      "grad_norm": 0.523765504360199,
      "learning_rate": 0.00012894789600215612,
      "loss": 2.4465,
      "step": 61880
    },
    {
      "epoch": 1.2732160903520118,
      "grad_norm": 0.41976794600486755,
      "learning_rate": 0.00012892663705499905,
      "loss": 2.4163,
      "step": 61890
    },
    {
      "epoch": 1.2734218098688794,
      "grad_norm": 0.42385604977607727,
      "learning_rate": 0.00012890537668104862,
      "loss": 2.4037,
      "step": 61900
    },
    {
      "epoch": 1.2736275293857473,
      "grad_norm": 0.41300880908966064,
      "learning_rate": 0.0001288841148813535,
      "loss": 2.3812,
      "step": 61910
    },
    {
      "epoch": 1.273833248902615,
      "grad_norm": 0.4376348853111267,
      "learning_rate": 0.00012886285165696233,
      "loss": 2.4177,
      "step": 61920
    },
    {
      "epoch": 1.2740389684194826,
      "grad_norm": 0.4053003191947937,
      "learning_rate": 0.00012884158700892398,
      "loss": 2.4134,
      "step": 61930
    },
    {
      "epoch": 1.2742446879363505,
      "grad_norm": 0.4114512801170349,
      "learning_rate": 0.00012882032093828734,
      "loss": 2.3887,
      "step": 61940
    },
    {
      "epoch": 1.2744504074532181,
      "grad_norm": 0.4474332332611084,
      "learning_rate": 0.0001287990534461013,
      "loss": 2.3964,
      "step": 61950
    },
    {
      "epoch": 1.2746561269700858,
      "grad_norm": 0.5185419917106628,
      "learning_rate": 0.0001287777845334149,
      "loss": 2.3968,
      "step": 61960
    },
    {
      "epoch": 1.2748618464869534,
      "grad_norm": 0.4311531186103821,
      "learning_rate": 0.0001287565142012772,
      "loss": 2.4293,
      "step": 61970
    },
    {
      "epoch": 1.2750675660038213,
      "grad_norm": 0.4151619076728821,
      "learning_rate": 0.00012873524245073738,
      "loss": 2.4488,
      "step": 61980
    },
    {
      "epoch": 1.275273285520689,
      "grad_norm": 0.49077939987182617,
      "learning_rate": 0.00012871396928284463,
      "loss": 2.3945,
      "step": 61990
    },
    {
      "epoch": 1.2754790050375566,
      "grad_norm": 0.4360385239124298,
      "learning_rate": 0.00012869269469864824,
      "loss": 2.4297,
      "step": 62000
    },
    {
      "epoch": 1.2756847245544245,
      "grad_norm": 0.44656312465667725,
      "learning_rate": 0.00012867141869919756,
      "loss": 2.3868,
      "step": 62010
    },
    {
      "epoch": 1.2758904440712922,
      "grad_norm": 0.417557954788208,
      "learning_rate": 0.00012865014128554206,
      "loss": 2.4335,
      "step": 62020
    },
    {
      "epoch": 1.2760961635881598,
      "grad_norm": 0.41524815559387207,
      "learning_rate": 0.0001286288624587312,
      "loss": 2.3752,
      "step": 62030
    },
    {
      "epoch": 1.2763018831050275,
      "grad_norm": 0.3996841013431549,
      "learning_rate": 0.0001286075822198146,
      "loss": 2.4366,
      "step": 62040
    },
    {
      "epoch": 1.276507602621895,
      "grad_norm": 0.3716267943382263,
      "learning_rate": 0.00012858630056984183,
      "loss": 2.3929,
      "step": 62050
    },
    {
      "epoch": 1.276713322138763,
      "grad_norm": 0.4262866973876953,
      "learning_rate": 0.00012856501750986262,
      "loss": 2.3756,
      "step": 62060
    },
    {
      "epoch": 1.2769190416556306,
      "grad_norm": 0.4091126620769501,
      "learning_rate": 0.0001285437330409268,
      "loss": 2.4069,
      "step": 62070
    },
    {
      "epoch": 1.2771247611724985,
      "grad_norm": 0.4466691315174103,
      "learning_rate": 0.00012852244716408414,
      "loss": 2.4091,
      "step": 62080
    },
    {
      "epoch": 1.2773304806893662,
      "grad_norm": 0.440390020608902,
      "learning_rate": 0.00012850115988038456,
      "loss": 2.4292,
      "step": 62090
    },
    {
      "epoch": 1.2775362002062338,
      "grad_norm": 0.4202682077884674,
      "learning_rate": 0.00012847987119087815,
      "loss": 2.3734,
      "step": 62100
    },
    {
      "epoch": 1.2777419197231015,
      "grad_norm": 0.43288475275039673,
      "learning_rate": 0.00012845858109661483,
      "loss": 2.3914,
      "step": 62110
    },
    {
      "epoch": 1.2779476392399691,
      "grad_norm": 0.424855500459671,
      "learning_rate": 0.00012843728959864483,
      "loss": 2.3928,
      "step": 62120
    },
    {
      "epoch": 1.278153358756837,
      "grad_norm": 0.55429607629776,
      "learning_rate": 0.00012841599669801828,
      "loss": 2.4397,
      "step": 62130
    },
    {
      "epoch": 1.2783590782737047,
      "grad_norm": 0.43251925706863403,
      "learning_rate": 0.00012839470239578543,
      "loss": 2.462,
      "step": 62140
    },
    {
      "epoch": 1.2785647977905725,
      "grad_norm": 0.42556509375572205,
      "learning_rate": 0.0001283734066929967,
      "loss": 2.3789,
      "step": 62150
    },
    {
      "epoch": 1.2787705173074402,
      "grad_norm": 0.4178243577480316,
      "learning_rate": 0.00012835210959070238,
      "loss": 2.339,
      "step": 62160
    },
    {
      "epoch": 1.2789762368243078,
      "grad_norm": 0.4256199896335602,
      "learning_rate": 0.00012833081108995303,
      "loss": 2.4307,
      "step": 62170
    },
    {
      "epoch": 1.2791819563411755,
      "grad_norm": 0.42457643151283264,
      "learning_rate": 0.00012830951119179913,
      "loss": 2.4801,
      "step": 62180
    },
    {
      "epoch": 1.2793876758580431,
      "grad_norm": 0.4145734906196594,
      "learning_rate": 0.00012828820989729134,
      "loss": 2.3629,
      "step": 62190
    },
    {
      "epoch": 1.279593395374911,
      "grad_norm": 0.4144589900970459,
      "learning_rate": 0.0001282669072074803,
      "loss": 2.4142,
      "step": 62200
    },
    {
      "epoch": 1.2797991148917787,
      "grad_norm": 0.41929128766059875,
      "learning_rate": 0.00012824560312341674,
      "loss": 2.4457,
      "step": 62210
    },
    {
      "epoch": 1.2800048344086463,
      "grad_norm": 0.4069450795650482,
      "learning_rate": 0.0001282242976461515,
      "loss": 2.4066,
      "step": 62220
    },
    {
      "epoch": 1.2802105539255142,
      "grad_norm": 0.438566654920578,
      "learning_rate": 0.0001282029907767355,
      "loss": 2.3809,
      "step": 62230
    },
    {
      "epoch": 1.2804162734423818,
      "grad_norm": 0.4276019036769867,
      "learning_rate": 0.00012818168251621962,
      "loss": 2.392,
      "step": 62240
    },
    {
      "epoch": 1.2806219929592495,
      "grad_norm": 0.43148714303970337,
      "learning_rate": 0.0001281603728656549,
      "loss": 2.3708,
      "step": 62250
    },
    {
      "epoch": 1.2808277124761172,
      "grad_norm": 0.4856273829936981,
      "learning_rate": 0.00012813906182609246,
      "loss": 2.3992,
      "step": 62260
    },
    {
      "epoch": 1.281033431992985,
      "grad_norm": 0.41516387462615967,
      "learning_rate": 0.00012811774939858346,
      "loss": 2.369,
      "step": 62270
    },
    {
      "epoch": 1.2812391515098527,
      "grad_norm": 0.4561837911605835,
      "learning_rate": 0.0001280964355841791,
      "loss": 2.4417,
      "step": 62280
    },
    {
      "epoch": 1.2814448710267203,
      "grad_norm": 0.4574247896671295,
      "learning_rate": 0.00012807512038393067,
      "loss": 2.3827,
      "step": 62290
    },
    {
      "epoch": 1.2816505905435882,
      "grad_norm": 0.612548291683197,
      "learning_rate": 0.00012805380379888952,
      "loss": 2.393,
      "step": 62300
    },
    {
      "epoch": 1.2818563100604559,
      "grad_norm": 0.4359402656555176,
      "learning_rate": 0.00012803248583010714,
      "loss": 2.3853,
      "step": 62310
    },
    {
      "epoch": 1.2820620295773235,
      "grad_norm": 0.4444691836833954,
      "learning_rate": 0.00012801116647863497,
      "loss": 2.4481,
      "step": 62320
    },
    {
      "epoch": 1.2822677490941912,
      "grad_norm": 0.402301162481308,
      "learning_rate": 0.00012798984574552462,
      "loss": 2.4495,
      "step": 62330
    },
    {
      "epoch": 1.282473468611059,
      "grad_norm": 0.42783060669898987,
      "learning_rate": 0.00012796852363182768,
      "loss": 2.4317,
      "step": 62340
    },
    {
      "epoch": 1.2826791881279267,
      "grad_norm": 0.4457963705062866,
      "learning_rate": 0.00012794720013859588,
      "loss": 2.4018,
      "step": 62350
    },
    {
      "epoch": 1.2828849076447943,
      "grad_norm": 0.42579516768455505,
      "learning_rate": 0.00012792587526688103,
      "loss": 2.3869,
      "step": 62360
    },
    {
      "epoch": 1.2830906271616622,
      "grad_norm": 0.43341514468193054,
      "learning_rate": 0.0001279045490177349,
      "loss": 2.4191,
      "step": 62370
    },
    {
      "epoch": 1.2832963466785299,
      "grad_norm": 0.4672344923019409,
      "learning_rate": 0.00012788322139220944,
      "loss": 2.4358,
      "step": 62380
    },
    {
      "epoch": 1.2835020661953975,
      "grad_norm": 0.4341747462749481,
      "learning_rate": 0.00012786189239135659,
      "loss": 2.3679,
      "step": 62390
    },
    {
      "epoch": 1.2837077857122652,
      "grad_norm": 0.41343259811401367,
      "learning_rate": 0.00012784056201622845,
      "loss": 2.422,
      "step": 62400
    },
    {
      "epoch": 1.2839135052291328,
      "grad_norm": 0.4138436019420624,
      "learning_rate": 0.00012781923026787705,
      "loss": 2.4045,
      "step": 62410
    },
    {
      "epoch": 1.2841192247460007,
      "grad_norm": 0.4363037645816803,
      "learning_rate": 0.0001277978971473547,
      "loss": 2.3969,
      "step": 62420
    },
    {
      "epoch": 1.2843249442628684,
      "grad_norm": 0.43445727229118347,
      "learning_rate": 0.0001277765626557135,
      "loss": 2.3195,
      "step": 62430
    },
    {
      "epoch": 1.2845306637797362,
      "grad_norm": 0.4119006097316742,
      "learning_rate": 0.00012775522679400582,
      "loss": 2.3947,
      "step": 62440
    },
    {
      "epoch": 1.284736383296604,
      "grad_norm": 0.41808396577835083,
      "learning_rate": 0.00012773388956328408,
      "loss": 2.3367,
      "step": 62450
    },
    {
      "epoch": 1.2849421028134715,
      "grad_norm": 0.41676151752471924,
      "learning_rate": 0.0001277125509646007,
      "loss": 2.4506,
      "step": 62460
    },
    {
      "epoch": 1.2851478223303392,
      "grad_norm": 0.4553024470806122,
      "learning_rate": 0.00012769121099900818,
      "loss": 2.4205,
      "step": 62470
    },
    {
      "epoch": 1.2853535418472068,
      "grad_norm": 0.40155112743377686,
      "learning_rate": 0.00012766986966755914,
      "loss": 2.4309,
      "step": 62480
    },
    {
      "epoch": 1.2855592613640747,
      "grad_norm": 0.41533124446868896,
      "learning_rate": 0.00012764852697130617,
      "loss": 2.3845,
      "step": 62490
    },
    {
      "epoch": 1.2857649808809424,
      "grad_norm": 0.38852185010910034,
      "learning_rate": 0.0001276271829113021,
      "loss": 2.4041,
      "step": 62500
    },
    {
      "epoch": 1.2859707003978103,
      "grad_norm": 0.43573251366615295,
      "learning_rate": 0.0001276058374885996,
      "loss": 2.3703,
      "step": 62510
    },
    {
      "epoch": 1.286176419914678,
      "grad_norm": 0.5415360331535339,
      "learning_rate": 0.0001275844907042516,
      "loss": 2.3706,
      "step": 62520
    },
    {
      "epoch": 1.2863821394315456,
      "grad_norm": 0.44733908772468567,
      "learning_rate": 0.00012756314255931095,
      "loss": 2.3617,
      "step": 62530
    },
    {
      "epoch": 1.2865878589484132,
      "grad_norm": 0.44268903136253357,
      "learning_rate": 0.00012754179305483069,
      "loss": 2.4071,
      "step": 62540
    },
    {
      "epoch": 1.2867935784652809,
      "grad_norm": 0.3918440639972687,
      "learning_rate": 0.00012752044219186388,
      "loss": 2.3662,
      "step": 62550
    },
    {
      "epoch": 1.2869992979821487,
      "grad_norm": 0.4877314269542694,
      "learning_rate": 0.00012749908997146362,
      "loss": 2.396,
      "step": 62560
    },
    {
      "epoch": 1.2872050174990164,
      "grad_norm": 0.4905760884284973,
      "learning_rate": 0.00012747773639468308,
      "loss": 2.4033,
      "step": 62570
    },
    {
      "epoch": 1.287410737015884,
      "grad_norm": 0.41895654797554016,
      "learning_rate": 0.00012745638146257558,
      "loss": 2.4295,
      "step": 62580
    },
    {
      "epoch": 1.287616456532752,
      "grad_norm": 0.4132547378540039,
      "learning_rate": 0.0001274350251761944,
      "loss": 2.4383,
      "step": 62590
    },
    {
      "epoch": 1.2878221760496196,
      "grad_norm": 0.4100160300731659,
      "learning_rate": 0.0001274136675365929,
      "loss": 2.4123,
      "step": 62600
    },
    {
      "epoch": 1.2880278955664872,
      "grad_norm": 0.4283561110496521,
      "learning_rate": 0.00012739230854482458,
      "loss": 2.3861,
      "step": 62610
    },
    {
      "epoch": 1.2882336150833549,
      "grad_norm": 0.4387841522693634,
      "learning_rate": 0.00012737094820194296,
      "loss": 2.3779,
      "step": 62620
    },
    {
      "epoch": 1.2884393346002228,
      "grad_norm": 0.4550726115703583,
      "learning_rate": 0.0001273495865090016,
      "loss": 2.4106,
      "step": 62630
    },
    {
      "epoch": 1.2886450541170904,
      "grad_norm": 0.4393076002597809,
      "learning_rate": 0.00012732822346705417,
      "loss": 2.4231,
      "step": 62640
    },
    {
      "epoch": 1.288850773633958,
      "grad_norm": 0.41441258788108826,
      "learning_rate": 0.0001273068590771544,
      "loss": 2.4239,
      "step": 62650
    },
    {
      "epoch": 1.289056493150826,
      "grad_norm": 0.44112569093704224,
      "learning_rate": 0.0001272854933403561,
      "loss": 2.4222,
      "step": 62660
    },
    {
      "epoch": 1.2892622126676936,
      "grad_norm": 0.398068904876709,
      "learning_rate": 0.00012726412625771309,
      "loss": 2.3957,
      "step": 62670
    },
    {
      "epoch": 1.2894679321845612,
      "grad_norm": 0.44024384021759033,
      "learning_rate": 0.00012724275783027934,
      "loss": 2.3459,
      "step": 62680
    },
    {
      "epoch": 1.289673651701429,
      "grad_norm": 0.4300827085971832,
      "learning_rate": 0.00012722138805910873,
      "loss": 2.4199,
      "step": 62690
    },
    {
      "epoch": 1.2898793712182968,
      "grad_norm": 0.4874214828014374,
      "learning_rate": 0.0001272000169452554,
      "loss": 2.3967,
      "step": 62700
    },
    {
      "epoch": 1.2900850907351644,
      "grad_norm": 0.4268217086791992,
      "learning_rate": 0.00012717864448977351,
      "loss": 2.4403,
      "step": 62710
    },
    {
      "epoch": 1.290290810252032,
      "grad_norm": 0.4109138250350952,
      "learning_rate": 0.00012715727069371715,
      "loss": 2.4605,
      "step": 62720
    },
    {
      "epoch": 1.2904965297689,
      "grad_norm": 0.41606125235557556,
      "learning_rate": 0.0001271358955581406,
      "loss": 2.4005,
      "step": 62730
    },
    {
      "epoch": 1.2907022492857676,
      "grad_norm": 0.40817293524742126,
      "learning_rate": 0.00012711451908409824,
      "loss": 2.4183,
      "step": 62740
    },
    {
      "epoch": 1.2909079688026353,
      "grad_norm": 0.42910996079444885,
      "learning_rate": 0.0001270931412726444,
      "loss": 2.3865,
      "step": 62750
    },
    {
      "epoch": 1.291113688319503,
      "grad_norm": 0.4544825255870819,
      "learning_rate": 0.00012707176212483354,
      "loss": 2.3911,
      "step": 62760
    },
    {
      "epoch": 1.2913194078363706,
      "grad_norm": 0.43702325224876404,
      "learning_rate": 0.00012705038164172015,
      "loss": 2.4149,
      "step": 62770
    },
    {
      "epoch": 1.2915251273532384,
      "grad_norm": 0.43647468090057373,
      "learning_rate": 0.00012702899982435886,
      "loss": 2.4245,
      "step": 62780
    },
    {
      "epoch": 1.291730846870106,
      "grad_norm": 0.4144565165042877,
      "learning_rate": 0.0001270076166738043,
      "loss": 2.4303,
      "step": 62790
    },
    {
      "epoch": 1.291936566386974,
      "grad_norm": 0.4207012355327606,
      "learning_rate": 0.00012698623219111122,
      "loss": 2.3949,
      "step": 62800
    },
    {
      "epoch": 1.2921422859038416,
      "grad_norm": 0.4819578528404236,
      "learning_rate": 0.0001269648463773343,
      "loss": 2.4976,
      "step": 62810
    },
    {
      "epoch": 1.2923480054207093,
      "grad_norm": 0.448037713766098,
      "learning_rate": 0.00012694345923352845,
      "loss": 2.4439,
      "step": 62820
    },
    {
      "epoch": 1.292553724937577,
      "grad_norm": 0.4009558856487274,
      "learning_rate": 0.00012692207076074862,
      "loss": 2.385,
      "step": 62830
    },
    {
      "epoch": 1.2927594444544446,
      "grad_norm": 0.4290512800216675,
      "learning_rate": 0.00012690068096004974,
      "loss": 2.3896,
      "step": 62840
    },
    {
      "epoch": 1.2929651639713124,
      "grad_norm": 0.4285016655921936,
      "learning_rate": 0.00012687928983248683,
      "loss": 2.4097,
      "step": 62850
    },
    {
      "epoch": 1.29317088348818,
      "grad_norm": 0.42071083188056946,
      "learning_rate": 0.00012685789737911508,
      "loss": 2.3564,
      "step": 62860
    },
    {
      "epoch": 1.293376603005048,
      "grad_norm": 0.43585047125816345,
      "learning_rate": 0.00012683650360098957,
      "loss": 2.4077,
      "step": 62870
    },
    {
      "epoch": 1.2935823225219156,
      "grad_norm": 0.42877182364463806,
      "learning_rate": 0.00012681510849916558,
      "loss": 2.4013,
      "step": 62880
    },
    {
      "epoch": 1.2937880420387833,
      "grad_norm": 0.448525607585907,
      "learning_rate": 0.00012679371207469843,
      "loss": 2.3766,
      "step": 62890
    },
    {
      "epoch": 1.293993761555651,
      "grad_norm": 1.0638058185577393,
      "learning_rate": 0.00012677231432864345,
      "loss": 2.4545,
      "step": 62900
    },
    {
      "epoch": 1.2941994810725186,
      "grad_norm": 0.43677181005477905,
      "learning_rate": 0.0001267509152620561,
      "loss": 2.4583,
      "step": 62910
    },
    {
      "epoch": 1.2944052005893865,
      "grad_norm": 0.4472714364528656,
      "learning_rate": 0.0001267295148759919,
      "loss": 2.3499,
      "step": 62920
    },
    {
      "epoch": 1.2946109201062541,
      "grad_norm": 0.4541676342487335,
      "learning_rate": 0.00012670811317150638,
      "loss": 2.4056,
      "step": 62930
    },
    {
      "epoch": 1.2948166396231218,
      "grad_norm": 0.45754274725914,
      "learning_rate": 0.00012668671014965516,
      "loss": 2.359,
      "step": 62940
    },
    {
      "epoch": 1.2950223591399896,
      "grad_norm": 0.3621901273727417,
      "learning_rate": 0.00012666530581149397,
      "loss": 2.3938,
      "step": 62950
    },
    {
      "epoch": 1.2952280786568573,
      "grad_norm": 0.4335891008377075,
      "learning_rate": 0.00012664390015807854,
      "loss": 2.4439,
      "step": 62960
    },
    {
      "epoch": 1.295433798173725,
      "grad_norm": 0.43318676948547363,
      "learning_rate": 0.00012662249319046474,
      "loss": 2.3813,
      "step": 62970
    },
    {
      "epoch": 1.2956395176905926,
      "grad_norm": 0.41320350766181946,
      "learning_rate": 0.00012660108490970836,
      "loss": 2.3703,
      "step": 62980
    },
    {
      "epoch": 1.2958452372074605,
      "grad_norm": 0.4116664528846741,
      "learning_rate": 0.0001265796753168655,
      "loss": 2.427,
      "step": 62990
    },
    {
      "epoch": 1.2960509567243281,
      "grad_norm": 0.4541627764701843,
      "learning_rate": 0.00012655826441299202,
      "loss": 2.3826,
      "step": 63000
    },
    {
      "epoch": 1.2962566762411958,
      "grad_norm": 0.45182546973228455,
      "learning_rate": 0.00012653685219914413,
      "loss": 2.4314,
      "step": 63010
    },
    {
      "epoch": 1.2964623957580637,
      "grad_norm": 0.442234069108963,
      "learning_rate": 0.00012651543867637791,
      "loss": 2.3735,
      "step": 63020
    },
    {
      "epoch": 1.2966681152749313,
      "grad_norm": 0.4168250262737274,
      "learning_rate": 0.00012649402384574962,
      "loss": 2.4479,
      "step": 63030
    },
    {
      "epoch": 1.296873834791799,
      "grad_norm": 0.43759286403656006,
      "learning_rate": 0.00012647260770831548,
      "loss": 2.4119,
      "step": 63040
    },
    {
      "epoch": 1.2970795543086666,
      "grad_norm": 0.43932849168777466,
      "learning_rate": 0.00012645119026513188,
      "loss": 2.3825,
      "step": 63050
    },
    {
      "epoch": 1.2972852738255345,
      "grad_norm": 0.40779539942741394,
      "learning_rate": 0.0001264297715172552,
      "loss": 2.3534,
      "step": 63060
    },
    {
      "epoch": 1.2974909933424021,
      "grad_norm": 0.4151683747768402,
      "learning_rate": 0.0001264083514657419,
      "loss": 2.3711,
      "step": 63070
    },
    {
      "epoch": 1.2976967128592698,
      "grad_norm": 0.44659292697906494,
      "learning_rate": 0.00012638693011164854,
      "loss": 2.3844,
      "step": 63080
    },
    {
      "epoch": 1.2979024323761377,
      "grad_norm": 0.4268505573272705,
      "learning_rate": 0.00012636550745603176,
      "loss": 2.418,
      "step": 63090
    },
    {
      "epoch": 1.2981081518930053,
      "grad_norm": 0.42741209268569946,
      "learning_rate": 0.00012634408349994808,
      "loss": 2.3618,
      "step": 63100
    },
    {
      "epoch": 1.298313871409873,
      "grad_norm": 0.4028007984161377,
      "learning_rate": 0.00012632265824445436,
      "loss": 2.3726,
      "step": 63110
    },
    {
      "epoch": 1.2985195909267406,
      "grad_norm": 0.4433932900428772,
      "learning_rate": 0.00012630123169060737,
      "loss": 2.4237,
      "step": 63120
    },
    {
      "epoch": 1.2987253104436085,
      "grad_norm": 0.43864259123802185,
      "learning_rate": 0.00012627980383946393,
      "loss": 2.4109,
      "step": 63130
    },
    {
      "epoch": 1.2989310299604762,
      "grad_norm": 0.4064183235168457,
      "learning_rate": 0.00012625837469208096,
      "loss": 2.3871,
      "step": 63140
    },
    {
      "epoch": 1.2991367494773438,
      "grad_norm": 0.40699121356010437,
      "learning_rate": 0.0001262369442495155,
      "loss": 2.4236,
      "step": 63150
    },
    {
      "epoch": 1.2993424689942117,
      "grad_norm": 0.4094238579273224,
      "learning_rate": 0.0001262155125128245,
      "loss": 2.3686,
      "step": 63160
    },
    {
      "epoch": 1.2995481885110793,
      "grad_norm": 0.46745339035987854,
      "learning_rate": 0.00012619407948306517,
      "loss": 2.4225,
      "step": 63170
    },
    {
      "epoch": 1.299753908027947,
      "grad_norm": 0.6327084302902222,
      "learning_rate": 0.0001261726451612946,
      "loss": 2.3856,
      "step": 63180
    },
    {
      "epoch": 1.2999596275448146,
      "grad_norm": 0.4598681628704071,
      "learning_rate": 0.00012615120954857008,
      "loss": 2.4134,
      "step": 63190
    },
    {
      "epoch": 1.3001653470616823,
      "grad_norm": 0.4221095144748688,
      "learning_rate": 0.00012612977264594893,
      "loss": 2.3751,
      "step": 63200
    },
    {
      "epoch": 1.3003710665785502,
      "grad_norm": 0.4295961856842041,
      "learning_rate": 0.00012610833445448843,
      "loss": 2.4038,
      "step": 63210
    },
    {
      "epoch": 1.3005767860954178,
      "grad_norm": 0.4200000464916229,
      "learning_rate": 0.0001260868949752461,
      "loss": 2.4335,
      "step": 63220
    },
    {
      "epoch": 1.3007825056122857,
      "grad_norm": 0.41610395908355713,
      "learning_rate": 0.00012606545420927942,
      "loss": 2.3986,
      "step": 63230
    },
    {
      "epoch": 1.3009882251291534,
      "grad_norm": 0.44406670331954956,
      "learning_rate": 0.0001260440121576459,
      "loss": 2.3946,
      "step": 63240
    },
    {
      "epoch": 1.301193944646021,
      "grad_norm": 0.38770097494125366,
      "learning_rate": 0.0001260225688214032,
      "loss": 2.4467,
      "step": 63250
    },
    {
      "epoch": 1.3013996641628887,
      "grad_norm": 0.4115148186683655,
      "learning_rate": 0.00012600112420160895,
      "loss": 2.3867,
      "step": 63260
    },
    {
      "epoch": 1.3016053836797563,
      "grad_norm": 0.4382297098636627,
      "learning_rate": 0.00012597967829932095,
      "loss": 2.382,
      "step": 63270
    },
    {
      "epoch": 1.3018111031966242,
      "grad_norm": 0.41161683201789856,
      "learning_rate": 0.000125958231115597,
      "loss": 2.3098,
      "step": 63280
    },
    {
      "epoch": 1.3020168227134918,
      "grad_norm": 0.4629530608654022,
      "learning_rate": 0.00012593678265149496,
      "loss": 2.4023,
      "step": 63290
    },
    {
      "epoch": 1.3022225422303595,
      "grad_norm": 0.4256855249404907,
      "learning_rate": 0.00012591533290807273,
      "loss": 2.413,
      "step": 63300
    },
    {
      "epoch": 1.3024282617472274,
      "grad_norm": 0.40326350927352905,
      "learning_rate": 0.00012589388188638843,
      "loss": 2.4095,
      "step": 63310
    },
    {
      "epoch": 1.302633981264095,
      "grad_norm": 0.41602566838264465,
      "learning_rate": 0.00012587242958749998,
      "loss": 2.4344,
      "step": 63320
    },
    {
      "epoch": 1.3028397007809627,
      "grad_norm": 0.45800548791885376,
      "learning_rate": 0.0001258509760124656,
      "loss": 2.4492,
      "step": 63330
    },
    {
      "epoch": 1.3030454202978303,
      "grad_norm": 0.4626368284225464,
      "learning_rate": 0.00012582952116234341,
      "loss": 2.4113,
      "step": 63340
    },
    {
      "epoch": 1.3032511398146982,
      "grad_norm": 0.447950154542923,
      "learning_rate": 0.00012580806503819172,
      "loss": 2.3841,
      "step": 63350
    },
    {
      "epoch": 1.3034568593315659,
      "grad_norm": 0.45283299684524536,
      "learning_rate": 0.0001257866076410688,
      "loss": 2.3683,
      "step": 63360
    },
    {
      "epoch": 1.3036625788484335,
      "grad_norm": 0.4104497730731964,
      "learning_rate": 0.00012576514897203307,
      "loss": 2.3678,
      "step": 63370
    },
    {
      "epoch": 1.3038682983653014,
      "grad_norm": 0.4448571503162384,
      "learning_rate": 0.00012574368903214294,
      "loss": 2.3922,
      "step": 63380
    },
    {
      "epoch": 1.304074017882169,
      "grad_norm": 0.44909271597862244,
      "learning_rate": 0.00012572222782245688,
      "loss": 2.3865,
      "step": 63390
    },
    {
      "epoch": 1.3042797373990367,
      "grad_norm": 0.42272675037384033,
      "learning_rate": 0.00012570076534403353,
      "loss": 2.4338,
      "step": 63400
    },
    {
      "epoch": 1.3044854569159043,
      "grad_norm": 0.4473695755004883,
      "learning_rate": 0.00012567930159793148,
      "loss": 2.3798,
      "step": 63410
    },
    {
      "epoch": 1.3046911764327722,
      "grad_norm": 0.6335265040397644,
      "learning_rate": 0.00012565783658520938,
      "loss": 2.3838,
      "step": 63420
    },
    {
      "epoch": 1.3048968959496399,
      "grad_norm": 0.43298032879829407,
      "learning_rate": 0.00012563637030692606,
      "loss": 2.4476,
      "step": 63430
    },
    {
      "epoch": 1.3051026154665075,
      "grad_norm": 0.41593390703201294,
      "learning_rate": 0.00012561490276414026,
      "loss": 2.3935,
      "step": 63440
    },
    {
      "epoch": 1.3053083349833754,
      "grad_norm": 0.48828139901161194,
      "learning_rate": 0.00012559343395791094,
      "loss": 2.4264,
      "step": 63450
    },
    {
      "epoch": 1.305514054500243,
      "grad_norm": 0.46665558218955994,
      "learning_rate": 0.00012557196388929693,
      "loss": 2.4202,
      "step": 63460
    },
    {
      "epoch": 1.3057197740171107,
      "grad_norm": 0.40582171082496643,
      "learning_rate": 0.00012555049255935733,
      "loss": 2.4125,
      "step": 63470
    },
    {
      "epoch": 1.3059254935339784,
      "grad_norm": 0.4383235573768616,
      "learning_rate": 0.00012552901996915115,
      "loss": 2.4151,
      "step": 63480
    },
    {
      "epoch": 1.3061312130508462,
      "grad_norm": 0.44194960594177246,
      "learning_rate": 0.00012550754611973756,
      "loss": 2.3556,
      "step": 63490
    },
    {
      "epoch": 1.3063369325677139,
      "grad_norm": 0.443593829870224,
      "learning_rate": 0.0001254860710121757,
      "loss": 2.4562,
      "step": 63500
    },
    {
      "epoch": 1.3065426520845815,
      "grad_norm": 0.41327691078186035,
      "learning_rate": 0.00012546459464752481,
      "loss": 2.3578,
      "step": 63510
    },
    {
      "epoch": 1.3067483716014494,
      "grad_norm": 0.4455341398715973,
      "learning_rate": 0.00012544311702684428,
      "loss": 2.4458,
      "step": 63520
    },
    {
      "epoch": 1.306954091118317,
      "grad_norm": 0.4313834011554718,
      "learning_rate": 0.00012542163815119342,
      "loss": 2.4414,
      "step": 63530
    },
    {
      "epoch": 1.3071598106351847,
      "grad_norm": 0.41328486800193787,
      "learning_rate": 0.00012540015802163165,
      "loss": 2.4571,
      "step": 63540
    },
    {
      "epoch": 1.3073655301520524,
      "grad_norm": 0.4262374937534332,
      "learning_rate": 0.00012537867663921852,
      "loss": 2.3964,
      "step": 63550
    },
    {
      "epoch": 1.30757124966892,
      "grad_norm": 0.4518372118473053,
      "learning_rate": 0.0001253571940050136,
      "loss": 2.3761,
      "step": 63560
    },
    {
      "epoch": 1.307776969185788,
      "grad_norm": 0.42613473534584045,
      "learning_rate": 0.00012533571012007645,
      "loss": 2.4604,
      "step": 63570
    },
    {
      "epoch": 1.3079826887026555,
      "grad_norm": 0.48237207531929016,
      "learning_rate": 0.00012531422498546677,
      "loss": 2.4443,
      "step": 63580
    },
    {
      "epoch": 1.3081884082195234,
      "grad_norm": 0.4262732267379761,
      "learning_rate": 0.00012529273860224433,
      "loss": 2.4509,
      "step": 63590
    },
    {
      "epoch": 1.308394127736391,
      "grad_norm": 0.41624802350997925,
      "learning_rate": 0.00012527125097146894,
      "loss": 2.4035,
      "step": 63600
    },
    {
      "epoch": 1.3085998472532587,
      "grad_norm": 0.44967496395111084,
      "learning_rate": 0.00012524976209420046,
      "loss": 2.4032,
      "step": 63610
    },
    {
      "epoch": 1.3088055667701264,
      "grad_norm": 0.4497905671596527,
      "learning_rate": 0.00012522827197149872,
      "loss": 2.4483,
      "step": 63620
    },
    {
      "epoch": 1.309011286286994,
      "grad_norm": 0.41957053542137146,
      "learning_rate": 0.00012520678060442392,
      "loss": 2.4291,
      "step": 63630
    },
    {
      "epoch": 1.309217005803862,
      "grad_norm": 0.4813379645347595,
      "learning_rate": 0.00012518528799403592,
      "loss": 2.3697,
      "step": 63640
    },
    {
      "epoch": 1.3094227253207296,
      "grad_norm": 0.4434901475906372,
      "learning_rate": 0.00012516379414139495,
      "loss": 2.3981,
      "step": 63650
    },
    {
      "epoch": 1.3096284448375972,
      "grad_norm": 0.49200475215911865,
      "learning_rate": 0.00012514229904756112,
      "loss": 2.4325,
      "step": 63660
    },
    {
      "epoch": 1.309834164354465,
      "grad_norm": 0.4258666932582855,
      "learning_rate": 0.00012512080271359467,
      "loss": 2.4392,
      "step": 63670
    },
    {
      "epoch": 1.3100398838713327,
      "grad_norm": 0.39283856749534607,
      "learning_rate": 0.00012509930514055593,
      "loss": 2.3939,
      "step": 63680
    },
    {
      "epoch": 1.3102456033882004,
      "grad_norm": 0.4753037989139557,
      "learning_rate": 0.00012507780632950525,
      "loss": 2.3904,
      "step": 63690
    },
    {
      "epoch": 1.310451322905068,
      "grad_norm": 0.4352634847164154,
      "learning_rate": 0.00012505630628150305,
      "loss": 2.3869,
      "step": 63700
    },
    {
      "epoch": 1.310657042421936,
      "grad_norm": 0.4130264222621918,
      "learning_rate": 0.00012503480499760977,
      "loss": 2.3847,
      "step": 63710
    },
    {
      "epoch": 1.3108627619388036,
      "grad_norm": 0.44403108954429626,
      "learning_rate": 0.000125013302478886,
      "loss": 2.4005,
      "step": 63720
    },
    {
      "epoch": 1.3110684814556712,
      "grad_norm": 0.4280286133289337,
      "learning_rate": 0.00012499179872639234,
      "loss": 2.3602,
      "step": 63730
    },
    {
      "epoch": 1.311274200972539,
      "grad_norm": 0.4239090085029602,
      "learning_rate": 0.0001249702937411894,
      "loss": 2.3539,
      "step": 63740
    },
    {
      "epoch": 1.3114799204894068,
      "grad_norm": 0.4456855058670044,
      "learning_rate": 0.00012494878752433798,
      "loss": 2.3991,
      "step": 63750
    },
    {
      "epoch": 1.3116856400062744,
      "grad_norm": 0.6208890676498413,
      "learning_rate": 0.0001249272800768988,
      "loss": 2.4052,
      "step": 63760
    },
    {
      "epoch": 1.311891359523142,
      "grad_norm": 0.42587077617645264,
      "learning_rate": 0.00012490577139993275,
      "loss": 2.3957,
      "step": 63770
    },
    {
      "epoch": 1.31209707904001,
      "grad_norm": 0.4695402681827545,
      "learning_rate": 0.0001248842614945007,
      "loss": 2.4223,
      "step": 63780
    },
    {
      "epoch": 1.3123027985568776,
      "grad_norm": 0.4959017336368561,
      "learning_rate": 0.00012486275036166367,
      "loss": 2.366,
      "step": 63790
    },
    {
      "epoch": 1.3125085180737452,
      "grad_norm": 0.4740949273109436,
      "learning_rate": 0.00012484123800248262,
      "loss": 2.3745,
      "step": 63800
    },
    {
      "epoch": 1.3127142375906131,
      "grad_norm": 0.4090837240219116,
      "learning_rate": 0.00012481972441801865,
      "loss": 2.482,
      "step": 63810
    },
    {
      "epoch": 1.3129199571074808,
      "grad_norm": 0.41837531328201294,
      "learning_rate": 0.00012479820960933295,
      "loss": 2.4471,
      "step": 63820
    },
    {
      "epoch": 1.3131256766243484,
      "grad_norm": 0.425976037979126,
      "learning_rate": 0.0001247766935774867,
      "loss": 2.3774,
      "step": 63830
    },
    {
      "epoch": 1.313331396141216,
      "grad_norm": 0.4633904993534088,
      "learning_rate": 0.00012475517632354118,
      "loss": 2.4182,
      "step": 63840
    },
    {
      "epoch": 1.313537115658084,
      "grad_norm": 0.38803887367248535,
      "learning_rate": 0.00012473365784855772,
      "loss": 2.46,
      "step": 63850
    },
    {
      "epoch": 1.3137428351749516,
      "grad_norm": 0.4128570556640625,
      "learning_rate": 0.00012471213815359766,
      "loss": 2.3652,
      "step": 63860
    },
    {
      "epoch": 1.3139485546918193,
      "grad_norm": 0.49522948265075684,
      "learning_rate": 0.00012469061723972252,
      "loss": 2.337,
      "step": 63870
    },
    {
      "epoch": 1.3141542742086871,
      "grad_norm": 0.42175716161727905,
      "learning_rate": 0.00012466909510799377,
      "loss": 2.446,
      "step": 63880
    },
    {
      "epoch": 1.3143599937255548,
      "grad_norm": 0.4826061427593231,
      "learning_rate": 0.00012464757175947302,
      "loss": 2.3734,
      "step": 63890
    },
    {
      "epoch": 1.3145657132424224,
      "grad_norm": 0.46514514088630676,
      "learning_rate": 0.00012462604719522182,
      "loss": 2.3931,
      "step": 63900
    },
    {
      "epoch": 1.31477143275929,
      "grad_norm": 0.42111504077911377,
      "learning_rate": 0.00012460452141630193,
      "loss": 2.3778,
      "step": 63910
    },
    {
      "epoch": 1.3149771522761577,
      "grad_norm": 0.45957762002944946,
      "learning_rate": 0.00012458299442377508,
      "loss": 2.4081,
      "step": 63920
    },
    {
      "epoch": 1.3151828717930256,
      "grad_norm": 0.4330631494522095,
      "learning_rate": 0.00012456146621870308,
      "loss": 2.3836,
      "step": 63930
    },
    {
      "epoch": 1.3153885913098933,
      "grad_norm": 0.44341927766799927,
      "learning_rate": 0.00012453993680214777,
      "loss": 2.4085,
      "step": 63940
    },
    {
      "epoch": 1.3155943108267611,
      "grad_norm": 0.3969747722148895,
      "learning_rate": 0.00012451840617517114,
      "loss": 2.3807,
      "step": 63950
    },
    {
      "epoch": 1.3158000303436288,
      "grad_norm": 0.4224759340286255,
      "learning_rate": 0.00012449687433883512,
      "loss": 2.4152,
      "step": 63960
    },
    {
      "epoch": 1.3160057498604965,
      "grad_norm": 0.40604835748672485,
      "learning_rate": 0.00012447534129420178,
      "loss": 2.4098,
      "step": 63970
    },
    {
      "epoch": 1.316211469377364,
      "grad_norm": 0.4307875633239746,
      "learning_rate": 0.00012445380704233323,
      "loss": 2.3458,
      "step": 63980
    },
    {
      "epoch": 1.3164171888942318,
      "grad_norm": 0.4020155668258667,
      "learning_rate": 0.00012443227158429165,
      "loss": 2.4205,
      "step": 63990
    },
    {
      "epoch": 1.3166229084110996,
      "grad_norm": 0.4511602520942688,
      "learning_rate": 0.00012441073492113923,
      "loss": 2.3529,
      "step": 64000
    },
    {
      "epoch": 1.3168286279279673,
      "grad_norm": 0.48918968439102173,
      "learning_rate": 0.0001243891970539383,
      "loss": 2.409,
      "step": 64010
    },
    {
      "epoch": 1.3170343474448352,
      "grad_norm": 0.3987397849559784,
      "learning_rate": 0.00012436765798375115,
      "loss": 2.3797,
      "step": 64020
    },
    {
      "epoch": 1.3172400669617028,
      "grad_norm": 0.4584687352180481,
      "learning_rate": 0.00012434611771164027,
      "loss": 2.384,
      "step": 64030
    },
    {
      "epoch": 1.3174457864785705,
      "grad_norm": 0.4086487591266632,
      "learning_rate": 0.00012432457623866804,
      "loss": 2.4246,
      "step": 64040
    },
    {
      "epoch": 1.3176515059954381,
      "grad_norm": 0.44798797369003296,
      "learning_rate": 0.00012430303356589706,
      "loss": 2.4246,
      "step": 64050
    },
    {
      "epoch": 1.3178572255123058,
      "grad_norm": 0.40536606311798096,
      "learning_rate": 0.0001242814896943898,
      "loss": 2.3809,
      "step": 64060
    },
    {
      "epoch": 1.3180629450291736,
      "grad_norm": 0.44044560194015503,
      "learning_rate": 0.000124259944625209,
      "loss": 2.4517,
      "step": 64070
    },
    {
      "epoch": 1.3182686645460413,
      "grad_norm": 0.42371559143066406,
      "learning_rate": 0.00012423839835941738,
      "loss": 2.4229,
      "step": 64080
    },
    {
      "epoch": 1.318474384062909,
      "grad_norm": 0.4028128981590271,
      "learning_rate": 0.0001242168508980776,
      "loss": 2.4471,
      "step": 64090
    },
    {
      "epoch": 1.3186801035797768,
      "grad_norm": 0.4178396165370941,
      "learning_rate": 0.00012419530224225253,
      "loss": 2.4147,
      "step": 64100
    },
    {
      "epoch": 1.3188858230966445,
      "grad_norm": 0.4132211208343506,
      "learning_rate": 0.00012417375239300507,
      "loss": 2.3936,
      "step": 64110
    },
    {
      "epoch": 1.3190915426135121,
      "grad_norm": 0.4022885859012604,
      "learning_rate": 0.00012415220135139813,
      "loss": 2.3917,
      "step": 64120
    },
    {
      "epoch": 1.3192972621303798,
      "grad_norm": 0.5339206457138062,
      "learning_rate": 0.0001241306491184947,
      "loss": 2.3886,
      "step": 64130
    },
    {
      "epoch": 1.3195029816472477,
      "grad_norm": 0.43647047877311707,
      "learning_rate": 0.00012410909569535785,
      "loss": 2.3453,
      "step": 64140
    },
    {
      "epoch": 1.3197087011641153,
      "grad_norm": 0.4220796227455139,
      "learning_rate": 0.0001240875410830507,
      "loss": 2.4213,
      "step": 64150
    },
    {
      "epoch": 1.319914420680983,
      "grad_norm": 0.4149441719055176,
      "learning_rate": 0.0001240659852826364,
      "loss": 2.3977,
      "step": 64160
    },
    {
      "epoch": 1.3201201401978508,
      "grad_norm": 0.38116222620010376,
      "learning_rate": 0.0001240444282951782,
      "loss": 2.4418,
      "step": 64170
    },
    {
      "epoch": 1.3203258597147185,
      "grad_norm": 0.3996255099773407,
      "learning_rate": 0.0001240228701217394,
      "loss": 2.3674,
      "step": 64180
    },
    {
      "epoch": 1.3205315792315862,
      "grad_norm": 0.47248750925064087,
      "learning_rate": 0.00012400131076338326,
      "loss": 2.3567,
      "step": 64190
    },
    {
      "epoch": 1.3207372987484538,
      "grad_norm": 0.4087834656238556,
      "learning_rate": 0.00012397975022117327,
      "loss": 2.3795,
      "step": 64200
    },
    {
      "epoch": 1.3209430182653217,
      "grad_norm": 0.4416442811489105,
      "learning_rate": 0.00012395818849617293,
      "loss": 2.3894,
      "step": 64210
    },
    {
      "epoch": 1.3211487377821893,
      "grad_norm": 0.4018564820289612,
      "learning_rate": 0.00012393662558944567,
      "loss": 2.4067,
      "step": 64220
    },
    {
      "epoch": 1.321354457299057,
      "grad_norm": 0.45781052112579346,
      "learning_rate": 0.0001239150615020551,
      "loss": 2.4069,
      "step": 64230
    },
    {
      "epoch": 1.3215601768159249,
      "grad_norm": 0.42370855808258057,
      "learning_rate": 0.00012389349623506487,
      "loss": 2.3889,
      "step": 64240
    },
    {
      "epoch": 1.3217658963327925,
      "grad_norm": 0.4217447340488434,
      "learning_rate": 0.0001238719297895387,
      "loss": 2.4267,
      "step": 64250
    },
    {
      "epoch": 1.3219716158496602,
      "grad_norm": 0.46649253368377686,
      "learning_rate": 0.0001238503621665403,
      "loss": 2.4026,
      "step": 64260
    },
    {
      "epoch": 1.3221773353665278,
      "grad_norm": 0.44974184036254883,
      "learning_rate": 0.0001238287933671335,
      "loss": 2.4571,
      "step": 64270
    },
    {
      "epoch": 1.3223830548833955,
      "grad_norm": 0.47535622119903564,
      "learning_rate": 0.00012380722339238218,
      "loss": 2.4137,
      "step": 64280
    },
    {
      "epoch": 1.3225887744002633,
      "grad_norm": 0.41117915511131287,
      "learning_rate": 0.00012378565224335027,
      "loss": 2.4021,
      "step": 64290
    },
    {
      "epoch": 1.322794493917131,
      "grad_norm": 0.38926857709884644,
      "learning_rate": 0.00012376407992110172,
      "loss": 2.4039,
      "step": 64300
    },
    {
      "epoch": 1.3230002134339989,
      "grad_norm": 0.39617034792900085,
      "learning_rate": 0.00012374250642670063,
      "loss": 2.3882,
      "step": 64310
    },
    {
      "epoch": 1.3232059329508665,
      "grad_norm": 0.43310844898223877,
      "learning_rate": 0.0001237209317612111,
      "loss": 2.3576,
      "step": 64320
    },
    {
      "epoch": 1.3234116524677342,
      "grad_norm": 0.4314500689506531,
      "learning_rate": 0.00012369935592569724,
      "loss": 2.4155,
      "step": 64330
    },
    {
      "epoch": 1.3236173719846018,
      "grad_norm": 0.4375821650028229,
      "learning_rate": 0.00012367777892122335,
      "loss": 2.39,
      "step": 64340
    },
    {
      "epoch": 1.3238230915014695,
      "grad_norm": 0.4521533250808716,
      "learning_rate": 0.00012365620074885359,
      "loss": 2.4312,
      "step": 64350
    },
    {
      "epoch": 1.3240288110183374,
      "grad_norm": 0.43865451216697693,
      "learning_rate": 0.00012363462140965238,
      "loss": 2.3807,
      "step": 64360
    },
    {
      "epoch": 1.324234530535205,
      "grad_norm": 0.5172328948974609,
      "learning_rate": 0.00012361304090468408,
      "loss": 2.3983,
      "step": 64370
    },
    {
      "epoch": 1.3244402500520729,
      "grad_norm": 0.43217042088508606,
      "learning_rate": 0.00012359145923501315,
      "loss": 2.3494,
      "step": 64380
    },
    {
      "epoch": 1.3246459695689405,
      "grad_norm": 0.42889657616615295,
      "learning_rate": 0.0001235698764017041,
      "loss": 2.401,
      "step": 64390
    },
    {
      "epoch": 1.3248516890858082,
      "grad_norm": 0.44537627696990967,
      "learning_rate": 0.0001235482924058215,
      "loss": 2.3991,
      "step": 64400
    },
    {
      "epoch": 1.3250574086026758,
      "grad_norm": 0.4413646161556244,
      "learning_rate": 0.00012352670724842993,
      "loss": 2.4282,
      "step": 64410
    },
    {
      "epoch": 1.3252631281195435,
      "grad_norm": 0.38524767756462097,
      "learning_rate": 0.00012350512093059414,
      "loss": 2.4676,
      "step": 64420
    },
    {
      "epoch": 1.3254688476364114,
      "grad_norm": 0.45480549335479736,
      "learning_rate": 0.0001234835334533788,
      "loss": 2.4561,
      "step": 64430
    },
    {
      "epoch": 1.325674567153279,
      "grad_norm": 0.4490712881088257,
      "learning_rate": 0.00012346194481784872,
      "loss": 2.3843,
      "step": 64440
    },
    {
      "epoch": 1.3258802866701467,
      "grad_norm": 0.4107406437397003,
      "learning_rate": 0.00012344035502506875,
      "loss": 2.3713,
      "step": 64450
    },
    {
      "epoch": 1.3260860061870146,
      "grad_norm": 0.4140285551548004,
      "learning_rate": 0.0001234187640761038,
      "loss": 2.3731,
      "step": 64460
    },
    {
      "epoch": 1.3262917257038822,
      "grad_norm": 0.44005560874938965,
      "learning_rate": 0.00012339717197201884,
      "loss": 2.4255,
      "step": 64470
    },
    {
      "epoch": 1.3264974452207499,
      "grad_norm": 0.4558951258659363,
      "learning_rate": 0.00012337557871387888,
      "loss": 2.3631,
      "step": 64480
    },
    {
      "epoch": 1.3267031647376175,
      "grad_norm": 0.5115065574645996,
      "learning_rate": 0.000123353984302749,
      "loss": 2.3992,
      "step": 64490
    },
    {
      "epoch": 1.3269088842544854,
      "grad_norm": 0.4030287265777588,
      "learning_rate": 0.00012333238873969438,
      "loss": 2.3981,
      "step": 64500
    },
    {
      "epoch": 1.327114603771353,
      "grad_norm": 0.42388156056404114,
      "learning_rate": 0.00012331079202578012,
      "loss": 2.3781,
      "step": 64510
    },
    {
      "epoch": 1.3273203232882207,
      "grad_norm": 0.45150062441825867,
      "learning_rate": 0.00012328919416207154,
      "loss": 2.4416,
      "step": 64520
    },
    {
      "epoch": 1.3275260428050886,
      "grad_norm": 0.40596380829811096,
      "learning_rate": 0.00012326759514963393,
      "loss": 2.4244,
      "step": 64530
    },
    {
      "epoch": 1.3277317623219562,
      "grad_norm": 0.42440134286880493,
      "learning_rate": 0.00012324599498953265,
      "loss": 2.4489,
      "step": 64540
    },
    {
      "epoch": 1.3279374818388239,
      "grad_norm": 0.4504433274269104,
      "learning_rate": 0.00012322439368283306,
      "loss": 2.3899,
      "step": 64550
    },
    {
      "epoch": 1.3281432013556915,
      "grad_norm": 0.43717074394226074,
      "learning_rate": 0.00012320279123060075,
      "loss": 2.467,
      "step": 64560
    },
    {
      "epoch": 1.3283489208725594,
      "grad_norm": 0.4404534101486206,
      "learning_rate": 0.00012318118763390116,
      "loss": 2.4178,
      "step": 64570
    },
    {
      "epoch": 1.328554640389427,
      "grad_norm": 0.47230395674705505,
      "learning_rate": 0.00012315958289379992,
      "loss": 2.4217,
      "step": 64580
    },
    {
      "epoch": 1.3287603599062947,
      "grad_norm": 0.43711310625076294,
      "learning_rate": 0.00012313797701136262,
      "loss": 2.4092,
      "step": 64590
    },
    {
      "epoch": 1.3289660794231626,
      "grad_norm": 0.5197145342826843,
      "learning_rate": 0.00012311636998765506,
      "loss": 2.4071,
      "step": 64600
    },
    {
      "epoch": 1.3291717989400302,
      "grad_norm": 0.42862480878829956,
      "learning_rate": 0.0001230947618237429,
      "loss": 2.3938,
      "step": 64610
    },
    {
      "epoch": 1.3293775184568979,
      "grad_norm": 0.4227966070175171,
      "learning_rate": 0.00012307315252069198,
      "loss": 2.4474,
      "step": 64620
    },
    {
      "epoch": 1.3295832379737655,
      "grad_norm": 0.44170984625816345,
      "learning_rate": 0.00012305154207956822,
      "loss": 2.4296,
      "step": 64630
    },
    {
      "epoch": 1.3297889574906334,
      "grad_norm": 0.4303295314311981,
      "learning_rate": 0.00012302993050143746,
      "loss": 2.4167,
      "step": 64640
    },
    {
      "epoch": 1.329994677007501,
      "grad_norm": 0.42676249146461487,
      "learning_rate": 0.00012300831778736576,
      "loss": 2.3347,
      "step": 64650
    },
    {
      "epoch": 1.3302003965243687,
      "grad_norm": 0.41107216477394104,
      "learning_rate": 0.00012298670393841914,
      "loss": 2.432,
      "step": 64660
    },
    {
      "epoch": 1.3304061160412366,
      "grad_norm": 0.44827497005462646,
      "learning_rate": 0.00012296508895566364,
      "loss": 2.4043,
      "step": 64670
    },
    {
      "epoch": 1.3306118355581043,
      "grad_norm": 0.44903960824012756,
      "learning_rate": 0.00012294347284016544,
      "loss": 2.3412,
      "step": 64680
    },
    {
      "epoch": 1.330817555074972,
      "grad_norm": 0.44533973932266235,
      "learning_rate": 0.00012292185559299078,
      "loss": 2.3857,
      "step": 64690
    },
    {
      "epoch": 1.3310232745918396,
      "grad_norm": 0.43279990553855896,
      "learning_rate": 0.0001229002372152059,
      "loss": 2.4052,
      "step": 64700
    },
    {
      "epoch": 1.3312289941087072,
      "grad_norm": 0.39480850100517273,
      "learning_rate": 0.00012287861770787705,
      "loss": 2.402,
      "step": 64710
    },
    {
      "epoch": 1.331434713625575,
      "grad_norm": 0.4260481894016266,
      "learning_rate": 0.00012285699707207073,
      "loss": 2.3862,
      "step": 64720
    },
    {
      "epoch": 1.3316404331424427,
      "grad_norm": 0.44346150755882263,
      "learning_rate": 0.00012283537530885326,
      "loss": 2.4527,
      "step": 64730
    },
    {
      "epoch": 1.3318461526593106,
      "grad_norm": 0.42482277750968933,
      "learning_rate": 0.0001228137524192912,
      "loss": 2.4396,
      "step": 64740
    },
    {
      "epoch": 1.3320518721761783,
      "grad_norm": 0.442975789308548,
      "learning_rate": 0.000122792128404451,
      "loss": 2.3697,
      "step": 64750
    },
    {
      "epoch": 1.332257591693046,
      "grad_norm": 0.42074933648109436,
      "learning_rate": 0.00012277050326539937,
      "loss": 2.3851,
      "step": 64760
    },
    {
      "epoch": 1.3324633112099136,
      "grad_norm": 0.427960604429245,
      "learning_rate": 0.00012274887700320286,
      "loss": 2.3547,
      "step": 64770
    },
    {
      "epoch": 1.3326690307267812,
      "grad_norm": 0.42462554574012756,
      "learning_rate": 0.00012272724961892822,
      "loss": 2.39,
      "step": 64780
    },
    {
      "epoch": 1.332874750243649,
      "grad_norm": 0.4491763114929199,
      "learning_rate": 0.00012270562111364218,
      "loss": 2.4243,
      "step": 64790
    },
    {
      "epoch": 1.3330804697605168,
      "grad_norm": 0.4097800850868225,
      "learning_rate": 0.00012268399148841164,
      "loss": 2.3649,
      "step": 64800
    },
    {
      "epoch": 1.3332861892773844,
      "grad_norm": 0.44716599583625793,
      "learning_rate": 0.00012266236074430338,
      "loss": 2.3754,
      "step": 64810
    },
    {
      "epoch": 1.3334919087942523,
      "grad_norm": 0.4026004672050476,
      "learning_rate": 0.00012264072888238436,
      "loss": 2.3487,
      "step": 64820
    },
    {
      "epoch": 1.33369762831112,
      "grad_norm": 0.38650017976760864,
      "learning_rate": 0.00012261909590372154,
      "loss": 2.4072,
      "step": 64830
    },
    {
      "epoch": 1.3339033478279876,
      "grad_norm": 0.4293275475502014,
      "learning_rate": 0.000122597461809382,
      "loss": 2.323,
      "step": 64840
    },
    {
      "epoch": 1.3341090673448552,
      "grad_norm": 0.43750622868537903,
      "learning_rate": 0.00012257582660043283,
      "loss": 2.3905,
      "step": 64850
    },
    {
      "epoch": 1.3343147868617231,
      "grad_norm": 0.43562397360801697,
      "learning_rate": 0.00012255419027794114,
      "loss": 2.4488,
      "step": 64860
    },
    {
      "epoch": 1.3345205063785908,
      "grad_norm": 0.4194098114967346,
      "learning_rate": 0.0001225325528429741,
      "loss": 2.3834,
      "step": 64870
    },
    {
      "epoch": 1.3347262258954584,
      "grad_norm": 0.4318685531616211,
      "learning_rate": 0.00012251091429659911,
      "loss": 2.4064,
      "step": 64880
    },
    {
      "epoch": 1.3349319454123263,
      "grad_norm": 0.43281784653663635,
      "learning_rate": 0.00012248927463988333,
      "loss": 2.36,
      "step": 64890
    },
    {
      "epoch": 1.335137664929194,
      "grad_norm": 0.4472925066947937,
      "learning_rate": 0.0001224676338738942,
      "loss": 2.4128,
      "step": 64900
    },
    {
      "epoch": 1.3353433844460616,
      "grad_norm": 0.40254950523376465,
      "learning_rate": 0.0001224459919996991,
      "loss": 2.4174,
      "step": 64910
    },
    {
      "epoch": 1.3355491039629293,
      "grad_norm": 0.4173813462257385,
      "learning_rate": 0.00012242434901836558,
      "loss": 2.4649,
      "step": 64920
    },
    {
      "epoch": 1.3357548234797971,
      "grad_norm": 0.43808332085609436,
      "learning_rate": 0.00012240270493096107,
      "loss": 2.3901,
      "step": 64930
    },
    {
      "epoch": 1.3359605429966648,
      "grad_norm": 0.4551191031932831,
      "learning_rate": 0.00012238105973855324,
      "loss": 2.3432,
      "step": 64940
    },
    {
      "epoch": 1.3361662625135324,
      "grad_norm": 0.43307679891586304,
      "learning_rate": 0.00012235941344220967,
      "loss": 2.3988,
      "step": 64950
    },
    {
      "epoch": 1.3363719820304003,
      "grad_norm": 0.42553555965423584,
      "learning_rate": 0.00012233776604299808,
      "loss": 2.4276,
      "step": 64960
    },
    {
      "epoch": 1.336577701547268,
      "grad_norm": 0.4229508936405182,
      "learning_rate": 0.00012231611754198623,
      "loss": 2.4352,
      "step": 64970
    },
    {
      "epoch": 1.3367834210641356,
      "grad_norm": 0.4282478392124176,
      "learning_rate": 0.00012229446794024193,
      "loss": 2.4008,
      "step": 64980
    },
    {
      "epoch": 1.3369891405810033,
      "grad_norm": 0.5127312541007996,
      "learning_rate": 0.00012227281723883296,
      "loss": 2.3909,
      "step": 64990
    },
    {
      "epoch": 1.3371948600978711,
      "grad_norm": 0.4227328896522522,
      "learning_rate": 0.00012225116543882733,
      "loss": 2.3731,
      "step": 65000
    },
    {
      "epoch": 1.3374005796147388,
      "grad_norm": 0.4679197371006012,
      "learning_rate": 0.00012222951254129297,
      "loss": 2.379,
      "step": 65010
    },
    {
      "epoch": 1.3376062991316064,
      "grad_norm": 0.44385263323783875,
      "learning_rate": 0.0001222078585472979,
      "loss": 2.4101,
      "step": 65020
    },
    {
      "epoch": 1.3378120186484743,
      "grad_norm": 0.4592258632183075,
      "learning_rate": 0.00012218620345791012,
      "loss": 2.4462,
      "step": 65030
    },
    {
      "epoch": 1.338017738165342,
      "grad_norm": 0.42743536829948425,
      "learning_rate": 0.0001221645472741979,
      "loss": 2.4202,
      "step": 65040
    },
    {
      "epoch": 1.3382234576822096,
      "grad_norm": 0.39881545305252075,
      "learning_rate": 0.0001221428899972293,
      "loss": 2.3701,
      "step": 65050
    },
    {
      "epoch": 1.3384291771990773,
      "grad_norm": 0.5093467235565186,
      "learning_rate": 0.00012212123162807262,
      "loss": 2.437,
      "step": 65060
    },
    {
      "epoch": 1.338634896715945,
      "grad_norm": 0.4306408166885376,
      "learning_rate": 0.00012209957216779613,
      "loss": 2.3886,
      "step": 65070
    },
    {
      "epoch": 1.3388406162328128,
      "grad_norm": 0.44029736518859863,
      "learning_rate": 0.00012207791161746815,
      "loss": 2.4465,
      "step": 65080
    },
    {
      "epoch": 1.3390463357496805,
      "grad_norm": 0.426827073097229,
      "learning_rate": 0.00012205624997815712,
      "loss": 2.3962,
      "step": 65090
    },
    {
      "epoch": 1.3392520552665483,
      "grad_norm": 0.46331796050071716,
      "learning_rate": 0.00012203458725093147,
      "loss": 2.3861,
      "step": 65100
    },
    {
      "epoch": 1.339457774783416,
      "grad_norm": 0.4477970004081726,
      "learning_rate": 0.0001220129234368597,
      "loss": 2.4345,
      "step": 65110
    },
    {
      "epoch": 1.3396634943002836,
      "grad_norm": 0.4587234854698181,
      "learning_rate": 0.0001219912585370104,
      "loss": 2.3576,
      "step": 65120
    },
    {
      "epoch": 1.3398692138171513,
      "grad_norm": 0.4225471317768097,
      "learning_rate": 0.0001219695925524521,
      "loss": 2.4073,
      "step": 65130
    },
    {
      "epoch": 1.340074933334019,
      "grad_norm": 0.47176387906074524,
      "learning_rate": 0.00012194792548425358,
      "loss": 2.4279,
      "step": 65140
    },
    {
      "epoch": 1.3402806528508868,
      "grad_norm": 0.403186172246933,
      "learning_rate": 0.00012192625733348345,
      "loss": 2.4202,
      "step": 65150
    },
    {
      "epoch": 1.3404863723677545,
      "grad_norm": 0.42621007561683655,
      "learning_rate": 0.00012190458810121054,
      "loss": 2.4438,
      "step": 65160
    },
    {
      "epoch": 1.3406920918846221,
      "grad_norm": 0.39758846163749695,
      "learning_rate": 0.0001218829177885037,
      "loss": 2.3896,
      "step": 65170
    },
    {
      "epoch": 1.34089781140149,
      "grad_norm": 0.45642155408859253,
      "learning_rate": 0.00012186124639643173,
      "loss": 2.4168,
      "step": 65180
    },
    {
      "epoch": 1.3411035309183577,
      "grad_norm": 0.40274277329444885,
      "learning_rate": 0.0001218395739260636,
      "loss": 2.4015,
      "step": 65190
    },
    {
      "epoch": 1.3413092504352253,
      "grad_norm": 0.5151703953742981,
      "learning_rate": 0.00012181790037846835,
      "loss": 2.3996,
      "step": 65200
    },
    {
      "epoch": 1.341514969952093,
      "grad_norm": 0.4570361375808716,
      "learning_rate": 0.00012179622575471493,
      "loss": 2.4308,
      "step": 65210
    },
    {
      "epoch": 1.3417206894689608,
      "grad_norm": 0.4434303641319275,
      "learning_rate": 0.00012177455005587246,
      "loss": 2.4619,
      "step": 65220
    },
    {
      "epoch": 1.3419264089858285,
      "grad_norm": 0.42542365193367004,
      "learning_rate": 0.00012175287328301012,
      "loss": 2.3642,
      "step": 65230
    },
    {
      "epoch": 1.3421321285026961,
      "grad_norm": 0.4449249804019928,
      "learning_rate": 0.00012173119543719707,
      "loss": 2.4225,
      "step": 65240
    },
    {
      "epoch": 1.342337848019564,
      "grad_norm": 0.46627116203308105,
      "learning_rate": 0.00012170951651950255,
      "loss": 2.421,
      "step": 65250
    },
    {
      "epoch": 1.3425435675364317,
      "grad_norm": 0.460617333650589,
      "learning_rate": 0.00012168783653099591,
      "loss": 2.3696,
      "step": 65260
    },
    {
      "epoch": 1.3427492870532993,
      "grad_norm": 0.4296024441719055,
      "learning_rate": 0.00012166615547274649,
      "loss": 2.3862,
      "step": 65270
    },
    {
      "epoch": 1.342955006570167,
      "grad_norm": 0.45119479298591614,
      "learning_rate": 0.00012164447334582365,
      "loss": 2.3343,
      "step": 65280
    },
    {
      "epoch": 1.3431607260870349,
      "grad_norm": 0.49852320551872253,
      "learning_rate": 0.0001216227901512969,
      "loss": 2.3657,
      "step": 65290
    },
    {
      "epoch": 1.3433664456039025,
      "grad_norm": 0.4253566861152649,
      "learning_rate": 0.00012160110589023577,
      "loss": 2.3819,
      "step": 65300
    },
    {
      "epoch": 1.3435721651207702,
      "grad_norm": 0.4525664746761322,
      "learning_rate": 0.00012157942056370976,
      "loss": 2.3883,
      "step": 65310
    },
    {
      "epoch": 1.343777884637638,
      "grad_norm": 0.39599940180778503,
      "learning_rate": 0.00012155773417278852,
      "loss": 2.4286,
      "step": 65320
    },
    {
      "epoch": 1.3439836041545057,
      "grad_norm": 0.3952501118183136,
      "learning_rate": 0.00012153604671854178,
      "loss": 2.3978,
      "step": 65330
    },
    {
      "epoch": 1.3441893236713733,
      "grad_norm": 0.425780326128006,
      "learning_rate": 0.00012151435820203919,
      "loss": 2.423,
      "step": 65340
    },
    {
      "epoch": 1.344395043188241,
      "grad_norm": 0.4237526059150696,
      "learning_rate": 0.00012149266862435054,
      "loss": 2.3988,
      "step": 65350
    },
    {
      "epoch": 1.3446007627051089,
      "grad_norm": 0.453671932220459,
      "learning_rate": 0.00012147097798654566,
      "loss": 2.4029,
      "step": 65360
    },
    {
      "epoch": 1.3448064822219765,
      "grad_norm": 0.45589888095855713,
      "learning_rate": 0.00012144928628969445,
      "loss": 2.4389,
      "step": 65370
    },
    {
      "epoch": 1.3450122017388442,
      "grad_norm": 0.46432918310165405,
      "learning_rate": 0.00012142759353486682,
      "loss": 2.4042,
      "step": 65380
    },
    {
      "epoch": 1.345217921255712,
      "grad_norm": 0.41209501028060913,
      "learning_rate": 0.00012140589972313278,
      "loss": 2.3936,
      "step": 65390
    },
    {
      "epoch": 1.3454236407725797,
      "grad_norm": 0.3968106508255005,
      "learning_rate": 0.00012138420485556234,
      "loss": 2.3813,
      "step": 65400
    },
    {
      "epoch": 1.3456293602894474,
      "grad_norm": 0.44019803404808044,
      "learning_rate": 0.00012136250893322562,
      "loss": 2.4092,
      "step": 65410
    },
    {
      "epoch": 1.345835079806315,
      "grad_norm": 0.45165687799453735,
      "learning_rate": 0.00012134081195719275,
      "loss": 2.39,
      "step": 65420
    },
    {
      "epoch": 1.3460407993231827,
      "grad_norm": 0.4710462987422943,
      "learning_rate": 0.00012131911392853395,
      "loss": 2.3728,
      "step": 65430
    },
    {
      "epoch": 1.3462465188400505,
      "grad_norm": 0.4374473989009857,
      "learning_rate": 0.00012129741484831935,
      "loss": 2.4786,
      "step": 65440
    },
    {
      "epoch": 1.3464522383569182,
      "grad_norm": 0.4201085865497589,
      "learning_rate": 0.00012127571471761938,
      "loss": 2.4204,
      "step": 65450
    },
    {
      "epoch": 1.346657957873786,
      "grad_norm": 0.4547068178653717,
      "learning_rate": 0.00012125401353750438,
      "loss": 2.3812,
      "step": 65460
    },
    {
      "epoch": 1.3468636773906537,
      "grad_norm": 0.42803847789764404,
      "learning_rate": 0.00012123231130904467,
      "loss": 2.3494,
      "step": 65470
    },
    {
      "epoch": 1.3470693969075214,
      "grad_norm": 0.42913076281547546,
      "learning_rate": 0.00012121060803331073,
      "loss": 2.3768,
      "step": 65480
    },
    {
      "epoch": 1.347275116424389,
      "grad_norm": 0.4277661144733429,
      "learning_rate": 0.00012118890371137313,
      "loss": 2.4002,
      "step": 65490
    },
    {
      "epoch": 1.3474808359412567,
      "grad_norm": 0.3842325210571289,
      "learning_rate": 0.00012116719834430233,
      "loss": 2.3565,
      "step": 65500
    },
    {
      "epoch": 1.3476865554581245,
      "grad_norm": 0.42208120226860046,
      "learning_rate": 0.000121145491933169,
      "loss": 2.4377,
      "step": 65510
    },
    {
      "epoch": 1.3478922749749922,
      "grad_norm": 0.45631143450737,
      "learning_rate": 0.00012112378447904378,
      "loss": 2.3629,
      "step": 65520
    },
    {
      "epoch": 1.34809799449186,
      "grad_norm": 0.4811975657939911,
      "learning_rate": 0.0001211020759829974,
      "loss": 2.4256,
      "step": 65530
    },
    {
      "epoch": 1.3483037140087277,
      "grad_norm": 0.4262157380580902,
      "learning_rate": 0.00012108036644610057,
      "loss": 2.4315,
      "step": 65540
    },
    {
      "epoch": 1.3485094335255954,
      "grad_norm": 0.5141247510910034,
      "learning_rate": 0.00012105865586942415,
      "loss": 2.4274,
      "step": 65550
    },
    {
      "epoch": 1.348715153042463,
      "grad_norm": 0.4314129948616028,
      "learning_rate": 0.000121036944254039,
      "loss": 2.4435,
      "step": 65560
    },
    {
      "epoch": 1.3489208725593307,
      "grad_norm": 0.4121115505695343,
      "learning_rate": 0.00012101523160101603,
      "loss": 2.4309,
      "step": 65570
    },
    {
      "epoch": 1.3491265920761986,
      "grad_norm": 0.3946932852268219,
      "learning_rate": 0.00012099351791142619,
      "loss": 2.4543,
      "step": 65580
    },
    {
      "epoch": 1.3493323115930662,
      "grad_norm": 0.41782087087631226,
      "learning_rate": 0.00012097180318634053,
      "loss": 2.3847,
      "step": 65590
    },
    {
      "epoch": 1.3495380311099339,
      "grad_norm": 0.43035081028938293,
      "learning_rate": 0.00012095008742683007,
      "loss": 2.399,
      "step": 65600
    },
    {
      "epoch": 1.3497437506268017,
      "grad_norm": 0.5614085793495178,
      "learning_rate": 0.00012092837063396599,
      "loss": 2.4176,
      "step": 65610
    },
    {
      "epoch": 1.3499494701436694,
      "grad_norm": 0.44515034556388855,
      "learning_rate": 0.00012090665280881945,
      "loss": 2.3719,
      "step": 65620
    },
    {
      "epoch": 1.350155189660537,
      "grad_norm": 0.4583323895931244,
      "learning_rate": 0.00012088493395246164,
      "loss": 2.4111,
      "step": 65630
    },
    {
      "epoch": 1.3503609091774047,
      "grad_norm": 0.41406193375587463,
      "learning_rate": 0.00012086321406596379,
      "loss": 2.3979,
      "step": 65640
    },
    {
      "epoch": 1.3505666286942726,
      "grad_norm": 0.440030962228775,
      "learning_rate": 0.00012084149315039737,
      "loss": 2.4214,
      "step": 65650
    },
    {
      "epoch": 1.3507723482111402,
      "grad_norm": 0.491510272026062,
      "learning_rate": 0.0001208197712068336,
      "loss": 2.4477,
      "step": 65660
    },
    {
      "epoch": 1.3509780677280079,
      "grad_norm": 0.4341754615306854,
      "learning_rate": 0.00012079804823634401,
      "loss": 2.3939,
      "step": 65670
    },
    {
      "epoch": 1.3511837872448758,
      "grad_norm": 0.4296690821647644,
      "learning_rate": 0.00012077632424000003,
      "loss": 2.3638,
      "step": 65680
    },
    {
      "epoch": 1.3513895067617434,
      "grad_norm": 0.42405712604522705,
      "learning_rate": 0.00012075459921887317,
      "loss": 2.3807,
      "step": 65690
    },
    {
      "epoch": 1.351595226278611,
      "grad_norm": 0.4114924967288971,
      "learning_rate": 0.00012073287317403505,
      "loss": 2.4017,
      "step": 65700
    },
    {
      "epoch": 1.3518009457954787,
      "grad_norm": 0.43368422985076904,
      "learning_rate": 0.00012071114610655728,
      "loss": 2.4324,
      "step": 65710
    },
    {
      "epoch": 1.3520066653123466,
      "grad_norm": 0.5738319754600525,
      "learning_rate": 0.00012068941801751152,
      "loss": 2.3919,
      "step": 65720
    },
    {
      "epoch": 1.3522123848292142,
      "grad_norm": 0.4116824269294739,
      "learning_rate": 0.00012066768890796952,
      "loss": 2.409,
      "step": 65730
    },
    {
      "epoch": 1.352418104346082,
      "grad_norm": 0.4037144184112549,
      "learning_rate": 0.00012064595877900304,
      "loss": 2.4137,
      "step": 65740
    },
    {
      "epoch": 1.3526238238629498,
      "grad_norm": 0.4008055329322815,
      "learning_rate": 0.00012062422763168394,
      "loss": 2.3942,
      "step": 65750
    },
    {
      "epoch": 1.3528295433798174,
      "grad_norm": 0.5008202195167542,
      "learning_rate": 0.00012060249546708406,
      "loss": 2.4106,
      "step": 65760
    },
    {
      "epoch": 1.353035262896685,
      "grad_norm": 0.42693156003952026,
      "learning_rate": 0.00012058076228627535,
      "loss": 2.4,
      "step": 65770
    },
    {
      "epoch": 1.3532409824135527,
      "grad_norm": 0.41757044196128845,
      "learning_rate": 0.00012055902809032982,
      "loss": 2.3259,
      "step": 65780
    },
    {
      "epoch": 1.3534467019304204,
      "grad_norm": 0.437212198972702,
      "learning_rate": 0.00012053729288031946,
      "loss": 2.3829,
      "step": 65790
    },
    {
      "epoch": 1.3536524214472883,
      "grad_norm": 0.42982354760169983,
      "learning_rate": 0.0001205155566573163,
      "loss": 2.3845,
      "step": 65800
    },
    {
      "epoch": 1.353858140964156,
      "grad_norm": 0.38527584075927734,
      "learning_rate": 0.0001204938194223926,
      "loss": 2.4357,
      "step": 65810
    },
    {
      "epoch": 1.3540638604810238,
      "grad_norm": 0.4385490417480469,
      "learning_rate": 0.00012047208117662043,
      "loss": 2.3835,
      "step": 65820
    },
    {
      "epoch": 1.3542695799978914,
      "grad_norm": 0.43084701895713806,
      "learning_rate": 0.00012045034192107205,
      "loss": 2.3843,
      "step": 65830
    },
    {
      "epoch": 1.354475299514759,
      "grad_norm": 0.5450106263160706,
      "learning_rate": 0.00012042860165681975,
      "loss": 2.3784,
      "step": 65840
    },
    {
      "epoch": 1.3546810190316267,
      "grad_norm": 0.46457383036613464,
      "learning_rate": 0.00012040686038493588,
      "loss": 2.4545,
      "step": 65850
    },
    {
      "epoch": 1.3548867385484944,
      "grad_norm": 0.4272562563419342,
      "learning_rate": 0.00012038511810649276,
      "loss": 2.4504,
      "step": 65860
    },
    {
      "epoch": 1.3550924580653623,
      "grad_norm": 0.4276297688484192,
      "learning_rate": 0.00012036337482256284,
      "loss": 2.4336,
      "step": 65870
    },
    {
      "epoch": 1.35529817758223,
      "grad_norm": 0.4003278315067291,
      "learning_rate": 0.00012034163053421863,
      "loss": 2.3573,
      "step": 65880
    },
    {
      "epoch": 1.3555038970990978,
      "grad_norm": 0.446282297372818,
      "learning_rate": 0.00012031988524253261,
      "loss": 2.4028,
      "step": 65890
    },
    {
      "epoch": 1.3557096166159655,
      "grad_norm": 0.39519309997558594,
      "learning_rate": 0.00012029813894857742,
      "loss": 2.4355,
      "step": 65900
    },
    {
      "epoch": 1.355915336132833,
      "grad_norm": 0.43536075949668884,
      "learning_rate": 0.00012027639165342563,
      "loss": 2.3906,
      "step": 65910
    },
    {
      "epoch": 1.3561210556497008,
      "grad_norm": 0.4556944668292999,
      "learning_rate": 0.00012025464335814991,
      "loss": 2.3767,
      "step": 65920
    },
    {
      "epoch": 1.3563267751665684,
      "grad_norm": 0.4523416757583618,
      "learning_rate": 0.00012023289406382303,
      "loss": 2.424,
      "step": 65930
    },
    {
      "epoch": 1.3565324946834363,
      "grad_norm": 0.3999606668949127,
      "learning_rate": 0.00012021114377151777,
      "loss": 2.3569,
      "step": 65940
    },
    {
      "epoch": 1.356738214200304,
      "grad_norm": 0.3772355318069458,
      "learning_rate": 0.00012018939248230688,
      "loss": 2.4463,
      "step": 65950
    },
    {
      "epoch": 1.3569439337171716,
      "grad_norm": 0.510334312915802,
      "learning_rate": 0.00012016764019726327,
      "loss": 2.4232,
      "step": 65960
    },
    {
      "epoch": 1.3571496532340395,
      "grad_norm": 0.4796558916568756,
      "learning_rate": 0.00012014588691745992,
      "loss": 2.468,
      "step": 65970
    },
    {
      "epoch": 1.3573553727509071,
      "grad_norm": 0.3955058753490448,
      "learning_rate": 0.0001201241326439697,
      "loss": 2.3916,
      "step": 65980
    },
    {
      "epoch": 1.3575610922677748,
      "grad_norm": 0.4572429358959198,
      "learning_rate": 0.00012010237737786572,
      "loss": 2.3968,
      "step": 65990
    },
    {
      "epoch": 1.3577668117846424,
      "grad_norm": 0.4228264391422272,
      "learning_rate": 0.00012008062112022099,
      "loss": 2.3938,
      "step": 66000
    },
    {
      "epoch": 1.3579725313015103,
      "grad_norm": 0.45345351099967957,
      "learning_rate": 0.00012005886387210865,
      "loss": 2.3689,
      "step": 66010
    },
    {
      "epoch": 1.358178250818378,
      "grad_norm": 0.39569956064224243,
      "learning_rate": 0.00012003710563460184,
      "loss": 2.4646,
      "step": 66020
    },
    {
      "epoch": 1.3583839703352456,
      "grad_norm": 0.40515461564064026,
      "learning_rate": 0.00012001534640877383,
      "loss": 2.4188,
      "step": 66030
    },
    {
      "epoch": 1.3585896898521135,
      "grad_norm": 0.39628365635871887,
      "learning_rate": 0.00011999358619569783,
      "loss": 2.3457,
      "step": 66040
    },
    {
      "epoch": 1.3587954093689811,
      "grad_norm": 0.3930414915084839,
      "learning_rate": 0.0001199718249964472,
      "loss": 2.341,
      "step": 66050
    },
    {
      "epoch": 1.3590011288858488,
      "grad_norm": 0.4469253718852997,
      "learning_rate": 0.00011995006281209525,
      "loss": 2.379,
      "step": 66060
    },
    {
      "epoch": 1.3592068484027164,
      "grad_norm": 0.42777931690216064,
      "learning_rate": 0.00011992829964371543,
      "loss": 2.3619,
      "step": 66070
    },
    {
      "epoch": 1.3594125679195843,
      "grad_norm": 0.44339194893836975,
      "learning_rate": 0.00011990653549238118,
      "loss": 2.4183,
      "step": 66080
    },
    {
      "epoch": 1.359618287436452,
      "grad_norm": 0.44725143909454346,
      "learning_rate": 0.000119884770359166,
      "loss": 2.4537,
      "step": 66090
    },
    {
      "epoch": 1.3598240069533196,
      "grad_norm": 0.4192875921726227,
      "learning_rate": 0.00011986300424514349,
      "loss": 2.4421,
      "step": 66100
    },
    {
      "epoch": 1.3600297264701875,
      "grad_norm": 0.3980553448200226,
      "learning_rate": 0.00011984123715138723,
      "loss": 2.3834,
      "step": 66110
    },
    {
      "epoch": 1.3602354459870551,
      "grad_norm": 0.4386323094367981,
      "learning_rate": 0.00011981946907897079,
      "loss": 2.4688,
      "step": 66120
    },
    {
      "epoch": 1.3604411655039228,
      "grad_norm": 0.4835016429424286,
      "learning_rate": 0.00011979770002896801,
      "loss": 2.4631,
      "step": 66130
    },
    {
      "epoch": 1.3606468850207905,
      "grad_norm": 0.42931774258613586,
      "learning_rate": 0.00011977593000245256,
      "loss": 2.3431,
      "step": 66140
    },
    {
      "epoch": 1.360852604537658,
      "grad_norm": 0.468977153301239,
      "learning_rate": 0.00011975415900049823,
      "loss": 2.4177,
      "step": 66150
    },
    {
      "epoch": 1.361058324054526,
      "grad_norm": 0.40696778893470764,
      "learning_rate": 0.00011973238702417888,
      "loss": 2.3894,
      "step": 66160
    },
    {
      "epoch": 1.3612640435713936,
      "grad_norm": 0.4283802807331085,
      "learning_rate": 0.00011971061407456843,
      "loss": 2.4057,
      "step": 66170
    },
    {
      "epoch": 1.3614697630882615,
      "grad_norm": 0.4715767502784729,
      "learning_rate": 0.00011968884015274078,
      "loss": 2.3745,
      "step": 66180
    },
    {
      "epoch": 1.3616754826051292,
      "grad_norm": 0.4304371476173401,
      "learning_rate": 0.00011966706525976996,
      "loss": 2.3624,
      "step": 66190
    },
    {
      "epoch": 1.3618812021219968,
      "grad_norm": 0.4265836477279663,
      "learning_rate": 0.00011964528939672997,
      "loss": 2.4361,
      "step": 66200
    },
    {
      "epoch": 1.3620869216388645,
      "grad_norm": 0.42511066794395447,
      "learning_rate": 0.0001196235125646949,
      "loss": 2.3569,
      "step": 66210
    },
    {
      "epoch": 1.3622926411557321,
      "grad_norm": 0.43838080763816833,
      "learning_rate": 0.00011960173476473888,
      "loss": 2.4097,
      "step": 66220
    },
    {
      "epoch": 1.3624983606726,
      "grad_norm": 0.40691694617271423,
      "learning_rate": 0.00011957995599793614,
      "loss": 2.4108,
      "step": 66230
    },
    {
      "epoch": 1.3627040801894676,
      "grad_norm": 0.42777517437934875,
      "learning_rate": 0.00011955817626536084,
      "loss": 2.335,
      "step": 66240
    },
    {
      "epoch": 1.3629097997063355,
      "grad_norm": 0.4298061430454254,
      "learning_rate": 0.0001195363955680873,
      "loss": 2.3594,
      "step": 66250
    },
    {
      "epoch": 1.3631155192232032,
      "grad_norm": 0.3859638273715973,
      "learning_rate": 0.00011951461390718981,
      "loss": 2.3223,
      "step": 66260
    },
    {
      "epoch": 1.3633212387400708,
      "grad_norm": 0.4145103096961975,
      "learning_rate": 0.00011949283128374278,
      "loss": 2.3749,
      "step": 66270
    },
    {
      "epoch": 1.3635269582569385,
      "grad_norm": 0.4377497434616089,
      "learning_rate": 0.00011947104769882056,
      "loss": 2.424,
      "step": 66280
    },
    {
      "epoch": 1.3637326777738061,
      "grad_norm": 0.44840243458747864,
      "learning_rate": 0.00011944926315349772,
      "loss": 2.3442,
      "step": 66290
    },
    {
      "epoch": 1.363938397290674,
      "grad_norm": 0.5114670395851135,
      "learning_rate": 0.00011942747764884868,
      "loss": 2.4055,
      "step": 66300
    },
    {
      "epoch": 1.3641441168075417,
      "grad_norm": 0.4351784586906433,
      "learning_rate": 0.00011940569118594805,
      "loss": 2.3998,
      "step": 66310
    },
    {
      "epoch": 1.3643498363244093,
      "grad_norm": 0.401673287153244,
      "learning_rate": 0.00011938390376587042,
      "loss": 2.3931,
      "step": 66320
    },
    {
      "epoch": 1.3645555558412772,
      "grad_norm": 0.5120635032653809,
      "learning_rate": 0.00011936211538969042,
      "loss": 2.4589,
      "step": 66330
    },
    {
      "epoch": 1.3647612753581448,
      "grad_norm": 0.4134317934513092,
      "learning_rate": 0.0001193403260584828,
      "loss": 2.3343,
      "step": 66340
    },
    {
      "epoch": 1.3649669948750125,
      "grad_norm": 0.45128557085990906,
      "learning_rate": 0.00011931853577332229,
      "loss": 2.4169,
      "step": 66350
    },
    {
      "epoch": 1.3651727143918801,
      "grad_norm": 0.4699687659740448,
      "learning_rate": 0.00011929674453528368,
      "loss": 2.4434,
      "step": 66360
    },
    {
      "epoch": 1.365378433908748,
      "grad_norm": 0.51222825050354,
      "learning_rate": 0.00011927495234544182,
      "loss": 2.4206,
      "step": 66370
    },
    {
      "epoch": 1.3655841534256157,
      "grad_norm": 0.4042864441871643,
      "learning_rate": 0.0001192531592048716,
      "loss": 2.4299,
      "step": 66380
    },
    {
      "epoch": 1.3657898729424833,
      "grad_norm": 0.5009496212005615,
      "learning_rate": 0.00011923136511464794,
      "loss": 2.3831,
      "step": 66390
    },
    {
      "epoch": 1.3659955924593512,
      "grad_norm": 0.4357515275478363,
      "learning_rate": 0.00011920957007584584,
      "loss": 2.3999,
      "step": 66400
    },
    {
      "epoch": 1.3662013119762189,
      "grad_norm": 0.482150673866272,
      "learning_rate": 0.00011918777408954032,
      "loss": 2.3526,
      "step": 66410
    },
    {
      "epoch": 1.3664070314930865,
      "grad_norm": 0.44082948565483093,
      "learning_rate": 0.00011916597715680651,
      "loss": 2.3969,
      "step": 66420
    },
    {
      "epoch": 1.3666127510099542,
      "grad_norm": 0.4765620827674866,
      "learning_rate": 0.00011914417927871944,
      "loss": 2.405,
      "step": 66430
    },
    {
      "epoch": 1.366818470526822,
      "grad_norm": 0.44313693046569824,
      "learning_rate": 0.00011912238045635436,
      "loss": 2.3954,
      "step": 66440
    },
    {
      "epoch": 1.3670241900436897,
      "grad_norm": 0.4506726562976837,
      "learning_rate": 0.00011910058069078643,
      "loss": 2.4344,
      "step": 66450
    },
    {
      "epoch": 1.3672299095605573,
      "grad_norm": 0.4026224613189697,
      "learning_rate": 0.00011907877998309096,
      "loss": 2.4503,
      "step": 66460
    },
    {
      "epoch": 1.3674356290774252,
      "grad_norm": 0.43693140149116516,
      "learning_rate": 0.00011905697833434323,
      "loss": 2.3276,
      "step": 66470
    },
    {
      "epoch": 1.3676413485942929,
      "grad_norm": 0.4291776120662689,
      "learning_rate": 0.0001190351757456186,
      "loss": 2.4475,
      "step": 66480
    },
    {
      "epoch": 1.3678470681111605,
      "grad_norm": 0.500687301158905,
      "learning_rate": 0.00011901337221799249,
      "loss": 2.3749,
      "step": 66490
    },
    {
      "epoch": 1.3680527876280282,
      "grad_norm": 0.3908686935901642,
      "learning_rate": 0.00011899156775254035,
      "loss": 2.4108,
      "step": 66500
    },
    {
      "epoch": 1.368258507144896,
      "grad_norm": 0.463538259267807,
      "learning_rate": 0.00011896976235033763,
      "loss": 2.4303,
      "step": 66510
    },
    {
      "epoch": 1.3684642266617637,
      "grad_norm": 0.41005292534828186,
      "learning_rate": 0.00011894795601245996,
      "loss": 2.3946,
      "step": 66520
    },
    {
      "epoch": 1.3686699461786314,
      "grad_norm": 0.4058610498905182,
      "learning_rate": 0.00011892614873998282,
      "loss": 2.4128,
      "step": 66530
    },
    {
      "epoch": 1.3688756656954992,
      "grad_norm": 0.4323239028453827,
      "learning_rate": 0.00011890434053398191,
      "loss": 2.4013,
      "step": 66540
    },
    {
      "epoch": 1.3690813852123669,
      "grad_norm": 0.44351592659950256,
      "learning_rate": 0.00011888253139553294,
      "loss": 2.4117,
      "step": 66550
    },
    {
      "epoch": 1.3692871047292345,
      "grad_norm": 0.4666197597980499,
      "learning_rate": 0.00011886072132571156,
      "loss": 2.4361,
      "step": 66560
    },
    {
      "epoch": 1.3694928242461022,
      "grad_norm": 0.4614686667919159,
      "learning_rate": 0.00011883891032559355,
      "loss": 2.3799,
      "step": 66570
    },
    {
      "epoch": 1.3696985437629698,
      "grad_norm": 0.455244243144989,
      "learning_rate": 0.0001188170983962548,
      "loss": 2.4308,
      "step": 66580
    },
    {
      "epoch": 1.3699042632798377,
      "grad_norm": 0.42202404141426086,
      "learning_rate": 0.00011879528553877111,
      "loss": 2.3906,
      "step": 66590
    },
    {
      "epoch": 1.3701099827967054,
      "grad_norm": 0.4125513434410095,
      "learning_rate": 0.0001187734717542184,
      "loss": 2.3664,
      "step": 66600
    },
    {
      "epoch": 1.3703157023135732,
      "grad_norm": 0.45964041352272034,
      "learning_rate": 0.00011875165704367263,
      "loss": 2.4571,
      "step": 66610
    },
    {
      "epoch": 1.370521421830441,
      "grad_norm": 0.4281730055809021,
      "learning_rate": 0.00011872984140820982,
      "loss": 2.4032,
      "step": 66620
    },
    {
      "epoch": 1.3707271413473086,
      "grad_norm": 0.4680934548377991,
      "learning_rate": 0.000118708024848906,
      "loss": 2.4285,
      "step": 66630
    },
    {
      "epoch": 1.3709328608641762,
      "grad_norm": 0.50316321849823,
      "learning_rate": 0.00011868620736683724,
      "loss": 2.4244,
      "step": 66640
    },
    {
      "epoch": 1.3711385803810439,
      "grad_norm": 0.4234848618507385,
      "learning_rate": 0.0001186643889630797,
      "loss": 2.4637,
      "step": 66650
    },
    {
      "epoch": 1.3713442998979117,
      "grad_norm": 0.4631170928478241,
      "learning_rate": 0.00011864256963870957,
      "loss": 2.4688,
      "step": 66660
    },
    {
      "epoch": 1.3715500194147794,
      "grad_norm": 0.4634808897972107,
      "learning_rate": 0.00011862074939480306,
      "loss": 2.4616,
      "step": 66670
    },
    {
      "epoch": 1.371755738931647,
      "grad_norm": 0.44137856364250183,
      "learning_rate": 0.00011859892823243648,
      "loss": 2.3979,
      "step": 66680
    },
    {
      "epoch": 1.371961458448515,
      "grad_norm": 0.4332430362701416,
      "learning_rate": 0.00011857710615268609,
      "loss": 2.3998,
      "step": 66690
    },
    {
      "epoch": 1.3721671779653826,
      "grad_norm": 0.44601210951805115,
      "learning_rate": 0.00011855528315662829,
      "loss": 2.4861,
      "step": 66700
    },
    {
      "epoch": 1.3723728974822502,
      "grad_norm": 0.44134509563446045,
      "learning_rate": 0.00011853345924533952,
      "loss": 2.3635,
      "step": 66710
    },
    {
      "epoch": 1.3725786169991179,
      "grad_norm": 0.4044734239578247,
      "learning_rate": 0.00011851163441989617,
      "loss": 2.4192,
      "step": 66720
    },
    {
      "epoch": 1.3727843365159857,
      "grad_norm": 0.4415106177330017,
      "learning_rate": 0.00011848980868137476,
      "loss": 2.4563,
      "step": 66730
    },
    {
      "epoch": 1.3729900560328534,
      "grad_norm": 0.417759507894516,
      "learning_rate": 0.00011846798203085188,
      "loss": 2.399,
      "step": 66740
    },
    {
      "epoch": 1.373195775549721,
      "grad_norm": 0.42940568923950195,
      "learning_rate": 0.00011844615446940406,
      "loss": 2.442,
      "step": 66750
    },
    {
      "epoch": 1.373401495066589,
      "grad_norm": 0.47550803422927856,
      "learning_rate": 0.00011842432599810796,
      "loss": 2.379,
      "step": 66760
    },
    {
      "epoch": 1.3736072145834566,
      "grad_norm": 0.4184146523475647,
      "learning_rate": 0.00011840249661804027,
      "loss": 2.4631,
      "step": 66770
    },
    {
      "epoch": 1.3738129341003242,
      "grad_norm": 0.40610453486442566,
      "learning_rate": 0.00011838066633027769,
      "loss": 2.4477,
      "step": 66780
    },
    {
      "epoch": 1.3740186536171919,
      "grad_norm": 0.41772258281707764,
      "learning_rate": 0.00011835883513589701,
      "loss": 2.4082,
      "step": 66790
    },
    {
      "epoch": 1.3742243731340598,
      "grad_norm": 0.4318113923072815,
      "learning_rate": 0.00011833700303597504,
      "loss": 2.4082,
      "step": 66800
    },
    {
      "epoch": 1.3744300926509274,
      "grad_norm": 0.46520277857780457,
      "learning_rate": 0.00011831517003158863,
      "loss": 2.4124,
      "step": 66810
    },
    {
      "epoch": 1.374635812167795,
      "grad_norm": 0.39301228523254395,
      "learning_rate": 0.00011829333612381468,
      "loss": 2.3765,
      "step": 66820
    },
    {
      "epoch": 1.374841531684663,
      "grad_norm": 0.44272512197494507,
      "learning_rate": 0.00011827150131373014,
      "loss": 2.3814,
      "step": 66830
    },
    {
      "epoch": 1.3750472512015306,
      "grad_norm": 0.44542044401168823,
      "learning_rate": 0.00011824966560241207,
      "loss": 2.3366,
      "step": 66840
    },
    {
      "epoch": 1.3752529707183982,
      "grad_norm": 1.1278126239776611,
      "learning_rate": 0.00011822782899093736,
      "loss": 2.4276,
      "step": 66850
    },
    {
      "epoch": 1.375458690235266,
      "grad_norm": 0.47049713134765625,
      "learning_rate": 0.00011820599148038322,
      "loss": 2.3974,
      "step": 66860
    },
    {
      "epoch": 1.3756644097521338,
      "grad_norm": 0.4415949583053589,
      "learning_rate": 0.00011818415307182676,
      "loss": 2.4415,
      "step": 66870
    },
    {
      "epoch": 1.3758701292690014,
      "grad_norm": 0.4732920229434967,
      "learning_rate": 0.00011816231376634508,
      "loss": 2.3958,
      "step": 66880
    },
    {
      "epoch": 1.376075848785869,
      "grad_norm": 0.48195892572402954,
      "learning_rate": 0.00011814047356501541,
      "loss": 2.3942,
      "step": 66890
    },
    {
      "epoch": 1.376281568302737,
      "grad_norm": 0.4473642408847809,
      "learning_rate": 0.00011811863246891509,
      "loss": 2.3676,
      "step": 66900
    },
    {
      "epoch": 1.3764872878196046,
      "grad_norm": 0.42543327808380127,
      "learning_rate": 0.00011809679047912132,
      "loss": 2.3867,
      "step": 66910
    },
    {
      "epoch": 1.3766930073364723,
      "grad_norm": 0.4621681571006775,
      "learning_rate": 0.00011807494759671151,
      "loss": 2.375,
      "step": 66920
    },
    {
      "epoch": 1.37689872685334,
      "grad_norm": 0.4649139940738678,
      "learning_rate": 0.00011805310382276304,
      "loss": 2.3463,
      "step": 66930
    },
    {
      "epoch": 1.3771044463702076,
      "grad_norm": 0.43032610416412354,
      "learning_rate": 0.00011803125915835331,
      "loss": 2.3906,
      "step": 66940
    },
    {
      "epoch": 1.3773101658870754,
      "grad_norm": 0.42546606063842773,
      "learning_rate": 0.00011800941360455986,
      "loss": 2.344,
      "step": 66950
    },
    {
      "epoch": 1.377515885403943,
      "grad_norm": 0.4403054118156433,
      "learning_rate": 0.00011798756716246015,
      "loss": 2.3709,
      "step": 66960
    },
    {
      "epoch": 1.377721604920811,
      "grad_norm": 0.451742947101593,
      "learning_rate": 0.00011796571983313178,
      "loss": 2.3874,
      "step": 66970
    },
    {
      "epoch": 1.3779273244376786,
      "grad_norm": 0.4209374785423279,
      "learning_rate": 0.00011794387161765234,
      "loss": 2.4811,
      "step": 66980
    },
    {
      "epoch": 1.3781330439545463,
      "grad_norm": 0.4763675034046173,
      "learning_rate": 0.00011792202251709951,
      "loss": 2.3806,
      "step": 66990
    },
    {
      "epoch": 1.378338763471414,
      "grad_norm": 0.39938920736312866,
      "learning_rate": 0.000117900172532551,
      "loss": 2.4277,
      "step": 67000
    },
    {
      "epoch": 1.3785444829882816,
      "grad_norm": 0.42661985754966736,
      "learning_rate": 0.00011787832166508446,
      "loss": 2.3426,
      "step": 67010
    },
    {
      "epoch": 1.3787502025051495,
      "grad_norm": 0.42752164602279663,
      "learning_rate": 0.00011785646991577776,
      "loss": 2.4376,
      "step": 67020
    },
    {
      "epoch": 1.378955922022017,
      "grad_norm": 0.3934394419193268,
      "learning_rate": 0.00011783461728570875,
      "loss": 2.4283,
      "step": 67030
    },
    {
      "epoch": 1.3791616415388848,
      "grad_norm": 0.44664761424064636,
      "learning_rate": 0.00011781276377595523,
      "loss": 2.3171,
      "step": 67040
    },
    {
      "epoch": 1.3793673610557526,
      "grad_norm": 0.4052031636238098,
      "learning_rate": 0.00011779090938759512,
      "loss": 2.3889,
      "step": 67050
    },
    {
      "epoch": 1.3795730805726203,
      "grad_norm": 0.46698057651519775,
      "learning_rate": 0.00011776905412170643,
      "loss": 2.3889,
      "step": 67060
    },
    {
      "epoch": 1.379778800089488,
      "grad_norm": 0.42807435989379883,
      "learning_rate": 0.00011774719797936708,
      "loss": 2.4386,
      "step": 67070
    },
    {
      "epoch": 1.3799845196063556,
      "grad_norm": 0.4595469534397125,
      "learning_rate": 0.00011772534096165522,
      "loss": 2.3987,
      "step": 67080
    },
    {
      "epoch": 1.3801902391232235,
      "grad_norm": 0.42632004618644714,
      "learning_rate": 0.00011770348306964885,
      "loss": 2.3797,
      "step": 67090
    },
    {
      "epoch": 1.3803959586400911,
      "grad_norm": 0.4428829848766327,
      "learning_rate": 0.00011768162430442614,
      "loss": 2.3536,
      "step": 67100
    },
    {
      "epoch": 1.3806016781569588,
      "grad_norm": 0.4284723401069641,
      "learning_rate": 0.00011765976466706526,
      "loss": 2.3959,
      "step": 67110
    },
    {
      "epoch": 1.3808073976738267,
      "grad_norm": 0.4334157705307007,
      "learning_rate": 0.00011763790415864442,
      "loss": 2.4092,
      "step": 67120
    },
    {
      "epoch": 1.3810131171906943,
      "grad_norm": 0.4087502360343933,
      "learning_rate": 0.00011761604278024188,
      "loss": 2.3472,
      "step": 67130
    },
    {
      "epoch": 1.381218836707562,
      "grad_norm": 0.43137088418006897,
      "learning_rate": 0.00011759418053293594,
      "loss": 2.4373,
      "step": 67140
    },
    {
      "epoch": 1.3814245562244296,
      "grad_norm": 0.46094217896461487,
      "learning_rate": 0.00011757231741780496,
      "loss": 2.4291,
      "step": 67150
    },
    {
      "epoch": 1.3816302757412975,
      "grad_norm": 0.4356786906719208,
      "learning_rate": 0.00011755045343592734,
      "loss": 2.4254,
      "step": 67160
    },
    {
      "epoch": 1.3818359952581651,
      "grad_norm": 0.41222718358039856,
      "learning_rate": 0.00011752858858838145,
      "loss": 2.4268,
      "step": 67170
    },
    {
      "epoch": 1.3820417147750328,
      "grad_norm": 0.42988207936286926,
      "learning_rate": 0.00011750672287624582,
      "loss": 2.3463,
      "step": 67180
    },
    {
      "epoch": 1.3822474342919007,
      "grad_norm": 0.5072152614593506,
      "learning_rate": 0.00011748485630059898,
      "loss": 2.4171,
      "step": 67190
    },
    {
      "epoch": 1.3824531538087683,
      "grad_norm": 0.43203675746917725,
      "learning_rate": 0.00011746298886251942,
      "loss": 2.4427,
      "step": 67200
    },
    {
      "epoch": 1.382658873325636,
      "grad_norm": 0.44189029932022095,
      "learning_rate": 0.00011744112056308578,
      "loss": 2.4161,
      "step": 67210
    },
    {
      "epoch": 1.3828645928425036,
      "grad_norm": 0.4234806299209595,
      "learning_rate": 0.00011741925140337675,
      "loss": 2.3788,
      "step": 67220
    },
    {
      "epoch": 1.3830703123593715,
      "grad_norm": 0.4820493161678314,
      "learning_rate": 0.00011739738138447093,
      "loss": 2.3989,
      "step": 67230
    },
    {
      "epoch": 1.3832760318762392,
      "grad_norm": 0.4271068871021271,
      "learning_rate": 0.0001173755105074471,
      "loss": 2.4112,
      "step": 67240
    },
    {
      "epoch": 1.3834817513931068,
      "grad_norm": 0.4846324026584625,
      "learning_rate": 0.00011735363877338404,
      "loss": 2.336,
      "step": 67250
    },
    {
      "epoch": 1.3836874709099747,
      "grad_norm": 0.4305168092250824,
      "learning_rate": 0.00011733176618336052,
      "loss": 2.4088,
      "step": 67260
    },
    {
      "epoch": 1.3838931904268423,
      "grad_norm": 0.4173238277435303,
      "learning_rate": 0.00011730989273845542,
      "loss": 2.4026,
      "step": 67270
    },
    {
      "epoch": 1.38409890994371,
      "grad_norm": 0.4340321123600006,
      "learning_rate": 0.00011728801843974766,
      "loss": 2.3714,
      "step": 67280
    },
    {
      "epoch": 1.3843046294605776,
      "grad_norm": 0.47170326113700867,
      "learning_rate": 0.00011726614328831616,
      "loss": 2.4038,
      "step": 67290
    },
    {
      "epoch": 1.3845103489774453,
      "grad_norm": 0.4302644431591034,
      "learning_rate": 0.00011724426728523987,
      "loss": 2.3585,
      "step": 67300
    },
    {
      "epoch": 1.3847160684943132,
      "grad_norm": 0.4579579532146454,
      "learning_rate": 0.00011722239043159786,
      "loss": 2.3613,
      "step": 67310
    },
    {
      "epoch": 1.3849217880111808,
      "grad_norm": 0.43072837591171265,
      "learning_rate": 0.00011720051272846921,
      "loss": 2.4107,
      "step": 67320
    },
    {
      "epoch": 1.3851275075280487,
      "grad_norm": 0.43671268224716187,
      "learning_rate": 0.00011717863417693295,
      "loss": 2.4056,
      "step": 67330
    },
    {
      "epoch": 1.3853332270449163,
      "grad_norm": 0.44005513191223145,
      "learning_rate": 0.00011715675477806831,
      "loss": 2.4287,
      "step": 67340
    },
    {
      "epoch": 1.385538946561784,
      "grad_norm": 0.41199684143066406,
      "learning_rate": 0.00011713487453295447,
      "loss": 2.4506,
      "step": 67350
    },
    {
      "epoch": 1.3857446660786517,
      "grad_norm": 0.41739383339881897,
      "learning_rate": 0.00011711299344267059,
      "loss": 2.4075,
      "step": 67360
    },
    {
      "epoch": 1.3859503855955193,
      "grad_norm": 0.45753395557403564,
      "learning_rate": 0.00011709111150829601,
      "loss": 2.4234,
      "step": 67370
    },
    {
      "epoch": 1.3861561051123872,
      "grad_norm": 0.3977104723453522,
      "learning_rate": 0.00011706922873091008,
      "loss": 2.404,
      "step": 67380
    },
    {
      "epoch": 1.3863618246292548,
      "grad_norm": 0.4087006151676178,
      "learning_rate": 0.00011704734511159207,
      "loss": 2.4273,
      "step": 67390
    },
    {
      "epoch": 1.3865675441461227,
      "grad_norm": 0.39815929532051086,
      "learning_rate": 0.00011702546065142144,
      "loss": 2.41,
      "step": 67400
    },
    {
      "epoch": 1.3867732636629904,
      "grad_norm": 0.4269503355026245,
      "learning_rate": 0.0001170035753514776,
      "loss": 2.3284,
      "step": 67410
    },
    {
      "epoch": 1.386978983179858,
      "grad_norm": 0.4318355917930603,
      "learning_rate": 0.00011698168921284005,
      "loss": 2.3728,
      "step": 67420
    },
    {
      "epoch": 1.3871847026967257,
      "grad_norm": 0.4477108120918274,
      "learning_rate": 0.0001169598022365883,
      "loss": 2.3877,
      "step": 67430
    },
    {
      "epoch": 1.3873904222135933,
      "grad_norm": 0.422245055437088,
      "learning_rate": 0.00011693791442380197,
      "loss": 2.3729,
      "step": 67440
    },
    {
      "epoch": 1.3875961417304612,
      "grad_norm": 0.4091383218765259,
      "learning_rate": 0.00011691602577556058,
      "loss": 2.3989,
      "step": 67450
    },
    {
      "epoch": 1.3878018612473288,
      "grad_norm": 0.3966370224952698,
      "learning_rate": 0.00011689413629294383,
      "loss": 2.352,
      "step": 67460
    },
    {
      "epoch": 1.3880075807641965,
      "grad_norm": 0.4453842341899872,
      "learning_rate": 0.00011687224597703139,
      "loss": 2.4025,
      "step": 67470
    },
    {
      "epoch": 1.3882133002810644,
      "grad_norm": 0.3895963728427887,
      "learning_rate": 0.00011685035482890303,
      "loss": 2.377,
      "step": 67480
    },
    {
      "epoch": 1.388419019797932,
      "grad_norm": 0.4298701584339142,
      "learning_rate": 0.00011682846284963844,
      "loss": 2.4545,
      "step": 67490
    },
    {
      "epoch": 1.3886247393147997,
      "grad_norm": 0.42392659187316895,
      "learning_rate": 0.00011680657004031751,
      "loss": 2.4549,
      "step": 67500
    },
    {
      "epoch": 1.3888304588316673,
      "grad_norm": 0.41712164878845215,
      "learning_rate": 0.00011678467640202009,
      "loss": 2.4269,
      "step": 67510
    },
    {
      "epoch": 1.3890361783485352,
      "grad_norm": 0.46414077281951904,
      "learning_rate": 0.00011676278193582604,
      "loss": 2.3574,
      "step": 67520
    },
    {
      "epoch": 1.3892418978654029,
      "grad_norm": 0.47360557317733765,
      "learning_rate": 0.00011674088664281524,
      "loss": 2.4137,
      "step": 67530
    },
    {
      "epoch": 1.3894476173822705,
      "grad_norm": 0.42474135756492615,
      "learning_rate": 0.0001167189905240678,
      "loss": 2.396,
      "step": 67540
    },
    {
      "epoch": 1.3896533368991384,
      "grad_norm": 0.4131163954734802,
      "learning_rate": 0.00011669709358066366,
      "loss": 2.3704,
      "step": 67550
    },
    {
      "epoch": 1.389859056416006,
      "grad_norm": 0.44326210021972656,
      "learning_rate": 0.00011667519581368284,
      "loss": 2.373,
      "step": 67560
    },
    {
      "epoch": 1.3900647759328737,
      "grad_norm": 0.42591017484664917,
      "learning_rate": 0.0001166532972242055,
      "loss": 2.4151,
      "step": 67570
    },
    {
      "epoch": 1.3902704954497414,
      "grad_norm": 0.4276483654975891,
      "learning_rate": 0.00011663139781331175,
      "loss": 2.411,
      "step": 67580
    },
    {
      "epoch": 1.3904762149666092,
      "grad_norm": 0.47318780422210693,
      "learning_rate": 0.0001166094975820818,
      "loss": 2.403,
      "step": 67590
    },
    {
      "epoch": 1.3906819344834769,
      "grad_norm": 0.44317251443862915,
      "learning_rate": 0.00011658759653159585,
      "loss": 2.411,
      "step": 67600
    },
    {
      "epoch": 1.3908876540003445,
      "grad_norm": 0.4437340199947357,
      "learning_rate": 0.00011656569466293414,
      "loss": 2.3848,
      "step": 67610
    },
    {
      "epoch": 1.3910933735172124,
      "grad_norm": 0.4231591522693634,
      "learning_rate": 0.00011654379197717695,
      "loss": 2.4236,
      "step": 67620
    },
    {
      "epoch": 1.39129909303408,
      "grad_norm": 0.42513522505760193,
      "learning_rate": 0.0001165218884754047,
      "loss": 2.3713,
      "step": 67630
    },
    {
      "epoch": 1.3915048125509477,
      "grad_norm": 0.43227049708366394,
      "learning_rate": 0.00011649998415869772,
      "loss": 2.4012,
      "step": 67640
    },
    {
      "epoch": 1.3917105320678154,
      "grad_norm": 0.44988924264907837,
      "learning_rate": 0.00011647807902813642,
      "loss": 2.3533,
      "step": 67650
    },
    {
      "epoch": 1.391916251584683,
      "grad_norm": 0.4952012896537781,
      "learning_rate": 0.00011645617308480125,
      "loss": 2.4124,
      "step": 67660
    },
    {
      "epoch": 1.392121971101551,
      "grad_norm": 0.45099157094955444,
      "learning_rate": 0.00011643426632977279,
      "loss": 2.4172,
      "step": 67670
    },
    {
      "epoch": 1.3923276906184185,
      "grad_norm": 0.4226737916469574,
      "learning_rate": 0.00011641235876413152,
      "loss": 2.4211,
      "step": 67680
    },
    {
      "epoch": 1.3925334101352864,
      "grad_norm": 0.38651254773139954,
      "learning_rate": 0.000116390450388958,
      "loss": 2.4055,
      "step": 67690
    },
    {
      "epoch": 1.392739129652154,
      "grad_norm": 0.41799575090408325,
      "learning_rate": 0.00011636854120533289,
      "loss": 2.4181,
      "step": 67700
    },
    {
      "epoch": 1.3929448491690217,
      "grad_norm": 0.42892375588417053,
      "learning_rate": 0.00011634663121433686,
      "loss": 2.3333,
      "step": 67710
    },
    {
      "epoch": 1.3931505686858894,
      "grad_norm": 0.4625597894191742,
      "learning_rate": 0.00011632472041705057,
      "loss": 2.4202,
      "step": 67720
    },
    {
      "epoch": 1.393356288202757,
      "grad_norm": 0.42433789372444153,
      "learning_rate": 0.0001163028088145548,
      "loss": 2.3652,
      "step": 67730
    },
    {
      "epoch": 1.393562007719625,
      "grad_norm": 0.4744194746017456,
      "learning_rate": 0.00011628089640793027,
      "loss": 2.4087,
      "step": 67740
    },
    {
      "epoch": 1.3937677272364926,
      "grad_norm": 0.5631009340286255,
      "learning_rate": 0.00011625898319825787,
      "loss": 2.4461,
      "step": 67750
    },
    {
      "epoch": 1.3939734467533604,
      "grad_norm": 0.46909037232398987,
      "learning_rate": 0.00011623706918661845,
      "loss": 2.3837,
      "step": 67760
    },
    {
      "epoch": 1.394179166270228,
      "grad_norm": 0.3884013295173645,
      "learning_rate": 0.00011621515437409287,
      "loss": 2.36,
      "step": 67770
    },
    {
      "epoch": 1.3943848857870957,
      "grad_norm": 0.39618563652038574,
      "learning_rate": 0.00011619323876176207,
      "loss": 2.4159,
      "step": 67780
    },
    {
      "epoch": 1.3945906053039634,
      "grad_norm": 0.4384031295776367,
      "learning_rate": 0.00011617132235070706,
      "loss": 2.4124,
      "step": 67790
    },
    {
      "epoch": 1.394796324820831,
      "grad_norm": 0.4306527078151703,
      "learning_rate": 0.00011614940514200885,
      "loss": 2.3825,
      "step": 67800
    },
    {
      "epoch": 1.395002044337699,
      "grad_norm": 0.45073023438453674,
      "learning_rate": 0.00011612748713674846,
      "loss": 2.4053,
      "step": 67810
    },
    {
      "epoch": 1.3952077638545666,
      "grad_norm": 0.3863425552845001,
      "learning_rate": 0.00011610556833600703,
      "loss": 2.387,
      "step": 67820
    },
    {
      "epoch": 1.3954134833714342,
      "grad_norm": 0.45888087153434753,
      "learning_rate": 0.00011608364874086568,
      "loss": 2.4082,
      "step": 67830
    },
    {
      "epoch": 1.395619202888302,
      "grad_norm": 0.40724658966064453,
      "learning_rate": 0.00011606172835240558,
      "loss": 2.3939,
      "step": 67840
    },
    {
      "epoch": 1.3958249224051698,
      "grad_norm": 0.502284049987793,
      "learning_rate": 0.00011603980717170795,
      "loss": 2.4091,
      "step": 67850
    },
    {
      "epoch": 1.3960306419220374,
      "grad_norm": 0.4374692738056183,
      "learning_rate": 0.00011601788519985403,
      "loss": 2.3962,
      "step": 67860
    },
    {
      "epoch": 1.396236361438905,
      "grad_norm": 0.4207003712654114,
      "learning_rate": 0.00011599596243792515,
      "loss": 2.443,
      "step": 67870
    },
    {
      "epoch": 1.396442080955773,
      "grad_norm": 0.4740752577781677,
      "learning_rate": 0.00011597403888700258,
      "loss": 2.3776,
      "step": 67880
    },
    {
      "epoch": 1.3966478004726406,
      "grad_norm": 0.44788822531700134,
      "learning_rate": 0.00011595211454816775,
      "loss": 2.4205,
      "step": 67890
    },
    {
      "epoch": 1.3968535199895082,
      "grad_norm": 0.39804723858833313,
      "learning_rate": 0.00011593018942250197,
      "loss": 2.404,
      "step": 67900
    },
    {
      "epoch": 1.3970592395063761,
      "grad_norm": 0.4284300208091736,
      "learning_rate": 0.00011590826351108681,
      "loss": 2.3638,
      "step": 67910
    },
    {
      "epoch": 1.3972649590232438,
      "grad_norm": 0.4294632375240326,
      "learning_rate": 0.00011588633681500371,
      "loss": 2.3395,
      "step": 67920
    },
    {
      "epoch": 1.3974706785401114,
      "grad_norm": 0.4118516445159912,
      "learning_rate": 0.00011586440933533418,
      "loss": 2.3736,
      "step": 67930
    },
    {
      "epoch": 1.397676398056979,
      "grad_norm": 0.44574475288391113,
      "learning_rate": 0.00011584248107315974,
      "loss": 2.361,
      "step": 67940
    },
    {
      "epoch": 1.397882117573847,
      "grad_norm": 0.4226830005645752,
      "learning_rate": 0.0001158205520295621,
      "loss": 2.3822,
      "step": 67950
    },
    {
      "epoch": 1.3980878370907146,
      "grad_norm": 0.4575018584728241,
      "learning_rate": 0.0001157986222056228,
      "loss": 2.4304,
      "step": 67960
    },
    {
      "epoch": 1.3982935566075823,
      "grad_norm": 0.45952391624450684,
      "learning_rate": 0.00011577669160242358,
      "loss": 2.419,
      "step": 67970
    },
    {
      "epoch": 1.3984992761244501,
      "grad_norm": 0.42808136343955994,
      "learning_rate": 0.00011575476022104612,
      "loss": 2.4372,
      "step": 67980
    },
    {
      "epoch": 1.3987049956413178,
      "grad_norm": 0.4496898055076599,
      "learning_rate": 0.00011573282806257222,
      "loss": 2.3625,
      "step": 67990
    },
    {
      "epoch": 1.3989107151581854,
      "grad_norm": 0.4096790850162506,
      "learning_rate": 0.00011571089512808362,
      "loss": 2.4,
      "step": 68000
    },
    {
      "epoch": 1.399116434675053,
      "grad_norm": 0.4440631866455078,
      "learning_rate": 0.00011568896141866216,
      "loss": 2.4659,
      "step": 68010
    },
    {
      "epoch": 1.3993221541919207,
      "grad_norm": 0.41758012771606445,
      "learning_rate": 0.00011566702693538975,
      "loss": 2.3709,
      "step": 68020
    },
    {
      "epoch": 1.3995278737087886,
      "grad_norm": 0.4234287738800049,
      "learning_rate": 0.0001156450916793483,
      "loss": 2.4,
      "step": 68030
    },
    {
      "epoch": 1.3997335932256563,
      "grad_norm": 0.40843674540519714,
      "learning_rate": 0.0001156231556516197,
      "loss": 2.3987,
      "step": 68040
    },
    {
      "epoch": 1.3999393127425241,
      "grad_norm": 0.44377633929252625,
      "learning_rate": 0.000115601218853286,
      "loss": 2.4255,
      "step": 68050
    },
    {
      "epoch": 1.4001450322593918,
      "grad_norm": 0.4159395098686218,
      "learning_rate": 0.00011557928128542914,
      "loss": 2.4142,
      "step": 68060
    },
    {
      "epoch": 1.4003507517762595,
      "grad_norm": 0.43104490637779236,
      "learning_rate": 0.00011555734294913126,
      "loss": 2.4101,
      "step": 68070
    },
    {
      "epoch": 1.400556471293127,
      "grad_norm": 0.4248744249343872,
      "learning_rate": 0.00011553540384547446,
      "loss": 2.3875,
      "step": 68080
    },
    {
      "epoch": 1.4007621908099948,
      "grad_norm": 0.43161648511886597,
      "learning_rate": 0.00011551346397554081,
      "loss": 2.3962,
      "step": 68090
    },
    {
      "epoch": 1.4009679103268626,
      "grad_norm": 0.4344114661216736,
      "learning_rate": 0.00011549152334041249,
      "loss": 2.3716,
      "step": 68100
    },
    {
      "epoch": 1.4011736298437303,
      "grad_norm": 0.4078247547149658,
      "learning_rate": 0.00011546958194117181,
      "loss": 2.3996,
      "step": 68110
    },
    {
      "epoch": 1.4013793493605982,
      "grad_norm": 0.4414845407009125,
      "learning_rate": 0.00011544763977890091,
      "loss": 2.4167,
      "step": 68120
    },
    {
      "epoch": 1.4015850688774658,
      "grad_norm": 0.41929420828819275,
      "learning_rate": 0.00011542569685468214,
      "loss": 2.3836,
      "step": 68130
    },
    {
      "epoch": 1.4017907883943335,
      "grad_norm": 0.443219393491745,
      "learning_rate": 0.0001154037531695978,
      "loss": 2.3867,
      "step": 68140
    },
    {
      "epoch": 1.4019965079112011,
      "grad_norm": 0.4612475633621216,
      "learning_rate": 0.00011538180872473024,
      "loss": 2.3302,
      "step": 68150
    },
    {
      "epoch": 1.4022022274280688,
      "grad_norm": 0.4206179976463318,
      "learning_rate": 0.00011535986352116189,
      "loss": 2.3896,
      "step": 68160
    },
    {
      "epoch": 1.4024079469449366,
      "grad_norm": 0.47087767720222473,
      "learning_rate": 0.00011533791755997517,
      "loss": 2.3766,
      "step": 68170
    },
    {
      "epoch": 1.4026136664618043,
      "grad_norm": 0.5051743388175964,
      "learning_rate": 0.00011531597084225254,
      "loss": 2.4199,
      "step": 68180
    },
    {
      "epoch": 1.402819385978672,
      "grad_norm": 0.4048447608947754,
      "learning_rate": 0.00011529402336907656,
      "loss": 2.4154,
      "step": 68190
    },
    {
      "epoch": 1.4030251054955398,
      "grad_norm": 0.44316565990448,
      "learning_rate": 0.00011527207514152973,
      "loss": 2.4576,
      "step": 68200
    },
    {
      "epoch": 1.4032308250124075,
      "grad_norm": 0.4133703112602234,
      "learning_rate": 0.00011525012616069468,
      "loss": 2.4265,
      "step": 68210
    },
    {
      "epoch": 1.4034365445292751,
      "grad_norm": 0.4292648732662201,
      "learning_rate": 0.00011522817642765398,
      "loss": 2.3442,
      "step": 68220
    },
    {
      "epoch": 1.4036422640461428,
      "grad_norm": 0.4495278298854828,
      "learning_rate": 0.00011520622594349033,
      "loss": 2.4179,
      "step": 68230
    },
    {
      "epoch": 1.4038479835630107,
      "grad_norm": 0.41507551074028015,
      "learning_rate": 0.00011518427470928643,
      "loss": 2.436,
      "step": 68240
    },
    {
      "epoch": 1.4040537030798783,
      "grad_norm": 0.42493587732315063,
      "learning_rate": 0.00011516232272612502,
      "loss": 2.4442,
      "step": 68250
    },
    {
      "epoch": 1.404259422596746,
      "grad_norm": 0.48207008838653564,
      "learning_rate": 0.00011514036999508879,
      "loss": 2.3823,
      "step": 68260
    },
    {
      "epoch": 1.4044651421136138,
      "grad_norm": 0.5840355753898621,
      "learning_rate": 0.00011511841651726067,
      "loss": 2.3874,
      "step": 68270
    },
    {
      "epoch": 1.4046708616304815,
      "grad_norm": 0.5258448719978333,
      "learning_rate": 0.00011509646229372344,
      "loss": 2.4014,
      "step": 68280
    },
    {
      "epoch": 1.4048765811473491,
      "grad_norm": 0.4215838313102722,
      "learning_rate": 0.00011507450732555998,
      "loss": 2.3593,
      "step": 68290
    },
    {
      "epoch": 1.4050823006642168,
      "grad_norm": 0.40275824069976807,
      "learning_rate": 0.00011505255161385322,
      "loss": 2.4006,
      "step": 68300
    },
    {
      "epoch": 1.4052880201810847,
      "grad_norm": 0.4598122239112854,
      "learning_rate": 0.00011503059515968613,
      "loss": 2.4243,
      "step": 68310
    },
    {
      "epoch": 1.4054937396979523,
      "grad_norm": 0.4569959044456482,
      "learning_rate": 0.00011500863796414166,
      "loss": 2.372,
      "step": 68320
    },
    {
      "epoch": 1.40569945921482,
      "grad_norm": 0.45639336109161377,
      "learning_rate": 0.00011498668002830289,
      "loss": 2.4229,
      "step": 68330
    },
    {
      "epoch": 1.4059051787316879,
      "grad_norm": 0.435213565826416,
      "learning_rate": 0.00011496472135325285,
      "loss": 2.3923,
      "step": 68340
    },
    {
      "epoch": 1.4061108982485555,
      "grad_norm": 0.43239930272102356,
      "learning_rate": 0.00011494276194007464,
      "loss": 2.3961,
      "step": 68350
    },
    {
      "epoch": 1.4063166177654232,
      "grad_norm": 0.45108601450920105,
      "learning_rate": 0.00011492080178985145,
      "loss": 2.4483,
      "step": 68360
    },
    {
      "epoch": 1.4065223372822908,
      "grad_norm": 0.4826197922229767,
      "learning_rate": 0.00011489884090366641,
      "loss": 2.3976,
      "step": 68370
    },
    {
      "epoch": 1.4067280567991587,
      "grad_norm": 0.473289430141449,
      "learning_rate": 0.00011487687928260272,
      "loss": 2.3655,
      "step": 68380
    },
    {
      "epoch": 1.4069337763160263,
      "grad_norm": 0.4494419991970062,
      "learning_rate": 0.00011485491692774362,
      "loss": 2.3796,
      "step": 68390
    },
    {
      "epoch": 1.407139495832894,
      "grad_norm": 0.39814841747283936,
      "learning_rate": 0.00011483295384017247,
      "loss": 2.4188,
      "step": 68400
    },
    {
      "epoch": 1.4073452153497619,
      "grad_norm": 0.4124779999256134,
      "learning_rate": 0.0001148109900209725,
      "loss": 2.3902,
      "step": 68410
    },
    {
      "epoch": 1.4075509348666295,
      "grad_norm": 0.41448017954826355,
      "learning_rate": 0.00011478902547122711,
      "loss": 2.3754,
      "step": 68420
    },
    {
      "epoch": 1.4077566543834972,
      "grad_norm": 0.48270323872566223,
      "learning_rate": 0.0001147670601920197,
      "loss": 2.4154,
      "step": 68430
    },
    {
      "epoch": 1.4079623739003648,
      "grad_norm": 0.47268444299697876,
      "learning_rate": 0.00011474509418443366,
      "loss": 2.432,
      "step": 68440
    },
    {
      "epoch": 1.4081680934172325,
      "grad_norm": 0.3951261341571808,
      "learning_rate": 0.00011472312744955249,
      "loss": 2.4048,
      "step": 68450
    },
    {
      "epoch": 1.4083738129341004,
      "grad_norm": 0.4391690492630005,
      "learning_rate": 0.00011470115998845967,
      "loss": 2.3891,
      "step": 68460
    },
    {
      "epoch": 1.408579532450968,
      "grad_norm": 0.42863377928733826,
      "learning_rate": 0.00011467919180223874,
      "loss": 2.4302,
      "step": 68470
    },
    {
      "epoch": 1.4087852519678359,
      "grad_norm": 0.4580513834953308,
      "learning_rate": 0.00011465722289197327,
      "loss": 2.4047,
      "step": 68480
    },
    {
      "epoch": 1.4089909714847035,
      "grad_norm": 0.3990149199962616,
      "learning_rate": 0.00011463525325874686,
      "loss": 2.3914,
      "step": 68490
    },
    {
      "epoch": 1.4091966910015712,
      "grad_norm": 0.40483108162879944,
      "learning_rate": 0.00011461328290364317,
      "loss": 2.3478,
      "step": 68500
    },
    {
      "epoch": 1.4094024105184388,
      "grad_norm": 0.447553813457489,
      "learning_rate": 0.00011459131182774588,
      "loss": 2.4006,
      "step": 68510
    },
    {
      "epoch": 1.4096081300353065,
      "grad_norm": 0.4152216911315918,
      "learning_rate": 0.00011456934003213866,
      "loss": 2.4256,
      "step": 68520
    },
    {
      "epoch": 1.4098138495521744,
      "grad_norm": 0.41208943724632263,
      "learning_rate": 0.00011454736751790531,
      "loss": 2.3637,
      "step": 68530
    },
    {
      "epoch": 1.410019569069042,
      "grad_norm": 0.40312349796295166,
      "learning_rate": 0.00011452539428612959,
      "loss": 2.4179,
      "step": 68540
    },
    {
      "epoch": 1.4102252885859097,
      "grad_norm": 0.4283086955547333,
      "learning_rate": 0.0001145034203378953,
      "loss": 2.3854,
      "step": 68550
    },
    {
      "epoch": 1.4104310081027776,
      "grad_norm": 0.4347045123577118,
      "learning_rate": 0.00011448144567428635,
      "loss": 2.389,
      "step": 68560
    },
    {
      "epoch": 1.4106367276196452,
      "grad_norm": 0.48243123292922974,
      "learning_rate": 0.0001144594702963866,
      "loss": 2.3947,
      "step": 68570
    },
    {
      "epoch": 1.4108424471365129,
      "grad_norm": 0.457497239112854,
      "learning_rate": 0.00011443749420527993,
      "loss": 2.3865,
      "step": 68580
    },
    {
      "epoch": 1.4110481666533805,
      "grad_norm": 0.4012765884399414,
      "learning_rate": 0.00011441551740205041,
      "loss": 2.4146,
      "step": 68590
    },
    {
      "epoch": 1.4112538861702484,
      "grad_norm": 0.4370534420013428,
      "learning_rate": 0.00011439353988778194,
      "loss": 2.4318,
      "step": 68600
    },
    {
      "epoch": 1.411459605687116,
      "grad_norm": 0.4259428083896637,
      "learning_rate": 0.00011437156166355858,
      "loss": 2.3778,
      "step": 68610
    },
    {
      "epoch": 1.4116653252039837,
      "grad_norm": 0.4238031208515167,
      "learning_rate": 0.00011434958273046441,
      "loss": 2.4466,
      "step": 68620
    },
    {
      "epoch": 1.4118710447208516,
      "grad_norm": 0.457958459854126,
      "learning_rate": 0.0001143276030895835,
      "loss": 2.4265,
      "step": 68630
    },
    {
      "epoch": 1.4120767642377192,
      "grad_norm": 0.4124492406845093,
      "learning_rate": 0.00011430562274200003,
      "loss": 2.4211,
      "step": 68640
    },
    {
      "epoch": 1.4122824837545869,
      "grad_norm": 0.4437956213951111,
      "learning_rate": 0.00011428364168879813,
      "loss": 2.4206,
      "step": 68650
    },
    {
      "epoch": 1.4124882032714545,
      "grad_norm": 0.45036670565605164,
      "learning_rate": 0.00011426165993106204,
      "loss": 2.3795,
      "step": 68660
    },
    {
      "epoch": 1.4126939227883224,
      "grad_norm": 0.46870970726013184,
      "learning_rate": 0.00011423967746987596,
      "loss": 2.43,
      "step": 68670
    },
    {
      "epoch": 1.41289964230519,
      "grad_norm": 0.4076065719127655,
      "learning_rate": 0.00011421769430632418,
      "loss": 2.4055,
      "step": 68680
    },
    {
      "epoch": 1.4131053618220577,
      "grad_norm": 0.44411176443099976,
      "learning_rate": 0.00011419571044149106,
      "loss": 2.4506,
      "step": 68690
    },
    {
      "epoch": 1.4133110813389256,
      "grad_norm": 0.4733801782131195,
      "learning_rate": 0.00011417372587646085,
      "loss": 2.4215,
      "step": 68700
    },
    {
      "epoch": 1.4135168008557932,
      "grad_norm": 0.4290022552013397,
      "learning_rate": 0.00011415174061231801,
      "loss": 2.3753,
      "step": 68710
    },
    {
      "epoch": 1.4137225203726609,
      "grad_norm": 0.41601261496543884,
      "learning_rate": 0.00011412975465014694,
      "loss": 2.3946,
      "step": 68720
    },
    {
      "epoch": 1.4139282398895285,
      "grad_norm": 0.4423918128013611,
      "learning_rate": 0.00011410776799103204,
      "loss": 2.4279,
      "step": 68730
    },
    {
      "epoch": 1.4141339594063964,
      "grad_norm": 0.4667176306247711,
      "learning_rate": 0.00011408578063605782,
      "loss": 2.4043,
      "step": 68740
    },
    {
      "epoch": 1.414339678923264,
      "grad_norm": 0.47455456852912903,
      "learning_rate": 0.00011406379258630884,
      "loss": 2.4093,
      "step": 68750
    },
    {
      "epoch": 1.4145453984401317,
      "grad_norm": 0.4446140229701996,
      "learning_rate": 0.00011404180384286957,
      "loss": 2.3847,
      "step": 68760
    },
    {
      "epoch": 1.4147511179569996,
      "grad_norm": 0.45430830121040344,
      "learning_rate": 0.00011401981440682466,
      "loss": 2.3898,
      "step": 68770
    },
    {
      "epoch": 1.4149568374738672,
      "grad_norm": 0.4364456236362457,
      "learning_rate": 0.00011399782427925867,
      "loss": 2.3739,
      "step": 68780
    },
    {
      "epoch": 1.415162556990735,
      "grad_norm": 0.4167288839817047,
      "learning_rate": 0.00011397583346125632,
      "loss": 2.3679,
      "step": 68790
    },
    {
      "epoch": 1.4153682765076026,
      "grad_norm": 0.4495302736759186,
      "learning_rate": 0.00011395384195390224,
      "loss": 2.3849,
      "step": 68800
    },
    {
      "epoch": 1.4155739960244702,
      "grad_norm": 0.4528725743293762,
      "learning_rate": 0.0001139318497582812,
      "loss": 2.406,
      "step": 68810
    },
    {
      "epoch": 1.415779715541338,
      "grad_norm": 0.4289984405040741,
      "learning_rate": 0.00011390985687547793,
      "loss": 2.4241,
      "step": 68820
    },
    {
      "epoch": 1.4159854350582057,
      "grad_norm": 0.4582480490207672,
      "learning_rate": 0.00011388786330657719,
      "loss": 2.4241,
      "step": 68830
    },
    {
      "epoch": 1.4161911545750736,
      "grad_norm": 0.40341606736183167,
      "learning_rate": 0.00011386586905266385,
      "loss": 2.415,
      "step": 68840
    },
    {
      "epoch": 1.4163968740919413,
      "grad_norm": 0.4166586697101593,
      "learning_rate": 0.00011384387411482277,
      "loss": 2.3875,
      "step": 68850
    },
    {
      "epoch": 1.416602593608809,
      "grad_norm": 0.49901363253593445,
      "learning_rate": 0.00011382187849413881,
      "loss": 2.387,
      "step": 68860
    },
    {
      "epoch": 1.4168083131256766,
      "grad_norm": 0.4522082209587097,
      "learning_rate": 0.00011379988219169686,
      "loss": 2.4718,
      "step": 68870
    },
    {
      "epoch": 1.4170140326425442,
      "grad_norm": 0.45567941665649414,
      "learning_rate": 0.00011377788520858199,
      "loss": 2.3894,
      "step": 68880
    },
    {
      "epoch": 1.417219752159412,
      "grad_norm": 0.43862035870552063,
      "learning_rate": 0.00011375588754587909,
      "loss": 2.3518,
      "step": 68890
    },
    {
      "epoch": 1.4174254716762797,
      "grad_norm": 0.41621464490890503,
      "learning_rate": 0.00011373388920467322,
      "loss": 2.4169,
      "step": 68900
    },
    {
      "epoch": 1.4176311911931474,
      "grad_norm": 0.43090876936912537,
      "learning_rate": 0.00011371189018604944,
      "loss": 2.3851,
      "step": 68910
    },
    {
      "epoch": 1.4178369107100153,
      "grad_norm": 0.5558209419250488,
      "learning_rate": 0.00011368989049109283,
      "loss": 2.4187,
      "step": 68920
    },
    {
      "epoch": 1.418042630226883,
      "grad_norm": 0.4846808910369873,
      "learning_rate": 0.00011366789012088852,
      "loss": 2.3963,
      "step": 68930
    },
    {
      "epoch": 1.4182483497437506,
      "grad_norm": 0.6378288865089417,
      "learning_rate": 0.00011364588907652167,
      "loss": 2.3871,
      "step": 68940
    },
    {
      "epoch": 1.4184540692606182,
      "grad_norm": 0.4387863576412201,
      "learning_rate": 0.0001136238873590775,
      "loss": 2.3994,
      "step": 68950
    },
    {
      "epoch": 1.418659788777486,
      "grad_norm": 0.4464837908744812,
      "learning_rate": 0.00011360188496964117,
      "loss": 2.4037,
      "step": 68960
    },
    {
      "epoch": 1.4188655082943538,
      "grad_norm": 0.43857327103614807,
      "learning_rate": 0.00011357988190929801,
      "loss": 2.3777,
      "step": 68970
    },
    {
      "epoch": 1.4190712278112214,
      "grad_norm": 0.4431949853897095,
      "learning_rate": 0.00011355787817913326,
      "loss": 2.3501,
      "step": 68980
    },
    {
      "epoch": 1.4192769473280893,
      "grad_norm": 0.4602559506893158,
      "learning_rate": 0.00011353587378023225,
      "loss": 2.4343,
      "step": 68990
    },
    {
      "epoch": 1.419482666844957,
      "grad_norm": 0.4484952688217163,
      "learning_rate": 0.00011351386871368035,
      "loss": 2.4055,
      "step": 69000
    },
    {
      "epoch": 1.4196883863618246,
      "grad_norm": 0.4250600337982178,
      "learning_rate": 0.00011349186298056297,
      "loss": 2.3758,
      "step": 69010
    },
    {
      "epoch": 1.4198941058786922,
      "grad_norm": 0.4570204019546509,
      "learning_rate": 0.00011346985658196552,
      "loss": 2.4605,
      "step": 69020
    },
    {
      "epoch": 1.4200998253955601,
      "grad_norm": 0.4120848476886749,
      "learning_rate": 0.00011344784951897341,
      "loss": 2.4454,
      "step": 69030
    },
    {
      "epoch": 1.4203055449124278,
      "grad_norm": 0.39782485365867615,
      "learning_rate": 0.0001134258417926722,
      "loss": 2.438,
      "step": 69040
    },
    {
      "epoch": 1.4205112644292954,
      "grad_norm": 0.405138224363327,
      "learning_rate": 0.00011340383340414735,
      "loss": 2.4539,
      "step": 69050
    },
    {
      "epoch": 1.4207169839461633,
      "grad_norm": 0.41083088517189026,
      "learning_rate": 0.00011338182435448443,
      "loss": 2.4453,
      "step": 69060
    },
    {
      "epoch": 1.420922703463031,
      "grad_norm": 0.42185479402542114,
      "learning_rate": 0.00011335981464476905,
      "loss": 2.3533,
      "step": 69070
    },
    {
      "epoch": 1.4211284229798986,
      "grad_norm": 0.41972437500953674,
      "learning_rate": 0.00011333780427608684,
      "loss": 2.4044,
      "step": 69080
    },
    {
      "epoch": 1.4213341424967663,
      "grad_norm": 0.4560341536998749,
      "learning_rate": 0.0001133157932495234,
      "loss": 2.4038,
      "step": 69090
    },
    {
      "epoch": 1.4215398620136341,
      "grad_norm": 0.41959747672080994,
      "learning_rate": 0.00011329378156616446,
      "loss": 2.4153,
      "step": 69100
    },
    {
      "epoch": 1.4217455815305018,
      "grad_norm": 0.43733644485473633,
      "learning_rate": 0.0001132717692270957,
      "loss": 2.4017,
      "step": 69110
    },
    {
      "epoch": 1.4219513010473694,
      "grad_norm": 0.45370590686798096,
      "learning_rate": 0.00011324975623340288,
      "loss": 2.4126,
      "step": 69120
    },
    {
      "epoch": 1.4221570205642373,
      "grad_norm": 0.4273190498352051,
      "learning_rate": 0.00011322774258617181,
      "loss": 2.4071,
      "step": 69130
    },
    {
      "epoch": 1.422362740081105,
      "grad_norm": 0.4163127541542053,
      "learning_rate": 0.0001132057282864883,
      "loss": 2.4391,
      "step": 69140
    },
    {
      "epoch": 1.4225684595979726,
      "grad_norm": 0.4416194558143616,
      "learning_rate": 0.00011318371333543811,
      "loss": 2.5063,
      "step": 69150
    },
    {
      "epoch": 1.4227741791148403,
      "grad_norm": 0.4619043171405792,
      "learning_rate": 0.00011316169773410724,
      "loss": 2.469,
      "step": 69160
    },
    {
      "epoch": 1.422979898631708,
      "grad_norm": 0.431572824716568,
      "learning_rate": 0.00011313968148358154,
      "loss": 2.3657,
      "step": 69170
    },
    {
      "epoch": 1.4231856181485758,
      "grad_norm": 0.42792943120002747,
      "learning_rate": 0.00011311766458494695,
      "loss": 2.4227,
      "step": 69180
    },
    {
      "epoch": 1.4233913376654435,
      "grad_norm": 0.4028758406639099,
      "learning_rate": 0.0001130956470392894,
      "loss": 2.4008,
      "step": 69190
    },
    {
      "epoch": 1.4235970571823113,
      "grad_norm": 0.4624662399291992,
      "learning_rate": 0.000113073628847695,
      "loss": 2.3858,
      "step": 69200
    },
    {
      "epoch": 1.423802776699179,
      "grad_norm": 0.45024076104164124,
      "learning_rate": 0.00011305161001124974,
      "loss": 2.3696,
      "step": 69210
    },
    {
      "epoch": 1.4240084962160466,
      "grad_norm": 0.4330618381500244,
      "learning_rate": 0.00011302959053103963,
      "loss": 2.346,
      "step": 69220
    },
    {
      "epoch": 1.4242142157329143,
      "grad_norm": 0.40852800011634827,
      "learning_rate": 0.00011300757040815085,
      "loss": 2.3868,
      "step": 69230
    },
    {
      "epoch": 1.424419935249782,
      "grad_norm": 0.43233850598335266,
      "learning_rate": 0.0001129855496436695,
      "loss": 2.3975,
      "step": 69240
    },
    {
      "epoch": 1.4246256547666498,
      "grad_norm": 0.45353958010673523,
      "learning_rate": 0.00011296352823868176,
      "loss": 2.4363,
      "step": 69250
    },
    {
      "epoch": 1.4248313742835175,
      "grad_norm": 0.420439213514328,
      "learning_rate": 0.0001129415061942738,
      "loss": 2.3545,
      "step": 69260
    },
    {
      "epoch": 1.4250370938003853,
      "grad_norm": 0.4393264055252075,
      "learning_rate": 0.00011291948351153187,
      "loss": 2.3364,
      "step": 69270
    },
    {
      "epoch": 1.425242813317253,
      "grad_norm": 0.5872249603271484,
      "learning_rate": 0.00011289746019154221,
      "loss": 2.4208,
      "step": 69280
    },
    {
      "epoch": 1.4254485328341207,
      "grad_norm": 0.47076961398124695,
      "learning_rate": 0.00011287543623539115,
      "loss": 2.3588,
      "step": 69290
    },
    {
      "epoch": 1.4256542523509883,
      "grad_norm": 0.5042780041694641,
      "learning_rate": 0.00011285341164416496,
      "loss": 2.4004,
      "step": 69300
    },
    {
      "epoch": 1.425859971867856,
      "grad_norm": 0.39398324489593506,
      "learning_rate": 0.00011283138641895001,
      "loss": 2.4135,
      "step": 69310
    },
    {
      "epoch": 1.4260656913847238,
      "grad_norm": 0.45165637135505676,
      "learning_rate": 0.00011280936056083269,
      "loss": 2.3988,
      "step": 69320
    },
    {
      "epoch": 1.4262714109015915,
      "grad_norm": 0.4106326997280121,
      "learning_rate": 0.00011278733407089946,
      "loss": 2.4097,
      "step": 69330
    },
    {
      "epoch": 1.4264771304184591,
      "grad_norm": 0.4249180257320404,
      "learning_rate": 0.00011276530695023668,
      "loss": 2.4124,
      "step": 69340
    },
    {
      "epoch": 1.426682849935327,
      "grad_norm": 0.48981374502182007,
      "learning_rate": 0.00011274327919993086,
      "loss": 2.392,
      "step": 69350
    },
    {
      "epoch": 1.4268885694521947,
      "grad_norm": 0.4872491657733917,
      "learning_rate": 0.00011272125082106857,
      "loss": 2.3678,
      "step": 69360
    },
    {
      "epoch": 1.4270942889690623,
      "grad_norm": 0.41804543137550354,
      "learning_rate": 0.00011269922181473627,
      "loss": 2.3795,
      "step": 69370
    },
    {
      "epoch": 1.42730000848593,
      "grad_norm": 0.4408727288246155,
      "learning_rate": 0.00011267719218202055,
      "loss": 2.3987,
      "step": 69380
    },
    {
      "epoch": 1.4275057280027978,
      "grad_norm": 0.44892534613609314,
      "learning_rate": 0.00011265516192400803,
      "loss": 2.386,
      "step": 69390
    },
    {
      "epoch": 1.4277114475196655,
      "grad_norm": 0.4271516501903534,
      "learning_rate": 0.00011263313104178535,
      "loss": 2.3496,
      "step": 69400
    },
    {
      "epoch": 1.4279171670365332,
      "grad_norm": 0.40670543909072876,
      "learning_rate": 0.00011261109953643913,
      "loss": 2.4056,
      "step": 69410
    },
    {
      "epoch": 1.428122886553401,
      "grad_norm": 0.44119778275489807,
      "learning_rate": 0.00011258906740905611,
      "loss": 2.4103,
      "step": 69420
    },
    {
      "epoch": 1.4283286060702687,
      "grad_norm": 0.44284602999687195,
      "learning_rate": 0.000112567034660723,
      "loss": 2.3753,
      "step": 69430
    },
    {
      "epoch": 1.4285343255871363,
      "grad_norm": 0.4361564517021179,
      "learning_rate": 0.00011254500129252654,
      "loss": 2.4053,
      "step": 69440
    },
    {
      "epoch": 1.428740045104004,
      "grad_norm": 0.3902980387210846,
      "learning_rate": 0.00011252296730555355,
      "loss": 2.3473,
      "step": 69450
    },
    {
      "epoch": 1.4289457646208719,
      "grad_norm": 0.4888557195663452,
      "learning_rate": 0.00011250093270089082,
      "loss": 2.3541,
      "step": 69460
    },
    {
      "epoch": 1.4291514841377395,
      "grad_norm": 0.43405845761299133,
      "learning_rate": 0.00011247889747962517,
      "loss": 2.3778,
      "step": 69470
    },
    {
      "epoch": 1.4293572036546072,
      "grad_norm": 0.6164171099662781,
      "learning_rate": 0.00011245686164284352,
      "loss": 2.3704,
      "step": 69480
    },
    {
      "epoch": 1.429562923171475,
      "grad_norm": 0.46964532136917114,
      "learning_rate": 0.0001124348251916328,
      "loss": 2.418,
      "step": 69490
    },
    {
      "epoch": 1.4297686426883427,
      "grad_norm": 0.4291011691093445,
      "learning_rate": 0.0001124127881270799,
      "loss": 2.4117,
      "step": 69500
    },
    {
      "epoch": 1.4299743622052103,
      "grad_norm": 0.4730566740036011,
      "learning_rate": 0.00011239075045027176,
      "loss": 2.4204,
      "step": 69510
    },
    {
      "epoch": 1.430180081722078,
      "grad_norm": 0.42590487003326416,
      "learning_rate": 0.00011236871216229548,
      "loss": 2.3447,
      "step": 69520
    },
    {
      "epoch": 1.4303858012389457,
      "grad_norm": 0.409324586391449,
      "learning_rate": 0.000112346673264238,
      "loss": 2.3578,
      "step": 69530
    },
    {
      "epoch": 1.4305915207558135,
      "grad_norm": 0.4634983539581299,
      "learning_rate": 0.00011232463375718642,
      "loss": 2.3915,
      "step": 69540
    },
    {
      "epoch": 1.4307972402726812,
      "grad_norm": 0.44901636242866516,
      "learning_rate": 0.00011230259364222782,
      "loss": 2.3756,
      "step": 69550
    },
    {
      "epoch": 1.431002959789549,
      "grad_norm": 0.4200117886066437,
      "learning_rate": 0.0001122805529204493,
      "loss": 2.4125,
      "step": 69560
    },
    {
      "epoch": 1.4312086793064167,
      "grad_norm": 0.41961681842803955,
      "learning_rate": 0.00011225851159293805,
      "loss": 2.3549,
      "step": 69570
    },
    {
      "epoch": 1.4314143988232844,
      "grad_norm": 0.45672616362571716,
      "learning_rate": 0.0001122364696607812,
      "loss": 2.3838,
      "step": 69580
    },
    {
      "epoch": 1.431620118340152,
      "grad_norm": 0.47266533970832825,
      "learning_rate": 0.00011221442712506601,
      "loss": 2.4066,
      "step": 69590
    },
    {
      "epoch": 1.4318258378570197,
      "grad_norm": 0.40115541219711304,
      "learning_rate": 0.00011219238398687966,
      "loss": 2.3802,
      "step": 69600
    },
    {
      "epoch": 1.4320315573738875,
      "grad_norm": 0.3931298553943634,
      "learning_rate": 0.00011217034024730946,
      "loss": 2.3687,
      "step": 69610
    },
    {
      "epoch": 1.4322372768907552,
      "grad_norm": 0.4401863217353821,
      "learning_rate": 0.00011214829590744271,
      "loss": 2.4194,
      "step": 69620
    },
    {
      "epoch": 1.432442996407623,
      "grad_norm": 0.4363136887550354,
      "learning_rate": 0.00011212625096836671,
      "loss": 2.4055,
      "step": 69630
    },
    {
      "epoch": 1.4326487159244907,
      "grad_norm": 0.41502517461776733,
      "learning_rate": 0.00011210420543116882,
      "loss": 2.4405,
      "step": 69640
    },
    {
      "epoch": 1.4328544354413584,
      "grad_norm": 0.4677179455757141,
      "learning_rate": 0.00011208215929693646,
      "loss": 2.3613,
      "step": 69650
    },
    {
      "epoch": 1.433060154958226,
      "grad_norm": 0.42443016171455383,
      "learning_rate": 0.00011206011256675702,
      "loss": 2.4368,
      "step": 69660
    },
    {
      "epoch": 1.4332658744750937,
      "grad_norm": 0.4093998670578003,
      "learning_rate": 0.00011203806524171791,
      "loss": 2.3958,
      "step": 69670
    },
    {
      "epoch": 1.4334715939919616,
      "grad_norm": 0.4147416353225708,
      "learning_rate": 0.00011201601732290669,
      "loss": 2.382,
      "step": 69680
    },
    {
      "epoch": 1.4336773135088292,
      "grad_norm": 0.45675504207611084,
      "learning_rate": 0.0001119939688114108,
      "loss": 2.4136,
      "step": 69690
    },
    {
      "epoch": 1.4338830330256969,
      "grad_norm": 0.48475581407546997,
      "learning_rate": 0.00011197191970831778,
      "loss": 2.3976,
      "step": 69700
    },
    {
      "epoch": 1.4340887525425647,
      "grad_norm": 0.43249160051345825,
      "learning_rate": 0.0001119498700147152,
      "loss": 2.3993,
      "step": 69710
    },
    {
      "epoch": 1.4342944720594324,
      "grad_norm": 0.39963123202323914,
      "learning_rate": 0.00011192781973169064,
      "loss": 2.4159,
      "step": 69720
    },
    {
      "epoch": 1.4345001915763,
      "grad_norm": 0.4458793103694916,
      "learning_rate": 0.00011190576886033173,
      "loss": 2.3569,
      "step": 69730
    },
    {
      "epoch": 1.4347059110931677,
      "grad_norm": 0.5166347026824951,
      "learning_rate": 0.00011188371740172615,
      "loss": 2.4281,
      "step": 69740
    },
    {
      "epoch": 1.4349116306100356,
      "grad_norm": 0.3940453827381134,
      "learning_rate": 0.00011186166535696152,
      "loss": 2.4274,
      "step": 69750
    },
    {
      "epoch": 1.4351173501269032,
      "grad_norm": 0.417115718126297,
      "learning_rate": 0.0001118396127271256,
      "loss": 2.4023,
      "step": 69760
    },
    {
      "epoch": 1.4353230696437709,
      "grad_norm": 0.4453034996986389,
      "learning_rate": 0.00011181755951330608,
      "loss": 2.4108,
      "step": 69770
    },
    {
      "epoch": 1.4355287891606388,
      "grad_norm": 0.477641224861145,
      "learning_rate": 0.00011179550571659079,
      "loss": 2.4735,
      "step": 69780
    },
    {
      "epoch": 1.4357345086775064,
      "grad_norm": 0.4421411454677582,
      "learning_rate": 0.0001117734513380674,
      "loss": 2.3671,
      "step": 69790
    },
    {
      "epoch": 1.435940228194374,
      "grad_norm": 0.43571290373802185,
      "learning_rate": 0.00011175139637882388,
      "loss": 2.3775,
      "step": 69800
    },
    {
      "epoch": 1.4361459477112417,
      "grad_norm": 0.4299193024635315,
      "learning_rate": 0.00011172934083994802,
      "loss": 2.3775,
      "step": 69810
    },
    {
      "epoch": 1.4363516672281096,
      "grad_norm": 0.4324856996536255,
      "learning_rate": 0.00011170728472252765,
      "loss": 2.3448,
      "step": 69820
    },
    {
      "epoch": 1.4365573867449772,
      "grad_norm": 0.5179688334465027,
      "learning_rate": 0.00011168522802765073,
      "loss": 2.439,
      "step": 69830
    },
    {
      "epoch": 1.436763106261845,
      "grad_norm": 0.41259345412254333,
      "learning_rate": 0.00011166317075640522,
      "loss": 2.3792,
      "step": 69840
    },
    {
      "epoch": 1.4369688257787128,
      "grad_norm": 0.5006593465805054,
      "learning_rate": 0.00011164111290987905,
      "loss": 2.3883,
      "step": 69850
    },
    {
      "epoch": 1.4371745452955804,
      "grad_norm": 0.4569980204105377,
      "learning_rate": 0.00011161905448916019,
      "loss": 2.4471,
      "step": 69860
    },
    {
      "epoch": 1.437380264812448,
      "grad_norm": 0.4403553605079651,
      "learning_rate": 0.00011159699549533668,
      "loss": 2.3937,
      "step": 69870
    },
    {
      "epoch": 1.4375859843293157,
      "grad_norm": 0.4390749931335449,
      "learning_rate": 0.0001115749359294966,
      "loss": 2.3755,
      "step": 69880
    },
    {
      "epoch": 1.4377917038461836,
      "grad_norm": 0.4466102719306946,
      "learning_rate": 0.00011155287579272799,
      "loss": 2.3669,
      "step": 69890
    },
    {
      "epoch": 1.4379974233630513,
      "grad_norm": 0.43003201484680176,
      "learning_rate": 0.000111530815086119,
      "loss": 2.3834,
      "step": 69900
    },
    {
      "epoch": 1.438203142879919,
      "grad_norm": 0.44611379504203796,
      "learning_rate": 0.00011150875381075771,
      "loss": 2.4016,
      "step": 69910
    },
    {
      "epoch": 1.4384088623967868,
      "grad_norm": 0.38717636466026306,
      "learning_rate": 0.00011148669196773234,
      "loss": 2.3901,
      "step": 69920
    },
    {
      "epoch": 1.4386145819136544,
      "grad_norm": 0.4361266791820526,
      "learning_rate": 0.00011146462955813101,
      "loss": 2.4183,
      "step": 69930
    },
    {
      "epoch": 1.438820301430522,
      "grad_norm": 0.4528929889202118,
      "learning_rate": 0.00011144256658304205,
      "loss": 2.4151,
      "step": 69940
    },
    {
      "epoch": 1.4390260209473897,
      "grad_norm": 0.46600738167762756,
      "learning_rate": 0.00011142050304355356,
      "loss": 2.4133,
      "step": 69950
    },
    {
      "epoch": 1.4392317404642574,
      "grad_norm": 0.3979988396167755,
      "learning_rate": 0.00011139843894075391,
      "loss": 2.3893,
      "step": 69960
    },
    {
      "epoch": 1.4394374599811253,
      "grad_norm": 0.41239190101623535,
      "learning_rate": 0.0001113763742757314,
      "loss": 2.4032,
      "step": 69970
    },
    {
      "epoch": 1.439643179497993,
      "grad_norm": 0.44048672914505005,
      "learning_rate": 0.00011135430904957433,
      "loss": 2.4055,
      "step": 69980
    },
    {
      "epoch": 1.4398488990148608,
      "grad_norm": 0.4063955545425415,
      "learning_rate": 0.00011133224326337106,
      "loss": 2.4385,
      "step": 69990
    },
    {
      "epoch": 1.4400546185317284,
      "grad_norm": 0.47871410846710205,
      "learning_rate": 0.00011131017691821,
      "loss": 2.3797,
      "step": 70000
    },
    {
      "epoch": 1.440260338048596,
      "grad_norm": 0.40207165479660034,
      "learning_rate": 0.00011128811001517951,
      "loss": 2.3737,
      "step": 70010
    },
    {
      "epoch": 1.4404660575654638,
      "grad_norm": 0.43991056084632874,
      "learning_rate": 0.0001112660425553681,
      "loss": 2.4136,
      "step": 70020
    },
    {
      "epoch": 1.4406717770823314,
      "grad_norm": 0.4618576169013977,
      "learning_rate": 0.00011124397453986419,
      "loss": 2.4258,
      "step": 70030
    },
    {
      "epoch": 1.4408774965991993,
      "grad_norm": 0.4075639843940735,
      "learning_rate": 0.0001112219059697563,
      "loss": 2.3581,
      "step": 70040
    },
    {
      "epoch": 1.441083216116067,
      "grad_norm": 0.43109312653541565,
      "learning_rate": 0.00011119983684613293,
      "loss": 2.4463,
      "step": 70050
    },
    {
      "epoch": 1.4412889356329346,
      "grad_norm": 0.4200993478298187,
      "learning_rate": 0.00011117776717008264,
      "loss": 2.3856,
      "step": 70060
    },
    {
      "epoch": 1.4414946551498025,
      "grad_norm": 0.421130895614624,
      "learning_rate": 0.00011115569694269403,
      "loss": 2.4638,
      "step": 70070
    },
    {
      "epoch": 1.4417003746666701,
      "grad_norm": 0.4532637298107147,
      "learning_rate": 0.00011113362616505563,
      "loss": 2.3976,
      "step": 70080
    },
    {
      "epoch": 1.4419060941835378,
      "grad_norm": 0.40069887042045593,
      "learning_rate": 0.00011111155483825615,
      "loss": 2.4003,
      "step": 70090
    },
    {
      "epoch": 1.4421118137004054,
      "grad_norm": 0.42045915126800537,
      "learning_rate": 0.00011108948296338424,
      "loss": 2.4431,
      "step": 70100
    },
    {
      "epoch": 1.4423175332172733,
      "grad_norm": 0.49442222714424133,
      "learning_rate": 0.00011106741054152856,
      "loss": 2.3727,
      "step": 70110
    },
    {
      "epoch": 1.442523252734141,
      "grad_norm": 0.4202464818954468,
      "learning_rate": 0.0001110453375737778,
      "loss": 2.436,
      "step": 70120
    },
    {
      "epoch": 1.4427289722510086,
      "grad_norm": 0.4379613995552063,
      "learning_rate": 0.00011102326406122076,
      "loss": 2.4182,
      "step": 70130
    },
    {
      "epoch": 1.4429346917678765,
      "grad_norm": 0.41306254267692566,
      "learning_rate": 0.00011100119000494616,
      "loss": 2.4514,
      "step": 70140
    },
    {
      "epoch": 1.4431404112847441,
      "grad_norm": 0.41720667481422424,
      "learning_rate": 0.0001109791154060428,
      "loss": 2.414,
      "step": 70150
    },
    {
      "epoch": 1.4433461308016118,
      "grad_norm": 0.42582184076309204,
      "learning_rate": 0.0001109570402655995,
      "loss": 2.3968,
      "step": 70160
    },
    {
      "epoch": 1.4435518503184794,
      "grad_norm": 0.47460734844207764,
      "learning_rate": 0.00011093496458470514,
      "loss": 2.4164,
      "step": 70170
    },
    {
      "epoch": 1.4437575698353473,
      "grad_norm": 0.4753202795982361,
      "learning_rate": 0.00011091288836444853,
      "loss": 2.3683,
      "step": 70180
    },
    {
      "epoch": 1.443963289352215,
      "grad_norm": 0.4725710451602936,
      "learning_rate": 0.00011089081160591864,
      "loss": 2.3793,
      "step": 70190
    },
    {
      "epoch": 1.4441690088690826,
      "grad_norm": 0.4330728352069855,
      "learning_rate": 0.00011086873431020433,
      "loss": 2.4353,
      "step": 70200
    },
    {
      "epoch": 1.4443747283859505,
      "grad_norm": 0.4207872450351715,
      "learning_rate": 0.00011084665647839461,
      "loss": 2.4133,
      "step": 70210
    },
    {
      "epoch": 1.4445804479028181,
      "grad_norm": 0.420168936252594,
      "learning_rate": 0.0001108245781115784,
      "loss": 2.3802,
      "step": 70220
    },
    {
      "epoch": 1.4447861674196858,
      "grad_norm": 0.43174195289611816,
      "learning_rate": 0.00011080249921084478,
      "loss": 2.4172,
      "step": 70230
    },
    {
      "epoch": 1.4449918869365534,
      "grad_norm": 0.443738728761673,
      "learning_rate": 0.00011078041977728271,
      "loss": 2.419,
      "step": 70240
    },
    {
      "epoch": 1.4451976064534213,
      "grad_norm": 0.43624499440193176,
      "learning_rate": 0.00011075833981198129,
      "loss": 2.3792,
      "step": 70250
    },
    {
      "epoch": 1.445403325970289,
      "grad_norm": 0.4405396282672882,
      "learning_rate": 0.0001107362593160296,
      "loss": 2.3532,
      "step": 70260
    },
    {
      "epoch": 1.4456090454871566,
      "grad_norm": 0.4763719141483307,
      "learning_rate": 0.00011071417829051673,
      "loss": 2.3823,
      "step": 70270
    },
    {
      "epoch": 1.4458147650040245,
      "grad_norm": 0.44087278842926025,
      "learning_rate": 0.00011069209673653182,
      "loss": 2.4728,
      "step": 70280
    },
    {
      "epoch": 1.4460204845208922,
      "grad_norm": 0.45991671085357666,
      "learning_rate": 0.0001106700146551641,
      "loss": 2.445,
      "step": 70290
    },
    {
      "epoch": 1.4462262040377598,
      "grad_norm": 0.4487013816833496,
      "learning_rate": 0.00011064793204750264,
      "loss": 2.4279,
      "step": 70300
    },
    {
      "epoch": 1.4464319235546275,
      "grad_norm": 0.4612817168235779,
      "learning_rate": 0.00011062584891463674,
      "loss": 2.4106,
      "step": 70310
    },
    {
      "epoch": 1.4466376430714951,
      "grad_norm": 0.4123859107494354,
      "learning_rate": 0.00011060376525765561,
      "loss": 2.4149,
      "step": 70320
    },
    {
      "epoch": 1.446843362588363,
      "grad_norm": 0.4672383964061737,
      "learning_rate": 0.00011058168107764852,
      "loss": 2.3907,
      "step": 70330
    },
    {
      "epoch": 1.4470490821052306,
      "grad_norm": 0.4202883243560791,
      "learning_rate": 0.00011055959637570478,
      "loss": 2.3697,
      "step": 70340
    },
    {
      "epoch": 1.4472548016220985,
      "grad_norm": 0.43487486243247986,
      "learning_rate": 0.00011053751115291369,
      "loss": 2.4267,
      "step": 70350
    },
    {
      "epoch": 1.4474605211389662,
      "grad_norm": 0.505186140537262,
      "learning_rate": 0.00011051542541036459,
      "loss": 2.4294,
      "step": 70360
    },
    {
      "epoch": 1.4476662406558338,
      "grad_norm": 0.5032680630683899,
      "learning_rate": 0.00011049333914914686,
      "loss": 2.3952,
      "step": 70370
    },
    {
      "epoch": 1.4478719601727015,
      "grad_norm": 0.40673527121543884,
      "learning_rate": 0.0001104712523703499,
      "loss": 2.4165,
      "step": 70380
    },
    {
      "epoch": 1.4480776796895691,
      "grad_norm": 0.42673763632774353,
      "learning_rate": 0.00011044916507506313,
      "loss": 2.3966,
      "step": 70390
    },
    {
      "epoch": 1.448283399206437,
      "grad_norm": 0.44124677777290344,
      "learning_rate": 0.00011042707726437596,
      "loss": 2.4719,
      "step": 70400
    },
    {
      "epoch": 1.4484891187233047,
      "grad_norm": 0.46803876757621765,
      "learning_rate": 0.0001104049889393779,
      "loss": 2.3859,
      "step": 70410
    },
    {
      "epoch": 1.4486948382401723,
      "grad_norm": 0.39439645409584045,
      "learning_rate": 0.00011038290010115843,
      "loss": 2.4308,
      "step": 70420
    },
    {
      "epoch": 1.4489005577570402,
      "grad_norm": 0.5029056668281555,
      "learning_rate": 0.00011036081075080711,
      "loss": 2.4427,
      "step": 70430
    },
    {
      "epoch": 1.4491062772739078,
      "grad_norm": 0.5242897272109985,
      "learning_rate": 0.00011033872088941339,
      "loss": 2.4394,
      "step": 70440
    },
    {
      "epoch": 1.4493119967907755,
      "grad_norm": 0.410317599773407,
      "learning_rate": 0.00011031663051806698,
      "loss": 2.3793,
      "step": 70450
    },
    {
      "epoch": 1.4495177163076431,
      "grad_norm": 0.39662110805511475,
      "learning_rate": 0.00011029453963785736,
      "loss": 2.4213,
      "step": 70460
    },
    {
      "epoch": 1.449723435824511,
      "grad_norm": 0.45154106616973877,
      "learning_rate": 0.00011027244824987422,
      "loss": 2.3559,
      "step": 70470
    },
    {
      "epoch": 1.4499291553413787,
      "grad_norm": 0.4676002264022827,
      "learning_rate": 0.00011025035635520718,
      "loss": 2.4382,
      "step": 70480
    },
    {
      "epoch": 1.4501348748582463,
      "grad_norm": 0.45324352383613586,
      "learning_rate": 0.00011022826395494592,
      "loss": 2.3728,
      "step": 70490
    },
    {
      "epoch": 1.4503405943751142,
      "grad_norm": 0.4522610306739807,
      "learning_rate": 0.00011020617105018015,
      "loss": 2.418,
      "step": 70500
    },
    {
      "epoch": 1.4505463138919819,
      "grad_norm": 0.40338829159736633,
      "learning_rate": 0.00011018407764199953,
      "loss": 2.4099,
      "step": 70510
    },
    {
      "epoch": 1.4507520334088495,
      "grad_norm": 0.429436057806015,
      "learning_rate": 0.0001101619837314939,
      "loss": 2.4332,
      "step": 70520
    },
    {
      "epoch": 1.4509577529257172,
      "grad_norm": 0.43732795119285583,
      "learning_rate": 0.00011013988931975298,
      "loss": 2.4188,
      "step": 70530
    },
    {
      "epoch": 1.451163472442585,
      "grad_norm": 0.47320613265037537,
      "learning_rate": 0.00011011779440786659,
      "loss": 2.3758,
      "step": 70540
    },
    {
      "epoch": 1.4513691919594527,
      "grad_norm": 0.42525631189346313,
      "learning_rate": 0.0001100956989969245,
      "loss": 2.3778,
      "step": 70550
    },
    {
      "epoch": 1.4515749114763203,
      "grad_norm": 0.45091739296913147,
      "learning_rate": 0.00011007360308801662,
      "loss": 2.3754,
      "step": 70560
    },
    {
      "epoch": 1.4517806309931882,
      "grad_norm": 0.3914840519428253,
      "learning_rate": 0.00011005150668223279,
      "loss": 2.4214,
      "step": 70570
    },
    {
      "epoch": 1.4519863505100559,
      "grad_norm": 0.42010682821273804,
      "learning_rate": 0.00011002940978066292,
      "loss": 2.3759,
      "step": 70580
    },
    {
      "epoch": 1.4521920700269235,
      "grad_norm": 0.41215696930885315,
      "learning_rate": 0.00011000731238439689,
      "loss": 2.3816,
      "step": 70590
    },
    {
      "epoch": 1.4523977895437912,
      "grad_norm": 0.46377357840538025,
      "learning_rate": 0.00010998521449452468,
      "loss": 2.38,
      "step": 70600
    },
    {
      "epoch": 1.452603509060659,
      "grad_norm": 0.44799739122390747,
      "learning_rate": 0.00010996311611213628,
      "loss": 2.42,
      "step": 70610
    },
    {
      "epoch": 1.4528092285775267,
      "grad_norm": 0.42238301038742065,
      "learning_rate": 0.00010994101723832165,
      "loss": 2.3955,
      "step": 70620
    },
    {
      "epoch": 1.4530149480943944,
      "grad_norm": 0.4673033058643341,
      "learning_rate": 0.0001099189178741708,
      "loss": 2.4191,
      "step": 70630
    },
    {
      "epoch": 1.4532206676112622,
      "grad_norm": 0.4802843928337097,
      "learning_rate": 0.00010989681802077376,
      "loss": 2.4202,
      "step": 70640
    },
    {
      "epoch": 1.4534263871281299,
      "grad_norm": 0.4226260483264923,
      "learning_rate": 0.00010987471767922064,
      "loss": 2.3401,
      "step": 70650
    },
    {
      "epoch": 1.4536321066449975,
      "grad_norm": 0.4320646822452545,
      "learning_rate": 0.00010985261685060151,
      "loss": 2.3686,
      "step": 70660
    },
    {
      "epoch": 1.4538378261618652,
      "grad_norm": 0.4444398880004883,
      "learning_rate": 0.00010983051553600647,
      "loss": 2.4393,
      "step": 70670
    },
    {
      "epoch": 1.4540435456787328,
      "grad_norm": 0.4692653715610504,
      "learning_rate": 0.00010980841373652569,
      "loss": 2.3761,
      "step": 70680
    },
    {
      "epoch": 1.4542492651956007,
      "grad_norm": 0.4359762966632843,
      "learning_rate": 0.0001097863114532493,
      "loss": 2.3832,
      "step": 70690
    },
    {
      "epoch": 1.4544549847124684,
      "grad_norm": 0.44669637084007263,
      "learning_rate": 0.00010976420868726749,
      "loss": 2.3635,
      "step": 70700
    },
    {
      "epoch": 1.4546607042293362,
      "grad_norm": 0.4400405287742615,
      "learning_rate": 0.0001097421054396705,
      "loss": 2.4286,
      "step": 70710
    },
    {
      "epoch": 1.454866423746204,
      "grad_norm": 0.4470328986644745,
      "learning_rate": 0.00010972000171154848,
      "loss": 2.4261,
      "step": 70720
    },
    {
      "epoch": 1.4550721432630715,
      "grad_norm": 0.4205114543437958,
      "learning_rate": 0.00010969789750399177,
      "loss": 2.4585,
      "step": 70730
    },
    {
      "epoch": 1.4552778627799392,
      "grad_norm": 0.43254050612449646,
      "learning_rate": 0.00010967579281809066,
      "loss": 2.3929,
      "step": 70740
    },
    {
      "epoch": 1.4554835822968069,
      "grad_norm": 0.42583954334259033,
      "learning_rate": 0.00010965368765493538,
      "loss": 2.3826,
      "step": 70750
    },
    {
      "epoch": 1.4556893018136747,
      "grad_norm": 0.4056050181388855,
      "learning_rate": 0.00010963158201561628,
      "loss": 2.4082,
      "step": 70760
    },
    {
      "epoch": 1.4558950213305424,
      "grad_norm": 0.4068637788295746,
      "learning_rate": 0.00010960947590122377,
      "loss": 2.4258,
      "step": 70770
    },
    {
      "epoch": 1.4561007408474103,
      "grad_norm": 0.4413180947303772,
      "learning_rate": 0.00010958736931284815,
      "loss": 2.4611,
      "step": 70780
    },
    {
      "epoch": 1.456306460364278,
      "grad_norm": 0.43915289640426636,
      "learning_rate": 0.00010956526225157985,
      "loss": 2.4207,
      "step": 70790
    },
    {
      "epoch": 1.4565121798811456,
      "grad_norm": 0.43029963970184326,
      "learning_rate": 0.00010954315471850929,
      "loss": 2.3488,
      "step": 70800
    },
    {
      "epoch": 1.4567178993980132,
      "grad_norm": 0.4890739321708679,
      "learning_rate": 0.00010952104671472692,
      "loss": 2.4065,
      "step": 70810
    },
    {
      "epoch": 1.4569236189148809,
      "grad_norm": 0.4426461160182953,
      "learning_rate": 0.00010949893824132318,
      "loss": 2.4185,
      "step": 70820
    },
    {
      "epoch": 1.4571293384317487,
      "grad_norm": 0.42900246381759644,
      "learning_rate": 0.00010947682929938858,
      "loss": 2.444,
      "step": 70830
    },
    {
      "epoch": 1.4573350579486164,
      "grad_norm": 0.4369579553604126,
      "learning_rate": 0.00010945471989001366,
      "loss": 2.443,
      "step": 70840
    },
    {
      "epoch": 1.457540777465484,
      "grad_norm": 0.4840281307697296,
      "learning_rate": 0.00010943261001428891,
      "loss": 2.4218,
      "step": 70850
    },
    {
      "epoch": 1.457746496982352,
      "grad_norm": 0.48482000827789307,
      "learning_rate": 0.0001094104996733049,
      "loss": 2.4136,
      "step": 70860
    },
    {
      "epoch": 1.4579522164992196,
      "grad_norm": 0.4058401584625244,
      "learning_rate": 0.00010938838886815226,
      "loss": 2.4251,
      "step": 70870
    },
    {
      "epoch": 1.4581579360160872,
      "grad_norm": 0.4349014461040497,
      "learning_rate": 0.00010936627759992151,
      "loss": 2.349,
      "step": 70880
    },
    {
      "epoch": 1.4583636555329549,
      "grad_norm": 0.4081551134586334,
      "learning_rate": 0.00010934416586970335,
      "loss": 2.4284,
      "step": 70890
    },
    {
      "epoch": 1.4585693750498228,
      "grad_norm": 0.4080178141593933,
      "learning_rate": 0.00010932205367858841,
      "loss": 2.3518,
      "step": 70900
    },
    {
      "epoch": 1.4587750945666904,
      "grad_norm": 0.4630163013935089,
      "learning_rate": 0.00010929994102766737,
      "loss": 2.3768,
      "step": 70910
    },
    {
      "epoch": 1.458980814083558,
      "grad_norm": 0.4892992079257965,
      "learning_rate": 0.00010927782791803088,
      "loss": 2.462,
      "step": 70920
    },
    {
      "epoch": 1.459186533600426,
      "grad_norm": 0.4454927444458008,
      "learning_rate": 0.00010925571435076974,
      "loss": 2.4126,
      "step": 70930
    },
    {
      "epoch": 1.4593922531172936,
      "grad_norm": 0.43295055627822876,
      "learning_rate": 0.00010923360032697463,
      "loss": 2.3931,
      "step": 70940
    },
    {
      "epoch": 1.4595979726341612,
      "grad_norm": 0.4198334515094757,
      "learning_rate": 0.00010921148584773633,
      "loss": 2.3923,
      "step": 70950
    },
    {
      "epoch": 1.459803692151029,
      "grad_norm": 0.4536208510398865,
      "learning_rate": 0.00010918937091414563,
      "loss": 2.4313,
      "step": 70960
    },
    {
      "epoch": 1.4600094116678968,
      "grad_norm": 0.4310471713542938,
      "learning_rate": 0.00010916725552729334,
      "loss": 2.3696,
      "step": 70970
    },
    {
      "epoch": 1.4602151311847644,
      "grad_norm": 0.4441155791282654,
      "learning_rate": 0.0001091451396882703,
      "loss": 2.4182,
      "step": 70980
    },
    {
      "epoch": 1.460420850701632,
      "grad_norm": 0.4860844910144806,
      "learning_rate": 0.00010912302339816733,
      "loss": 2.4433,
      "step": 70990
    },
    {
      "epoch": 1.4606265702185,
      "grad_norm": 0.4412427246570587,
      "learning_rate": 0.00010910090665807535,
      "loss": 2.4151,
      "step": 71000
    },
    {
      "epoch": 1.4608322897353676,
      "grad_norm": 0.42379137873649597,
      "learning_rate": 0.00010907878946908523,
      "loss": 2.3963,
      "step": 71010
    },
    {
      "epoch": 1.4610380092522353,
      "grad_norm": 0.4746706485748291,
      "learning_rate": 0.00010905667183228792,
      "loss": 2.407,
      "step": 71020
    },
    {
      "epoch": 1.461243728769103,
      "grad_norm": 0.43879979848861694,
      "learning_rate": 0.00010903455374877433,
      "loss": 2.3699,
      "step": 71030
    },
    {
      "epoch": 1.4614494482859706,
      "grad_norm": 0.4565240442752838,
      "learning_rate": 0.0001090124352196354,
      "loss": 2.4317,
      "step": 71040
    },
    {
      "epoch": 1.4616551678028384,
      "grad_norm": 0.4534306228160858,
      "learning_rate": 0.00010899031624596218,
      "loss": 2.4568,
      "step": 71050
    },
    {
      "epoch": 1.461860887319706,
      "grad_norm": 0.44539767503738403,
      "learning_rate": 0.0001089681968288457,
      "loss": 2.4288,
      "step": 71060
    },
    {
      "epoch": 1.462066606836574,
      "grad_norm": 0.40597259998321533,
      "learning_rate": 0.00010894607696937689,
      "loss": 2.4176,
      "step": 71070
    },
    {
      "epoch": 1.4622723263534416,
      "grad_norm": 0.424723356962204,
      "learning_rate": 0.00010892395666864684,
      "loss": 2.3902,
      "step": 71080
    },
    {
      "epoch": 1.4624780458703093,
      "grad_norm": 0.4726783335208893,
      "learning_rate": 0.00010890183592774668,
      "loss": 2.3981,
      "step": 71090
    },
    {
      "epoch": 1.462683765387177,
      "grad_norm": 0.389454185962677,
      "learning_rate": 0.00010887971474776741,
      "loss": 2.431,
      "step": 71100
    },
    {
      "epoch": 1.4628894849040446,
      "grad_norm": 0.4246957302093506,
      "learning_rate": 0.00010885759312980024,
      "loss": 2.433,
      "step": 71110
    },
    {
      "epoch": 1.4630952044209125,
      "grad_norm": 0.4717521071434021,
      "learning_rate": 0.00010883547107493626,
      "loss": 2.3649,
      "step": 71120
    },
    {
      "epoch": 1.46330092393778,
      "grad_norm": 0.40446698665618896,
      "learning_rate": 0.00010881334858426664,
      "loss": 2.391,
      "step": 71130
    },
    {
      "epoch": 1.463506643454648,
      "grad_norm": 0.40434637665748596,
      "learning_rate": 0.00010879122565888255,
      "loss": 2.3738,
      "step": 71140
    },
    {
      "epoch": 1.4637123629715156,
      "grad_norm": 0.6245056390762329,
      "learning_rate": 0.00010876910229987522,
      "loss": 2.436,
      "step": 71150
    },
    {
      "epoch": 1.4639180824883833,
      "grad_norm": 0.4661966860294342,
      "learning_rate": 0.00010874697850833586,
      "loss": 2.4457,
      "step": 71160
    },
    {
      "epoch": 1.464123802005251,
      "grad_norm": 0.4381188750267029,
      "learning_rate": 0.00010872485428535571,
      "loss": 2.3965,
      "step": 71170
    },
    {
      "epoch": 1.4643295215221186,
      "grad_norm": 0.45943188667297363,
      "learning_rate": 0.00010870272963202604,
      "loss": 2.3472,
      "step": 71180
    },
    {
      "epoch": 1.4645352410389865,
      "grad_norm": 0.46618103981018066,
      "learning_rate": 0.00010868060454943816,
      "loss": 2.3623,
      "step": 71190
    },
    {
      "epoch": 1.4647409605558541,
      "grad_norm": 0.4373440146446228,
      "learning_rate": 0.00010865847903868333,
      "loss": 2.4179,
      "step": 71200
    },
    {
      "epoch": 1.4649466800727218,
      "grad_norm": 0.43521058559417725,
      "learning_rate": 0.00010863635310085291,
      "loss": 2.4201,
      "step": 71210
    },
    {
      "epoch": 1.4651523995895896,
      "grad_norm": 0.4323611557483673,
      "learning_rate": 0.0001086142267370383,
      "loss": 2.4747,
      "step": 71220
    },
    {
      "epoch": 1.4653581191064573,
      "grad_norm": 0.4500255584716797,
      "learning_rate": 0.0001085920999483308,
      "loss": 2.4187,
      "step": 71230
    },
    {
      "epoch": 1.465563838623325,
      "grad_norm": 0.42970210313796997,
      "learning_rate": 0.0001085699727358218,
      "loss": 2.387,
      "step": 71240
    },
    {
      "epoch": 1.4657695581401926,
      "grad_norm": 0.41673099994659424,
      "learning_rate": 0.00010854784510060278,
      "loss": 2.3667,
      "step": 71250
    },
    {
      "epoch": 1.4659752776570605,
      "grad_norm": 0.4119321405887604,
      "learning_rate": 0.00010852571704376513,
      "loss": 2.3878,
      "step": 71260
    },
    {
      "epoch": 1.4661809971739281,
      "grad_norm": 0.4360271394252777,
      "learning_rate": 0.00010850358856640033,
      "loss": 2.4442,
      "step": 71270
    },
    {
      "epoch": 1.4663867166907958,
      "grad_norm": 0.4688520133495331,
      "learning_rate": 0.00010848145966959984,
      "loss": 2.4088,
      "step": 71280
    },
    {
      "epoch": 1.4665924362076637,
      "grad_norm": 0.41126328706741333,
      "learning_rate": 0.00010845933035445516,
      "loss": 2.335,
      "step": 71290
    },
    {
      "epoch": 1.4667981557245313,
      "grad_norm": 0.4269625246524811,
      "learning_rate": 0.0001084372006220578,
      "loss": 2.4127,
      "step": 71300
    },
    {
      "epoch": 1.467003875241399,
      "grad_norm": 0.46045276522636414,
      "learning_rate": 0.00010841507047349932,
      "loss": 2.3667,
      "step": 71310
    },
    {
      "epoch": 1.4672095947582666,
      "grad_norm": 0.4554521441459656,
      "learning_rate": 0.00010839293990987132,
      "loss": 2.4151,
      "step": 71320
    },
    {
      "epoch": 1.4674153142751345,
      "grad_norm": 0.48244571685791016,
      "learning_rate": 0.00010837080893226526,
      "loss": 2.3964,
      "step": 71330
    },
    {
      "epoch": 1.4676210337920021,
      "grad_norm": 0.4161219000816345,
      "learning_rate": 0.00010834867754177282,
      "loss": 2.3811,
      "step": 71340
    },
    {
      "epoch": 1.4678267533088698,
      "grad_norm": 0.4296422600746155,
      "learning_rate": 0.00010832654573948565,
      "loss": 2.372,
      "step": 71350
    },
    {
      "epoch": 1.4680324728257377,
      "grad_norm": 0.45703819394111633,
      "learning_rate": 0.00010830441352649531,
      "loss": 2.4738,
      "step": 71360
    },
    {
      "epoch": 1.4682381923426053,
      "grad_norm": 0.44147491455078125,
      "learning_rate": 0.00010828228090389352,
      "loss": 2.4105,
      "step": 71370
    },
    {
      "epoch": 1.468443911859473,
      "grad_norm": 0.4159700572490692,
      "learning_rate": 0.00010826014787277197,
      "loss": 2.4113,
      "step": 71380
    },
    {
      "epoch": 1.4686496313763406,
      "grad_norm": 0.4924801290035248,
      "learning_rate": 0.00010823801443422231,
      "loss": 2.3835,
      "step": 71390
    },
    {
      "epoch": 1.4688553508932083,
      "grad_norm": 0.5518187284469604,
      "learning_rate": 0.00010821588058933629,
      "loss": 2.3677,
      "step": 71400
    },
    {
      "epoch": 1.4690610704100762,
      "grad_norm": 0.4283577501773834,
      "learning_rate": 0.00010819374633920567,
      "loss": 2.4041,
      "step": 71410
    },
    {
      "epoch": 1.4692667899269438,
      "grad_norm": 0.41890498995780945,
      "learning_rate": 0.00010817161168492217,
      "loss": 2.3762,
      "step": 71420
    },
    {
      "epoch": 1.4694725094438117,
      "grad_norm": 0.4208131730556488,
      "learning_rate": 0.00010814947662757761,
      "loss": 2.3993,
      "step": 71430
    },
    {
      "epoch": 1.4696782289606793,
      "grad_norm": 0.4274563789367676,
      "learning_rate": 0.00010812734116826377,
      "loss": 2.3637,
      "step": 71440
    },
    {
      "epoch": 1.469883948477547,
      "grad_norm": 0.4068351686000824,
      "learning_rate": 0.00010810520530807249,
      "loss": 2.3714,
      "step": 71450
    },
    {
      "epoch": 1.4700896679944147,
      "grad_norm": 0.6014686822891235,
      "learning_rate": 0.0001080830690480956,
      "loss": 2.379,
      "step": 71460
    },
    {
      "epoch": 1.4702953875112823,
      "grad_norm": 0.4260067045688629,
      "learning_rate": 0.00010806093238942494,
      "loss": 2.4239,
      "step": 71470
    },
    {
      "epoch": 1.4705011070281502,
      "grad_norm": 0.4943352937698364,
      "learning_rate": 0.00010803879533315244,
      "loss": 2.4251,
      "step": 71480
    },
    {
      "epoch": 1.4707068265450178,
      "grad_norm": 0.43629634380340576,
      "learning_rate": 0.00010801665788036994,
      "loss": 2.4184,
      "step": 71490
    },
    {
      "epoch": 1.4709125460618857,
      "grad_norm": 0.4194718599319458,
      "learning_rate": 0.00010799452003216941,
      "loss": 2.3592,
      "step": 71500
    },
    {
      "epoch": 1.4711182655787534,
      "grad_norm": 0.4160512089729309,
      "learning_rate": 0.00010797238178964279,
      "loss": 2.3501,
      "step": 71510
    },
    {
      "epoch": 1.471323985095621,
      "grad_norm": 0.5155396461486816,
      "learning_rate": 0.00010795024315388202,
      "loss": 2.3805,
      "step": 71520
    },
    {
      "epoch": 1.4715297046124887,
      "grad_norm": 0.41144391894340515,
      "learning_rate": 0.00010792810412597904,
      "loss": 2.4262,
      "step": 71530
    },
    {
      "epoch": 1.4717354241293563,
      "grad_norm": 0.3866310715675354,
      "learning_rate": 0.00010790596470702592,
      "loss": 2.3284,
      "step": 71540
    },
    {
      "epoch": 1.4719411436462242,
      "grad_norm": 0.449300616979599,
      "learning_rate": 0.00010788382489811464,
      "loss": 2.3838,
      "step": 71550
    },
    {
      "epoch": 1.4721468631630918,
      "grad_norm": 0.42619788646698,
      "learning_rate": 0.00010786168470033724,
      "loss": 2.3976,
      "step": 71560
    },
    {
      "epoch": 1.4723525826799595,
      "grad_norm": 0.43730640411376953,
      "learning_rate": 0.00010783954411478576,
      "loss": 2.3794,
      "step": 71570
    },
    {
      "epoch": 1.4725583021968274,
      "grad_norm": 0.42862221598625183,
      "learning_rate": 0.00010781740314255229,
      "loss": 2.4303,
      "step": 71580
    },
    {
      "epoch": 1.472764021713695,
      "grad_norm": 0.4426872432231903,
      "learning_rate": 0.00010779526178472894,
      "loss": 2.4188,
      "step": 71590
    },
    {
      "epoch": 1.4729697412305627,
      "grad_norm": 0.44644302129745483,
      "learning_rate": 0.00010777312004240778,
      "loss": 2.4068,
      "step": 71600
    },
    {
      "epoch": 1.4731754607474303,
      "grad_norm": 0.4066923260688782,
      "learning_rate": 0.00010775097791668099,
      "loss": 2.4111,
      "step": 71610
    },
    {
      "epoch": 1.4733811802642982,
      "grad_norm": 0.44362950325012207,
      "learning_rate": 0.0001077288354086407,
      "loss": 2.4261,
      "step": 71620
    },
    {
      "epoch": 1.4735868997811659,
      "grad_norm": 0.4780294597148895,
      "learning_rate": 0.00010770669251937904,
      "loss": 2.3623,
      "step": 71630
    },
    {
      "epoch": 1.4737926192980335,
      "grad_norm": 0.4210876226425171,
      "learning_rate": 0.0001076845492499883,
      "loss": 2.358,
      "step": 71640
    },
    {
      "epoch": 1.4739983388149014,
      "grad_norm": 0.4244108498096466,
      "learning_rate": 0.00010766240560156053,
      "loss": 2.3982,
      "step": 71650
    },
    {
      "epoch": 1.474204058331769,
      "grad_norm": 0.4126015305519104,
      "learning_rate": 0.00010764026157518811,
      "loss": 2.4098,
      "step": 71660
    },
    {
      "epoch": 1.4744097778486367,
      "grad_norm": 0.419544517993927,
      "learning_rate": 0.00010761811717196324,
      "loss": 2.3828,
      "step": 71670
    },
    {
      "epoch": 1.4746154973655043,
      "grad_norm": 0.3922123908996582,
      "learning_rate": 0.00010759597239297811,
      "loss": 2.3564,
      "step": 71680
    },
    {
      "epoch": 1.4748212168823722,
      "grad_norm": 0.4180081784725189,
      "learning_rate": 0.00010757382723932505,
      "loss": 2.4144,
      "step": 71690
    },
    {
      "epoch": 1.4750269363992399,
      "grad_norm": 0.4107227325439453,
      "learning_rate": 0.00010755168171209641,
      "loss": 2.3998,
      "step": 71700
    },
    {
      "epoch": 1.4752326559161075,
      "grad_norm": 0.45866817235946655,
      "learning_rate": 0.00010752953581238446,
      "loss": 2.4277,
      "step": 71710
    },
    {
      "epoch": 1.4754383754329754,
      "grad_norm": 0.4000890552997589,
      "learning_rate": 0.00010750738954128153,
      "loss": 2.4675,
      "step": 71720
    },
    {
      "epoch": 1.475644094949843,
      "grad_norm": 0.43184328079223633,
      "learning_rate": 0.00010748524289987998,
      "loss": 2.4207,
      "step": 71730
    },
    {
      "epoch": 1.4758498144667107,
      "grad_norm": 0.4352113902568817,
      "learning_rate": 0.00010746309588927217,
      "loss": 2.3972,
      "step": 71740
    },
    {
      "epoch": 1.4760555339835784,
      "grad_norm": 0.4436365067958832,
      "learning_rate": 0.00010744094851055053,
      "loss": 2.3378,
      "step": 71750
    },
    {
      "epoch": 1.4762612535004462,
      "grad_norm": 0.44626152515411377,
      "learning_rate": 0.00010741880076480744,
      "loss": 2.4182,
      "step": 71760
    },
    {
      "epoch": 1.4764669730173139,
      "grad_norm": 0.415023535490036,
      "learning_rate": 0.00010739665265313534,
      "loss": 2.3731,
      "step": 71770
    },
    {
      "epoch": 1.4766726925341815,
      "grad_norm": 0.4673987030982971,
      "learning_rate": 0.00010737450417662667,
      "loss": 2.3601,
      "step": 71780
    },
    {
      "epoch": 1.4768784120510494,
      "grad_norm": 0.46896103024482727,
      "learning_rate": 0.00010735235533637389,
      "loss": 2.3957,
      "step": 71790
    },
    {
      "epoch": 1.477084131567917,
      "grad_norm": 0.43399837613105774,
      "learning_rate": 0.00010733020613346951,
      "loss": 2.3913,
      "step": 71800
    },
    {
      "epoch": 1.4772898510847847,
      "grad_norm": 0.43416792154312134,
      "learning_rate": 0.00010730805656900597,
      "loss": 2.3693,
      "step": 71810
    },
    {
      "epoch": 1.4774955706016524,
      "grad_norm": 0.469563364982605,
      "learning_rate": 0.00010728590664407583,
      "loss": 2.4597,
      "step": 71820
    },
    {
      "epoch": 1.47770129011852,
      "grad_norm": 0.47839605808258057,
      "learning_rate": 0.00010726375635977165,
      "loss": 2.369,
      "step": 71830
    },
    {
      "epoch": 1.477907009635388,
      "grad_norm": 0.4780712425708771,
      "learning_rate": 0.00010724160571718593,
      "loss": 2.4418,
      "step": 71840
    },
    {
      "epoch": 1.4781127291522556,
      "grad_norm": 0.48093846440315247,
      "learning_rate": 0.00010721945471741126,
      "loss": 2.4129,
      "step": 71850
    },
    {
      "epoch": 1.4783184486691234,
      "grad_norm": 0.4146549701690674,
      "learning_rate": 0.00010719730336154023,
      "loss": 2.3503,
      "step": 71860
    },
    {
      "epoch": 1.478524168185991,
      "grad_norm": 0.4015342891216278,
      "learning_rate": 0.00010717515165066547,
      "loss": 2.4228,
      "step": 71870
    },
    {
      "epoch": 1.4787298877028587,
      "grad_norm": 0.45233795046806335,
      "learning_rate": 0.00010715299958587954,
      "loss": 2.4281,
      "step": 71880
    },
    {
      "epoch": 1.4789356072197264,
      "grad_norm": 0.49734556674957275,
      "learning_rate": 0.00010713084716827515,
      "loss": 2.4279,
      "step": 71890
    },
    {
      "epoch": 1.479141326736594,
      "grad_norm": 0.3960162103176117,
      "learning_rate": 0.00010710869439894491,
      "loss": 2.3328,
      "step": 71900
    },
    {
      "epoch": 1.479347046253462,
      "grad_norm": 0.45025119185447693,
      "learning_rate": 0.00010708654127898153,
      "loss": 2.3714,
      "step": 71910
    },
    {
      "epoch": 1.4795527657703296,
      "grad_norm": 0.4301404058933258,
      "learning_rate": 0.00010706438780947768,
      "loss": 2.4173,
      "step": 71920
    },
    {
      "epoch": 1.4797584852871972,
      "grad_norm": 0.4614141583442688,
      "learning_rate": 0.00010704223399152606,
      "loss": 2.3765,
      "step": 71930
    },
    {
      "epoch": 1.479964204804065,
      "grad_norm": 0.4670636057853699,
      "learning_rate": 0.00010702007982621942,
      "loss": 2.3721,
      "step": 71940
    },
    {
      "epoch": 1.4801699243209328,
      "grad_norm": 0.40649154782295227,
      "learning_rate": 0.0001069979253146505,
      "loss": 2.455,
      "step": 71950
    },
    {
      "epoch": 1.4803756438378004,
      "grad_norm": 0.44652029871940613,
      "learning_rate": 0.00010697577045791207,
      "loss": 2.3978,
      "step": 71960
    },
    {
      "epoch": 1.480581363354668,
      "grad_norm": 0.43379104137420654,
      "learning_rate": 0.00010695361525709686,
      "loss": 2.4114,
      "step": 71970
    },
    {
      "epoch": 1.480787082871536,
      "grad_norm": 0.42354968190193176,
      "learning_rate": 0.00010693145971329774,
      "loss": 2.4068,
      "step": 71980
    },
    {
      "epoch": 1.4809928023884036,
      "grad_norm": 0.42266061902046204,
      "learning_rate": 0.00010690930382760748,
      "loss": 2.3884,
      "step": 71990
    },
    {
      "epoch": 1.4811985219052712,
      "grad_norm": 0.4108355641365051,
      "learning_rate": 0.0001068871476011189,
      "loss": 2.4149,
      "step": 72000
    },
    {
      "epoch": 1.4814042414221391,
      "grad_norm": 0.4221375286579132,
      "learning_rate": 0.00010686499103492487,
      "loss": 2.3984,
      "step": 72010
    },
    {
      "epoch": 1.4816099609390068,
      "grad_norm": 0.4265334904193878,
      "learning_rate": 0.00010684283413011824,
      "loss": 2.4235,
      "step": 72020
    },
    {
      "epoch": 1.4818156804558744,
      "grad_norm": 0.42256858944892883,
      "learning_rate": 0.0001068206768877919,
      "loss": 2.3716,
      "step": 72030
    },
    {
      "epoch": 1.482021399972742,
      "grad_norm": 0.4076002240180969,
      "learning_rate": 0.0001067985193090387,
      "loss": 2.3666,
      "step": 72040
    },
    {
      "epoch": 1.48222711948961,
      "grad_norm": 0.44920551776885986,
      "learning_rate": 0.00010677636139495161,
      "loss": 2.4115,
      "step": 72050
    },
    {
      "epoch": 1.4824328390064776,
      "grad_norm": 0.4443891942501068,
      "learning_rate": 0.00010675420314662356,
      "loss": 2.3913,
      "step": 72060
    },
    {
      "epoch": 1.4826385585233453,
      "grad_norm": 0.434651255607605,
      "learning_rate": 0.00010673204456514745,
      "loss": 2.3758,
      "step": 72070
    },
    {
      "epoch": 1.4828442780402131,
      "grad_norm": 0.4963539242744446,
      "learning_rate": 0.0001067098856516163,
      "loss": 2.3778,
      "step": 72080
    },
    {
      "epoch": 1.4830499975570808,
      "grad_norm": 0.4221900999546051,
      "learning_rate": 0.00010668772640712301,
      "loss": 2.4056,
      "step": 72090
    },
    {
      "epoch": 1.4832557170739484,
      "grad_norm": 0.45754680037498474,
      "learning_rate": 0.00010666556683276063,
      "loss": 2.4354,
      "step": 72100
    },
    {
      "epoch": 1.483461436590816,
      "grad_norm": 0.4563036859035492,
      "learning_rate": 0.00010664340692962218,
      "loss": 2.3928,
      "step": 72110
    },
    {
      "epoch": 1.483667156107684,
      "grad_norm": 0.4452810287475586,
      "learning_rate": 0.00010662124669880069,
      "loss": 2.4311,
      "step": 72120
    },
    {
      "epoch": 1.4838728756245516,
      "grad_norm": 0.43217232823371887,
      "learning_rate": 0.00010659908614138912,
      "loss": 2.3875,
      "step": 72130
    },
    {
      "epoch": 1.4840785951414193,
      "grad_norm": 0.41604477167129517,
      "learning_rate": 0.00010657692525848062,
      "loss": 2.3357,
      "step": 72140
    },
    {
      "epoch": 1.4842843146582871,
      "grad_norm": 0.42236101627349854,
      "learning_rate": 0.00010655476405116826,
      "loss": 2.3999,
      "step": 72150
    },
    {
      "epoch": 1.4844900341751548,
      "grad_norm": 0.4636717438697815,
      "learning_rate": 0.00010653260252054508,
      "loss": 2.4405,
      "step": 72160
    },
    {
      "epoch": 1.4846957536920224,
      "grad_norm": 0.44732800126075745,
      "learning_rate": 0.00010651044066770421,
      "loss": 2.4161,
      "step": 72170
    },
    {
      "epoch": 1.48490147320889,
      "grad_norm": 0.43473100662231445,
      "learning_rate": 0.00010648827849373881,
      "loss": 2.3903,
      "step": 72180
    },
    {
      "epoch": 1.4851071927257578,
      "grad_norm": 0.4779510200023651,
      "learning_rate": 0.00010646611599974199,
      "loss": 2.3674,
      "step": 72190
    },
    {
      "epoch": 1.4853129122426256,
      "grad_norm": 0.4404909014701843,
      "learning_rate": 0.0001064439531868069,
      "loss": 2.3603,
      "step": 72200
    },
    {
      "epoch": 1.4855186317594933,
      "grad_norm": 0.41172417998313904,
      "learning_rate": 0.00010642179005602668,
      "loss": 2.4375,
      "step": 72210
    },
    {
      "epoch": 1.4857243512763612,
      "grad_norm": 0.4012851119041443,
      "learning_rate": 0.00010639962660849458,
      "loss": 2.3864,
      "step": 72220
    },
    {
      "epoch": 1.4859300707932288,
      "grad_norm": 0.42578980326652527,
      "learning_rate": 0.00010637746284530377,
      "loss": 2.3204,
      "step": 72230
    },
    {
      "epoch": 1.4861357903100965,
      "grad_norm": 0.4266952574253082,
      "learning_rate": 0.00010635529876754748,
      "loss": 2.4248,
      "step": 72240
    },
    {
      "epoch": 1.4863415098269641,
      "grad_norm": 0.4992837905883789,
      "learning_rate": 0.00010633313437631888,
      "loss": 2.4545,
      "step": 72250
    },
    {
      "epoch": 1.4865472293438318,
      "grad_norm": 0.4199306070804596,
      "learning_rate": 0.00010631096967271132,
      "loss": 2.3805,
      "step": 72260
    },
    {
      "epoch": 1.4867529488606996,
      "grad_norm": 0.4060368835926056,
      "learning_rate": 0.00010628880465781803,
      "loss": 2.4109,
      "step": 72270
    },
    {
      "epoch": 1.4869586683775673,
      "grad_norm": 0.4324721097946167,
      "learning_rate": 0.00010626663933273227,
      "loss": 2.3516,
      "step": 72280
    },
    {
      "epoch": 1.487164387894435,
      "grad_norm": 0.4556824564933777,
      "learning_rate": 0.00010624447369854731,
      "loss": 2.3473,
      "step": 72290
    },
    {
      "epoch": 1.4873701074113028,
      "grad_norm": 0.4298757314682007,
      "learning_rate": 0.00010622230775635649,
      "loss": 2.386,
      "step": 72300
    },
    {
      "epoch": 1.4875758269281705,
      "grad_norm": 0.418352872133255,
      "learning_rate": 0.00010620014150725319,
      "loss": 2.5067,
      "step": 72310
    },
    {
      "epoch": 1.4877815464450381,
      "grad_norm": 0.4656204283237457,
      "learning_rate": 0.00010617797495233066,
      "loss": 2.3544,
      "step": 72320
    },
    {
      "epoch": 1.4879872659619058,
      "grad_norm": 0.4058244824409485,
      "learning_rate": 0.00010615580809268229,
      "loss": 2.4062,
      "step": 72330
    },
    {
      "epoch": 1.4881929854787737,
      "grad_norm": 0.44047877192497253,
      "learning_rate": 0.00010613364092940145,
      "loss": 2.3888,
      "step": 72340
    },
    {
      "epoch": 1.4883987049956413,
      "grad_norm": 0.4652279317378998,
      "learning_rate": 0.00010611147346358152,
      "loss": 2.3509,
      "step": 72350
    },
    {
      "epoch": 1.488604424512509,
      "grad_norm": 0.5610227584838867,
      "learning_rate": 0.00010608930569631592,
      "loss": 2.4086,
      "step": 72360
    },
    {
      "epoch": 1.4888101440293768,
      "grad_norm": 0.516130268573761,
      "learning_rate": 0.00010606713762869807,
      "loss": 2.3951,
      "step": 72370
    },
    {
      "epoch": 1.4890158635462445,
      "grad_norm": 0.43130818009376526,
      "learning_rate": 0.00010604496926182135,
      "loss": 2.3261,
      "step": 72380
    },
    {
      "epoch": 1.4892215830631121,
      "grad_norm": 0.4266369938850403,
      "learning_rate": 0.00010602280059677924,
      "loss": 2.3253,
      "step": 72390
    },
    {
      "epoch": 1.4894273025799798,
      "grad_norm": 0.528009295463562,
      "learning_rate": 0.00010600063163466523,
      "loss": 2.3843,
      "step": 72400
    },
    {
      "epoch": 1.4896330220968477,
      "grad_norm": 0.46423789858818054,
      "learning_rate": 0.00010597846237657275,
      "loss": 2.367,
      "step": 72410
    },
    {
      "epoch": 1.4898387416137153,
      "grad_norm": 0.5535693764686584,
      "learning_rate": 0.00010595629282359527,
      "loss": 2.3801,
      "step": 72420
    },
    {
      "epoch": 1.490044461130583,
      "grad_norm": 0.49805888533592224,
      "learning_rate": 0.00010593412297682635,
      "loss": 2.3666,
      "step": 72430
    },
    {
      "epoch": 1.4902501806474509,
      "grad_norm": 0.44103458523750305,
      "learning_rate": 0.00010591195283735948,
      "loss": 2.4113,
      "step": 72440
    },
    {
      "epoch": 1.4904559001643185,
      "grad_norm": 0.47032374143600464,
      "learning_rate": 0.00010588978240628817,
      "loss": 2.4176,
      "step": 72450
    },
    {
      "epoch": 1.4906616196811862,
      "grad_norm": 0.42058274149894714,
      "learning_rate": 0.00010586761168470599,
      "loss": 2.3811,
      "step": 72460
    },
    {
      "epoch": 1.4908673391980538,
      "grad_norm": 0.41659635305404663,
      "learning_rate": 0.00010584544067370654,
      "loss": 2.4102,
      "step": 72470
    },
    {
      "epoch": 1.4910730587149217,
      "grad_norm": 0.43297067284584045,
      "learning_rate": 0.00010582326937438332,
      "loss": 2.3721,
      "step": 72480
    },
    {
      "epoch": 1.4912787782317893,
      "grad_norm": 0.42776891589164734,
      "learning_rate": 0.00010580109778782996,
      "loss": 2.3403,
      "step": 72490
    },
    {
      "epoch": 1.491484497748657,
      "grad_norm": 0.45303231477737427,
      "learning_rate": 0.00010577892591514007,
      "loss": 2.3904,
      "step": 72500
    },
    {
      "epoch": 1.4916902172655249,
      "grad_norm": 0.4066210389137268,
      "learning_rate": 0.00010575675375740726,
      "loss": 2.3718,
      "step": 72510
    },
    {
      "epoch": 1.4918959367823925,
      "grad_norm": 0.4615000784397125,
      "learning_rate": 0.00010573458131572513,
      "loss": 2.363,
      "step": 72520
    },
    {
      "epoch": 1.4921016562992602,
      "grad_norm": 0.4783383309841156,
      "learning_rate": 0.00010571240859118741,
      "loss": 2.4238,
      "step": 72530
    },
    {
      "epoch": 1.4923073758161278,
      "grad_norm": 0.42439162731170654,
      "learning_rate": 0.00010569023558488762,
      "loss": 2.4473,
      "step": 72540
    },
    {
      "epoch": 1.4925130953329955,
      "grad_norm": 0.4351927638053894,
      "learning_rate": 0.00010566806229791957,
      "loss": 2.4404,
      "step": 72550
    },
    {
      "epoch": 1.4927188148498634,
      "grad_norm": 0.44533076882362366,
      "learning_rate": 0.00010564588873137691,
      "loss": 2.4075,
      "step": 72560
    },
    {
      "epoch": 1.492924534366731,
      "grad_norm": 0.43375489115715027,
      "learning_rate": 0.0001056237148863533,
      "loss": 2.4747,
      "step": 72570
    },
    {
      "epoch": 1.4931302538835989,
      "grad_norm": 0.42616933584213257,
      "learning_rate": 0.00010560154076394244,
      "loss": 2.43,
      "step": 72580
    },
    {
      "epoch": 1.4933359734004665,
      "grad_norm": 0.46244099736213684,
      "learning_rate": 0.00010557936636523817,
      "loss": 2.3545,
      "step": 72590
    },
    {
      "epoch": 1.4935416929173342,
      "grad_norm": 0.42316335439682007,
      "learning_rate": 0.00010555719169133413,
      "loss": 2.4306,
      "step": 72600
    },
    {
      "epoch": 1.4937474124342018,
      "grad_norm": 0.41300344467163086,
      "learning_rate": 0.0001055350167433241,
      "loss": 2.4252,
      "step": 72610
    },
    {
      "epoch": 1.4939531319510695,
      "grad_norm": 0.44309690594673157,
      "learning_rate": 0.00010551284152230188,
      "loss": 2.4641,
      "step": 72620
    },
    {
      "epoch": 1.4941588514679374,
      "grad_norm": 0.46597591042518616,
      "learning_rate": 0.0001054906660293612,
      "loss": 2.377,
      "step": 72630
    },
    {
      "epoch": 1.494364570984805,
      "grad_norm": 0.45359402894973755,
      "learning_rate": 0.00010546849026559589,
      "loss": 2.3992,
      "step": 72640
    },
    {
      "epoch": 1.494570290501673,
      "grad_norm": 0.4214991331100464,
      "learning_rate": 0.00010544631423209977,
      "loss": 2.3817,
      "step": 72650
    },
    {
      "epoch": 1.4947760100185405,
      "grad_norm": 0.4338170289993286,
      "learning_rate": 0.00010542413792996664,
      "loss": 2.3173,
      "step": 72660
    },
    {
      "epoch": 1.4949817295354082,
      "grad_norm": 0.4983941912651062,
      "learning_rate": 0.00010540196136029034,
      "loss": 2.4023,
      "step": 72670
    },
    {
      "epoch": 1.4951874490522759,
      "grad_norm": 0.5101333856582642,
      "learning_rate": 0.00010537978452416473,
      "loss": 2.4607,
      "step": 72680
    },
    {
      "epoch": 1.4953931685691435,
      "grad_norm": 0.4224434494972229,
      "learning_rate": 0.00010535760742268369,
      "loss": 2.3585,
      "step": 72690
    },
    {
      "epoch": 1.4955988880860114,
      "grad_norm": 0.4122762084007263,
      "learning_rate": 0.000105335430056941,
      "loss": 2.3755,
      "step": 72700
    },
    {
      "epoch": 1.495804607602879,
      "grad_norm": 0.43082019686698914,
      "learning_rate": 0.00010531325242803067,
      "loss": 2.4624,
      "step": 72710
    },
    {
      "epoch": 1.4960103271197467,
      "grad_norm": 0.42986562848091125,
      "learning_rate": 0.00010529107453704656,
      "loss": 2.426,
      "step": 72720
    },
    {
      "epoch": 1.4962160466366146,
      "grad_norm": 0.482965350151062,
      "learning_rate": 0.00010526889638508257,
      "loss": 2.3507,
      "step": 72730
    },
    {
      "epoch": 1.4964217661534822,
      "grad_norm": 0.42387834191322327,
      "learning_rate": 0.0001052467179732326,
      "loss": 2.3961,
      "step": 72740
    },
    {
      "epoch": 1.4966274856703499,
      "grad_norm": 0.5934590101242065,
      "learning_rate": 0.00010522453930259066,
      "loss": 2.4329,
      "step": 72750
    },
    {
      "epoch": 1.4968332051872175,
      "grad_norm": 0.43346285820007324,
      "learning_rate": 0.00010520236037425066,
      "loss": 2.3539,
      "step": 72760
    },
    {
      "epoch": 1.4970389247040854,
      "grad_norm": 0.4308556020259857,
      "learning_rate": 0.00010518018118930657,
      "loss": 2.4273,
      "step": 72770
    },
    {
      "epoch": 1.497244644220953,
      "grad_norm": 0.4230883717536926,
      "learning_rate": 0.00010515800174885238,
      "loss": 2.416,
      "step": 72780
    },
    {
      "epoch": 1.4974503637378207,
      "grad_norm": 0.43930691480636597,
      "learning_rate": 0.00010513582205398204,
      "loss": 2.3742,
      "step": 72790
    },
    {
      "epoch": 1.4976560832546886,
      "grad_norm": 0.42691338062286377,
      "learning_rate": 0.0001051136421057896,
      "loss": 2.4127,
      "step": 72800
    },
    {
      "epoch": 1.4978618027715562,
      "grad_norm": 0.4523080885410309,
      "learning_rate": 0.00010509146190536907,
      "loss": 2.4054,
      "step": 72810
    },
    {
      "epoch": 1.4980675222884239,
      "grad_norm": 0.43796971440315247,
      "learning_rate": 0.00010506928145381447,
      "loss": 2.4523,
      "step": 72820
    },
    {
      "epoch": 1.4982732418052915,
      "grad_norm": 0.3930628001689911,
      "learning_rate": 0.00010504710075221984,
      "loss": 2.4356,
      "step": 72830
    },
    {
      "epoch": 1.4984789613221594,
      "grad_norm": 0.43833938241004944,
      "learning_rate": 0.00010502491980167922,
      "loss": 2.3892,
      "step": 72840
    },
    {
      "epoch": 1.498684680839027,
      "grad_norm": 0.425225168466568,
      "learning_rate": 0.00010500273860328671,
      "loss": 2.4189,
      "step": 72850
    },
    {
      "epoch": 1.4988904003558947,
      "grad_norm": 0.41513946652412415,
      "learning_rate": 0.00010498055715813633,
      "loss": 2.3956,
      "step": 72860
    },
    {
      "epoch": 1.4990961198727626,
      "grad_norm": 0.41072651743888855,
      "learning_rate": 0.00010495837546732224,
      "loss": 2.4524,
      "step": 72870
    },
    {
      "epoch": 1.4993018393896302,
      "grad_norm": 0.4980691969394684,
      "learning_rate": 0.00010493619353193851,
      "loss": 2.429,
      "step": 72880
    },
    {
      "epoch": 1.499507558906498,
      "grad_norm": 0.418971985578537,
      "learning_rate": 0.00010491401135307924,
      "loss": 2.3792,
      "step": 72890
    },
    {
      "epoch": 1.4997132784233655,
      "grad_norm": 0.4203011393547058,
      "learning_rate": 0.00010489182893183856,
      "loss": 2.4413,
      "step": 72900
    },
    {
      "epoch": 1.4999189979402332,
      "grad_norm": 0.3997569978237152,
      "learning_rate": 0.00010486964626931064,
      "loss": 2.3847,
      "step": 72910
    },
    {
      "epoch": 1.500124717457101,
      "grad_norm": 0.4212319850921631,
      "learning_rate": 0.00010484746336658957,
      "loss": 2.433,
      "step": 72920
    },
    {
      "epoch": 1.5003304369739687,
      "grad_norm": 0.4550490379333496,
      "learning_rate": 0.00010482528022476959,
      "loss": 2.394,
      "step": 72930
    },
    {
      "epoch": 1.5005361564908366,
      "grad_norm": 0.4481683373451233,
      "learning_rate": 0.00010480309684494478,
      "loss": 2.4084,
      "step": 72940
    },
    {
      "epoch": 1.5007418760077043,
      "grad_norm": 0.4458814263343811,
      "learning_rate": 0.00010478091322820943,
      "loss": 2.391,
      "step": 72950
    },
    {
      "epoch": 1.500947595524572,
      "grad_norm": 0.49462899565696716,
      "learning_rate": 0.00010475872937565765,
      "loss": 2.414,
      "step": 72960
    },
    {
      "epoch": 1.5011533150414396,
      "grad_norm": 0.40022265911102295,
      "learning_rate": 0.00010473654528838368,
      "loss": 2.3307,
      "step": 72970
    },
    {
      "epoch": 1.5013590345583072,
      "grad_norm": 0.5602350234985352,
      "learning_rate": 0.00010471436096748175,
      "loss": 2.3709,
      "step": 72980
    },
    {
      "epoch": 1.501564754075175,
      "grad_norm": 0.4317474961280823,
      "learning_rate": 0.0001046921764140461,
      "loss": 2.3605,
      "step": 72990
    },
    {
      "epoch": 1.5017704735920427,
      "grad_norm": 0.4537900686264038,
      "learning_rate": 0.00010466999162917093,
      "loss": 2.4508,
      "step": 73000
    },
    {
      "epoch": 1.5019761931089106,
      "grad_norm": 0.4349061846733093,
      "learning_rate": 0.00010464780661395055,
      "loss": 2.3815,
      "step": 73010
    },
    {
      "epoch": 1.5021819126257783,
      "grad_norm": 0.46375131607055664,
      "learning_rate": 0.00010462562136947915,
      "loss": 2.4099,
      "step": 73020
    },
    {
      "epoch": 1.502387632142646,
      "grad_norm": 0.4550143778324127,
      "learning_rate": 0.00010460343589685109,
      "loss": 2.3982,
      "step": 73030
    },
    {
      "epoch": 1.5025933516595136,
      "grad_norm": 0.4390958547592163,
      "learning_rate": 0.00010458125019716066,
      "loss": 2.3561,
      "step": 73040
    },
    {
      "epoch": 1.5027990711763812,
      "grad_norm": 0.427087664604187,
      "learning_rate": 0.00010455906427150206,
      "loss": 2.4292,
      "step": 73050
    },
    {
      "epoch": 1.503004790693249,
      "grad_norm": 0.4746648967266083,
      "learning_rate": 0.00010453687812096966,
      "loss": 2.3944,
      "step": 73060
    },
    {
      "epoch": 1.5032105102101168,
      "grad_norm": 0.4445745348930359,
      "learning_rate": 0.00010451469174665785,
      "loss": 2.4124,
      "step": 73070
    },
    {
      "epoch": 1.5034162297269846,
      "grad_norm": 0.4549763798713684,
      "learning_rate": 0.00010449250514966085,
      "loss": 2.3859,
      "step": 73080
    },
    {
      "epoch": 1.5036219492438523,
      "grad_norm": 0.45604240894317627,
      "learning_rate": 0.00010447031833107306,
      "loss": 2.3374,
      "step": 73090
    },
    {
      "epoch": 1.50382766876072,
      "grad_norm": 0.45200949907302856,
      "learning_rate": 0.00010444813129198882,
      "loss": 2.3911,
      "step": 73100
    },
    {
      "epoch": 1.5040333882775876,
      "grad_norm": 0.3868255615234375,
      "learning_rate": 0.0001044259440335025,
      "loss": 2.4051,
      "step": 73110
    },
    {
      "epoch": 1.5042391077944552,
      "grad_norm": 0.43865343928337097,
      "learning_rate": 0.00010440375655670849,
      "loss": 2.3984,
      "step": 73120
    },
    {
      "epoch": 1.504444827311323,
      "grad_norm": 0.4847489297389984,
      "learning_rate": 0.00010438156886270117,
      "loss": 2.4478,
      "step": 73130
    },
    {
      "epoch": 1.5046505468281908,
      "grad_norm": 0.4266837537288666,
      "learning_rate": 0.00010435938095257494,
      "loss": 2.3994,
      "step": 73140
    },
    {
      "epoch": 1.5048562663450586,
      "grad_norm": 0.4725000560283661,
      "learning_rate": 0.00010433719282742418,
      "loss": 2.4122,
      "step": 73150
    },
    {
      "epoch": 1.5050619858619263,
      "grad_norm": 0.4809796214103699,
      "learning_rate": 0.00010431500448834335,
      "loss": 2.3998,
      "step": 73160
    },
    {
      "epoch": 1.505267705378794,
      "grad_norm": 0.4376120865345001,
      "learning_rate": 0.00010429281593642687,
      "loss": 2.3964,
      "step": 73170
    },
    {
      "epoch": 1.5054734248956616,
      "grad_norm": 0.45493432879447937,
      "learning_rate": 0.00010427062717276914,
      "loss": 2.3789,
      "step": 73180
    },
    {
      "epoch": 1.5056791444125293,
      "grad_norm": 0.4126580059528351,
      "learning_rate": 0.00010424843819846466,
      "loss": 2.3864,
      "step": 73190
    },
    {
      "epoch": 1.505884863929397,
      "grad_norm": 0.510573148727417,
      "learning_rate": 0.00010422624901460791,
      "loss": 2.4216,
      "step": 73200
    },
    {
      "epoch": 1.5060905834462648,
      "grad_norm": 0.4312215745449066,
      "learning_rate": 0.00010420405962229329,
      "loss": 2.3778,
      "step": 73210
    },
    {
      "epoch": 1.5062963029631324,
      "grad_norm": 0.43398746848106384,
      "learning_rate": 0.0001041818700226153,
      "loss": 2.448,
      "step": 73220
    },
    {
      "epoch": 1.5065020224800003,
      "grad_norm": 0.39623117446899414,
      "learning_rate": 0.0001041596802166685,
      "loss": 2.3276,
      "step": 73230
    },
    {
      "epoch": 1.506707741996868,
      "grad_norm": 0.43719661235809326,
      "learning_rate": 0.00010413749020554733,
      "loss": 2.36,
      "step": 73240
    },
    {
      "epoch": 1.5069134615137356,
      "grad_norm": 0.46896833181381226,
      "learning_rate": 0.0001041152999903463,
      "loss": 2.409,
      "step": 73250
    },
    {
      "epoch": 1.5071191810306033,
      "grad_norm": 0.42932605743408203,
      "learning_rate": 0.00010409310957215995,
      "loss": 2.369,
      "step": 73260
    },
    {
      "epoch": 1.507324900547471,
      "grad_norm": 0.4351414442062378,
      "learning_rate": 0.0001040709189520828,
      "loss": 2.3862,
      "step": 73270
    },
    {
      "epoch": 1.5075306200643388,
      "grad_norm": 0.4799153804779053,
      "learning_rate": 0.00010404872813120941,
      "loss": 2.4226,
      "step": 73280
    },
    {
      "epoch": 1.5077363395812065,
      "grad_norm": 0.49269768595695496,
      "learning_rate": 0.00010402653711063433,
      "loss": 2.3824,
      "step": 73290
    },
    {
      "epoch": 1.5079420590980743,
      "grad_norm": 0.41886481642723083,
      "learning_rate": 0.00010400434589145212,
      "loss": 2.3579,
      "step": 73300
    },
    {
      "epoch": 1.508147778614942,
      "grad_norm": 0.4230598211288452,
      "learning_rate": 0.00010398215447475732,
      "loss": 2.4185,
      "step": 73310
    },
    {
      "epoch": 1.5083534981318096,
      "grad_norm": 0.41988274455070496,
      "learning_rate": 0.00010395996286164455,
      "loss": 2.3624,
      "step": 73320
    },
    {
      "epoch": 1.5085592176486773,
      "grad_norm": 0.4412515461444855,
      "learning_rate": 0.00010393777105320843,
      "loss": 2.3539,
      "step": 73330
    },
    {
      "epoch": 1.508764937165545,
      "grad_norm": 0.45409291982650757,
      "learning_rate": 0.00010391557905054347,
      "loss": 2.4097,
      "step": 73340
    },
    {
      "epoch": 1.5089706566824128,
      "grad_norm": 0.4731701612472534,
      "learning_rate": 0.00010389338685474437,
      "loss": 2.3847,
      "step": 73350
    },
    {
      "epoch": 1.5091763761992805,
      "grad_norm": 0.42903968691825867,
      "learning_rate": 0.00010387119446690572,
      "loss": 2.3843,
      "step": 73360
    },
    {
      "epoch": 1.5093820957161483,
      "grad_norm": 0.4274403750896454,
      "learning_rate": 0.00010384900188812212,
      "loss": 2.4196,
      "step": 73370
    },
    {
      "epoch": 1.509587815233016,
      "grad_norm": 0.42785629630088806,
      "learning_rate": 0.00010382680911948821,
      "loss": 2.3879,
      "step": 73380
    },
    {
      "epoch": 1.5097935347498836,
      "grad_norm": 0.4601548910140991,
      "learning_rate": 0.00010380461616209869,
      "loss": 2.4446,
      "step": 73390
    },
    {
      "epoch": 1.5099992542667513,
      "grad_norm": 0.4376932382583618,
      "learning_rate": 0.00010378242301704819,
      "loss": 2.4147,
      "step": 73400
    },
    {
      "epoch": 1.510204973783619,
      "grad_norm": 0.43737247586250305,
      "learning_rate": 0.00010376022968543136,
      "loss": 2.3988,
      "step": 73410
    },
    {
      "epoch": 1.5104106933004868,
      "grad_norm": 0.4227789342403412,
      "learning_rate": 0.0001037380361683429,
      "loss": 2.3958,
      "step": 73420
    },
    {
      "epoch": 1.5106164128173545,
      "grad_norm": 0.4499083161354065,
      "learning_rate": 0.00010371584246687746,
      "loss": 2.3879,
      "step": 73430
    },
    {
      "epoch": 1.5108221323342224,
      "grad_norm": 0.4359649121761322,
      "learning_rate": 0.0001036936485821298,
      "loss": 2.4205,
      "step": 73440
    },
    {
      "epoch": 1.51102785185109,
      "grad_norm": 0.45552921295166016,
      "learning_rate": 0.00010367145451519456,
      "loss": 2.4302,
      "step": 73450
    },
    {
      "epoch": 1.5112335713679577,
      "grad_norm": 0.43378686904907227,
      "learning_rate": 0.00010364926026716648,
      "loss": 2.38,
      "step": 73460
    },
    {
      "epoch": 1.5114392908848253,
      "grad_norm": 0.4389709234237671,
      "learning_rate": 0.00010362706583914028,
      "loss": 2.3709,
      "step": 73470
    },
    {
      "epoch": 1.511645010401693,
      "grad_norm": 0.398773193359375,
      "learning_rate": 0.00010360487123221068,
      "loss": 2.3924,
      "step": 73480
    },
    {
      "epoch": 1.5118507299185606,
      "grad_norm": 0.43944400548934937,
      "learning_rate": 0.00010358267644747244,
      "loss": 2.4345,
      "step": 73490
    },
    {
      "epoch": 1.5120564494354285,
      "grad_norm": 0.41622257232666016,
      "learning_rate": 0.00010356048148602026,
      "loss": 2.3885,
      "step": 73500
    },
    {
      "epoch": 1.5122621689522964,
      "grad_norm": 0.4466257691383362,
      "learning_rate": 0.00010353828634894896,
      "loss": 2.377,
      "step": 73510
    },
    {
      "epoch": 1.512467888469164,
      "grad_norm": 0.42038676142692566,
      "learning_rate": 0.00010351609103735329,
      "loss": 2.4061,
      "step": 73520
    },
    {
      "epoch": 1.5126736079860317,
      "grad_norm": 0.44800665974617004,
      "learning_rate": 0.00010349389555232801,
      "loss": 2.3655,
      "step": 73530
    },
    {
      "epoch": 1.5128793275028993,
      "grad_norm": 0.4131543040275574,
      "learning_rate": 0.00010347169989496785,
      "loss": 2.4113,
      "step": 73540
    },
    {
      "epoch": 1.513085047019767,
      "grad_norm": 0.41997990012168884,
      "learning_rate": 0.00010344950406636771,
      "loss": 2.4588,
      "step": 73550
    },
    {
      "epoch": 1.5132907665366346,
      "grad_norm": 0.4278985559940338,
      "learning_rate": 0.00010342730806762229,
      "loss": 2.3861,
      "step": 73560
    },
    {
      "epoch": 1.5134964860535025,
      "grad_norm": 0.5048426389694214,
      "learning_rate": 0.00010340511189982647,
      "loss": 2.3307,
      "step": 73570
    },
    {
      "epoch": 1.5137022055703702,
      "grad_norm": 0.4564167559146881,
      "learning_rate": 0.00010338291556407503,
      "loss": 2.4236,
      "step": 73580
    },
    {
      "epoch": 1.513907925087238,
      "grad_norm": 0.4091559052467346,
      "learning_rate": 0.00010336071906146276,
      "loss": 2.4153,
      "step": 73590
    },
    {
      "epoch": 1.5141136446041057,
      "grad_norm": 0.42805689573287964,
      "learning_rate": 0.00010333852239308457,
      "loss": 2.3525,
      "step": 73600
    },
    {
      "epoch": 1.5143193641209733,
      "grad_norm": 0.4295455813407898,
      "learning_rate": 0.00010331632556003525,
      "loss": 2.4474,
      "step": 73610
    },
    {
      "epoch": 1.514525083637841,
      "grad_norm": 0.4073311686515808,
      "learning_rate": 0.00010329412856340969,
      "loss": 2.41,
      "step": 73620
    },
    {
      "epoch": 1.5147308031547086,
      "grad_norm": 0.44166895747184753,
      "learning_rate": 0.00010327193140430269,
      "loss": 2.3749,
      "step": 73630
    },
    {
      "epoch": 1.5149365226715765,
      "grad_norm": 0.43868374824523926,
      "learning_rate": 0.00010324973408380916,
      "loss": 2.4127,
      "step": 73640
    },
    {
      "epoch": 1.5151422421884442,
      "grad_norm": 0.43405359983444214,
      "learning_rate": 0.00010322753660302396,
      "loss": 2.4036,
      "step": 73650
    },
    {
      "epoch": 1.515347961705312,
      "grad_norm": 0.39769861102104187,
      "learning_rate": 0.00010320533896304196,
      "loss": 2.3602,
      "step": 73660
    },
    {
      "epoch": 1.5155536812221797,
      "grad_norm": 0.4419786334037781,
      "learning_rate": 0.00010318314116495802,
      "loss": 2.412,
      "step": 73670
    },
    {
      "epoch": 1.5157594007390474,
      "grad_norm": 0.44522061944007874,
      "learning_rate": 0.00010316094320986714,
      "loss": 2.384,
      "step": 73680
    },
    {
      "epoch": 1.515965120255915,
      "grad_norm": 0.47803714871406555,
      "learning_rate": 0.00010313874509886409,
      "loss": 2.4551,
      "step": 73690
    },
    {
      "epoch": 1.5161708397727827,
      "grad_norm": 0.4150640368461609,
      "learning_rate": 0.0001031165468330439,
      "loss": 2.3655,
      "step": 73700
    },
    {
      "epoch": 1.5163765592896505,
      "grad_norm": 0.42978522181510925,
      "learning_rate": 0.0001030943484135014,
      "loss": 2.4106,
      "step": 73710
    },
    {
      "epoch": 1.5165822788065182,
      "grad_norm": 0.43694204092025757,
      "learning_rate": 0.00010307214984133158,
      "loss": 2.3697,
      "step": 73720
    },
    {
      "epoch": 1.516787998323386,
      "grad_norm": 0.4678499102592468,
      "learning_rate": 0.00010304995111762934,
      "loss": 2.3985,
      "step": 73730
    },
    {
      "epoch": 1.5169937178402537,
      "grad_norm": 0.4277859032154083,
      "learning_rate": 0.00010302775224348964,
      "loss": 2.4521,
      "step": 73740
    },
    {
      "epoch": 1.5171994373571214,
      "grad_norm": 0.39820370078086853,
      "learning_rate": 0.0001030055532200074,
      "loss": 2.4116,
      "step": 73750
    },
    {
      "epoch": 1.517405156873989,
      "grad_norm": 0.42140820622444153,
      "learning_rate": 0.00010298335404827762,
      "loss": 2.4208,
      "step": 73760
    },
    {
      "epoch": 1.5176108763908567,
      "grad_norm": 0.4295946955680847,
      "learning_rate": 0.00010296115472939522,
      "loss": 2.4152,
      "step": 73770
    },
    {
      "epoch": 1.5178165959077246,
      "grad_norm": 0.456305593252182,
      "learning_rate": 0.00010293895526445525,
      "loss": 2.4643,
      "step": 73780
    },
    {
      "epoch": 1.5180223154245922,
      "grad_norm": 0.44743654131889343,
      "learning_rate": 0.00010291675565455256,
      "loss": 2.4083,
      "step": 73790
    },
    {
      "epoch": 1.51822803494146,
      "grad_norm": 0.4219737648963928,
      "learning_rate": 0.00010289455590078223,
      "loss": 2.4156,
      "step": 73800
    },
    {
      "epoch": 1.5184337544583277,
      "grad_norm": 0.419148713350296,
      "learning_rate": 0.00010287235600423926,
      "loss": 2.4052,
      "step": 73810
    },
    {
      "epoch": 1.5186394739751954,
      "grad_norm": 0.4406397044658661,
      "learning_rate": 0.00010285015596601862,
      "loss": 2.4232,
      "step": 73820
    },
    {
      "epoch": 1.518845193492063,
      "grad_norm": 0.4501132369041443,
      "learning_rate": 0.00010282795578721527,
      "loss": 2.411,
      "step": 73830
    },
    {
      "epoch": 1.5190509130089307,
      "grad_norm": 0.41126078367233276,
      "learning_rate": 0.0001028057554689243,
      "loss": 2.3462,
      "step": 73840
    },
    {
      "epoch": 1.5192566325257986,
      "grad_norm": 0.5170413851737976,
      "learning_rate": 0.00010278355501224071,
      "loss": 2.4185,
      "step": 73850
    },
    {
      "epoch": 1.5194623520426662,
      "grad_norm": 0.44265440106391907,
      "learning_rate": 0.00010276135441825951,
      "loss": 2.4456,
      "step": 73860
    },
    {
      "epoch": 1.519668071559534,
      "grad_norm": 0.4557504653930664,
      "learning_rate": 0.00010273915368807574,
      "loss": 2.404,
      "step": 73870
    },
    {
      "epoch": 1.5198737910764017,
      "grad_norm": 0.42778581380844116,
      "learning_rate": 0.00010271695282278447,
      "loss": 2.4168,
      "step": 73880
    },
    {
      "epoch": 1.5200795105932694,
      "grad_norm": 0.49403679370880127,
      "learning_rate": 0.0001026947518234807,
      "loss": 2.3807,
      "step": 73890
    },
    {
      "epoch": 1.520285230110137,
      "grad_norm": 0.524904727935791,
      "learning_rate": 0.00010267255069125949,
      "loss": 2.3711,
      "step": 73900
    },
    {
      "epoch": 1.5204909496270047,
      "grad_norm": 0.45346230268478394,
      "learning_rate": 0.00010265034942721594,
      "loss": 2.43,
      "step": 73910
    },
    {
      "epoch": 1.5206966691438724,
      "grad_norm": 0.417709082365036,
      "learning_rate": 0.00010262814803244509,
      "loss": 2.4695,
      "step": 73920
    },
    {
      "epoch": 1.5209023886607402,
      "grad_norm": 0.4520464837551117,
      "learning_rate": 0.00010260594650804203,
      "loss": 2.4164,
      "step": 73930
    },
    {
      "epoch": 1.521108108177608,
      "grad_norm": 0.44806843996047974,
      "learning_rate": 0.00010258374485510185,
      "loss": 2.424,
      "step": 73940
    },
    {
      "epoch": 1.5213138276944758,
      "grad_norm": 0.4436345398426056,
      "learning_rate": 0.00010256154307471955,
      "loss": 2.3775,
      "step": 73950
    },
    {
      "epoch": 1.5215195472113434,
      "grad_norm": 0.4328887462615967,
      "learning_rate": 0.00010253934116799033,
      "loss": 2.403,
      "step": 73960
    },
    {
      "epoch": 1.521725266728211,
      "grad_norm": 0.4033236503601074,
      "learning_rate": 0.00010251713913600927,
      "loss": 2.3494,
      "step": 73970
    },
    {
      "epoch": 1.5219309862450787,
      "grad_norm": 0.501003086566925,
      "learning_rate": 0.00010249493697987143,
      "loss": 2.4587,
      "step": 73980
    },
    {
      "epoch": 1.5221367057619464,
      "grad_norm": 0.4203729033470154,
      "learning_rate": 0.00010247273470067191,
      "loss": 2.3913,
      "step": 73990
    },
    {
      "epoch": 1.5223424252788142,
      "grad_norm": 0.45683807134628296,
      "learning_rate": 0.00010245053229950592,
      "loss": 2.3525,
      "step": 74000
    },
    {
      "epoch": 1.522548144795682,
      "grad_norm": 0.4331408143043518,
      "learning_rate": 0.0001024283297774685,
      "loss": 2.4237,
      "step": 74010
    },
    {
      "epoch": 1.5227538643125498,
      "grad_norm": 0.4070467948913574,
      "learning_rate": 0.00010240612713565481,
      "loss": 2.4099,
      "step": 74020
    },
    {
      "epoch": 1.5229595838294174,
      "grad_norm": 0.46244388818740845,
      "learning_rate": 0.00010238392437515998,
      "loss": 2.3569,
      "step": 74030
    },
    {
      "epoch": 1.523165303346285,
      "grad_norm": 0.4162323772907257,
      "learning_rate": 0.00010236172149707914,
      "loss": 2.356,
      "step": 74040
    },
    {
      "epoch": 1.5233710228631527,
      "grad_norm": 0.43556737899780273,
      "learning_rate": 0.00010233951850250745,
      "loss": 2.4094,
      "step": 74050
    },
    {
      "epoch": 1.5235767423800204,
      "grad_norm": 0.4255262613296509,
      "learning_rate": 0.00010231731539254006,
      "loss": 2.4064,
      "step": 74060
    },
    {
      "epoch": 1.5237824618968883,
      "grad_norm": 0.42091459035873413,
      "learning_rate": 0.00010229511216827214,
      "loss": 2.4276,
      "step": 74070
    },
    {
      "epoch": 1.523988181413756,
      "grad_norm": 0.4216153025627136,
      "learning_rate": 0.00010227290883079883,
      "loss": 2.4014,
      "step": 74080
    },
    {
      "epoch": 1.5241939009306238,
      "grad_norm": 0.4232322871685028,
      "learning_rate": 0.00010225070538121533,
      "loss": 2.4337,
      "step": 74090
    },
    {
      "epoch": 1.5243996204474914,
      "grad_norm": 0.4221246540546417,
      "learning_rate": 0.00010222850182061681,
      "loss": 2.4175,
      "step": 74100
    },
    {
      "epoch": 1.524605339964359,
      "grad_norm": 0.45186570286750793,
      "learning_rate": 0.00010220629815009842,
      "loss": 2.4059,
      "step": 74110
    },
    {
      "epoch": 1.5248110594812267,
      "grad_norm": 0.4674743115901947,
      "learning_rate": 0.00010218409437075536,
      "loss": 2.3901,
      "step": 74120
    },
    {
      "epoch": 1.5250167789980944,
      "grad_norm": 0.4230055809020996,
      "learning_rate": 0.00010216189048368283,
      "loss": 2.4098,
      "step": 74130
    },
    {
      "epoch": 1.5252224985149623,
      "grad_norm": 0.4402208924293518,
      "learning_rate": 0.00010213968648997602,
      "loss": 2.4427,
      "step": 74140
    },
    {
      "epoch": 1.52542821803183,
      "grad_norm": 0.4361867606639862,
      "learning_rate": 0.00010211748239073013,
      "loss": 2.3638,
      "step": 74150
    },
    {
      "epoch": 1.5256339375486978,
      "grad_norm": 0.412044495344162,
      "learning_rate": 0.00010209527818704041,
      "loss": 2.397,
      "step": 74160
    },
    {
      "epoch": 1.5258396570655655,
      "grad_norm": 0.45679971575737,
      "learning_rate": 0.00010207307388000201,
      "loss": 2.4113,
      "step": 74170
    },
    {
      "epoch": 1.526045376582433,
      "grad_norm": 0.4245985150337219,
      "learning_rate": 0.00010205086947071017,
      "loss": 2.3895,
      "step": 74180
    },
    {
      "epoch": 1.5262510960993008,
      "grad_norm": 0.4694490134716034,
      "learning_rate": 0.0001020286649602601,
      "loss": 2.3896,
      "step": 74190
    },
    {
      "epoch": 1.5264568156161684,
      "grad_norm": 0.44280996918678284,
      "learning_rate": 0.00010200646034974706,
      "loss": 2.4311,
      "step": 74200
    },
    {
      "epoch": 1.5266625351330363,
      "grad_norm": 0.49445995688438416,
      "learning_rate": 0.00010198425564026626,
      "loss": 2.4233,
      "step": 74210
    },
    {
      "epoch": 1.526868254649904,
      "grad_norm": 0.45078879594802856,
      "learning_rate": 0.00010196205083291295,
      "loss": 2.4304,
      "step": 74220
    },
    {
      "epoch": 1.5270739741667718,
      "grad_norm": 0.4149036407470703,
      "learning_rate": 0.00010193984592878236,
      "loss": 2.3866,
      "step": 74230
    },
    {
      "epoch": 1.5272796936836395,
      "grad_norm": 0.4257006049156189,
      "learning_rate": 0.00010191764092896974,
      "loss": 2.421,
      "step": 74240
    },
    {
      "epoch": 1.5274854132005071,
      "grad_norm": 0.446647047996521,
      "learning_rate": 0.00010189543583457032,
      "loss": 2.3913,
      "step": 74250
    },
    {
      "epoch": 1.5276911327173748,
      "grad_norm": 0.45197659730911255,
      "learning_rate": 0.00010187323064667944,
      "loss": 2.3592,
      "step": 74260
    },
    {
      "epoch": 1.5278968522342424,
      "grad_norm": 0.4036196172237396,
      "learning_rate": 0.00010185102536639224,
      "loss": 2.3529,
      "step": 74270
    },
    {
      "epoch": 1.52810257175111,
      "grad_norm": 0.4729403257369995,
      "learning_rate": 0.00010182881999480407,
      "loss": 2.3654,
      "step": 74280
    },
    {
      "epoch": 1.528308291267978,
      "grad_norm": 0.4217415750026703,
      "learning_rate": 0.0001018066145330102,
      "loss": 2.4034,
      "step": 74290
    },
    {
      "epoch": 1.5285140107848458,
      "grad_norm": 0.4225972592830658,
      "learning_rate": 0.00010178440898210584,
      "loss": 2.3936,
      "step": 74300
    },
    {
      "epoch": 1.5287197303017135,
      "grad_norm": 0.4391164183616638,
      "learning_rate": 0.00010176220334318632,
      "loss": 2.4426,
      "step": 74310
    },
    {
      "epoch": 1.5289254498185811,
      "grad_norm": 0.45703640580177307,
      "learning_rate": 0.00010173999761734692,
      "loss": 2.4474,
      "step": 74320
    },
    {
      "epoch": 1.5291311693354488,
      "grad_norm": 0.44507381319999695,
      "learning_rate": 0.0001017177918056829,
      "loss": 2.3989,
      "step": 74330
    },
    {
      "epoch": 1.5293368888523164,
      "grad_norm": 0.45897582173347473,
      "learning_rate": 0.0001016955859092896,
      "loss": 2.4598,
      "step": 74340
    },
    {
      "epoch": 1.529542608369184,
      "grad_norm": 0.4538080394268036,
      "learning_rate": 0.00010167337992926227,
      "loss": 2.4316,
      "step": 74350
    },
    {
      "epoch": 1.529748327886052,
      "grad_norm": 0.4247382581233978,
      "learning_rate": 0.00010165117386669622,
      "loss": 2.3947,
      "step": 74360
    },
    {
      "epoch": 1.5299540474029196,
      "grad_norm": 0.46145838499069214,
      "learning_rate": 0.00010162896772268677,
      "loss": 2.4535,
      "step": 74370
    },
    {
      "epoch": 1.5301597669197875,
      "grad_norm": 0.3973166346549988,
      "learning_rate": 0.00010160676149832921,
      "loss": 2.4163,
      "step": 74380
    },
    {
      "epoch": 1.5303654864366552,
      "grad_norm": 0.4192381799221039,
      "learning_rate": 0.00010158455519471887,
      "loss": 2.4107,
      "step": 74390
    },
    {
      "epoch": 1.5305712059535228,
      "grad_norm": 0.4176437258720398,
      "learning_rate": 0.00010156234881295106,
      "loss": 2.3758,
      "step": 74400
    },
    {
      "epoch": 1.5307769254703905,
      "grad_norm": 0.5619348883628845,
      "learning_rate": 0.0001015401423541211,
      "loss": 2.3754,
      "step": 74410
    },
    {
      "epoch": 1.5309826449872581,
      "grad_norm": 0.4391249120235443,
      "learning_rate": 0.00010151793581932432,
      "loss": 2.405,
      "step": 74420
    },
    {
      "epoch": 1.531188364504126,
      "grad_norm": 0.4922439157962799,
      "learning_rate": 0.000101495729209656,
      "loss": 2.3772,
      "step": 74430
    },
    {
      "epoch": 1.5313940840209936,
      "grad_norm": 0.49207523465156555,
      "learning_rate": 0.00010147352252621154,
      "loss": 2.3455,
      "step": 74440
    },
    {
      "epoch": 1.5315998035378615,
      "grad_norm": 0.4252963364124298,
      "learning_rate": 0.00010145131577008624,
      "loss": 2.4206,
      "step": 74450
    },
    {
      "epoch": 1.5318055230547292,
      "grad_norm": 0.4255329668521881,
      "learning_rate": 0.00010142910894237545,
      "loss": 2.3739,
      "step": 74460
    },
    {
      "epoch": 1.5320112425715968,
      "grad_norm": 0.4467957317829132,
      "learning_rate": 0.00010140690204417447,
      "loss": 2.3388,
      "step": 74470
    },
    {
      "epoch": 1.5322169620884645,
      "grad_norm": 0.48508191108703613,
      "learning_rate": 0.00010138469507657871,
      "loss": 2.3702,
      "step": 74480
    },
    {
      "epoch": 1.5324226816053321,
      "grad_norm": 0.48792603611946106,
      "learning_rate": 0.00010136248804068346,
      "loss": 2.4363,
      "step": 74490
    },
    {
      "epoch": 1.5326284011222,
      "grad_norm": 0.4584515392780304,
      "learning_rate": 0.00010134028093758412,
      "loss": 2.4351,
      "step": 74500
    },
    {
      "epoch": 1.5328341206390677,
      "grad_norm": 0.48225945234298706,
      "learning_rate": 0.00010131807376837603,
      "loss": 2.4256,
      "step": 74510
    },
    {
      "epoch": 1.5330398401559355,
      "grad_norm": 0.4064081907272339,
      "learning_rate": 0.00010129586653415453,
      "loss": 2.4356,
      "step": 74520
    },
    {
      "epoch": 1.5332455596728032,
      "grad_norm": 0.4182635545730591,
      "learning_rate": 0.000101273659236015,
      "loss": 2.4092,
      "step": 74530
    },
    {
      "epoch": 1.5334512791896708,
      "grad_norm": 0.4323152005672455,
      "learning_rate": 0.00010125145187505281,
      "loss": 2.3757,
      "step": 74540
    },
    {
      "epoch": 1.5336569987065385,
      "grad_norm": 0.4579138457775116,
      "learning_rate": 0.0001012292444523633,
      "loss": 2.3265,
      "step": 74550
    },
    {
      "epoch": 1.5338627182234061,
      "grad_norm": 0.4169035255908966,
      "learning_rate": 0.00010120703696904186,
      "loss": 2.4146,
      "step": 74560
    },
    {
      "epoch": 1.534068437740274,
      "grad_norm": 0.42443564534187317,
      "learning_rate": 0.00010118482942618388,
      "loss": 2.3659,
      "step": 74570
    },
    {
      "epoch": 1.5342741572571417,
      "grad_norm": 0.45126378536224365,
      "learning_rate": 0.00010116262182488475,
      "loss": 2.4048,
      "step": 74580
    },
    {
      "epoch": 1.5344798767740095,
      "grad_norm": 0.4217182993888855,
      "learning_rate": 0.00010114041416623974,
      "loss": 2.4049,
      "step": 74590
    },
    {
      "epoch": 1.5346855962908772,
      "grad_norm": 0.42158031463623047,
      "learning_rate": 0.00010111820645134436,
      "loss": 2.4282,
      "step": 74600
    },
    {
      "epoch": 1.5348913158077448,
      "grad_norm": 0.41590023040771484,
      "learning_rate": 0.00010109599868129397,
      "loss": 2.4092,
      "step": 74610
    },
    {
      "epoch": 1.5350970353246125,
      "grad_norm": 0.4434858560562134,
      "learning_rate": 0.00010107379085718391,
      "loss": 2.412,
      "step": 74620
    },
    {
      "epoch": 1.5353027548414802,
      "grad_norm": 0.4679545760154724,
      "learning_rate": 0.0001010515829801096,
      "loss": 2.4476,
      "step": 74630
    },
    {
      "epoch": 1.5355084743583478,
      "grad_norm": 0.4655930995941162,
      "learning_rate": 0.00010102937505116643,
      "loss": 2.4206,
      "step": 74640
    },
    {
      "epoch": 1.5357141938752157,
      "grad_norm": 0.4054516851902008,
      "learning_rate": 0.00010100716707144982,
      "loss": 2.4029,
      "step": 74650
    },
    {
      "epoch": 1.5359199133920836,
      "grad_norm": 0.45761823654174805,
      "learning_rate": 0.00010098495904205512,
      "loss": 2.4271,
      "step": 74660
    },
    {
      "epoch": 1.5361256329089512,
      "grad_norm": 0.43373191356658936,
      "learning_rate": 0.00010096275096407777,
      "loss": 2.3015,
      "step": 74670
    },
    {
      "epoch": 1.5363313524258189,
      "grad_norm": 0.4357754588127136,
      "learning_rate": 0.00010094054283861316,
      "loss": 2.3831,
      "step": 74680
    },
    {
      "epoch": 1.5365370719426865,
      "grad_norm": 0.4408247768878937,
      "learning_rate": 0.00010091833466675668,
      "loss": 2.3858,
      "step": 74690
    },
    {
      "epoch": 1.5367427914595542,
      "grad_norm": 0.5422930121421814,
      "learning_rate": 0.00010089612644960378,
      "loss": 2.3777,
      "step": 74700
    },
    {
      "epoch": 1.5369485109764218,
      "grad_norm": 0.49639320373535156,
      "learning_rate": 0.00010087391818824987,
      "loss": 2.4303,
      "step": 74710
    },
    {
      "epoch": 1.5371542304932897,
      "grad_norm": 0.4618666470050812,
      "learning_rate": 0.00010085170988379027,
      "loss": 2.3473,
      "step": 74720
    },
    {
      "epoch": 1.5373599500101574,
      "grad_norm": 0.4399760365486145,
      "learning_rate": 0.0001008295015373205,
      "loss": 2.3922,
      "step": 74730
    },
    {
      "epoch": 1.5375656695270252,
      "grad_norm": 0.43736764788627625,
      "learning_rate": 0.00010080729314993595,
      "loss": 2.3705,
      "step": 74740
    },
    {
      "epoch": 1.5377713890438929,
      "grad_norm": 0.45082101225852966,
      "learning_rate": 0.00010078508472273202,
      "loss": 2.4338,
      "step": 74750
    },
    {
      "epoch": 1.5379771085607605,
      "grad_norm": 0.43858134746551514,
      "learning_rate": 0.00010076287625680412,
      "loss": 2.3348,
      "step": 74760
    },
    {
      "epoch": 1.5381828280776282,
      "grad_norm": 0.41724705696105957,
      "learning_rate": 0.00010074066775324773,
      "loss": 2.3925,
      "step": 74770
    },
    {
      "epoch": 1.5383885475944958,
      "grad_norm": 0.4663828909397125,
      "learning_rate": 0.00010071845921315822,
      "loss": 2.4073,
      "step": 74780
    },
    {
      "epoch": 1.5385942671113637,
      "grad_norm": 0.4023648798465729,
      "learning_rate": 0.000100696250637631,
      "loss": 2.3515,
      "step": 74790
    },
    {
      "epoch": 1.5387999866282314,
      "grad_norm": 0.5191786885261536,
      "learning_rate": 0.00010067404202776157,
      "loss": 2.376,
      "step": 74800
    },
    {
      "epoch": 1.5390057061450992,
      "grad_norm": 0.5490447878837585,
      "learning_rate": 0.0001006518333846453,
      "loss": 2.3989,
      "step": 74810
    },
    {
      "epoch": 1.539211425661967,
      "grad_norm": 0.4561571478843689,
      "learning_rate": 0.00010062962470937764,
      "loss": 2.398,
      "step": 74820
    },
    {
      "epoch": 1.5394171451788345,
      "grad_norm": 0.44037967920303345,
      "learning_rate": 0.00010060741600305403,
      "loss": 2.4,
      "step": 74830
    },
    {
      "epoch": 1.5396228646957022,
      "grad_norm": 0.4504142105579376,
      "learning_rate": 0.0001005852072667699,
      "loss": 2.3277,
      "step": 74840
    },
    {
      "epoch": 1.5398285842125699,
      "grad_norm": 0.45402488112449646,
      "learning_rate": 0.00010056299850162068,
      "loss": 2.4068,
      "step": 74850
    },
    {
      "epoch": 1.5400343037294377,
      "grad_norm": 0.4533767104148865,
      "learning_rate": 0.0001005407897087018,
      "loss": 2.4115,
      "step": 74860
    },
    {
      "epoch": 1.5402400232463054,
      "grad_norm": 0.46166539192199707,
      "learning_rate": 0.00010051858088910875,
      "loss": 2.42,
      "step": 74870
    },
    {
      "epoch": 1.5404457427631733,
      "grad_norm": 0.4676152169704437,
      "learning_rate": 0.0001004963720439369,
      "loss": 2.4461,
      "step": 74880
    },
    {
      "epoch": 1.540651462280041,
      "grad_norm": 0.48728638887405396,
      "learning_rate": 0.0001004741631742817,
      "loss": 2.3825,
      "step": 74890
    },
    {
      "epoch": 1.5408571817969086,
      "grad_norm": 0.5131513476371765,
      "learning_rate": 0.00010045195428123868,
      "loss": 2.4564,
      "step": 74900
    },
    {
      "epoch": 1.5410629013137762,
      "grad_norm": 0.4690200686454773,
      "learning_rate": 0.00010042974536590318,
      "loss": 2.4356,
      "step": 74910
    },
    {
      "epoch": 1.5412686208306439,
      "grad_norm": 0.4291416108608246,
      "learning_rate": 0.00010040753642937067,
      "loss": 2.4678,
      "step": 74920
    },
    {
      "epoch": 1.5414743403475117,
      "grad_norm": 0.4654698669910431,
      "learning_rate": 0.00010038532747273664,
      "loss": 2.3905,
      "step": 74930
    },
    {
      "epoch": 1.5416800598643794,
      "grad_norm": 0.4289013743400574,
      "learning_rate": 0.00010036311849709648,
      "loss": 2.3698,
      "step": 74940
    },
    {
      "epoch": 1.5418857793812473,
      "grad_norm": 0.4413636326789856,
      "learning_rate": 0.00010034090950354567,
      "loss": 2.4065,
      "step": 74950
    },
    {
      "epoch": 1.542091498898115,
      "grad_norm": 0.47673720121383667,
      "learning_rate": 0.00010031870049317964,
      "loss": 2.415,
      "step": 74960
    },
    {
      "epoch": 1.5422972184149826,
      "grad_norm": 0.4578138291835785,
      "learning_rate": 0.00010029649146709386,
      "loss": 2.3893,
      "step": 74970
    },
    {
      "epoch": 1.5425029379318502,
      "grad_norm": 0.4392336905002594,
      "learning_rate": 0.00010027428242638378,
      "loss": 2.4314,
      "step": 74980
    },
    {
      "epoch": 1.5427086574487179,
      "grad_norm": 0.4617549180984497,
      "learning_rate": 0.00010025207337214484,
      "loss": 2.4264,
      "step": 74990
    },
    {
      "epoch": 1.5429143769655855,
      "grad_norm": 0.41954609751701355,
      "learning_rate": 0.0001002298643054725,
      "loss": 2.3666,
      "step": 75000
    },
    {
      "epoch": 1.5431200964824534,
      "grad_norm": 0.44401413202285767,
      "learning_rate": 0.00010020765522746218,
      "loss": 2.4109,
      "step": 75010
    },
    {
      "epoch": 1.5433258159993213,
      "grad_norm": 0.42653200030326843,
      "learning_rate": 0.00010018544613920939,
      "loss": 2.3583,
      "step": 75020
    },
    {
      "epoch": 1.543531535516189,
      "grad_norm": 0.4505240023136139,
      "learning_rate": 0.00010016323704180957,
      "loss": 2.3448,
      "step": 75030
    },
    {
      "epoch": 1.5437372550330566,
      "grad_norm": 0.4387526512145996,
      "learning_rate": 0.00010014102793635809,
      "loss": 2.3859,
      "step": 75040
    },
    {
      "epoch": 1.5439429745499242,
      "grad_norm": 0.4421657919883728,
      "learning_rate": 0.00010011881882395052,
      "loss": 2.3916,
      "step": 75050
    },
    {
      "epoch": 1.544148694066792,
      "grad_norm": 0.4134786128997803,
      "learning_rate": 0.00010009660970568226,
      "loss": 2.4129,
      "step": 75060
    },
    {
      "epoch": 1.5443544135836595,
      "grad_norm": 0.4464247226715088,
      "learning_rate": 0.00010007440058264876,
      "loss": 2.3774,
      "step": 75070
    },
    {
      "epoch": 1.5445601331005274,
      "grad_norm": 0.4751197099685669,
      "learning_rate": 0.00010005219145594547,
      "loss": 2.4365,
      "step": 75080
    },
    {
      "epoch": 1.544765852617395,
      "grad_norm": 0.41724294424057007,
      "learning_rate": 0.0001000299823266679,
      "loss": 2.3534,
      "step": 75090
    },
    {
      "epoch": 1.544971572134263,
      "grad_norm": 0.4554019570350647,
      "learning_rate": 0.00010000777319591144,
      "loss": 2.3515,
      "step": 75100
    },
    {
      "epoch": 1.5451772916511306,
      "grad_norm": 0.49828869104385376,
      "learning_rate": 9.998556406477156e-05,
      "loss": 2.4503,
      "step": 75110
    },
    {
      "epoch": 1.5453830111679983,
      "grad_norm": 0.5363897085189819,
      "learning_rate": 9.996335493434373e-05,
      "loss": 2.3637,
      "step": 75120
    },
    {
      "epoch": 1.545588730684866,
      "grad_norm": 0.4504016637802124,
      "learning_rate": 9.994114580572344e-05,
      "loss": 2.4554,
      "step": 75130
    },
    {
      "epoch": 1.5457944502017336,
      "grad_norm": 0.4848601222038269,
      "learning_rate": 9.991893668000605e-05,
      "loss": 2.4097,
      "step": 75140
    },
    {
      "epoch": 1.5460001697186014,
      "grad_norm": 0.4708326458930969,
      "learning_rate": 9.989672755828709e-05,
      "loss": 2.357,
      "step": 75150
    },
    {
      "epoch": 1.546205889235469,
      "grad_norm": 0.4430053234100342,
      "learning_rate": 9.987451844166204e-05,
      "loss": 2.3864,
      "step": 75160
    },
    {
      "epoch": 1.546411608752337,
      "grad_norm": 0.48159173130989075,
      "learning_rate": 9.985230933122625e-05,
      "loss": 2.3578,
      "step": 75170
    },
    {
      "epoch": 1.5466173282692046,
      "grad_norm": 0.4661301374435425,
      "learning_rate": 9.983010022807527e-05,
      "loss": 2.4414,
      "step": 75180
    },
    {
      "epoch": 1.5468230477860723,
      "grad_norm": 0.48752859234809875,
      "learning_rate": 9.980789113330452e-05,
      "loss": 2.3647,
      "step": 75190
    },
    {
      "epoch": 1.54702876730294,
      "grad_norm": 0.4188389778137207,
      "learning_rate": 9.978568204800943e-05,
      "loss": 2.438,
      "step": 75200
    },
    {
      "epoch": 1.5472344868198076,
      "grad_norm": 0.4150080680847168,
      "learning_rate": 9.976347297328551e-05,
      "loss": 2.4313,
      "step": 75210
    },
    {
      "epoch": 1.5474402063366755,
      "grad_norm": 0.4550503194332123,
      "learning_rate": 9.974126391022815e-05,
      "loss": 2.4508,
      "step": 75220
    },
    {
      "epoch": 1.547645925853543,
      "grad_norm": 0.42432406544685364,
      "learning_rate": 9.971905485993282e-05,
      "loss": 2.3453,
      "step": 75230
    },
    {
      "epoch": 1.547851645370411,
      "grad_norm": 0.44054099917411804,
      "learning_rate": 9.969684582349503e-05,
      "loss": 2.3688,
      "step": 75240
    },
    {
      "epoch": 1.5480573648872786,
      "grad_norm": 0.41188517212867737,
      "learning_rate": 9.967463680201017e-05,
      "loss": 2.3631,
      "step": 75250
    },
    {
      "epoch": 1.5482630844041463,
      "grad_norm": 0.4013218283653259,
      "learning_rate": 9.965242779657367e-05,
      "loss": 2.4881,
      "step": 75260
    },
    {
      "epoch": 1.548468803921014,
      "grad_norm": 0.4422210454940796,
      "learning_rate": 9.963021880828106e-05,
      "loss": 2.3643,
      "step": 75270
    },
    {
      "epoch": 1.5486745234378816,
      "grad_norm": 0.4231259524822235,
      "learning_rate": 9.960800983822771e-05,
      "loss": 2.3973,
      "step": 75280
    },
    {
      "epoch": 1.5488802429547495,
      "grad_norm": 0.45117175579071045,
      "learning_rate": 9.958580088750913e-05,
      "loss": 2.4033,
      "step": 75290
    },
    {
      "epoch": 1.5490859624716171,
      "grad_norm": 0.4585901200771332,
      "learning_rate": 9.956359195722068e-05,
      "loss": 2.4121,
      "step": 75300
    },
    {
      "epoch": 1.549291681988485,
      "grad_norm": 0.45494344830513,
      "learning_rate": 9.954138304845789e-05,
      "loss": 2.4033,
      "step": 75310
    },
    {
      "epoch": 1.5494974015053526,
      "grad_norm": 0.4536222517490387,
      "learning_rate": 9.95191741623162e-05,
      "loss": 2.373,
      "step": 75320
    },
    {
      "epoch": 1.5497031210222203,
      "grad_norm": 0.4570307433605194,
      "learning_rate": 9.949696529989098e-05,
      "loss": 2.3631,
      "step": 75330
    },
    {
      "epoch": 1.549908840539088,
      "grad_norm": 0.44601336121559143,
      "learning_rate": 9.947475646227774e-05,
      "loss": 2.3862,
      "step": 75340
    },
    {
      "epoch": 1.5501145600559556,
      "grad_norm": 0.46850332617759705,
      "learning_rate": 9.945254765057193e-05,
      "loss": 2.4322,
      "step": 75350
    },
    {
      "epoch": 1.5503202795728235,
      "grad_norm": 0.4240826964378357,
      "learning_rate": 9.94303388658689e-05,
      "loss": 2.3766,
      "step": 75360
    },
    {
      "epoch": 1.5505259990896911,
      "grad_norm": 0.47422540187835693,
      "learning_rate": 9.940813010926422e-05,
      "loss": 2.3925,
      "step": 75370
    },
    {
      "epoch": 1.550731718606559,
      "grad_norm": 0.4255675971508026,
      "learning_rate": 9.938592138185323e-05,
      "loss": 2.3857,
      "step": 75380
    },
    {
      "epoch": 1.5509374381234267,
      "grad_norm": 0.43083417415618896,
      "learning_rate": 9.936371268473136e-05,
      "loss": 2.3951,
      "step": 75390
    },
    {
      "epoch": 1.5511431576402943,
      "grad_norm": 0.49648863077163696,
      "learning_rate": 9.934150401899413e-05,
      "loss": 2.3874,
      "step": 75400
    },
    {
      "epoch": 1.551348877157162,
      "grad_norm": 0.5136768817901611,
      "learning_rate": 9.931929538573688e-05,
      "loss": 2.3717,
      "step": 75410
    },
    {
      "epoch": 1.5515545966740296,
      "grad_norm": 0.4750809967517853,
      "learning_rate": 9.929708678605506e-05,
      "loss": 2.4273,
      "step": 75420
    },
    {
      "epoch": 1.5517603161908973,
      "grad_norm": 0.4341435432434082,
      "learning_rate": 9.927487822104417e-05,
      "loss": 2.3734,
      "step": 75430
    },
    {
      "epoch": 1.5519660357077651,
      "grad_norm": 0.4566624164581299,
      "learning_rate": 9.925266969179956e-05,
      "loss": 2.3514,
      "step": 75440
    },
    {
      "epoch": 1.552171755224633,
      "grad_norm": 0.4724881649017334,
      "learning_rate": 9.923046119941671e-05,
      "loss": 2.4307,
      "step": 75450
    },
    {
      "epoch": 1.5523774747415007,
      "grad_norm": 0.4127591550350189,
      "learning_rate": 9.920825274499097e-05,
      "loss": 2.3959,
      "step": 75460
    },
    {
      "epoch": 1.5525831942583683,
      "grad_norm": 0.43406614661216736,
      "learning_rate": 9.918604432961783e-05,
      "loss": 2.3391,
      "step": 75470
    },
    {
      "epoch": 1.552788913775236,
      "grad_norm": 0.46547937393188477,
      "learning_rate": 9.91638359543927e-05,
      "loss": 2.3839,
      "step": 75480
    },
    {
      "epoch": 1.5529946332921036,
      "grad_norm": 0.4255160391330719,
      "learning_rate": 9.914162762041094e-05,
      "loss": 2.4139,
      "step": 75490
    },
    {
      "epoch": 1.5532003528089713,
      "grad_norm": 0.5769979953765869,
      "learning_rate": 9.911941932876805e-05,
      "loss": 2.3729,
      "step": 75500
    },
    {
      "epoch": 1.5534060723258392,
      "grad_norm": 0.42852190136909485,
      "learning_rate": 9.909721108055942e-05,
      "loss": 2.3815,
      "step": 75510
    },
    {
      "epoch": 1.5536117918427068,
      "grad_norm": 0.45002925395965576,
      "learning_rate": 9.907500287688041e-05,
      "loss": 2.4371,
      "step": 75520
    },
    {
      "epoch": 1.5538175113595747,
      "grad_norm": 0.43887102603912354,
      "learning_rate": 9.90527947188265e-05,
      "loss": 2.3792,
      "step": 75530
    },
    {
      "epoch": 1.5540232308764423,
      "grad_norm": 0.4979856014251709,
      "learning_rate": 9.903058660749304e-05,
      "loss": 2.4131,
      "step": 75540
    },
    {
      "epoch": 1.55422895039331,
      "grad_norm": 0.4280499815940857,
      "learning_rate": 9.900837854397546e-05,
      "loss": 2.3962,
      "step": 75550
    },
    {
      "epoch": 1.5544346699101776,
      "grad_norm": 0.4608277380466461,
      "learning_rate": 9.89861705293692e-05,
      "loss": 2.3789,
      "step": 75560
    },
    {
      "epoch": 1.5546403894270453,
      "grad_norm": 0.4621098041534424,
      "learning_rate": 9.896396256476958e-05,
      "loss": 2.4086,
      "step": 75570
    },
    {
      "epoch": 1.5548461089439132,
      "grad_norm": 0.49224853515625,
      "learning_rate": 9.894175465127204e-05,
      "loss": 2.3879,
      "step": 75580
    },
    {
      "epoch": 1.5550518284607808,
      "grad_norm": 0.4812970459461212,
      "learning_rate": 9.891954678997202e-05,
      "loss": 2.4216,
      "step": 75590
    },
    {
      "epoch": 1.5552575479776487,
      "grad_norm": 0.42813369631767273,
      "learning_rate": 9.889733898196484e-05,
      "loss": 2.3894,
      "step": 75600
    },
    {
      "epoch": 1.5554632674945164,
      "grad_norm": 0.44798949360847473,
      "learning_rate": 9.887513122834594e-05,
      "loss": 2.3984,
      "step": 75610
    },
    {
      "epoch": 1.555668987011384,
      "grad_norm": 0.42695069313049316,
      "learning_rate": 9.885292353021063e-05,
      "loss": 2.3828,
      "step": 75620
    },
    {
      "epoch": 1.5558747065282517,
      "grad_norm": 0.4458436369895935,
      "learning_rate": 9.88307158886544e-05,
      "loss": 2.419,
      "step": 75630
    },
    {
      "epoch": 1.5560804260451193,
      "grad_norm": 0.43799519538879395,
      "learning_rate": 9.88085083047726e-05,
      "loss": 2.3887,
      "step": 75640
    },
    {
      "epoch": 1.5562861455619872,
      "grad_norm": 0.44921839237213135,
      "learning_rate": 9.878630077966054e-05,
      "loss": 2.4418,
      "step": 75650
    },
    {
      "epoch": 1.5564918650788548,
      "grad_norm": 0.4605385363101959,
      "learning_rate": 9.876409331441367e-05,
      "loss": 2.409,
      "step": 75660
    },
    {
      "epoch": 1.5566975845957227,
      "grad_norm": 0.42053231596946716,
      "learning_rate": 9.874188591012736e-05,
      "loss": 2.3857,
      "step": 75670
    },
    {
      "epoch": 1.5569033041125904,
      "grad_norm": 0.41061171889305115,
      "learning_rate": 9.871967856789693e-05,
      "loss": 2.3382,
      "step": 75680
    },
    {
      "epoch": 1.557109023629458,
      "grad_norm": 0.4184305965900421,
      "learning_rate": 9.869747128881781e-05,
      "loss": 2.4545,
      "step": 75690
    },
    {
      "epoch": 1.5573147431463257,
      "grad_norm": 0.441611111164093,
      "learning_rate": 9.867526407398533e-05,
      "loss": 2.3672,
      "step": 75700
    },
    {
      "epoch": 1.5575204626631933,
      "grad_norm": 0.45618587732315063,
      "learning_rate": 9.865305692449481e-05,
      "loss": 2.4238,
      "step": 75710
    },
    {
      "epoch": 1.5577261821800612,
      "grad_norm": 0.4305392801761627,
      "learning_rate": 9.863084984144172e-05,
      "loss": 2.3716,
      "step": 75720
    },
    {
      "epoch": 1.5579319016969289,
      "grad_norm": 0.4337064325809479,
      "learning_rate": 9.86086428259213e-05,
      "loss": 2.3967,
      "step": 75730
    },
    {
      "epoch": 1.5581376212137967,
      "grad_norm": 0.44514036178588867,
      "learning_rate": 9.858643587902894e-05,
      "loss": 2.4159,
      "step": 75740
    },
    {
      "epoch": 1.5583433407306644,
      "grad_norm": 0.467837393283844,
      "learning_rate": 9.856422900186004e-05,
      "loss": 2.3833,
      "step": 75750
    },
    {
      "epoch": 1.558549060247532,
      "grad_norm": 0.4142826795578003,
      "learning_rate": 9.854202219550988e-05,
      "loss": 2.359,
      "step": 75760
    },
    {
      "epoch": 1.5587547797643997,
      "grad_norm": 0.41836294531822205,
      "learning_rate": 9.851981546107383e-05,
      "loss": 2.3473,
      "step": 75770
    },
    {
      "epoch": 1.5589604992812673,
      "grad_norm": 0.4914371967315674,
      "learning_rate": 9.849760879964717e-05,
      "loss": 2.4153,
      "step": 75780
    },
    {
      "epoch": 1.559166218798135,
      "grad_norm": 0.4750656485557556,
      "learning_rate": 9.847540221232532e-05,
      "loss": 2.4044,
      "step": 75790
    },
    {
      "epoch": 1.5593719383150029,
      "grad_norm": 0.4174750745296478,
      "learning_rate": 9.845319570020359e-05,
      "loss": 2.3644,
      "step": 75800
    },
    {
      "epoch": 1.5595776578318707,
      "grad_norm": 0.5075048208236694,
      "learning_rate": 9.843098926437723e-05,
      "loss": 2.3603,
      "step": 75810
    },
    {
      "epoch": 1.5597833773487384,
      "grad_norm": 0.4648072123527527,
      "learning_rate": 9.840878290594165e-05,
      "loss": 2.4044,
      "step": 75820
    },
    {
      "epoch": 1.559989096865606,
      "grad_norm": 0.4505099058151245,
      "learning_rate": 9.838657662599217e-05,
      "loss": 2.377,
      "step": 75830
    },
    {
      "epoch": 1.5601948163824737,
      "grad_norm": 0.427060604095459,
      "learning_rate": 9.836437042562401e-05,
      "loss": 2.3892,
      "step": 75840
    },
    {
      "epoch": 1.5604005358993414,
      "grad_norm": 0.46460872888565063,
      "learning_rate": 9.834216430593259e-05,
      "loss": 2.4184,
      "step": 75850
    },
    {
      "epoch": 1.560606255416209,
      "grad_norm": 0.4416182339191437,
      "learning_rate": 9.831995826801316e-05,
      "loss": 2.3832,
      "step": 75860
    },
    {
      "epoch": 1.5608119749330769,
      "grad_norm": 0.4242452383041382,
      "learning_rate": 9.829775231296102e-05,
      "loss": 2.4216,
      "step": 75870
    },
    {
      "epoch": 1.5610176944499445,
      "grad_norm": 0.4209219515323639,
      "learning_rate": 9.82755464418715e-05,
      "loss": 2.3536,
      "step": 75880
    },
    {
      "epoch": 1.5612234139668124,
      "grad_norm": 0.4333542585372925,
      "learning_rate": 9.825334065583986e-05,
      "loss": 2.4145,
      "step": 75890
    },
    {
      "epoch": 1.56142913348368,
      "grad_norm": 0.40954864025115967,
      "learning_rate": 9.823113495596145e-05,
      "loss": 2.3967,
      "step": 75900
    },
    {
      "epoch": 1.5616348530005477,
      "grad_norm": 0.5021211504936218,
      "learning_rate": 9.820892934333145e-05,
      "loss": 2.3762,
      "step": 75910
    },
    {
      "epoch": 1.5618405725174154,
      "grad_norm": 0.41943204402923584,
      "learning_rate": 9.818672381904524e-05,
      "loss": 2.3666,
      "step": 75920
    },
    {
      "epoch": 1.562046292034283,
      "grad_norm": 0.4325807988643646,
      "learning_rate": 9.816451838419807e-05,
      "loss": 2.3533,
      "step": 75930
    },
    {
      "epoch": 1.562252011551151,
      "grad_norm": 0.4360114336013794,
      "learning_rate": 9.814231303988517e-05,
      "loss": 2.4056,
      "step": 75940
    },
    {
      "epoch": 1.5624577310680186,
      "grad_norm": 0.3899472951889038,
      "learning_rate": 9.812010778720187e-05,
      "loss": 2.4095,
      "step": 75950
    },
    {
      "epoch": 1.5626634505848864,
      "grad_norm": 0.447585791349411,
      "learning_rate": 9.809790262724344e-05,
      "loss": 2.3836,
      "step": 75960
    },
    {
      "epoch": 1.562869170101754,
      "grad_norm": 0.4819311499595642,
      "learning_rate": 9.807569756110504e-05,
      "loss": 2.3593,
      "step": 75970
    },
    {
      "epoch": 1.5630748896186217,
      "grad_norm": 0.4008401036262512,
      "learning_rate": 9.805349258988204e-05,
      "loss": 2.4061,
      "step": 75980
    },
    {
      "epoch": 1.5632806091354894,
      "grad_norm": 0.4401094317436218,
      "learning_rate": 9.803128771466965e-05,
      "loss": 2.4201,
      "step": 75990
    },
    {
      "epoch": 1.563486328652357,
      "grad_norm": 0.436982661485672,
      "learning_rate": 9.800908293656306e-05,
      "loss": 2.3765,
      "step": 76000
    },
    {
      "epoch": 1.563692048169225,
      "grad_norm": 0.45269057154655457,
      "learning_rate": 9.79868782566576e-05,
      "loss": 2.4133,
      "step": 76010
    },
    {
      "epoch": 1.5638977676860926,
      "grad_norm": 0.4742269217967987,
      "learning_rate": 9.796467367604846e-05,
      "loss": 2.3486,
      "step": 76020
    },
    {
      "epoch": 1.5641034872029604,
      "grad_norm": 0.47289755940437317,
      "learning_rate": 9.794246919583085e-05,
      "loss": 2.4078,
      "step": 76030
    },
    {
      "epoch": 1.564309206719828,
      "grad_norm": 0.4165264368057251,
      "learning_rate": 9.792026481710006e-05,
      "loss": 2.3567,
      "step": 76040
    },
    {
      "epoch": 1.5645149262366957,
      "grad_norm": 0.45728033781051636,
      "learning_rate": 9.789806054095127e-05,
      "loss": 2.3912,
      "step": 76050
    },
    {
      "epoch": 1.5647206457535634,
      "grad_norm": 0.43791571259498596,
      "learning_rate": 9.78758563684797e-05,
      "loss": 2.4159,
      "step": 76060
    },
    {
      "epoch": 1.564926365270431,
      "grad_norm": 0.46271365880966187,
      "learning_rate": 9.785365230078052e-05,
      "loss": 2.4055,
      "step": 76070
    },
    {
      "epoch": 1.565132084787299,
      "grad_norm": 0.41482117772102356,
      "learning_rate": 9.783144833894901e-05,
      "loss": 2.3294,
      "step": 76080
    },
    {
      "epoch": 1.5653378043041666,
      "grad_norm": 0.44837579131126404,
      "learning_rate": 9.780924448408034e-05,
      "loss": 2.3672,
      "step": 76090
    },
    {
      "epoch": 1.5655435238210345,
      "grad_norm": 0.4608306884765625,
      "learning_rate": 9.778704073726968e-05,
      "loss": 2.4048,
      "step": 76100
    },
    {
      "epoch": 1.565749243337902,
      "grad_norm": 0.45308172702789307,
      "learning_rate": 9.776483709961224e-05,
      "loss": 2.426,
      "step": 76110
    },
    {
      "epoch": 1.5659549628547698,
      "grad_norm": 0.45240727066993713,
      "learning_rate": 9.774263357220323e-05,
      "loss": 2.3827,
      "step": 76120
    },
    {
      "epoch": 1.5661606823716374,
      "grad_norm": 0.4993012547492981,
      "learning_rate": 9.772043015613778e-05,
      "loss": 2.3802,
      "step": 76130
    },
    {
      "epoch": 1.566366401888505,
      "grad_norm": 0.44019120931625366,
      "learning_rate": 9.769822685251113e-05,
      "loss": 2.3849,
      "step": 76140
    },
    {
      "epoch": 1.5665721214053727,
      "grad_norm": 0.4254876673221588,
      "learning_rate": 9.767602366241836e-05,
      "loss": 2.3498,
      "step": 76150
    },
    {
      "epoch": 1.5667778409222406,
      "grad_norm": 0.4500456750392914,
      "learning_rate": 9.765382058695466e-05,
      "loss": 2.3477,
      "step": 76160
    },
    {
      "epoch": 1.5669835604391085,
      "grad_norm": 0.42108190059661865,
      "learning_rate": 9.763161762721528e-05,
      "loss": 2.3995,
      "step": 76170
    },
    {
      "epoch": 1.5671892799559761,
      "grad_norm": 0.4590087831020355,
      "learning_rate": 9.760941478429526e-05,
      "loss": 2.4136,
      "step": 76180
    },
    {
      "epoch": 1.5673949994728438,
      "grad_norm": 0.46325311064720154,
      "learning_rate": 9.758721205928975e-05,
      "loss": 2.3582,
      "step": 76190
    },
    {
      "epoch": 1.5676007189897114,
      "grad_norm": 0.4372516870498657,
      "learning_rate": 9.756500945329396e-05,
      "loss": 2.4703,
      "step": 76200
    },
    {
      "epoch": 1.567806438506579,
      "grad_norm": 0.4050755798816681,
      "learning_rate": 9.754280696740298e-05,
      "loss": 2.3808,
      "step": 76210
    },
    {
      "epoch": 1.5680121580234467,
      "grad_norm": 0.42041441798210144,
      "learning_rate": 9.752060460271195e-05,
      "loss": 2.414,
      "step": 76220
    },
    {
      "epoch": 1.5682178775403146,
      "grad_norm": 0.45553311705589294,
      "learning_rate": 9.749840236031596e-05,
      "loss": 2.4729,
      "step": 76230
    },
    {
      "epoch": 1.5684235970571823,
      "grad_norm": 0.4308948814868927,
      "learning_rate": 9.747620024131015e-05,
      "loss": 2.4066,
      "step": 76240
    },
    {
      "epoch": 1.5686293165740501,
      "grad_norm": 0.4226028323173523,
      "learning_rate": 9.745399824678966e-05,
      "loss": 2.4087,
      "step": 76250
    },
    {
      "epoch": 1.5688350360909178,
      "grad_norm": 0.44351816177368164,
      "learning_rate": 9.743179637784951e-05,
      "loss": 2.4415,
      "step": 76260
    },
    {
      "epoch": 1.5690407556077854,
      "grad_norm": 0.43595415353775024,
      "learning_rate": 9.740959463558489e-05,
      "loss": 2.3568,
      "step": 76270
    },
    {
      "epoch": 1.569246475124653,
      "grad_norm": 0.44051891565322876,
      "learning_rate": 9.738739302109086e-05,
      "loss": 2.4055,
      "step": 76280
    },
    {
      "epoch": 1.5694521946415207,
      "grad_norm": 0.4154336154460907,
      "learning_rate": 9.736519153546246e-05,
      "loss": 2.4141,
      "step": 76290
    },
    {
      "epoch": 1.5696579141583886,
      "grad_norm": 0.4755004942417145,
      "learning_rate": 9.734299017979484e-05,
      "loss": 2.3813,
      "step": 76300
    },
    {
      "epoch": 1.5698636336752563,
      "grad_norm": 0.40707576274871826,
      "learning_rate": 9.7320788955183e-05,
      "loss": 2.344,
      "step": 76310
    },
    {
      "epoch": 1.5700693531921242,
      "grad_norm": 0.4512518346309662,
      "learning_rate": 9.729858786272203e-05,
      "loss": 2.3743,
      "step": 76320
    },
    {
      "epoch": 1.5702750727089918,
      "grad_norm": 0.42680156230926514,
      "learning_rate": 9.727638690350706e-05,
      "loss": 2.3809,
      "step": 76330
    },
    {
      "epoch": 1.5704807922258595,
      "grad_norm": 0.4595571756362915,
      "learning_rate": 9.725418607863305e-05,
      "loss": 2.3745,
      "step": 76340
    },
    {
      "epoch": 1.570686511742727,
      "grad_norm": 0.40648791193962097,
      "learning_rate": 9.723198538919505e-05,
      "loss": 2.4334,
      "step": 76350
    },
    {
      "epoch": 1.5708922312595948,
      "grad_norm": 0.4520823359489441,
      "learning_rate": 9.720978483628816e-05,
      "loss": 2.4396,
      "step": 76360
    },
    {
      "epoch": 1.5710979507764626,
      "grad_norm": 0.4153949022293091,
      "learning_rate": 9.718758442100739e-05,
      "loss": 2.3498,
      "step": 76370
    },
    {
      "epoch": 1.5713036702933303,
      "grad_norm": 0.3930545449256897,
      "learning_rate": 9.716538414444775e-05,
      "loss": 2.4899,
      "step": 76380
    },
    {
      "epoch": 1.5715093898101982,
      "grad_norm": 0.4044816195964813,
      "learning_rate": 9.714318400770423e-05,
      "loss": 2.3996,
      "step": 76390
    },
    {
      "epoch": 1.5717151093270658,
      "grad_norm": 0.4261344075202942,
      "learning_rate": 9.712098401187188e-05,
      "loss": 2.4311,
      "step": 76400
    },
    {
      "epoch": 1.5719208288439335,
      "grad_norm": 0.4479275047779083,
      "learning_rate": 9.709878415804575e-05,
      "loss": 2.4149,
      "step": 76410
    },
    {
      "epoch": 1.5721265483608011,
      "grad_norm": 0.4522547721862793,
      "learning_rate": 9.707658444732073e-05,
      "loss": 2.3777,
      "step": 76420
    },
    {
      "epoch": 1.5723322678776688,
      "grad_norm": 0.4523887634277344,
      "learning_rate": 9.705438488079188e-05,
      "loss": 2.4328,
      "step": 76430
    },
    {
      "epoch": 1.5725379873945367,
      "grad_norm": 0.4213165044784546,
      "learning_rate": 9.703218545955421e-05,
      "loss": 2.304,
      "step": 76440
    },
    {
      "epoch": 1.5727437069114043,
      "grad_norm": 0.42499950528144836,
      "learning_rate": 9.70099861847026e-05,
      "loss": 2.3752,
      "step": 76450
    },
    {
      "epoch": 1.5729494264282722,
      "grad_norm": 0.5181381702423096,
      "learning_rate": 9.698778705733213e-05,
      "loss": 2.3849,
      "step": 76460
    },
    {
      "epoch": 1.5731551459451398,
      "grad_norm": 0.443956196308136,
      "learning_rate": 9.69655880785377e-05,
      "loss": 2.4351,
      "step": 76470
    },
    {
      "epoch": 1.5733608654620075,
      "grad_norm": 0.4552041292190552,
      "learning_rate": 9.694338924941425e-05,
      "loss": 2.4415,
      "step": 76480
    },
    {
      "epoch": 1.5735665849788751,
      "grad_norm": 0.4499484896659851,
      "learning_rate": 9.692119057105678e-05,
      "loss": 2.4499,
      "step": 76490
    },
    {
      "epoch": 1.5737723044957428,
      "grad_norm": 0.4348275661468506,
      "learning_rate": 9.689899204456018e-05,
      "loss": 2.4073,
      "step": 76500
    },
    {
      "epoch": 1.5739780240126104,
      "grad_norm": 0.4478079378604889,
      "learning_rate": 9.68767936710194e-05,
      "loss": 2.3752,
      "step": 76510
    },
    {
      "epoch": 1.5741837435294783,
      "grad_norm": 0.4658116400241852,
      "learning_rate": 9.68545954515294e-05,
      "loss": 2.4255,
      "step": 76520
    },
    {
      "epoch": 1.5743894630463462,
      "grad_norm": 0.43144553899765015,
      "learning_rate": 9.683239738718503e-05,
      "loss": 2.4127,
      "step": 76530
    },
    {
      "epoch": 1.5745951825632138,
      "grad_norm": 0.4221688210964203,
      "learning_rate": 9.681019947908127e-05,
      "loss": 2.3776,
      "step": 76540
    },
    {
      "epoch": 1.5748009020800815,
      "grad_norm": 0.4431159794330597,
      "learning_rate": 9.678800172831296e-05,
      "loss": 2.4103,
      "step": 76550
    },
    {
      "epoch": 1.5750066215969492,
      "grad_norm": 0.46675530076026917,
      "learning_rate": 9.676580413597502e-05,
      "loss": 2.4109,
      "step": 76560
    },
    {
      "epoch": 1.5752123411138168,
      "grad_norm": 0.4216275215148926,
      "learning_rate": 9.674360670316238e-05,
      "loss": 2.4108,
      "step": 76570
    },
    {
      "epoch": 1.5754180606306845,
      "grad_norm": 0.3947902321815491,
      "learning_rate": 9.672140943096982e-05,
      "loss": 2.3981,
      "step": 76580
    },
    {
      "epoch": 1.5756237801475523,
      "grad_norm": 0.4187385141849518,
      "learning_rate": 9.669921232049229e-05,
      "loss": 2.3408,
      "step": 76590
    },
    {
      "epoch": 1.57582949966442,
      "grad_norm": 0.45103225111961365,
      "learning_rate": 9.667701537282466e-05,
      "loss": 2.3836,
      "step": 76600
    },
    {
      "epoch": 1.5760352191812879,
      "grad_norm": 0.41822364926338196,
      "learning_rate": 9.665481858906169e-05,
      "loss": 2.4257,
      "step": 76610
    },
    {
      "epoch": 1.5762409386981555,
      "grad_norm": 0.45752859115600586,
      "learning_rate": 9.663262197029834e-05,
      "loss": 2.4298,
      "step": 76620
    },
    {
      "epoch": 1.5764466582150232,
      "grad_norm": 0.7703759670257568,
      "learning_rate": 9.661042551762939e-05,
      "loss": 2.4196,
      "step": 76630
    },
    {
      "epoch": 1.5766523777318908,
      "grad_norm": 0.47894760966300964,
      "learning_rate": 9.658822923214964e-05,
      "loss": 2.3922,
      "step": 76640
    },
    {
      "epoch": 1.5768580972487585,
      "grad_norm": 0.4387817978858948,
      "learning_rate": 9.6566033114954e-05,
      "loss": 2.4131,
      "step": 76650
    },
    {
      "epoch": 1.5770638167656263,
      "grad_norm": 0.422093003988266,
      "learning_rate": 9.654383716713722e-05,
      "loss": 2.4192,
      "step": 76660
    },
    {
      "epoch": 1.577269536282494,
      "grad_norm": 0.4168285131454468,
      "learning_rate": 9.652164138979409e-05,
      "loss": 2.4236,
      "step": 76670
    },
    {
      "epoch": 1.5774752557993619,
      "grad_norm": 0.4282934069633484,
      "learning_rate": 9.649944578401948e-05,
      "loss": 2.3533,
      "step": 76680
    },
    {
      "epoch": 1.5776809753162295,
      "grad_norm": 0.42475807666778564,
      "learning_rate": 9.647725035090811e-05,
      "loss": 2.3722,
      "step": 76690
    },
    {
      "epoch": 1.5778866948330972,
      "grad_norm": 0.429286390542984,
      "learning_rate": 9.64550550915548e-05,
      "loss": 2.3889,
      "step": 76700
    },
    {
      "epoch": 1.5780924143499648,
      "grad_norm": 0.42899808287620544,
      "learning_rate": 9.643286000705427e-05,
      "loss": 2.3871,
      "step": 76710
    },
    {
      "epoch": 1.5782981338668325,
      "grad_norm": 0.4230251908302307,
      "learning_rate": 9.641066509850135e-05,
      "loss": 2.4378,
      "step": 76720
    },
    {
      "epoch": 1.5785038533837004,
      "grad_norm": 0.4655991196632385,
      "learning_rate": 9.638847036699076e-05,
      "loss": 2.4094,
      "step": 76730
    },
    {
      "epoch": 1.578709572900568,
      "grad_norm": 0.44431546330451965,
      "learning_rate": 9.636627581361722e-05,
      "loss": 2.4078,
      "step": 76740
    },
    {
      "epoch": 1.578915292417436,
      "grad_norm": 0.4350389540195465,
      "learning_rate": 9.63440814394755e-05,
      "loss": 2.403,
      "step": 76750
    },
    {
      "epoch": 1.5791210119343035,
      "grad_norm": 0.42109084129333496,
      "learning_rate": 9.632188724566035e-05,
      "loss": 2.3725,
      "step": 76760
    },
    {
      "epoch": 1.5793267314511712,
      "grad_norm": 0.4811893105506897,
      "learning_rate": 9.629969323326641e-05,
      "loss": 2.3613,
      "step": 76770
    },
    {
      "epoch": 1.5795324509680388,
      "grad_norm": 0.4687410891056061,
      "learning_rate": 9.627749940338849e-05,
      "loss": 2.4229,
      "step": 76780
    },
    {
      "epoch": 1.5797381704849065,
      "grad_norm": 0.4921758472919464,
      "learning_rate": 9.62553057571212e-05,
      "loss": 2.3727,
      "step": 76790
    },
    {
      "epoch": 1.5799438900017744,
      "grad_norm": 0.4617849886417389,
      "learning_rate": 9.623311229555927e-05,
      "loss": 2.3628,
      "step": 76800
    },
    {
      "epoch": 1.580149609518642,
      "grad_norm": 0.4481968581676483,
      "learning_rate": 9.621091901979741e-05,
      "loss": 2.4338,
      "step": 76810
    },
    {
      "epoch": 1.58035532903551,
      "grad_norm": 0.4129750728607178,
      "learning_rate": 9.618872593093027e-05,
      "loss": 2.4295,
      "step": 76820
    },
    {
      "epoch": 1.5805610485523776,
      "grad_norm": 0.4336237907409668,
      "learning_rate": 9.616653303005247e-05,
      "loss": 2.3281,
      "step": 76830
    },
    {
      "epoch": 1.5807667680692452,
      "grad_norm": 0.4443763196468353,
      "learning_rate": 9.614434031825876e-05,
      "loss": 2.3519,
      "step": 76840
    },
    {
      "epoch": 1.5809724875861129,
      "grad_norm": 0.44326674938201904,
      "learning_rate": 9.61221477966437e-05,
      "loss": 2.3976,
      "step": 76850
    },
    {
      "epoch": 1.5811782071029805,
      "grad_norm": 0.4269079864025116,
      "learning_rate": 9.609995546630197e-05,
      "loss": 2.3823,
      "step": 76860
    },
    {
      "epoch": 1.5813839266198482,
      "grad_norm": 0.4749622941017151,
      "learning_rate": 9.607776332832815e-05,
      "loss": 2.3889,
      "step": 76870
    },
    {
      "epoch": 1.581589646136716,
      "grad_norm": 0.4450027048587799,
      "learning_rate": 9.605557138381692e-05,
      "loss": 2.4394,
      "step": 76880
    },
    {
      "epoch": 1.581795365653584,
      "grad_norm": 0.4423503279685974,
      "learning_rate": 9.603337963386287e-05,
      "loss": 2.4028,
      "step": 76890
    },
    {
      "epoch": 1.5820010851704516,
      "grad_norm": 0.4551771879196167,
      "learning_rate": 9.601118807956055e-05,
      "loss": 2.422,
      "step": 76900
    },
    {
      "epoch": 1.5822068046873192,
      "grad_norm": 0.4319045841693878,
      "learning_rate": 9.59889967220046e-05,
      "loss": 2.4037,
      "step": 76910
    },
    {
      "epoch": 1.5824125242041869,
      "grad_norm": 0.4645071029663086,
      "learning_rate": 9.596680556228963e-05,
      "loss": 2.378,
      "step": 76920
    },
    {
      "epoch": 1.5826182437210545,
      "grad_norm": 0.4493224620819092,
      "learning_rate": 9.59446146015101e-05,
      "loss": 2.4166,
      "step": 76930
    },
    {
      "epoch": 1.5828239632379222,
      "grad_norm": 0.44988107681274414,
      "learning_rate": 9.592242384076067e-05,
      "loss": 2.3684,
      "step": 76940
    },
    {
      "epoch": 1.58302968275479,
      "grad_norm": 0.4375024139881134,
      "learning_rate": 9.590023328113584e-05,
      "loss": 2.4266,
      "step": 76950
    },
    {
      "epoch": 1.5832354022716577,
      "grad_norm": 0.47081929445266724,
      "learning_rate": 9.587804292373016e-05,
      "loss": 2.3881,
      "step": 76960
    },
    {
      "epoch": 1.5834411217885256,
      "grad_norm": 0.47101396322250366,
      "learning_rate": 9.585585276963817e-05,
      "loss": 2.373,
      "step": 76970
    },
    {
      "epoch": 1.5836468413053932,
      "grad_norm": 0.43130025267601013,
      "learning_rate": 9.583366281995437e-05,
      "loss": 2.3667,
      "step": 76980
    },
    {
      "epoch": 1.583852560822261,
      "grad_norm": 0.5128623843193054,
      "learning_rate": 9.581147307577327e-05,
      "loss": 2.4324,
      "step": 76990
    },
    {
      "epoch": 1.5840582803391285,
      "grad_norm": 0.4289824962615967,
      "learning_rate": 9.57892835381894e-05,
      "loss": 2.388,
      "step": 77000
    },
    {
      "epoch": 1.5842639998559962,
      "grad_norm": 0.4624232351779938,
      "learning_rate": 9.576709420829722e-05,
      "loss": 2.4254,
      "step": 77010
    },
    {
      "epoch": 1.584469719372864,
      "grad_norm": 0.4287353754043579,
      "learning_rate": 9.574490508719123e-05,
      "loss": 2.4032,
      "step": 77020
    },
    {
      "epoch": 1.5846754388897317,
      "grad_norm": 0.4468177556991577,
      "learning_rate": 9.572271617596584e-05,
      "loss": 2.3486,
      "step": 77030
    },
    {
      "epoch": 1.5848811584065996,
      "grad_norm": 0.4167952835559845,
      "learning_rate": 9.570052747571558e-05,
      "loss": 2.4453,
      "step": 77040
    },
    {
      "epoch": 1.5850868779234673,
      "grad_norm": 0.39119526743888855,
      "learning_rate": 9.567833898753489e-05,
      "loss": 2.2937,
      "step": 77050
    },
    {
      "epoch": 1.585292597440335,
      "grad_norm": 0.4291723370552063,
      "learning_rate": 9.565615071251814e-05,
      "loss": 2.4452,
      "step": 77060
    },
    {
      "epoch": 1.5854983169572026,
      "grad_norm": 0.47786590456962585,
      "learning_rate": 9.563396265175986e-05,
      "loss": 2.4347,
      "step": 77070
    },
    {
      "epoch": 1.5857040364740702,
      "grad_norm": 0.44874051213264465,
      "learning_rate": 9.561177480635439e-05,
      "loss": 2.3626,
      "step": 77080
    },
    {
      "epoch": 1.585909755990938,
      "grad_norm": 0.46035075187683105,
      "learning_rate": 9.558958717739612e-05,
      "loss": 2.3786,
      "step": 77090
    },
    {
      "epoch": 1.5861154755078057,
      "grad_norm": 0.4293965697288513,
      "learning_rate": 9.556739976597953e-05,
      "loss": 2.4025,
      "step": 77100
    },
    {
      "epoch": 1.5863211950246736,
      "grad_norm": 0.46091368794441223,
      "learning_rate": 9.554521257319893e-05,
      "loss": 2.402,
      "step": 77110
    },
    {
      "epoch": 1.5865269145415413,
      "grad_norm": 0.4337177872657776,
      "learning_rate": 9.55230256001487e-05,
      "loss": 2.426,
      "step": 77120
    },
    {
      "epoch": 1.586732634058409,
      "grad_norm": 0.45587024092674255,
      "learning_rate": 9.550083884792327e-05,
      "loss": 2.3561,
      "step": 77130
    },
    {
      "epoch": 1.5869383535752766,
      "grad_norm": 0.4460020959377289,
      "learning_rate": 9.547865231761691e-05,
      "loss": 2.4342,
      "step": 77140
    },
    {
      "epoch": 1.5871440730921442,
      "grad_norm": 0.423587441444397,
      "learning_rate": 9.545646601032401e-05,
      "loss": 2.399,
      "step": 77150
    },
    {
      "epoch": 1.587349792609012,
      "grad_norm": 0.4775717854499817,
      "learning_rate": 9.543427992713885e-05,
      "loss": 2.4038,
      "step": 77160
    },
    {
      "epoch": 1.5875555121258798,
      "grad_norm": 0.5077868700027466,
      "learning_rate": 9.54120940691558e-05,
      "loss": 2.448,
      "step": 77170
    },
    {
      "epoch": 1.5877612316427476,
      "grad_norm": 0.5621193051338196,
      "learning_rate": 9.538990843746917e-05,
      "loss": 2.4206,
      "step": 77180
    },
    {
      "epoch": 1.5879669511596153,
      "grad_norm": 0.42511865496635437,
      "learning_rate": 9.536772303317319e-05,
      "loss": 2.4151,
      "step": 77190
    },
    {
      "epoch": 1.588172670676483,
      "grad_norm": 0.39594191312789917,
      "learning_rate": 9.534553785736222e-05,
      "loss": 2.3448,
      "step": 77200
    },
    {
      "epoch": 1.5883783901933506,
      "grad_norm": 0.4267396926879883,
      "learning_rate": 9.532335291113052e-05,
      "loss": 2.4295,
      "step": 77210
    },
    {
      "epoch": 1.5885841097102182,
      "grad_norm": 0.4333932101726532,
      "learning_rate": 9.530116819557228e-05,
      "loss": 2.472,
      "step": 77220
    },
    {
      "epoch": 1.5887898292270861,
      "grad_norm": 0.5137143731117249,
      "learning_rate": 9.527898371178186e-05,
      "loss": 2.3909,
      "step": 77230
    },
    {
      "epoch": 1.5889955487439538,
      "grad_norm": 0.427066832780838,
      "learning_rate": 9.525679946085345e-05,
      "loss": 2.4351,
      "step": 77240
    },
    {
      "epoch": 1.5892012682608216,
      "grad_norm": 0.4723808765411377,
      "learning_rate": 9.523461544388122e-05,
      "loss": 2.3782,
      "step": 77250
    },
    {
      "epoch": 1.5894069877776893,
      "grad_norm": 0.46101245284080505,
      "learning_rate": 9.52124316619595e-05,
      "loss": 2.4151,
      "step": 77260
    },
    {
      "epoch": 1.589612707294557,
      "grad_norm": 0.46015438437461853,
      "learning_rate": 9.51902481161824e-05,
      "loss": 2.427,
      "step": 77270
    },
    {
      "epoch": 1.5898184268114246,
      "grad_norm": 0.4422660768032074,
      "learning_rate": 9.516806480764414e-05,
      "loss": 2.3942,
      "step": 77280
    },
    {
      "epoch": 1.5900241463282923,
      "grad_norm": 0.43891775608062744,
      "learning_rate": 9.514588173743897e-05,
      "loss": 2.4258,
      "step": 77290
    },
    {
      "epoch": 1.59022986584516,
      "grad_norm": 0.40488356351852417,
      "learning_rate": 9.512369890666096e-05,
      "loss": 2.3743,
      "step": 77300
    },
    {
      "epoch": 1.5904355853620278,
      "grad_norm": 0.6106822490692139,
      "learning_rate": 9.510151631640434e-05,
      "loss": 2.3502,
      "step": 77310
    },
    {
      "epoch": 1.5906413048788957,
      "grad_norm": 0.49927860498428345,
      "learning_rate": 9.507933396776318e-05,
      "loss": 2.3919,
      "step": 77320
    },
    {
      "epoch": 1.5908470243957633,
      "grad_norm": 0.4365006387233734,
      "learning_rate": 9.505715186183168e-05,
      "loss": 2.4026,
      "step": 77330
    },
    {
      "epoch": 1.591052743912631,
      "grad_norm": 0.4127120077610016,
      "learning_rate": 9.503496999970397e-05,
      "loss": 2.3932,
      "step": 77340
    },
    {
      "epoch": 1.5912584634294986,
      "grad_norm": 0.5262876749038696,
      "learning_rate": 9.50127883824741e-05,
      "loss": 2.3661,
      "step": 77350
    },
    {
      "epoch": 1.5914641829463663,
      "grad_norm": 0.4531816840171814,
      "learning_rate": 9.499060701123621e-05,
      "loss": 2.3792,
      "step": 77360
    },
    {
      "epoch": 1.591669902463234,
      "grad_norm": 0.45620614290237427,
      "learning_rate": 9.49684258870844e-05,
      "loss": 2.3967,
      "step": 77370
    },
    {
      "epoch": 1.5918756219801018,
      "grad_norm": 0.42213231325149536,
      "learning_rate": 9.494624501111267e-05,
      "loss": 2.4648,
      "step": 77380
    },
    {
      "epoch": 1.5920813414969694,
      "grad_norm": 0.4573129117488861,
      "learning_rate": 9.49240643844152e-05,
      "loss": 2.4156,
      "step": 77390
    },
    {
      "epoch": 1.5922870610138373,
      "grad_norm": 0.44652456045150757,
      "learning_rate": 9.490188400808594e-05,
      "loss": 2.3891,
      "step": 77400
    },
    {
      "epoch": 1.592492780530705,
      "grad_norm": 0.4344356954097748,
      "learning_rate": 9.487970388321893e-05,
      "loss": 2.3795,
      "step": 77410
    },
    {
      "epoch": 1.5926985000475726,
      "grad_norm": 0.4005643427371979,
      "learning_rate": 9.485752401090829e-05,
      "loss": 2.3667,
      "step": 77420
    },
    {
      "epoch": 1.5929042195644403,
      "grad_norm": 0.45129501819610596,
      "learning_rate": 9.483534439224794e-05,
      "loss": 2.3437,
      "step": 77430
    },
    {
      "epoch": 1.593109939081308,
      "grad_norm": 0.4242151379585266,
      "learning_rate": 9.481316502833188e-05,
      "loss": 2.3761,
      "step": 77440
    },
    {
      "epoch": 1.5933156585981758,
      "grad_norm": 0.4228009581565857,
      "learning_rate": 9.479098592025416e-05,
      "loss": 2.3261,
      "step": 77450
    },
    {
      "epoch": 1.5935213781150435,
      "grad_norm": 0.4278789162635803,
      "learning_rate": 9.47688070691087e-05,
      "loss": 2.386,
      "step": 77460
    },
    {
      "epoch": 1.5937270976319113,
      "grad_norm": 0.45437610149383545,
      "learning_rate": 9.47466284759895e-05,
      "loss": 2.4398,
      "step": 77470
    },
    {
      "epoch": 1.593932817148779,
      "grad_norm": 0.41785743832588196,
      "learning_rate": 9.472445014199046e-05,
      "loss": 2.3645,
      "step": 77480
    },
    {
      "epoch": 1.5941385366656466,
      "grad_norm": 0.4002079665660858,
      "learning_rate": 9.470227206820557e-05,
      "loss": 2.3577,
      "step": 77490
    },
    {
      "epoch": 1.5943442561825143,
      "grad_norm": 0.4679226577281952,
      "learning_rate": 9.468009425572873e-05,
      "loss": 2.3577,
      "step": 77500
    },
    {
      "epoch": 1.594549975699382,
      "grad_norm": 0.4235219657421112,
      "learning_rate": 9.465791670565382e-05,
      "loss": 2.4304,
      "step": 77510
    },
    {
      "epoch": 1.5947556952162498,
      "grad_norm": 0.43997859954833984,
      "learning_rate": 9.46357394190748e-05,
      "loss": 2.4211,
      "step": 77520
    },
    {
      "epoch": 1.5949614147331175,
      "grad_norm": 0.46926349401474,
      "learning_rate": 9.461356239708554e-05,
      "loss": 2.4353,
      "step": 77530
    },
    {
      "epoch": 1.5951671342499854,
      "grad_norm": 0.4463724195957184,
      "learning_rate": 9.459138564077984e-05,
      "loss": 2.3603,
      "step": 77540
    },
    {
      "epoch": 1.595372853766853,
      "grad_norm": 0.4213446080684662,
      "learning_rate": 9.456920915125167e-05,
      "loss": 2.4367,
      "step": 77550
    },
    {
      "epoch": 1.5955785732837207,
      "grad_norm": 0.4335644543170929,
      "learning_rate": 9.454703292959478e-05,
      "loss": 2.3873,
      "step": 77560
    },
    {
      "epoch": 1.5957842928005883,
      "grad_norm": 0.44698280096054077,
      "learning_rate": 9.452485697690302e-05,
      "loss": 2.3585,
      "step": 77570
    },
    {
      "epoch": 1.595990012317456,
      "grad_norm": 0.44413840770721436,
      "learning_rate": 9.450268129427028e-05,
      "loss": 2.4489,
      "step": 77580
    },
    {
      "epoch": 1.5961957318343238,
      "grad_norm": 0.48003292083740234,
      "learning_rate": 9.448050588279029e-05,
      "loss": 2.4017,
      "step": 77590
    },
    {
      "epoch": 1.5964014513511915,
      "grad_norm": 0.4394254982471466,
      "learning_rate": 9.445833074355685e-05,
      "loss": 2.381,
      "step": 77600
    },
    {
      "epoch": 1.5966071708680594,
      "grad_norm": 0.4311788082122803,
      "learning_rate": 9.44361558776638e-05,
      "loss": 2.4306,
      "step": 77610
    },
    {
      "epoch": 1.596812890384927,
      "grad_norm": 0.45241254568099976,
      "learning_rate": 9.441398128620483e-05,
      "loss": 2.414,
      "step": 77620
    },
    {
      "epoch": 1.5970186099017947,
      "grad_norm": 0.4111859202384949,
      "learning_rate": 9.439180697027375e-05,
      "loss": 2.4043,
      "step": 77630
    },
    {
      "epoch": 1.5972243294186623,
      "grad_norm": 0.4497215449810028,
      "learning_rate": 9.436963293096424e-05,
      "loss": 2.4053,
      "step": 77640
    },
    {
      "epoch": 1.59743004893553,
      "grad_norm": 0.4954569637775421,
      "learning_rate": 9.434745916937006e-05,
      "loss": 2.4166,
      "step": 77650
    },
    {
      "epoch": 1.5976357684523976,
      "grad_norm": 0.4331207573413849,
      "learning_rate": 9.432528568658495e-05,
      "loss": 2.3432,
      "step": 77660
    },
    {
      "epoch": 1.5978414879692655,
      "grad_norm": 0.44140762090682983,
      "learning_rate": 9.430311248370251e-05,
      "loss": 2.3158,
      "step": 77670
    },
    {
      "epoch": 1.5980472074861334,
      "grad_norm": 0.4457415044307709,
      "learning_rate": 9.428093956181654e-05,
      "loss": 2.4214,
      "step": 77680
    },
    {
      "epoch": 1.598252927003001,
      "grad_norm": 0.41642633080482483,
      "learning_rate": 9.425876692202066e-05,
      "loss": 2.3422,
      "step": 77690
    },
    {
      "epoch": 1.5984586465198687,
      "grad_norm": 0.4337560534477234,
      "learning_rate": 9.423659456540847e-05,
      "loss": 2.4047,
      "step": 77700
    },
    {
      "epoch": 1.5986643660367363,
      "grad_norm": 0.43898388743400574,
      "learning_rate": 9.421442249307372e-05,
      "loss": 2.434,
      "step": 77710
    },
    {
      "epoch": 1.598870085553604,
      "grad_norm": 0.44042420387268066,
      "learning_rate": 9.419225070610995e-05,
      "loss": 2.3895,
      "step": 77720
    },
    {
      "epoch": 1.5990758050704716,
      "grad_norm": 0.4355253279209137,
      "learning_rate": 9.41700792056108e-05,
      "loss": 2.3905,
      "step": 77730
    },
    {
      "epoch": 1.5992815245873395,
      "grad_norm": 0.4311872124671936,
      "learning_rate": 9.414790799266991e-05,
      "loss": 2.4103,
      "step": 77740
    },
    {
      "epoch": 1.5994872441042072,
      "grad_norm": 0.42881083488464355,
      "learning_rate": 9.412573706838079e-05,
      "loss": 2.3645,
      "step": 77750
    },
    {
      "epoch": 1.599692963621075,
      "grad_norm": 0.4260299503803253,
      "learning_rate": 9.410356643383704e-05,
      "loss": 2.3502,
      "step": 77760
    },
    {
      "epoch": 1.5998986831379427,
      "grad_norm": 0.399673193693161,
      "learning_rate": 9.408139609013227e-05,
      "loss": 2.3835,
      "step": 77770
    },
    {
      "epoch": 1.6001044026548104,
      "grad_norm": 0.4531306326389313,
      "learning_rate": 9.405922603835996e-05,
      "loss": 2.2657,
      "step": 77780
    },
    {
      "epoch": 1.600310122171678,
      "grad_norm": 0.4711640179157257,
      "learning_rate": 9.403705627961368e-05,
      "loss": 2.3924,
      "step": 77790
    },
    {
      "epoch": 1.6005158416885457,
      "grad_norm": 0.4375234544277191,
      "learning_rate": 9.401488681498685e-05,
      "loss": 2.3348,
      "step": 77800
    },
    {
      "epoch": 1.6007215612054135,
      "grad_norm": 0.44083651900291443,
      "learning_rate": 9.39927176455731e-05,
      "loss": 2.4491,
      "step": 77810
    },
    {
      "epoch": 1.6009272807222812,
      "grad_norm": 0.47785964608192444,
      "learning_rate": 9.397054877246585e-05,
      "loss": 2.3988,
      "step": 77820
    },
    {
      "epoch": 1.601133000239149,
      "grad_norm": 0.45532095432281494,
      "learning_rate": 9.394838019675853e-05,
      "loss": 2.3876,
      "step": 77830
    },
    {
      "epoch": 1.6013387197560167,
      "grad_norm": 0.44388264417648315,
      "learning_rate": 9.392621191954467e-05,
      "loss": 2.3727,
      "step": 77840
    },
    {
      "epoch": 1.6015444392728844,
      "grad_norm": 0.4170823395252228,
      "learning_rate": 9.390404394191769e-05,
      "loss": 2.3571,
      "step": 77850
    },
    {
      "epoch": 1.601750158789752,
      "grad_norm": 0.47386398911476135,
      "learning_rate": 9.388187626497096e-05,
      "loss": 2.3949,
      "step": 77860
    },
    {
      "epoch": 1.6019558783066197,
      "grad_norm": 0.44855421781539917,
      "learning_rate": 9.385970888979799e-05,
      "loss": 2.3279,
      "step": 77870
    },
    {
      "epoch": 1.6021615978234875,
      "grad_norm": 0.42908725142478943,
      "learning_rate": 9.383754181749208e-05,
      "loss": 2.3825,
      "step": 77880
    },
    {
      "epoch": 1.6023673173403552,
      "grad_norm": 0.4013203978538513,
      "learning_rate": 9.381537504914664e-05,
      "loss": 2.3953,
      "step": 77890
    },
    {
      "epoch": 1.602573036857223,
      "grad_norm": 0.4824850857257843,
      "learning_rate": 9.379320858585509e-05,
      "loss": 2.4796,
      "step": 77900
    },
    {
      "epoch": 1.6027787563740907,
      "grad_norm": 0.4276995360851288,
      "learning_rate": 9.377104242871071e-05,
      "loss": 2.3822,
      "step": 77910
    },
    {
      "epoch": 1.6029844758909584,
      "grad_norm": 0.42622727155685425,
      "learning_rate": 9.374887657880684e-05,
      "loss": 2.3699,
      "step": 77920
    },
    {
      "epoch": 1.603190195407826,
      "grad_norm": 0.4111088514328003,
      "learning_rate": 9.372671103723686e-05,
      "loss": 2.3971,
      "step": 77930
    },
    {
      "epoch": 1.6033959149246937,
      "grad_norm": 0.4674871563911438,
      "learning_rate": 9.370454580509404e-05,
      "loss": 2.4615,
      "step": 77940
    },
    {
      "epoch": 1.6036016344415616,
      "grad_norm": 0.392742395401001,
      "learning_rate": 9.368238088347168e-05,
      "loss": 2.3878,
      "step": 77950
    },
    {
      "epoch": 1.6038073539584292,
      "grad_norm": 0.4366053640842438,
      "learning_rate": 9.366021627346301e-05,
      "loss": 2.4515,
      "step": 77960
    },
    {
      "epoch": 1.604013073475297,
      "grad_norm": 0.4282976984977722,
      "learning_rate": 9.363805197616133e-05,
      "loss": 2.4063,
      "step": 77970
    },
    {
      "epoch": 1.6042187929921647,
      "grad_norm": 0.46896257996559143,
      "learning_rate": 9.361588799265992e-05,
      "loss": 2.3876,
      "step": 77980
    },
    {
      "epoch": 1.6044245125090324,
      "grad_norm": 0.44576990604400635,
      "learning_rate": 9.359372432405189e-05,
      "loss": 2.339,
      "step": 77990
    },
    {
      "epoch": 1.6046302320259,
      "grad_norm": 0.4283539950847626,
      "learning_rate": 9.357156097143058e-05,
      "loss": 2.3967,
      "step": 78000
    },
    {
      "epoch": 1.6048359515427677,
      "grad_norm": 0.5451328158378601,
      "learning_rate": 9.354939793588914e-05,
      "loss": 2.3658,
      "step": 78010
    },
    {
      "epoch": 1.6050416710596354,
      "grad_norm": 0.4281195402145386,
      "learning_rate": 9.352723521852071e-05,
      "loss": 2.3779,
      "step": 78020
    },
    {
      "epoch": 1.6052473905765032,
      "grad_norm": 0.46107548475265503,
      "learning_rate": 9.350507282041855e-05,
      "loss": 2.4507,
      "step": 78030
    },
    {
      "epoch": 1.605453110093371,
      "grad_norm": 0.426220178604126,
      "learning_rate": 9.348291074267572e-05,
      "loss": 2.4453,
      "step": 78040
    },
    {
      "epoch": 1.6056588296102388,
      "grad_norm": 0.4189872741699219,
      "learning_rate": 9.346074898638537e-05,
      "loss": 2.4377,
      "step": 78050
    },
    {
      "epoch": 1.6058645491271064,
      "grad_norm": 0.4278193414211273,
      "learning_rate": 9.343858755264068e-05,
      "loss": 2.373,
      "step": 78060
    },
    {
      "epoch": 1.606070268643974,
      "grad_norm": 0.5396686792373657,
      "learning_rate": 9.341642644253469e-05,
      "loss": 2.3911,
      "step": 78070
    },
    {
      "epoch": 1.6062759881608417,
      "grad_norm": 0.52057945728302,
      "learning_rate": 9.339426565716049e-05,
      "loss": 2.3663,
      "step": 78080
    },
    {
      "epoch": 1.6064817076777094,
      "grad_norm": 0.40560802817344666,
      "learning_rate": 9.33721051976112e-05,
      "loss": 2.4136,
      "step": 78090
    },
    {
      "epoch": 1.6066874271945772,
      "grad_norm": 0.4464465081691742,
      "learning_rate": 9.334994506497984e-05,
      "loss": 2.4343,
      "step": 78100
    },
    {
      "epoch": 1.606893146711445,
      "grad_norm": 0.49593672156333923,
      "learning_rate": 9.332778526035946e-05,
      "loss": 2.367,
      "step": 78110
    },
    {
      "epoch": 1.6070988662283128,
      "grad_norm": 0.4250307083129883,
      "learning_rate": 9.330562578484303e-05,
      "loss": 2.3981,
      "step": 78120
    },
    {
      "epoch": 1.6073045857451804,
      "grad_norm": 0.4852369427680969,
      "learning_rate": 9.328346663952362e-05,
      "loss": 2.3973,
      "step": 78130
    },
    {
      "epoch": 1.607510305262048,
      "grad_norm": 0.430546373128891,
      "learning_rate": 9.326130782549422e-05,
      "loss": 2.4007,
      "step": 78140
    },
    {
      "epoch": 1.6077160247789157,
      "grad_norm": 0.42270058393478394,
      "learning_rate": 9.323914934384776e-05,
      "loss": 2.4376,
      "step": 78150
    },
    {
      "epoch": 1.6079217442957834,
      "grad_norm": 0.44879695773124695,
      "learning_rate": 9.321699119567722e-05,
      "loss": 2.4311,
      "step": 78160
    },
    {
      "epoch": 1.6081274638126513,
      "grad_norm": 0.44864243268966675,
      "learning_rate": 9.319483338207558e-05,
      "loss": 2.3664,
      "step": 78170
    },
    {
      "epoch": 1.608333183329519,
      "grad_norm": 0.44057518243789673,
      "learning_rate": 9.317267590413567e-05,
      "loss": 2.4242,
      "step": 78180
    },
    {
      "epoch": 1.6085389028463868,
      "grad_norm": 0.43554919958114624,
      "learning_rate": 9.31505187629505e-05,
      "loss": 2.4035,
      "step": 78190
    },
    {
      "epoch": 1.6087446223632544,
      "grad_norm": 0.4486759603023529,
      "learning_rate": 9.31283619596129e-05,
      "loss": 2.4139,
      "step": 78200
    },
    {
      "epoch": 1.608950341880122,
      "grad_norm": 0.4272155463695526,
      "learning_rate": 9.310620549521573e-05,
      "loss": 2.3607,
      "step": 78210
    },
    {
      "epoch": 1.6091560613969897,
      "grad_norm": 0.4561502933502197,
      "learning_rate": 9.308404937085191e-05,
      "loss": 2.416,
      "step": 78220
    },
    {
      "epoch": 1.6093617809138574,
      "grad_norm": 0.4746667444705963,
      "learning_rate": 9.306189358761423e-05,
      "loss": 2.3801,
      "step": 78230
    },
    {
      "epoch": 1.6095675004307253,
      "grad_norm": 0.46095260977745056,
      "learning_rate": 9.303973814659552e-05,
      "loss": 2.3599,
      "step": 78240
    },
    {
      "epoch": 1.609773219947593,
      "grad_norm": 0.44878214597702026,
      "learning_rate": 9.301758304888864e-05,
      "loss": 2.3752,
      "step": 78250
    },
    {
      "epoch": 1.6099789394644608,
      "grad_norm": 0.40158116817474365,
      "learning_rate": 9.299542829558632e-05,
      "loss": 2.4201,
      "step": 78260
    },
    {
      "epoch": 1.6101846589813285,
      "grad_norm": 0.4414135217666626,
      "learning_rate": 9.297327388778137e-05,
      "loss": 2.3565,
      "step": 78270
    },
    {
      "epoch": 1.610390378498196,
      "grad_norm": 0.4585202634334564,
      "learning_rate": 9.295111982656647e-05,
      "loss": 2.3848,
      "step": 78280
    },
    {
      "epoch": 1.6105960980150638,
      "grad_norm": 0.42754602432250977,
      "learning_rate": 9.292896611303445e-05,
      "loss": 2.359,
      "step": 78290
    },
    {
      "epoch": 1.6108018175319314,
      "grad_norm": 0.46588146686553955,
      "learning_rate": 9.290681274827803e-05,
      "loss": 2.3939,
      "step": 78300
    },
    {
      "epoch": 1.6110075370487993,
      "grad_norm": 0.43224549293518066,
      "learning_rate": 9.288465973338983e-05,
      "loss": 2.4138,
      "step": 78310
    },
    {
      "epoch": 1.611213256565667,
      "grad_norm": 0.41612735390663147,
      "learning_rate": 9.286250706946264e-05,
      "loss": 2.3785,
      "step": 78320
    },
    {
      "epoch": 1.6114189760825348,
      "grad_norm": 0.4743543267250061,
      "learning_rate": 9.284035475758904e-05,
      "loss": 2.3881,
      "step": 78330
    },
    {
      "epoch": 1.6116246955994025,
      "grad_norm": 0.42759084701538086,
      "learning_rate": 9.281820279886172e-05,
      "loss": 2.4369,
      "step": 78340
    },
    {
      "epoch": 1.6118304151162701,
      "grad_norm": 0.4349581003189087,
      "learning_rate": 9.279605119437336e-05,
      "loss": 2.3659,
      "step": 78350
    },
    {
      "epoch": 1.6120361346331378,
      "grad_norm": 0.49170535802841187,
      "learning_rate": 9.277389994521652e-05,
      "loss": 2.3901,
      "step": 78360
    },
    {
      "epoch": 1.6122418541500054,
      "grad_norm": 0.4728257656097412,
      "learning_rate": 9.275174905248381e-05,
      "loss": 2.416,
      "step": 78370
    },
    {
      "epoch": 1.612447573666873,
      "grad_norm": 0.46593326330184937,
      "learning_rate": 9.272959851726784e-05,
      "loss": 2.4366,
      "step": 78380
    },
    {
      "epoch": 1.612653293183741,
      "grad_norm": 0.4183283746242523,
      "learning_rate": 9.270744834066114e-05,
      "loss": 2.3898,
      "step": 78390
    },
    {
      "epoch": 1.6128590127006088,
      "grad_norm": 0.4171672463417053,
      "learning_rate": 9.26852985237563e-05,
      "loss": 2.3699,
      "step": 78400
    },
    {
      "epoch": 1.6130647322174765,
      "grad_norm": 0.4649817943572998,
      "learning_rate": 9.266314906764578e-05,
      "loss": 2.3609,
      "step": 78410
    },
    {
      "epoch": 1.6132704517343441,
      "grad_norm": 0.46199288964271545,
      "learning_rate": 9.264099997342216e-05,
      "loss": 2.4313,
      "step": 78420
    },
    {
      "epoch": 1.6134761712512118,
      "grad_norm": 0.4068167805671692,
      "learning_rate": 9.261885124217793e-05,
      "loss": 2.356,
      "step": 78430
    },
    {
      "epoch": 1.6136818907680794,
      "grad_norm": 0.4279193878173828,
      "learning_rate": 9.259670287500549e-05,
      "loss": 2.3954,
      "step": 78440
    },
    {
      "epoch": 1.613887610284947,
      "grad_norm": 0.4298340082168579,
      "learning_rate": 9.257455487299739e-05,
      "loss": 2.407,
      "step": 78450
    },
    {
      "epoch": 1.614093329801815,
      "grad_norm": 0.44333699345588684,
      "learning_rate": 9.255240723724606e-05,
      "loss": 2.4341,
      "step": 78460
    },
    {
      "epoch": 1.6142990493186826,
      "grad_norm": 0.48886626958847046,
      "learning_rate": 9.253025996884384e-05,
      "loss": 2.3906,
      "step": 78470
    },
    {
      "epoch": 1.6145047688355505,
      "grad_norm": 0.4623458683490753,
      "learning_rate": 9.250811306888324e-05,
      "loss": 2.3937,
      "step": 78480
    },
    {
      "epoch": 1.6147104883524181,
      "grad_norm": 0.47901785373687744,
      "learning_rate": 9.248596653845656e-05,
      "loss": 2.4039,
      "step": 78490
    },
    {
      "epoch": 1.6149162078692858,
      "grad_norm": 0.41486650705337524,
      "learning_rate": 9.24638203786562e-05,
      "loss": 2.4256,
      "step": 78500
    },
    {
      "epoch": 1.6151219273861535,
      "grad_norm": 0.44225677847862244,
      "learning_rate": 9.244167459057455e-05,
      "loss": 2.3557,
      "step": 78510
    },
    {
      "epoch": 1.615327646903021,
      "grad_norm": 0.4376375675201416,
      "learning_rate": 9.24195291753039e-05,
      "loss": 2.4353,
      "step": 78520
    },
    {
      "epoch": 1.615533366419889,
      "grad_norm": 0.4476190507411957,
      "learning_rate": 9.239738413393652e-05,
      "loss": 2.3942,
      "step": 78530
    },
    {
      "epoch": 1.6157390859367566,
      "grad_norm": 0.46108150482177734,
      "learning_rate": 9.23752394675648e-05,
      "loss": 2.3746,
      "step": 78540
    },
    {
      "epoch": 1.6159448054536245,
      "grad_norm": 0.4113774597644806,
      "learning_rate": 9.235309517728096e-05,
      "loss": 2.3989,
      "step": 78550
    },
    {
      "epoch": 1.6161505249704922,
      "grad_norm": 0.45821496844291687,
      "learning_rate": 9.233095126417728e-05,
      "loss": 2.4505,
      "step": 78560
    },
    {
      "epoch": 1.6163562444873598,
      "grad_norm": 0.4655640721321106,
      "learning_rate": 9.230880772934593e-05,
      "loss": 2.3851,
      "step": 78570
    },
    {
      "epoch": 1.6165619640042275,
      "grad_norm": 0.4082637131214142,
      "learning_rate": 9.228666457387921e-05,
      "loss": 2.3784,
      "step": 78580
    },
    {
      "epoch": 1.6167676835210951,
      "grad_norm": 0.45409414172172546,
      "learning_rate": 9.226452179886931e-05,
      "loss": 2.3521,
      "step": 78590
    },
    {
      "epoch": 1.616973403037963,
      "grad_norm": 0.3991275131702423,
      "learning_rate": 9.224237940540838e-05,
      "loss": 2.3963,
      "step": 78600
    },
    {
      "epoch": 1.6171791225548307,
      "grad_norm": 0.46072638034820557,
      "learning_rate": 9.222023739458859e-05,
      "loss": 2.3557,
      "step": 78610
    },
    {
      "epoch": 1.6173848420716985,
      "grad_norm": 0.4268902540206909,
      "learning_rate": 9.219809576750214e-05,
      "loss": 2.4171,
      "step": 78620
    },
    {
      "epoch": 1.6175905615885662,
      "grad_norm": 0.4342042803764343,
      "learning_rate": 9.217595452524105e-05,
      "loss": 2.3733,
      "step": 78630
    },
    {
      "epoch": 1.6177962811054338,
      "grad_norm": 0.3951047658920288,
      "learning_rate": 9.215381366889753e-05,
      "loss": 2.4078,
      "step": 78640
    },
    {
      "epoch": 1.6180020006223015,
      "grad_norm": 0.4134627878665924,
      "learning_rate": 9.21316731995636e-05,
      "loss": 2.4838,
      "step": 78650
    },
    {
      "epoch": 1.6182077201391691,
      "grad_norm": 0.5617588758468628,
      "learning_rate": 9.210953311833135e-05,
      "loss": 2.419,
      "step": 78660
    },
    {
      "epoch": 1.618413439656037,
      "grad_norm": 0.44871869683265686,
      "learning_rate": 9.208739342629286e-05,
      "loss": 2.4448,
      "step": 78670
    },
    {
      "epoch": 1.6186191591729047,
      "grad_norm": 0.42586153745651245,
      "learning_rate": 9.206525412454013e-05,
      "loss": 2.3425,
      "step": 78680
    },
    {
      "epoch": 1.6188248786897725,
      "grad_norm": 0.5215223431587219,
      "learning_rate": 9.204311521416514e-05,
      "loss": 2.4254,
      "step": 78690
    },
    {
      "epoch": 1.6190305982066402,
      "grad_norm": 0.43494877219200134,
      "learning_rate": 9.202097669625996e-05,
      "loss": 2.3368,
      "step": 78700
    },
    {
      "epoch": 1.6192363177235078,
      "grad_norm": 0.479218453168869,
      "learning_rate": 9.19988385719165e-05,
      "loss": 2.411,
      "step": 78710
    },
    {
      "epoch": 1.6194420372403755,
      "grad_norm": 0.43419215083122253,
      "learning_rate": 9.197670084222676e-05,
      "loss": 2.4013,
      "step": 78720
    },
    {
      "epoch": 1.6196477567572432,
      "grad_norm": 0.45629093050956726,
      "learning_rate": 9.195456350828259e-05,
      "loss": 2.4053,
      "step": 78730
    },
    {
      "epoch": 1.6198534762741108,
      "grad_norm": 0.4253072738647461,
      "learning_rate": 9.193242657117599e-05,
      "loss": 2.3195,
      "step": 78740
    },
    {
      "epoch": 1.6200591957909787,
      "grad_norm": 0.42756396532058716,
      "learning_rate": 9.191029003199884e-05,
      "loss": 2.366,
      "step": 78750
    },
    {
      "epoch": 1.6202649153078466,
      "grad_norm": 0.4968438446521759,
      "learning_rate": 9.188815389184297e-05,
      "loss": 2.4027,
      "step": 78760
    },
    {
      "epoch": 1.6204706348247142,
      "grad_norm": 0.41944098472595215,
      "learning_rate": 9.186601815180027e-05,
      "loss": 2.3372,
      "step": 78770
    },
    {
      "epoch": 1.6206763543415819,
      "grad_norm": 0.47290170192718506,
      "learning_rate": 9.18438828129626e-05,
      "loss": 2.418,
      "step": 78780
    },
    {
      "epoch": 1.6208820738584495,
      "grad_norm": 0.43638551235198975,
      "learning_rate": 9.182174787642171e-05,
      "loss": 2.3735,
      "step": 78790
    },
    {
      "epoch": 1.6210877933753172,
      "grad_norm": 0.49987709522247314,
      "learning_rate": 9.179961334326945e-05,
      "loss": 2.3714,
      "step": 78800
    },
    {
      "epoch": 1.6212935128921848,
      "grad_norm": 0.41712498664855957,
      "learning_rate": 9.177747921459757e-05,
      "loss": 2.4033,
      "step": 78810
    },
    {
      "epoch": 1.6214992324090527,
      "grad_norm": 0.45832762122154236,
      "learning_rate": 9.175534549149782e-05,
      "loss": 2.3975,
      "step": 78820
    },
    {
      "epoch": 1.6217049519259206,
      "grad_norm": 0.4185415208339691,
      "learning_rate": 9.173321217506198e-05,
      "loss": 2.4212,
      "step": 78830
    },
    {
      "epoch": 1.6219106714427882,
      "grad_norm": 0.4342973530292511,
      "learning_rate": 9.171107926638171e-05,
      "loss": 2.4302,
      "step": 78840
    },
    {
      "epoch": 1.6221163909596559,
      "grad_norm": 0.424111932516098,
      "learning_rate": 9.168894676654871e-05,
      "loss": 2.3971,
      "step": 78850
    },
    {
      "epoch": 1.6223221104765235,
      "grad_norm": 0.39419806003570557,
      "learning_rate": 9.166681467665472e-05,
      "loss": 2.4546,
      "step": 78860
    },
    {
      "epoch": 1.6225278299933912,
      "grad_norm": 0.5119538903236389,
      "learning_rate": 9.164468299779135e-05,
      "loss": 2.3613,
      "step": 78870
    },
    {
      "epoch": 1.6227335495102588,
      "grad_norm": 0.43066713213920593,
      "learning_rate": 9.162255173105024e-05,
      "loss": 2.417,
      "step": 78880
    },
    {
      "epoch": 1.6229392690271267,
      "grad_norm": 0.41492676734924316,
      "learning_rate": 9.160042087752295e-05,
      "loss": 2.3813,
      "step": 78890
    },
    {
      "epoch": 1.6231449885439944,
      "grad_norm": 0.41400378942489624,
      "learning_rate": 9.157829043830116e-05,
      "loss": 2.3803,
      "step": 78900
    },
    {
      "epoch": 1.6233507080608622,
      "grad_norm": 0.4072139859199524,
      "learning_rate": 9.155616041447644e-05,
      "loss": 2.3589,
      "step": 78910
    },
    {
      "epoch": 1.6235564275777299,
      "grad_norm": 0.43771010637283325,
      "learning_rate": 9.153403080714025e-05,
      "loss": 2.3999,
      "step": 78920
    },
    {
      "epoch": 1.6237621470945975,
      "grad_norm": 0.4772617518901825,
      "learning_rate": 9.151190161738423e-05,
      "loss": 2.4374,
      "step": 78930
    },
    {
      "epoch": 1.6239678666114652,
      "grad_norm": 0.4106041491031647,
      "learning_rate": 9.148977284629985e-05,
      "loss": 2.3928,
      "step": 78940
    },
    {
      "epoch": 1.6241735861283328,
      "grad_norm": 0.4295150637626648,
      "learning_rate": 9.146764449497857e-05,
      "loss": 2.3513,
      "step": 78950
    },
    {
      "epoch": 1.6243793056452007,
      "grad_norm": 0.4797850251197815,
      "learning_rate": 9.144551656451193e-05,
      "loss": 2.3748,
      "step": 78960
    },
    {
      "epoch": 1.6245850251620684,
      "grad_norm": 0.42717787623405457,
      "learning_rate": 9.142338905599132e-05,
      "loss": 2.4075,
      "step": 78970
    },
    {
      "epoch": 1.6247907446789362,
      "grad_norm": 0.42706912755966187,
      "learning_rate": 9.140126197050815e-05,
      "loss": 2.3576,
      "step": 78980
    },
    {
      "epoch": 1.624996464195804,
      "grad_norm": 0.43475770950317383,
      "learning_rate": 9.137913530915394e-05,
      "loss": 2.43,
      "step": 78990
    },
    {
      "epoch": 1.6252021837126716,
      "grad_norm": 0.4482762813568115,
      "learning_rate": 9.135700907301998e-05,
      "loss": 2.4714,
      "step": 79000
    },
    {
      "epoch": 1.6254079032295392,
      "grad_norm": 0.4629632234573364,
      "learning_rate": 9.133488326319764e-05,
      "loss": 2.3909,
      "step": 79010
    },
    {
      "epoch": 1.6256136227464069,
      "grad_norm": 0.4367976188659668,
      "learning_rate": 9.131275788077833e-05,
      "loss": 2.428,
      "step": 79020
    },
    {
      "epoch": 1.6258193422632747,
      "grad_norm": 0.4371575713157654,
      "learning_rate": 9.129063292685332e-05,
      "loss": 2.3716,
      "step": 79030
    },
    {
      "epoch": 1.6260250617801424,
      "grad_norm": 0.5310661196708679,
      "learning_rate": 9.126850840251393e-05,
      "loss": 2.3807,
      "step": 79040
    },
    {
      "epoch": 1.6262307812970103,
      "grad_norm": 0.47500377893447876,
      "learning_rate": 9.124638430885141e-05,
      "loss": 2.3727,
      "step": 79050
    },
    {
      "epoch": 1.626436500813878,
      "grad_norm": 0.4775243103504181,
      "learning_rate": 9.122426064695707e-05,
      "loss": 2.4034,
      "step": 79060
    },
    {
      "epoch": 1.6266422203307456,
      "grad_norm": 0.4412144720554352,
      "learning_rate": 9.120213741792216e-05,
      "loss": 2.4029,
      "step": 79070
    },
    {
      "epoch": 1.6268479398476132,
      "grad_norm": 0.48621368408203125,
      "learning_rate": 9.118001462283782e-05,
      "loss": 2.4391,
      "step": 79080
    },
    {
      "epoch": 1.6270536593644809,
      "grad_norm": 0.4392574429512024,
      "learning_rate": 9.115789226279531e-05,
      "loss": 2.4431,
      "step": 79090
    },
    {
      "epoch": 1.6272593788813488,
      "grad_norm": 0.4514312446117401,
      "learning_rate": 9.113577033888582e-05,
      "loss": 2.3894,
      "step": 79100
    },
    {
      "epoch": 1.6274650983982164,
      "grad_norm": 0.45031705498695374,
      "learning_rate": 9.111364885220042e-05,
      "loss": 2.4102,
      "step": 79110
    },
    {
      "epoch": 1.6276708179150843,
      "grad_norm": 0.5173177123069763,
      "learning_rate": 9.109152780383037e-05,
      "loss": 2.396,
      "step": 79120
    },
    {
      "epoch": 1.627876537431952,
      "grad_norm": 0.43939682841300964,
      "learning_rate": 9.106940719486666e-05,
      "loss": 2.3871,
      "step": 79130
    },
    {
      "epoch": 1.6280822569488196,
      "grad_norm": 0.43431568145751953,
      "learning_rate": 9.104728702640041e-05,
      "loss": 2.3909,
      "step": 79140
    },
    {
      "epoch": 1.6282879764656872,
      "grad_norm": 0.46953171491622925,
      "learning_rate": 9.102516729952275e-05,
      "loss": 2.4455,
      "step": 79150
    },
    {
      "epoch": 1.628493695982555,
      "grad_norm": 0.44289660453796387,
      "learning_rate": 9.100304801532466e-05,
      "loss": 2.3374,
      "step": 79160
    },
    {
      "epoch": 1.6286994154994225,
      "grad_norm": 0.41741371154785156,
      "learning_rate": 9.098092917489717e-05,
      "loss": 2.4071,
      "step": 79170
    },
    {
      "epoch": 1.6289051350162904,
      "grad_norm": 0.4086253046989441,
      "learning_rate": 9.095881077933133e-05,
      "loss": 2.3941,
      "step": 79180
    },
    {
      "epoch": 1.6291108545331583,
      "grad_norm": 0.4315020740032196,
      "learning_rate": 9.093669282971806e-05,
      "loss": 2.4249,
      "step": 79190
    },
    {
      "epoch": 1.629316574050026,
      "grad_norm": 0.45887911319732666,
      "learning_rate": 9.091457532714837e-05,
      "loss": 2.4675,
      "step": 79200
    },
    {
      "epoch": 1.6295222935668936,
      "grad_norm": 0.4722684323787689,
      "learning_rate": 9.089245827271312e-05,
      "loss": 2.3768,
      "step": 79210
    },
    {
      "epoch": 1.6297280130837613,
      "grad_norm": 0.5307769775390625,
      "learning_rate": 9.08703416675033e-05,
      "loss": 2.409,
      "step": 79220
    },
    {
      "epoch": 1.629933732600629,
      "grad_norm": 0.4687439799308777,
      "learning_rate": 9.08482255126098e-05,
      "loss": 2.4013,
      "step": 79230
    },
    {
      "epoch": 1.6301394521174966,
      "grad_norm": 0.4385717809200287,
      "learning_rate": 9.082610980912341e-05,
      "loss": 2.3971,
      "step": 79240
    },
    {
      "epoch": 1.6303451716343644,
      "grad_norm": 0.47856011986732483,
      "learning_rate": 9.080399455813507e-05,
      "loss": 2.4218,
      "step": 79250
    },
    {
      "epoch": 1.630550891151232,
      "grad_norm": 0.4759253263473511,
      "learning_rate": 9.078187976073557e-05,
      "loss": 2.365,
      "step": 79260
    },
    {
      "epoch": 1.6307566106681,
      "grad_norm": 0.494017094373703,
      "learning_rate": 9.075976541801569e-05,
      "loss": 2.3922,
      "step": 79270
    },
    {
      "epoch": 1.6309623301849676,
      "grad_norm": 0.4623148739337921,
      "learning_rate": 9.073765153106625e-05,
      "loss": 2.4149,
      "step": 79280
    },
    {
      "epoch": 1.6311680497018353,
      "grad_norm": 0.4720275402069092,
      "learning_rate": 9.071553810097798e-05,
      "loss": 2.4339,
      "step": 79290
    },
    {
      "epoch": 1.631373769218703,
      "grad_norm": 0.45504245162010193,
      "learning_rate": 9.069342512884157e-05,
      "loss": 2.3972,
      "step": 79300
    },
    {
      "epoch": 1.6315794887355706,
      "grad_norm": 0.43680539727211,
      "learning_rate": 9.067131261574786e-05,
      "loss": 2.408,
      "step": 79310
    },
    {
      "epoch": 1.6317852082524384,
      "grad_norm": 0.4258859157562256,
      "learning_rate": 9.064920056278743e-05,
      "loss": 2.44,
      "step": 79320
    },
    {
      "epoch": 1.631990927769306,
      "grad_norm": 0.4114309251308441,
      "learning_rate": 9.062708897105096e-05,
      "loss": 2.3569,
      "step": 79330
    },
    {
      "epoch": 1.632196647286174,
      "grad_norm": 0.4343993067741394,
      "learning_rate": 9.060497784162916e-05,
      "loss": 2.4318,
      "step": 79340
    },
    {
      "epoch": 1.6324023668030416,
      "grad_norm": 0.49200981855392456,
      "learning_rate": 9.058286717561258e-05,
      "loss": 2.3767,
      "step": 79350
    },
    {
      "epoch": 1.6326080863199093,
      "grad_norm": 0.443203330039978,
      "learning_rate": 9.056075697409186e-05,
      "loss": 2.3646,
      "step": 79360
    },
    {
      "epoch": 1.632813805836777,
      "grad_norm": 0.4512871205806732,
      "learning_rate": 9.053864723815753e-05,
      "loss": 2.4124,
      "step": 79370
    },
    {
      "epoch": 1.6330195253536446,
      "grad_norm": 0.5520487427711487,
      "learning_rate": 9.051653796890018e-05,
      "loss": 2.3602,
      "step": 79380
    },
    {
      "epoch": 1.6332252448705125,
      "grad_norm": 0.531674861907959,
      "learning_rate": 9.049442916741038e-05,
      "loss": 2.3655,
      "step": 79390
    },
    {
      "epoch": 1.6334309643873801,
      "grad_norm": 0.5084935426712036,
      "learning_rate": 9.047232083477853e-05,
      "loss": 2.3524,
      "step": 79400
    },
    {
      "epoch": 1.633636683904248,
      "grad_norm": 0.4475769102573395,
      "learning_rate": 9.045021297209519e-05,
      "loss": 2.4115,
      "step": 79410
    },
    {
      "epoch": 1.6338424034211156,
      "grad_norm": 0.44210296869277954,
      "learning_rate": 9.042810558045084e-05,
      "loss": 2.3631,
      "step": 79420
    },
    {
      "epoch": 1.6340481229379833,
      "grad_norm": 0.4574344754219055,
      "learning_rate": 9.040599866093582e-05,
      "loss": 2.4145,
      "step": 79430
    },
    {
      "epoch": 1.634253842454851,
      "grad_norm": 0.46289870142936707,
      "learning_rate": 9.038389221464066e-05,
      "loss": 2.4118,
      "step": 79440
    },
    {
      "epoch": 1.6344595619717186,
      "grad_norm": 0.4459186792373657,
      "learning_rate": 9.036178624265568e-05,
      "loss": 2.4142,
      "step": 79450
    },
    {
      "epoch": 1.6346652814885865,
      "grad_norm": 0.39994552731513977,
      "learning_rate": 9.033968074607122e-05,
      "loss": 2.4435,
      "step": 79460
    },
    {
      "epoch": 1.6348710010054541,
      "grad_norm": 0.4161217212677002,
      "learning_rate": 9.031757572597774e-05,
      "loss": 2.3584,
      "step": 79470
    },
    {
      "epoch": 1.635076720522322,
      "grad_norm": 0.4709247946739197,
      "learning_rate": 9.029547118346544e-05,
      "loss": 2.4026,
      "step": 79480
    },
    {
      "epoch": 1.6352824400391897,
      "grad_norm": 0.4300234317779541,
      "learning_rate": 9.027336711962469e-05,
      "loss": 2.3917,
      "step": 79490
    },
    {
      "epoch": 1.6354881595560573,
      "grad_norm": 0.5342622995376587,
      "learning_rate": 9.025126353554571e-05,
      "loss": 2.3919,
      "step": 79500
    },
    {
      "epoch": 1.635693879072925,
      "grad_norm": 0.44145941734313965,
      "learning_rate": 9.022916043231878e-05,
      "loss": 2.3458,
      "step": 79510
    },
    {
      "epoch": 1.6358995985897926,
      "grad_norm": 0.4294780492782593,
      "learning_rate": 9.020705781103414e-05,
      "loss": 2.4149,
      "step": 79520
    },
    {
      "epoch": 1.6361053181066603,
      "grad_norm": 0.4354000687599182,
      "learning_rate": 9.018495567278193e-05,
      "loss": 2.4606,
      "step": 79530
    },
    {
      "epoch": 1.6363110376235281,
      "grad_norm": 0.45724013447761536,
      "learning_rate": 9.016285401865241e-05,
      "loss": 2.3938,
      "step": 79540
    },
    {
      "epoch": 1.636516757140396,
      "grad_norm": 0.4847581088542938,
      "learning_rate": 9.01407528497357e-05,
      "loss": 2.425,
      "step": 79550
    },
    {
      "epoch": 1.6367224766572637,
      "grad_norm": 0.44858384132385254,
      "learning_rate": 9.011865216712188e-05,
      "loss": 2.4116,
      "step": 79560
    },
    {
      "epoch": 1.6369281961741313,
      "grad_norm": 0.42267343401908875,
      "learning_rate": 9.009655197190117e-05,
      "loss": 2.3968,
      "step": 79570
    },
    {
      "epoch": 1.637133915690999,
      "grad_norm": 0.47501039505004883,
      "learning_rate": 9.007445226516353e-05,
      "loss": 2.3777,
      "step": 79580
    },
    {
      "epoch": 1.6373396352078666,
      "grad_norm": 0.4364514946937561,
      "learning_rate": 9.005235304799908e-05,
      "loss": 2.4178,
      "step": 79590
    },
    {
      "epoch": 1.6375453547247343,
      "grad_norm": 0.4295808672904968,
      "learning_rate": 9.003025432149788e-05,
      "loss": 2.3803,
      "step": 79600
    },
    {
      "epoch": 1.6377510742416022,
      "grad_norm": 0.4661666452884674,
      "learning_rate": 9.000815608674987e-05,
      "loss": 2.4132,
      "step": 79610
    },
    {
      "epoch": 1.6379567937584698,
      "grad_norm": 0.5052476525306702,
      "learning_rate": 8.998605834484506e-05,
      "loss": 2.3976,
      "step": 79620
    },
    {
      "epoch": 1.6381625132753377,
      "grad_norm": 0.43115055561065674,
      "learning_rate": 8.996396109687347e-05,
      "loss": 2.4389,
      "step": 79630
    },
    {
      "epoch": 1.6383682327922053,
      "grad_norm": 0.4438820481300354,
      "learning_rate": 8.994186434392495e-05,
      "loss": 2.4478,
      "step": 79640
    },
    {
      "epoch": 1.638573952309073,
      "grad_norm": 0.43498578667640686,
      "learning_rate": 8.991976808708949e-05,
      "loss": 2.3691,
      "step": 79650
    },
    {
      "epoch": 1.6387796718259406,
      "grad_norm": 0.44101861119270325,
      "learning_rate": 8.989767232745689e-05,
      "loss": 2.3277,
      "step": 79660
    },
    {
      "epoch": 1.6389853913428083,
      "grad_norm": 0.5157310962677002,
      "learning_rate": 8.987557706611709e-05,
      "loss": 2.4256,
      "step": 79670
    },
    {
      "epoch": 1.6391911108596762,
      "grad_norm": 0.43815818428993225,
      "learning_rate": 8.98534823041599e-05,
      "loss": 2.3952,
      "step": 79680
    },
    {
      "epoch": 1.6393968303765438,
      "grad_norm": 0.4718325734138489,
      "learning_rate": 8.983138804267513e-05,
      "loss": 2.3505,
      "step": 79690
    },
    {
      "epoch": 1.6396025498934117,
      "grad_norm": 0.5035778284072876,
      "learning_rate": 8.980929428275258e-05,
      "loss": 2.3878,
      "step": 79700
    },
    {
      "epoch": 1.6398082694102794,
      "grad_norm": 0.445859432220459,
      "learning_rate": 8.978720102548203e-05,
      "loss": 2.3447,
      "step": 79710
    },
    {
      "epoch": 1.640013988927147,
      "grad_norm": 0.47798824310302734,
      "learning_rate": 8.976510827195316e-05,
      "loss": 2.399,
      "step": 79720
    },
    {
      "epoch": 1.6402197084440147,
      "grad_norm": 0.475598007440567,
      "learning_rate": 8.974301602325577e-05,
      "loss": 2.3767,
      "step": 79730
    },
    {
      "epoch": 1.6404254279608823,
      "grad_norm": 0.4133172035217285,
      "learning_rate": 8.972092428047947e-05,
      "loss": 2.4144,
      "step": 79740
    },
    {
      "epoch": 1.6406311474777502,
      "grad_norm": 0.4382341504096985,
      "learning_rate": 8.969883304471396e-05,
      "loss": 2.3945,
      "step": 79750
    },
    {
      "epoch": 1.6408368669946178,
      "grad_norm": 0.46088334918022156,
      "learning_rate": 8.967674231704891e-05,
      "loss": 2.4885,
      "step": 79760
    },
    {
      "epoch": 1.6410425865114857,
      "grad_norm": 0.4317924976348877,
      "learning_rate": 8.965465209857389e-05,
      "loss": 2.3745,
      "step": 79770
    },
    {
      "epoch": 1.6412483060283534,
      "grad_norm": 0.44865408539772034,
      "learning_rate": 8.963256239037849e-05,
      "loss": 2.4353,
      "step": 79780
    },
    {
      "epoch": 1.641454025545221,
      "grad_norm": 0.45618265867233276,
      "learning_rate": 8.961047319355233e-05,
      "loss": 2.3917,
      "step": 79790
    },
    {
      "epoch": 1.6416597450620887,
      "grad_norm": 0.49000421166419983,
      "learning_rate": 8.958838450918489e-05,
      "loss": 2.3744,
      "step": 79800
    },
    {
      "epoch": 1.6418654645789563,
      "grad_norm": 0.46856045722961426,
      "learning_rate": 8.956629633836571e-05,
      "loss": 2.3877,
      "step": 79810
    },
    {
      "epoch": 1.6420711840958242,
      "grad_norm": 0.46007877588272095,
      "learning_rate": 8.954420868218427e-05,
      "loss": 2.3994,
      "step": 79820
    },
    {
      "epoch": 1.6422769036126919,
      "grad_norm": 0.42445695400238037,
      "learning_rate": 8.952212154173004e-05,
      "loss": 2.3867,
      "step": 79830
    },
    {
      "epoch": 1.6424826231295597,
      "grad_norm": 0.402689665555954,
      "learning_rate": 8.950003491809247e-05,
      "loss": 2.3907,
      "step": 79840
    },
    {
      "epoch": 1.6426883426464274,
      "grad_norm": 0.43585970997810364,
      "learning_rate": 8.947794881236091e-05,
      "loss": 2.3517,
      "step": 79850
    },
    {
      "epoch": 1.642894062163295,
      "grad_norm": 0.4690772294998169,
      "learning_rate": 8.945586322562484e-05,
      "loss": 2.4804,
      "step": 79860
    },
    {
      "epoch": 1.6430997816801627,
      "grad_norm": 0.44233641028404236,
      "learning_rate": 8.943377815897359e-05,
      "loss": 2.4327,
      "step": 79870
    },
    {
      "epoch": 1.6433055011970303,
      "grad_norm": 0.4371258020401001,
      "learning_rate": 8.941169361349643e-05,
      "loss": 2.398,
      "step": 79880
    },
    {
      "epoch": 1.643511220713898,
      "grad_norm": 0.46543774008750916,
      "learning_rate": 8.93896095902828e-05,
      "loss": 2.4547,
      "step": 79890
    },
    {
      "epoch": 1.6437169402307659,
      "grad_norm": 0.4528031051158905,
      "learning_rate": 8.936752609042186e-05,
      "loss": 2.4225,
      "step": 79900
    },
    {
      "epoch": 1.6439226597476337,
      "grad_norm": 0.4452376961708069,
      "learning_rate": 8.934544311500291e-05,
      "loss": 2.3656,
      "step": 79910
    },
    {
      "epoch": 1.6441283792645014,
      "grad_norm": 0.5079780220985413,
      "learning_rate": 8.932336066511522e-05,
      "loss": 2.3692,
      "step": 79920
    },
    {
      "epoch": 1.644334098781369,
      "grad_norm": 0.42605000734329224,
      "learning_rate": 8.930127874184795e-05,
      "loss": 2.408,
      "step": 79930
    },
    {
      "epoch": 1.6445398182982367,
      "grad_norm": 0.4982507824897766,
      "learning_rate": 8.92791973462903e-05,
      "loss": 2.3561,
      "step": 79940
    },
    {
      "epoch": 1.6447455378151044,
      "grad_norm": 0.4664243161678314,
      "learning_rate": 8.925711647953143e-05,
      "loss": 2.4203,
      "step": 79950
    },
    {
      "epoch": 1.644951257331972,
      "grad_norm": 0.45877203345298767,
      "learning_rate": 8.923503614266046e-05,
      "loss": 2.3862,
      "step": 79960
    },
    {
      "epoch": 1.6451569768488399,
      "grad_norm": 0.44273364543914795,
      "learning_rate": 8.921295633676652e-05,
      "loss": 2.3795,
      "step": 79970
    },
    {
      "epoch": 1.6453626963657075,
      "grad_norm": 0.4445191025733948,
      "learning_rate": 8.919087706293861e-05,
      "loss": 2.4401,
      "step": 79980
    },
    {
      "epoch": 1.6455684158825754,
      "grad_norm": 0.46074995398521423,
      "learning_rate": 8.916879832226586e-05,
      "loss": 2.3585,
      "step": 79990
    },
    {
      "epoch": 1.645774135399443,
      "grad_norm": 0.39968422055244446,
      "learning_rate": 8.914672011583728e-05,
      "loss": 2.3787,
      "step": 80000
    },
    {
      "epoch": 1.6459798549163107,
      "grad_norm": 0.3911283612251282,
      "learning_rate": 8.912464244474182e-05,
      "loss": 2.367,
      "step": 80010
    },
    {
      "epoch": 1.6461855744331784,
      "grad_norm": 0.5774726271629333,
      "learning_rate": 8.91025653100685e-05,
      "loss": 2.3324,
      "step": 80020
    },
    {
      "epoch": 1.646391293950046,
      "grad_norm": 0.4666389226913452,
      "learning_rate": 8.908048871290628e-05,
      "loss": 2.3684,
      "step": 80030
    },
    {
      "epoch": 1.646597013466914,
      "grad_norm": 0.39808428287506104,
      "learning_rate": 8.905841265434399e-05,
      "loss": 2.3731,
      "step": 80040
    },
    {
      "epoch": 1.6468027329837815,
      "grad_norm": 0.4409288763999939,
      "learning_rate": 8.903633713547064e-05,
      "loss": 2.3867,
      "step": 80050
    },
    {
      "epoch": 1.6470084525006494,
      "grad_norm": 0.5487921833992004,
      "learning_rate": 8.901426215737499e-05,
      "loss": 2.4124,
      "step": 80060
    },
    {
      "epoch": 1.647214172017517,
      "grad_norm": 0.397931843996048,
      "learning_rate": 8.899218772114591e-05,
      "loss": 2.3572,
      "step": 80070
    },
    {
      "epoch": 1.6474198915343847,
      "grad_norm": 0.44350022077560425,
      "learning_rate": 8.897011382787228e-05,
      "loss": 2.4398,
      "step": 80080
    },
    {
      "epoch": 1.6476256110512524,
      "grad_norm": 0.41565635800361633,
      "learning_rate": 8.894804047864279e-05,
      "loss": 2.4073,
      "step": 80090
    },
    {
      "epoch": 1.64783133056812,
      "grad_norm": 0.4808714985847473,
      "learning_rate": 8.892596767454623e-05,
      "loss": 2.4354,
      "step": 80100
    },
    {
      "epoch": 1.648037050084988,
      "grad_norm": 0.46741753816604614,
      "learning_rate": 8.890389541667136e-05,
      "loss": 2.3921,
      "step": 80110
    },
    {
      "epoch": 1.6482427696018556,
      "grad_norm": 0.4479019045829773,
      "learning_rate": 8.888182370610687e-05,
      "loss": 2.4036,
      "step": 80120
    },
    {
      "epoch": 1.6484484891187234,
      "grad_norm": 0.41596585512161255,
      "learning_rate": 8.885975254394143e-05,
      "loss": 2.3501,
      "step": 80130
    },
    {
      "epoch": 1.648654208635591,
      "grad_norm": 0.4730653166770935,
      "learning_rate": 8.883768193126366e-05,
      "loss": 2.3993,
      "step": 80140
    },
    {
      "epoch": 1.6488599281524587,
      "grad_norm": 0.42212405800819397,
      "learning_rate": 8.881561186916223e-05,
      "loss": 2.4049,
      "step": 80150
    },
    {
      "epoch": 1.6490656476693264,
      "grad_norm": 0.41348227858543396,
      "learning_rate": 8.879354235872575e-05,
      "loss": 2.3787,
      "step": 80160
    },
    {
      "epoch": 1.649271367186194,
      "grad_norm": 0.44959694147109985,
      "learning_rate": 8.87714734010427e-05,
      "loss": 2.3888,
      "step": 80170
    },
    {
      "epoch": 1.649477086703062,
      "grad_norm": 0.4223635196685791,
      "learning_rate": 8.87494049972017e-05,
      "loss": 2.3627,
      "step": 80180
    },
    {
      "epoch": 1.6496828062199296,
      "grad_norm": 0.4051724970340729,
      "learning_rate": 8.872733714829127e-05,
      "loss": 2.3854,
      "step": 80190
    },
    {
      "epoch": 1.6498885257367975,
      "grad_norm": 0.46644675731658936,
      "learning_rate": 8.870526985539982e-05,
      "loss": 2.4026,
      "step": 80200
    },
    {
      "epoch": 1.650094245253665,
      "grad_norm": 0.43606796860694885,
      "learning_rate": 8.868320311961591e-05,
      "loss": 2.338,
      "step": 80210
    },
    {
      "epoch": 1.6502999647705328,
      "grad_norm": 0.4508567452430725,
      "learning_rate": 8.86611369420279e-05,
      "loss": 2.4032,
      "step": 80220
    },
    {
      "epoch": 1.6505056842874004,
      "grad_norm": 0.45676207542419434,
      "learning_rate": 8.863907132372419e-05,
      "loss": 2.4003,
      "step": 80230
    },
    {
      "epoch": 1.650711403804268,
      "grad_norm": 0.398425430059433,
      "learning_rate": 8.861700626579322e-05,
      "loss": 2.4235,
      "step": 80240
    },
    {
      "epoch": 1.6509171233211357,
      "grad_norm": 0.4270334243774414,
      "learning_rate": 8.859494176932329e-05,
      "loss": 2.385,
      "step": 80250
    },
    {
      "epoch": 1.6511228428380036,
      "grad_norm": 0.43173807859420776,
      "learning_rate": 8.857287783540271e-05,
      "loss": 2.3708,
      "step": 80260
    },
    {
      "epoch": 1.6513285623548715,
      "grad_norm": 0.41293570399284363,
      "learning_rate": 8.855081446511983e-05,
      "loss": 2.3594,
      "step": 80270
    },
    {
      "epoch": 1.6515342818717391,
      "grad_norm": 0.4244062602519989,
      "learning_rate": 8.852875165956287e-05,
      "loss": 2.3922,
      "step": 80280
    },
    {
      "epoch": 1.6517400013886068,
      "grad_norm": 0.4367733895778656,
      "learning_rate": 8.85066894198201e-05,
      "loss": 2.429,
      "step": 80290
    },
    {
      "epoch": 1.6519457209054744,
      "grad_norm": 0.46470052003860474,
      "learning_rate": 8.848462774697967e-05,
      "loss": 2.375,
      "step": 80300
    },
    {
      "epoch": 1.652151440422342,
      "grad_norm": 0.4025140702724457,
      "learning_rate": 8.84625666421298e-05,
      "loss": 2.3906,
      "step": 80310
    },
    {
      "epoch": 1.6523571599392097,
      "grad_norm": 0.4934040307998657,
      "learning_rate": 8.844050610635869e-05,
      "loss": 2.3755,
      "step": 80320
    },
    {
      "epoch": 1.6525628794560776,
      "grad_norm": 0.4485342502593994,
      "learning_rate": 8.841844614075438e-05,
      "loss": 2.396,
      "step": 80330
    },
    {
      "epoch": 1.6527685989729453,
      "grad_norm": 0.44459331035614014,
      "learning_rate": 8.839638674640503e-05,
      "loss": 2.4254,
      "step": 80340
    },
    {
      "epoch": 1.6529743184898131,
      "grad_norm": 0.488203763961792,
      "learning_rate": 8.83743279243987e-05,
      "loss": 2.2999,
      "step": 80350
    },
    {
      "epoch": 1.6531800380066808,
      "grad_norm": 0.40135619044303894,
      "learning_rate": 8.835226967582339e-05,
      "loss": 2.409,
      "step": 80360
    },
    {
      "epoch": 1.6533857575235484,
      "grad_norm": 0.49283427000045776,
      "learning_rate": 8.833021200176718e-05,
      "loss": 2.3572,
      "step": 80370
    },
    {
      "epoch": 1.653591477040416,
      "grad_norm": 0.4344841539859772,
      "learning_rate": 8.830815490331799e-05,
      "loss": 2.3553,
      "step": 80380
    },
    {
      "epoch": 1.6537971965572837,
      "grad_norm": 0.4353683292865753,
      "learning_rate": 8.828609838156382e-05,
      "loss": 2.3379,
      "step": 80390
    },
    {
      "epoch": 1.6540029160741516,
      "grad_norm": 0.44917112588882446,
      "learning_rate": 8.826404243759259e-05,
      "loss": 2.4219,
      "step": 80400
    },
    {
      "epoch": 1.6542086355910193,
      "grad_norm": 0.47724369168281555,
      "learning_rate": 8.824198707249218e-05,
      "loss": 2.394,
      "step": 80410
    },
    {
      "epoch": 1.6544143551078871,
      "grad_norm": 0.4219721853733063,
      "learning_rate": 8.821993228735045e-05,
      "loss": 2.4096,
      "step": 80420
    },
    {
      "epoch": 1.6546200746247548,
      "grad_norm": 0.4172053337097168,
      "learning_rate": 8.819787808325531e-05,
      "loss": 2.4245,
      "step": 80430
    },
    {
      "epoch": 1.6548257941416225,
      "grad_norm": 0.5203684568405151,
      "learning_rate": 8.817582446129453e-05,
      "loss": 2.4094,
      "step": 80440
    },
    {
      "epoch": 1.65503151365849,
      "grad_norm": 0.4482841193675995,
      "learning_rate": 8.81537714225559e-05,
      "loss": 2.3908,
      "step": 80450
    },
    {
      "epoch": 1.6552372331753578,
      "grad_norm": 0.4547981023788452,
      "learning_rate": 8.813171896812711e-05,
      "loss": 2.4172,
      "step": 80460
    },
    {
      "epoch": 1.6554429526922256,
      "grad_norm": 0.462859183549881,
      "learning_rate": 8.810966709909601e-05,
      "loss": 2.35,
      "step": 80470
    },
    {
      "epoch": 1.6556486722090933,
      "grad_norm": 0.4065026044845581,
      "learning_rate": 8.808761581655023e-05,
      "loss": 2.3878,
      "step": 80480
    },
    {
      "epoch": 1.6558543917259612,
      "grad_norm": 0.47504618763923645,
      "learning_rate": 8.806556512157741e-05,
      "loss": 2.4427,
      "step": 80490
    },
    {
      "epoch": 1.6560601112428288,
      "grad_norm": 0.4399566948413849,
      "learning_rate": 8.804351501526525e-05,
      "loss": 2.3917,
      "step": 80500
    },
    {
      "epoch": 1.6562658307596965,
      "grad_norm": 0.4531823992729187,
      "learning_rate": 8.802146549870137e-05,
      "loss": 2.3705,
      "step": 80510
    },
    {
      "epoch": 1.6564715502765641,
      "grad_norm": 0.4446776211261749,
      "learning_rate": 8.799941657297327e-05,
      "loss": 2.3685,
      "step": 80520
    },
    {
      "epoch": 1.6566772697934318,
      "grad_norm": 0.4390145242214203,
      "learning_rate": 8.79773682391686e-05,
      "loss": 2.3944,
      "step": 80530
    },
    {
      "epoch": 1.6568829893102996,
      "grad_norm": 0.53586345911026,
      "learning_rate": 8.795532049837482e-05,
      "loss": 2.414,
      "step": 80540
    },
    {
      "epoch": 1.6570887088271673,
      "grad_norm": 0.5334054231643677,
      "learning_rate": 8.793327335167941e-05,
      "loss": 2.3932,
      "step": 80550
    },
    {
      "epoch": 1.6572944283440352,
      "grad_norm": 0.42284658551216125,
      "learning_rate": 8.791122680016993e-05,
      "loss": 2.3795,
      "step": 80560
    },
    {
      "epoch": 1.6575001478609028,
      "grad_norm": 0.4528566300868988,
      "learning_rate": 8.788918084493373e-05,
      "loss": 2.4257,
      "step": 80570
    },
    {
      "epoch": 1.6577058673777705,
      "grad_norm": 0.4382237493991852,
      "learning_rate": 8.786713548705822e-05,
      "loss": 2.4551,
      "step": 80580
    },
    {
      "epoch": 1.6579115868946381,
      "grad_norm": 0.42465338110923767,
      "learning_rate": 8.784509072763084e-05,
      "loss": 2.3638,
      "step": 80590
    },
    {
      "epoch": 1.6581173064115058,
      "grad_norm": 0.43994736671447754,
      "learning_rate": 8.782304656773887e-05,
      "loss": 2.39,
      "step": 80600
    },
    {
      "epoch": 1.6583230259283737,
      "grad_norm": 0.4343601167201996,
      "learning_rate": 8.780100300846969e-05,
      "loss": 2.3613,
      "step": 80610
    },
    {
      "epoch": 1.6585287454452413,
      "grad_norm": 0.4874061942100525,
      "learning_rate": 8.777896005091051e-05,
      "loss": 2.412,
      "step": 80620
    },
    {
      "epoch": 1.6587344649621092,
      "grad_norm": 0.4366775453090668,
      "learning_rate": 8.775691769614865e-05,
      "loss": 2.3442,
      "step": 80630
    },
    {
      "epoch": 1.6589401844789768,
      "grad_norm": 0.554188072681427,
      "learning_rate": 8.773487594527135e-05,
      "loss": 2.3988,
      "step": 80640
    },
    {
      "epoch": 1.6591459039958445,
      "grad_norm": 0.4372284710407257,
      "learning_rate": 8.771283479936575e-05,
      "loss": 2.4153,
      "step": 80650
    },
    {
      "epoch": 1.6593516235127121,
      "grad_norm": 0.4421587586402893,
      "learning_rate": 8.769079425951909e-05,
      "loss": 2.3615,
      "step": 80660
    },
    {
      "epoch": 1.6595573430295798,
      "grad_norm": 0.4486265182495117,
      "learning_rate": 8.766875432681843e-05,
      "loss": 2.3516,
      "step": 80670
    },
    {
      "epoch": 1.6597630625464475,
      "grad_norm": 0.4329063296318054,
      "learning_rate": 8.764671500235091e-05,
      "loss": 2.3597,
      "step": 80680
    },
    {
      "epoch": 1.6599687820633153,
      "grad_norm": 0.45556679368019104,
      "learning_rate": 8.762467628720367e-05,
      "loss": 2.4033,
      "step": 80690
    },
    {
      "epoch": 1.6601745015801832,
      "grad_norm": 0.4485301971435547,
      "learning_rate": 8.76026381824637e-05,
      "loss": 2.3777,
      "step": 80700
    },
    {
      "epoch": 1.6603802210970509,
      "grad_norm": 0.48294249176979065,
      "learning_rate": 8.7580600689218e-05,
      "loss": 2.4295,
      "step": 80710
    },
    {
      "epoch": 1.6605859406139185,
      "grad_norm": 0.43552181124687195,
      "learning_rate": 8.755856380855363e-05,
      "loss": 2.4449,
      "step": 80720
    },
    {
      "epoch": 1.6607916601307862,
      "grad_norm": 0.44648444652557373,
      "learning_rate": 8.753652754155749e-05,
      "loss": 2.392,
      "step": 80730
    },
    {
      "epoch": 1.6609973796476538,
      "grad_norm": 0.4729451537132263,
      "learning_rate": 8.751449188931655e-05,
      "loss": 2.4884,
      "step": 80740
    },
    {
      "epoch": 1.6612030991645215,
      "grad_norm": 0.4043550491333008,
      "learning_rate": 8.749245685291763e-05,
      "loss": 2.414,
      "step": 80750
    },
    {
      "epoch": 1.6614088186813893,
      "grad_norm": 0.4441748261451721,
      "learning_rate": 8.747042243344769e-05,
      "loss": 2.4179,
      "step": 80760
    },
    {
      "epoch": 1.661614538198257,
      "grad_norm": 0.4195857048034668,
      "learning_rate": 8.744838863199358e-05,
      "loss": 2.405,
      "step": 80770
    },
    {
      "epoch": 1.6618202577151249,
      "grad_norm": 0.44092121720314026,
      "learning_rate": 8.742635544964198e-05,
      "loss": 2.3648,
      "step": 80780
    },
    {
      "epoch": 1.6620259772319925,
      "grad_norm": 0.40292832255363464,
      "learning_rate": 8.740432288747978e-05,
      "loss": 2.4219,
      "step": 80790
    },
    {
      "epoch": 1.6622316967488602,
      "grad_norm": 0.4705767035484314,
      "learning_rate": 8.738229094659372e-05,
      "loss": 2.4237,
      "step": 80800
    },
    {
      "epoch": 1.6624374162657278,
      "grad_norm": 0.45165616273880005,
      "learning_rate": 8.736025962807046e-05,
      "loss": 2.3326,
      "step": 80810
    },
    {
      "epoch": 1.6626431357825955,
      "grad_norm": 0.4452822804450989,
      "learning_rate": 8.733822893299673e-05,
      "loss": 2.4486,
      "step": 80820
    },
    {
      "epoch": 1.6628488552994634,
      "grad_norm": 0.43580368161201477,
      "learning_rate": 8.731619886245916e-05,
      "loss": 2.3559,
      "step": 80830
    },
    {
      "epoch": 1.663054574816331,
      "grad_norm": 0.4835425615310669,
      "learning_rate": 8.729416941754436e-05,
      "loss": 2.4048,
      "step": 80840
    },
    {
      "epoch": 1.6632602943331989,
      "grad_norm": 0.4232366979122162,
      "learning_rate": 8.727214059933898e-05,
      "loss": 2.3454,
      "step": 80850
    },
    {
      "epoch": 1.6634660138500665,
      "grad_norm": 0.40401241183280945,
      "learning_rate": 8.725011240892953e-05,
      "loss": 2.3602,
      "step": 80860
    },
    {
      "epoch": 1.6636717333669342,
      "grad_norm": 0.4745978116989136,
      "learning_rate": 8.722808484740254e-05,
      "loss": 2.4107,
      "step": 80870
    },
    {
      "epoch": 1.6638774528838018,
      "grad_norm": 0.4484427869319916,
      "learning_rate": 8.720605791584457e-05,
      "loss": 2.3818,
      "step": 80880
    },
    {
      "epoch": 1.6640831724006695,
      "grad_norm": 0.4747551381587982,
      "learning_rate": 8.718403161534201e-05,
      "loss": 2.4704,
      "step": 80890
    },
    {
      "epoch": 1.6642888919175374,
      "grad_norm": 0.47550004720687866,
      "learning_rate": 8.716200594698137e-05,
      "loss": 2.4056,
      "step": 80900
    },
    {
      "epoch": 1.664494611434405,
      "grad_norm": 0.4094170928001404,
      "learning_rate": 8.713998091184895e-05,
      "loss": 2.3288,
      "step": 80910
    },
    {
      "epoch": 1.664700330951273,
      "grad_norm": 0.44800394773483276,
      "learning_rate": 8.711795651103123e-05,
      "loss": 2.4234,
      "step": 80920
    },
    {
      "epoch": 1.6649060504681406,
      "grad_norm": 0.4473719596862793,
      "learning_rate": 8.709593274561454e-05,
      "loss": 2.3999,
      "step": 80930
    },
    {
      "epoch": 1.6651117699850082,
      "grad_norm": 0.4470546841621399,
      "learning_rate": 8.707390961668513e-05,
      "loss": 2.3929,
      "step": 80940
    },
    {
      "epoch": 1.6653174895018759,
      "grad_norm": 0.439249724149704,
      "learning_rate": 8.705188712532932e-05,
      "loss": 2.3766,
      "step": 80950
    },
    {
      "epoch": 1.6655232090187435,
      "grad_norm": 0.48756635189056396,
      "learning_rate": 8.70298652726334e-05,
      "loss": 2.4001,
      "step": 80960
    },
    {
      "epoch": 1.6657289285356114,
      "grad_norm": 0.5628942847251892,
      "learning_rate": 8.70078440596835e-05,
      "loss": 2.3749,
      "step": 80970
    },
    {
      "epoch": 1.665934648052479,
      "grad_norm": 0.5148284435272217,
      "learning_rate": 8.698582348756589e-05,
      "loss": 2.4236,
      "step": 80980
    },
    {
      "epoch": 1.666140367569347,
      "grad_norm": 0.42746463418006897,
      "learning_rate": 8.696380355736668e-05,
      "loss": 2.4126,
      "step": 80990
    },
    {
      "epoch": 1.6663460870862146,
      "grad_norm": 0.44740012288093567,
      "learning_rate": 8.694178427017198e-05,
      "loss": 2.421,
      "step": 81000
    },
    {
      "epoch": 1.6665518066030822,
      "grad_norm": 0.6090501546859741,
      "learning_rate": 8.691976562706794e-05,
      "loss": 2.3819,
      "step": 81010
    },
    {
      "epoch": 1.6667575261199499,
      "grad_norm": 0.4738714396953583,
      "learning_rate": 8.689774762914055e-05,
      "loss": 2.3686,
      "step": 81020
    },
    {
      "epoch": 1.6669632456368175,
      "grad_norm": 0.5564582347869873,
      "learning_rate": 8.687573027747587e-05,
      "loss": 2.4274,
      "step": 81030
    },
    {
      "epoch": 1.6671689651536852,
      "grad_norm": 0.49278780817985535,
      "learning_rate": 8.685371357315993e-05,
      "loss": 2.411,
      "step": 81040
    },
    {
      "epoch": 1.667374684670553,
      "grad_norm": 0.4508129954338074,
      "learning_rate": 8.683169751727865e-05,
      "loss": 2.3529,
      "step": 81050
    },
    {
      "epoch": 1.667580404187421,
      "grad_norm": 0.4100661873817444,
      "learning_rate": 8.680968211091798e-05,
      "loss": 2.3998,
      "step": 81060
    },
    {
      "epoch": 1.6677861237042886,
      "grad_norm": 0.44432130455970764,
      "learning_rate": 8.678766735516377e-05,
      "loss": 2.3824,
      "step": 81070
    },
    {
      "epoch": 1.6679918432211562,
      "grad_norm": 0.44990938901901245,
      "learning_rate": 8.676565325110196e-05,
      "loss": 2.3681,
      "step": 81080
    },
    {
      "epoch": 1.6681975627380239,
      "grad_norm": 0.4737631380558014,
      "learning_rate": 8.674363979981838e-05,
      "loss": 2.3751,
      "step": 81090
    },
    {
      "epoch": 1.6684032822548915,
      "grad_norm": 0.48101088404655457,
      "learning_rate": 8.672162700239876e-05,
      "loss": 2.3646,
      "step": 81100
    },
    {
      "epoch": 1.6686090017717592,
      "grad_norm": 0.4378139078617096,
      "learning_rate": 8.669961485992894e-05,
      "loss": 2.4413,
      "step": 81110
    },
    {
      "epoch": 1.668814721288627,
      "grad_norm": 0.4435892403125763,
      "learning_rate": 8.667760337349466e-05,
      "loss": 2.4424,
      "step": 81120
    },
    {
      "epoch": 1.6690204408054947,
      "grad_norm": 0.4255160093307495,
      "learning_rate": 8.665559254418158e-05,
      "loss": 2.3371,
      "step": 81130
    },
    {
      "epoch": 1.6692261603223626,
      "grad_norm": 0.46552225947380066,
      "learning_rate": 8.663358237307546e-05,
      "loss": 2.4378,
      "step": 81140
    },
    {
      "epoch": 1.6694318798392302,
      "grad_norm": 0.46089085936546326,
      "learning_rate": 8.661157286126182e-05,
      "loss": 2.3698,
      "step": 81150
    },
    {
      "epoch": 1.669637599356098,
      "grad_norm": 0.41453999280929565,
      "learning_rate": 8.658956400982632e-05,
      "loss": 2.4031,
      "step": 81160
    },
    {
      "epoch": 1.6698433188729656,
      "grad_norm": 0.4630875587463379,
      "learning_rate": 8.65675558198546e-05,
      "loss": 2.3431,
      "step": 81170
    },
    {
      "epoch": 1.6700490383898332,
      "grad_norm": 0.4453141987323761,
      "learning_rate": 8.654554829243213e-05,
      "loss": 2.3936,
      "step": 81180
    },
    {
      "epoch": 1.670254757906701,
      "grad_norm": 0.40303072333335876,
      "learning_rate": 8.652354142864443e-05,
      "loss": 2.403,
      "step": 81190
    },
    {
      "epoch": 1.6704604774235687,
      "grad_norm": 0.4620029032230377,
      "learning_rate": 8.650153522957704e-05,
      "loss": 2.365,
      "step": 81200
    },
    {
      "epoch": 1.6706661969404366,
      "grad_norm": 0.7015976309776306,
      "learning_rate": 8.647952969631532e-05,
      "loss": 2.3927,
      "step": 81210
    },
    {
      "epoch": 1.6708719164573043,
      "grad_norm": 0.44113004207611084,
      "learning_rate": 8.645752482994476e-05,
      "loss": 2.4086,
      "step": 81220
    },
    {
      "epoch": 1.671077635974172,
      "grad_norm": 0.4140985608100891,
      "learning_rate": 8.643552063155064e-05,
      "loss": 2.3875,
      "step": 81230
    },
    {
      "epoch": 1.6712833554910396,
      "grad_norm": 0.456371545791626,
      "learning_rate": 8.641351710221841e-05,
      "loss": 2.4294,
      "step": 81240
    },
    {
      "epoch": 1.6714890750079072,
      "grad_norm": 0.38372403383255005,
      "learning_rate": 8.639151424303336e-05,
      "loss": 2.3965,
      "step": 81250
    },
    {
      "epoch": 1.671694794524775,
      "grad_norm": 0.43130064010620117,
      "learning_rate": 8.636951205508071e-05,
      "loss": 2.3897,
      "step": 81260
    },
    {
      "epoch": 1.6719005140416427,
      "grad_norm": 0.4235428273677826,
      "learning_rate": 8.634751053944578e-05,
      "loss": 2.3798,
      "step": 81270
    },
    {
      "epoch": 1.6721062335585106,
      "grad_norm": 0.43200382590293884,
      "learning_rate": 8.632550969721378e-05,
      "loss": 2.353,
      "step": 81280
    },
    {
      "epoch": 1.6723119530753783,
      "grad_norm": 0.4234538972377777,
      "learning_rate": 8.630350952946982e-05,
      "loss": 2.4241,
      "step": 81290
    },
    {
      "epoch": 1.672517672592246,
      "grad_norm": 0.4914931058883667,
      "learning_rate": 8.628151003729916e-05,
      "loss": 2.3489,
      "step": 81300
    },
    {
      "epoch": 1.6727233921091136,
      "grad_norm": 0.45919933915138245,
      "learning_rate": 8.62595112217868e-05,
      "loss": 2.4178,
      "step": 81310
    },
    {
      "epoch": 1.6729291116259812,
      "grad_norm": 0.5438203811645508,
      "learning_rate": 8.623751308401787e-05,
      "loss": 2.3608,
      "step": 81320
    },
    {
      "epoch": 1.673134831142849,
      "grad_norm": 0.4392958879470825,
      "learning_rate": 8.621551562507748e-05,
      "loss": 2.3755,
      "step": 81330
    },
    {
      "epoch": 1.6733405506597168,
      "grad_norm": 0.43337002396583557,
      "learning_rate": 8.619351884605053e-05,
      "loss": 2.3683,
      "step": 81340
    },
    {
      "epoch": 1.6735462701765846,
      "grad_norm": 0.5330460667610168,
      "learning_rate": 8.617152274802206e-05,
      "loss": 2.3837,
      "step": 81350
    },
    {
      "epoch": 1.6737519896934523,
      "grad_norm": 0.4203130006790161,
      "learning_rate": 8.614952733207706e-05,
      "loss": 2.4679,
      "step": 81360
    },
    {
      "epoch": 1.67395770921032,
      "grad_norm": 0.4121587574481964,
      "learning_rate": 8.612753259930038e-05,
      "loss": 2.4268,
      "step": 81370
    },
    {
      "epoch": 1.6741634287271876,
      "grad_norm": 0.46497243642807007,
      "learning_rate": 8.610553855077695e-05,
      "loss": 2.3419,
      "step": 81380
    },
    {
      "epoch": 1.6743691482440552,
      "grad_norm": 0.39622485637664795,
      "learning_rate": 8.608354518759152e-05,
      "loss": 2.3826,
      "step": 81390
    },
    {
      "epoch": 1.674574867760923,
      "grad_norm": 0.4638689458370209,
      "learning_rate": 8.606155251082901e-05,
      "loss": 2.4312,
      "step": 81400
    },
    {
      "epoch": 1.6747805872777908,
      "grad_norm": 0.4326043128967285,
      "learning_rate": 8.603956052157418e-05,
      "loss": 2.4194,
      "step": 81410
    },
    {
      "epoch": 1.6749863067946587,
      "grad_norm": 0.4524528980255127,
      "learning_rate": 8.60175692209117e-05,
      "loss": 2.4614,
      "step": 81420
    },
    {
      "epoch": 1.6751920263115263,
      "grad_norm": 0.430084764957428,
      "learning_rate": 8.599557860992636e-05,
      "loss": 2.4054,
      "step": 81430
    },
    {
      "epoch": 1.675397745828394,
      "grad_norm": 0.40278518199920654,
      "learning_rate": 8.597358868970285e-05,
      "loss": 2.3869,
      "step": 81440
    },
    {
      "epoch": 1.6756034653452616,
      "grad_norm": 0.4421044588088989,
      "learning_rate": 8.595159946132572e-05,
      "loss": 2.3916,
      "step": 81450
    },
    {
      "epoch": 1.6758091848621293,
      "grad_norm": 0.44360750913619995,
      "learning_rate": 8.59296109258797e-05,
      "loss": 2.4158,
      "step": 81460
    },
    {
      "epoch": 1.676014904378997,
      "grad_norm": 0.42783457040786743,
      "learning_rate": 8.590762308444923e-05,
      "loss": 2.3802,
      "step": 81470
    },
    {
      "epoch": 1.6762206238958648,
      "grad_norm": 0.45478367805480957,
      "learning_rate": 8.588563593811892e-05,
      "loss": 2.426,
      "step": 81480
    },
    {
      "epoch": 1.6764263434127324,
      "grad_norm": 0.4382849335670471,
      "learning_rate": 8.586364948797335e-05,
      "loss": 2.4515,
      "step": 81490
    },
    {
      "epoch": 1.6766320629296003,
      "grad_norm": 0.47862306237220764,
      "learning_rate": 8.584166373509684e-05,
      "loss": 2.3332,
      "step": 81500
    },
    {
      "epoch": 1.676837782446468,
      "grad_norm": 0.4303577244281769,
      "learning_rate": 8.58196786805739e-05,
      "loss": 2.4055,
      "step": 81510
    },
    {
      "epoch": 1.6770435019633356,
      "grad_norm": 0.43054619431495667,
      "learning_rate": 8.579769432548901e-05,
      "loss": 2.4195,
      "step": 81520
    },
    {
      "epoch": 1.6772492214802033,
      "grad_norm": 0.4032582640647888,
      "learning_rate": 8.57757106709264e-05,
      "loss": 2.3459,
      "step": 81530
    },
    {
      "epoch": 1.677454940997071,
      "grad_norm": 0.5327931642532349,
      "learning_rate": 8.575372771797052e-05,
      "loss": 2.3431,
      "step": 81540
    },
    {
      "epoch": 1.6776606605139388,
      "grad_norm": 0.4439396262168884,
      "learning_rate": 8.573174546770557e-05,
      "loss": 2.4046,
      "step": 81550
    },
    {
      "epoch": 1.6778663800308065,
      "grad_norm": 0.45033055543899536,
      "learning_rate": 8.570976392121588e-05,
      "loss": 2.4041,
      "step": 81560
    },
    {
      "epoch": 1.6780720995476743,
      "grad_norm": 0.4488789737224579,
      "learning_rate": 8.56877830795857e-05,
      "loss": 2.3603,
      "step": 81570
    },
    {
      "epoch": 1.678277819064542,
      "grad_norm": 0.4334533214569092,
      "learning_rate": 8.566580294389911e-05,
      "loss": 2.4581,
      "step": 81580
    },
    {
      "epoch": 1.6784835385814096,
      "grad_norm": 0.4429038166999817,
      "learning_rate": 8.564382351524041e-05,
      "loss": 2.3954,
      "step": 81590
    },
    {
      "epoch": 1.6786892580982773,
      "grad_norm": 0.41454941034317017,
      "learning_rate": 8.562184479469367e-05,
      "loss": 2.3759,
      "step": 81600
    },
    {
      "epoch": 1.678894977615145,
      "grad_norm": 0.39640259742736816,
      "learning_rate": 8.559986678334293e-05,
      "loss": 2.4004,
      "step": 81610
    },
    {
      "epoch": 1.6791006971320128,
      "grad_norm": 0.4450981914997101,
      "learning_rate": 8.557788948227237e-05,
      "loss": 2.3956,
      "step": 81620
    },
    {
      "epoch": 1.6793064166488805,
      "grad_norm": 0.4559621214866638,
      "learning_rate": 8.555591289256585e-05,
      "loss": 2.3682,
      "step": 81630
    },
    {
      "epoch": 1.6795121361657483,
      "grad_norm": 0.4144151508808136,
      "learning_rate": 8.553393701530744e-05,
      "loss": 2.3852,
      "step": 81640
    },
    {
      "epoch": 1.679717855682616,
      "grad_norm": 0.44130566716194153,
      "learning_rate": 8.551196185158115e-05,
      "loss": 2.4063,
      "step": 81650
    },
    {
      "epoch": 1.6799235751994837,
      "grad_norm": 0.41671207547187805,
      "learning_rate": 8.548998740247076e-05,
      "loss": 2.3467,
      "step": 81660
    },
    {
      "epoch": 1.6801292947163513,
      "grad_norm": 0.421077162027359,
      "learning_rate": 8.546801366906023e-05,
      "loss": 2.4244,
      "step": 81670
    },
    {
      "epoch": 1.680335014233219,
      "grad_norm": 0.4615296721458435,
      "learning_rate": 8.544604065243347e-05,
      "loss": 2.3754,
      "step": 81680
    },
    {
      "epoch": 1.6805407337500868,
      "grad_norm": 0.41101014614105225,
      "learning_rate": 8.542406835367413e-05,
      "loss": 2.3005,
      "step": 81690
    },
    {
      "epoch": 1.6807464532669545,
      "grad_norm": 0.4345467686653137,
      "learning_rate": 8.540209677386612e-05,
      "loss": 2.384,
      "step": 81700
    },
    {
      "epoch": 1.6809521727838224,
      "grad_norm": 0.47875818610191345,
      "learning_rate": 8.538012591409312e-05,
      "loss": 2.4038,
      "step": 81710
    },
    {
      "epoch": 1.68115789230069,
      "grad_norm": 0.4381117522716522,
      "learning_rate": 8.535815577543879e-05,
      "loss": 2.4089,
      "step": 81720
    },
    {
      "epoch": 1.6813636118175577,
      "grad_norm": 0.4110429584980011,
      "learning_rate": 8.533618635898691e-05,
      "loss": 2.4292,
      "step": 81730
    },
    {
      "epoch": 1.6815693313344253,
      "grad_norm": 0.427397221326828,
      "learning_rate": 8.5314217665821e-05,
      "loss": 2.4121,
      "step": 81740
    },
    {
      "epoch": 1.681775050851293,
      "grad_norm": 0.43552154302597046,
      "learning_rate": 8.529224969702473e-05,
      "loss": 2.3457,
      "step": 81750
    },
    {
      "epoch": 1.6819807703681606,
      "grad_norm": 0.41702893376350403,
      "learning_rate": 8.527028245368166e-05,
      "loss": 2.3274,
      "step": 81760
    },
    {
      "epoch": 1.6821864898850285,
      "grad_norm": 0.40983086824417114,
      "learning_rate": 8.524831593687523e-05,
      "loss": 2.3995,
      "step": 81770
    },
    {
      "epoch": 1.6823922094018964,
      "grad_norm": 0.44559526443481445,
      "learning_rate": 8.522635014768907e-05,
      "loss": 2.4015,
      "step": 81780
    },
    {
      "epoch": 1.682597928918764,
      "grad_norm": 0.417953759431839,
      "learning_rate": 8.520438508720647e-05,
      "loss": 2.3418,
      "step": 81790
    },
    {
      "epoch": 1.6828036484356317,
      "grad_norm": 0.4305911660194397,
      "learning_rate": 8.518242075651094e-05,
      "loss": 2.3647,
      "step": 81800
    },
    {
      "epoch": 1.6830093679524993,
      "grad_norm": 0.5094807744026184,
      "learning_rate": 8.516045715668591e-05,
      "loss": 2.4534,
      "step": 81810
    },
    {
      "epoch": 1.683215087469367,
      "grad_norm": 0.4482531249523163,
      "learning_rate": 8.51384942888146e-05,
      "loss": 2.3649,
      "step": 81820
    },
    {
      "epoch": 1.6834208069862346,
      "grad_norm": 0.45545849204063416,
      "learning_rate": 8.511653215398042e-05,
      "loss": 2.4289,
      "step": 81830
    },
    {
      "epoch": 1.6836265265031025,
      "grad_norm": 0.4451329708099365,
      "learning_rate": 8.509457075326658e-05,
      "loss": 2.442,
      "step": 81840
    },
    {
      "epoch": 1.6838322460199702,
      "grad_norm": 0.4009213447570801,
      "learning_rate": 8.507261008775632e-05,
      "loss": 2.3028,
      "step": 81850
    },
    {
      "epoch": 1.684037965536838,
      "grad_norm": 0.44765907526016235,
      "learning_rate": 8.505065015853291e-05,
      "loss": 2.3422,
      "step": 81860
    },
    {
      "epoch": 1.6842436850537057,
      "grad_norm": 0.4330396354198456,
      "learning_rate": 8.502869096667942e-05,
      "loss": 2.4143,
      "step": 81870
    },
    {
      "epoch": 1.6844494045705733,
      "grad_norm": 0.4512299597263336,
      "learning_rate": 8.500673251327903e-05,
      "loss": 2.3932,
      "step": 81880
    },
    {
      "epoch": 1.684655124087441,
      "grad_norm": 0.44800490140914917,
      "learning_rate": 8.498477479941485e-05,
      "loss": 2.4378,
      "step": 81890
    },
    {
      "epoch": 1.6848608436043087,
      "grad_norm": 0.4809979498386383,
      "learning_rate": 8.496281782616988e-05,
      "loss": 2.3982,
      "step": 81900
    },
    {
      "epoch": 1.6850665631211765,
      "grad_norm": 0.4078883230686188,
      "learning_rate": 8.49408615946272e-05,
      "loss": 2.4054,
      "step": 81910
    },
    {
      "epoch": 1.6852722826380442,
      "grad_norm": 0.5398961901664734,
      "learning_rate": 8.491890610586969e-05,
      "loss": 2.4528,
      "step": 81920
    },
    {
      "epoch": 1.685478002154912,
      "grad_norm": 0.399865984916687,
      "learning_rate": 8.489695136098037e-05,
      "loss": 2.3761,
      "step": 81930
    },
    {
      "epoch": 1.6856837216717797,
      "grad_norm": 0.41041824221611023,
      "learning_rate": 8.487499736104221e-05,
      "loss": 2.379,
      "step": 81940
    },
    {
      "epoch": 1.6858894411886474,
      "grad_norm": 0.4542931318283081,
      "learning_rate": 8.485304410713794e-05,
      "loss": 2.3747,
      "step": 81950
    },
    {
      "epoch": 1.686095160705515,
      "grad_norm": 0.4380328059196472,
      "learning_rate": 8.483109160035045e-05,
      "loss": 2.3589,
      "step": 81960
    },
    {
      "epoch": 1.6863008802223827,
      "grad_norm": 0.4828983247280121,
      "learning_rate": 8.480913984176263e-05,
      "loss": 2.384,
      "step": 81970
    },
    {
      "epoch": 1.6865065997392505,
      "grad_norm": 0.45063772797584534,
      "learning_rate": 8.47871888324571e-05,
      "loss": 2.3847,
      "step": 81980
    },
    {
      "epoch": 1.6867123192561182,
      "grad_norm": 0.4482897222042084,
      "learning_rate": 8.476523857351667e-05,
      "loss": 2.3846,
      "step": 81990
    },
    {
      "epoch": 1.686918038772986,
      "grad_norm": 0.4705054759979248,
      "learning_rate": 8.4743289066024e-05,
      "loss": 2.4401,
      "step": 82000
    },
    {
      "epoch": 1.6871237582898537,
      "grad_norm": 0.4076203405857086,
      "learning_rate": 8.472134031106171e-05,
      "loss": 2.3361,
      "step": 82010
    },
    {
      "epoch": 1.6873294778067214,
      "grad_norm": 0.8971551656723022,
      "learning_rate": 8.469939230971249e-05,
      "loss": 2.4108,
      "step": 82020
    },
    {
      "epoch": 1.687535197323589,
      "grad_norm": 0.4017806947231293,
      "learning_rate": 8.467744506305884e-05,
      "loss": 2.4302,
      "step": 82030
    },
    {
      "epoch": 1.6877409168404567,
      "grad_norm": 0.4285152852535248,
      "learning_rate": 8.465549857218332e-05,
      "loss": 2.3734,
      "step": 82040
    },
    {
      "epoch": 1.6879466363573246,
      "grad_norm": 0.47200247645378113,
      "learning_rate": 8.463355283816846e-05,
      "loss": 2.3944,
      "step": 82050
    },
    {
      "epoch": 1.6881523558741922,
      "grad_norm": 0.42320433259010315,
      "learning_rate": 8.46116078620967e-05,
      "loss": 2.381,
      "step": 82060
    },
    {
      "epoch": 1.68835807539106,
      "grad_norm": 0.4508289694786072,
      "learning_rate": 8.458966364505048e-05,
      "loss": 2.4051,
      "step": 82070
    },
    {
      "epoch": 1.6885637949079277,
      "grad_norm": 0.449712336063385,
      "learning_rate": 8.456772018811213e-05,
      "loss": 2.3889,
      "step": 82080
    },
    {
      "epoch": 1.6887695144247954,
      "grad_norm": 0.43792015314102173,
      "learning_rate": 8.454577749236408e-05,
      "loss": 2.413,
      "step": 82090
    },
    {
      "epoch": 1.688975233941663,
      "grad_norm": 0.45496344566345215,
      "learning_rate": 8.452383555888862e-05,
      "loss": 2.374,
      "step": 82100
    },
    {
      "epoch": 1.6891809534585307,
      "grad_norm": 0.47284796833992004,
      "learning_rate": 8.450189438876797e-05,
      "loss": 2.4779,
      "step": 82110
    },
    {
      "epoch": 1.6893866729753984,
      "grad_norm": 0.457705557346344,
      "learning_rate": 8.447995398308444e-05,
      "loss": 2.3577,
      "step": 82120
    },
    {
      "epoch": 1.6895923924922662,
      "grad_norm": 0.4231518805027008,
      "learning_rate": 8.445801434292024e-05,
      "loss": 2.3767,
      "step": 82130
    },
    {
      "epoch": 1.689798112009134,
      "grad_norm": 0.478528767824173,
      "learning_rate": 8.443607546935745e-05,
      "loss": 2.4011,
      "step": 82140
    },
    {
      "epoch": 1.6900038315260018,
      "grad_norm": 0.4259265065193176,
      "learning_rate": 8.44141373634783e-05,
      "loss": 2.3692,
      "step": 82150
    },
    {
      "epoch": 1.6902095510428694,
      "grad_norm": 0.467547744512558,
      "learning_rate": 8.439220002636478e-05,
      "loss": 2.3651,
      "step": 82160
    },
    {
      "epoch": 1.690415270559737,
      "grad_norm": 0.46549201011657715,
      "learning_rate": 8.437026345909896e-05,
      "loss": 2.3316,
      "step": 82170
    },
    {
      "epoch": 1.6906209900766047,
      "grad_norm": 0.4746347963809967,
      "learning_rate": 8.434832766276292e-05,
      "loss": 2.4019,
      "step": 82180
    },
    {
      "epoch": 1.6908267095934724,
      "grad_norm": 0.4693044126033783,
      "learning_rate": 8.432639263843856e-05,
      "loss": 2.4014,
      "step": 82190
    },
    {
      "epoch": 1.6910324291103402,
      "grad_norm": 0.44136255979537964,
      "learning_rate": 8.430445838720781e-05,
      "loss": 2.4253,
      "step": 82200
    },
    {
      "epoch": 1.6912381486272081,
      "grad_norm": 0.4097796678543091,
      "learning_rate": 8.428252491015267e-05,
      "loss": 2.3992,
      "step": 82210
    },
    {
      "epoch": 1.6914438681440758,
      "grad_norm": 0.4436033070087433,
      "learning_rate": 8.426059220835489e-05,
      "loss": 2.4121,
      "step": 82220
    },
    {
      "epoch": 1.6916495876609434,
      "grad_norm": 0.41094911098480225,
      "learning_rate": 8.423866028289633e-05,
      "loss": 2.3388,
      "step": 82230
    },
    {
      "epoch": 1.691855307177811,
      "grad_norm": 0.4751652479171753,
      "learning_rate": 8.421672913485874e-05,
      "loss": 2.3335,
      "step": 82240
    },
    {
      "epoch": 1.6920610266946787,
      "grad_norm": 0.4110182821750641,
      "learning_rate": 8.419479876532392e-05,
      "loss": 2.4423,
      "step": 82250
    },
    {
      "epoch": 1.6922667462115464,
      "grad_norm": 0.4682205021381378,
      "learning_rate": 8.417286917537357e-05,
      "loss": 2.4083,
      "step": 82260
    },
    {
      "epoch": 1.6924724657284143,
      "grad_norm": 0.46047869324684143,
      "learning_rate": 8.415094036608928e-05,
      "loss": 2.4205,
      "step": 82270
    },
    {
      "epoch": 1.692678185245282,
      "grad_norm": 0.42099958658218384,
      "learning_rate": 8.412901233855277e-05,
      "loss": 2.3268,
      "step": 82280
    },
    {
      "epoch": 1.6928839047621498,
      "grad_norm": 0.4252931475639343,
      "learning_rate": 8.410708509384561e-05,
      "loss": 2.355,
      "step": 82290
    },
    {
      "epoch": 1.6930896242790174,
      "grad_norm": 0.4419105350971222,
      "learning_rate": 8.40851586330493e-05,
      "loss": 2.3857,
      "step": 82300
    },
    {
      "epoch": 1.693295343795885,
      "grad_norm": 0.4488200545310974,
      "learning_rate": 8.406323295724542e-05,
      "loss": 2.4065,
      "step": 82310
    },
    {
      "epoch": 1.6935010633127527,
      "grad_norm": 0.40895524621009827,
      "learning_rate": 8.40413080675154e-05,
      "loss": 2.3685,
      "step": 82320
    },
    {
      "epoch": 1.6937067828296204,
      "grad_norm": 0.4950079023838043,
      "learning_rate": 8.401938396494066e-05,
      "loss": 2.4265,
      "step": 82330
    },
    {
      "epoch": 1.6939125023464883,
      "grad_norm": 0.45274752378463745,
      "learning_rate": 8.39974606506027e-05,
      "loss": 2.4042,
      "step": 82340
    },
    {
      "epoch": 1.694118221863356,
      "grad_norm": 0.45528483390808105,
      "learning_rate": 8.397553812558274e-05,
      "loss": 2.4359,
      "step": 82350
    },
    {
      "epoch": 1.6943239413802238,
      "grad_norm": 0.4700593054294586,
      "learning_rate": 8.395361639096216e-05,
      "loss": 2.3906,
      "step": 82360
    },
    {
      "epoch": 1.6945296608970914,
      "grad_norm": 0.4141802489757538,
      "learning_rate": 8.393169544782227e-05,
      "loss": 2.4283,
      "step": 82370
    },
    {
      "epoch": 1.694735380413959,
      "grad_norm": 0.4286515414714813,
      "learning_rate": 8.390977529724428e-05,
      "loss": 2.3961,
      "step": 82380
    },
    {
      "epoch": 1.6949410999308268,
      "grad_norm": 0.42876890301704407,
      "learning_rate": 8.38878559403094e-05,
      "loss": 2.4173,
      "step": 82390
    },
    {
      "epoch": 1.6951468194476944,
      "grad_norm": 0.5107398629188538,
      "learning_rate": 8.386593737809875e-05,
      "loss": 2.3586,
      "step": 82400
    },
    {
      "epoch": 1.6953525389645623,
      "grad_norm": 0.4350513219833374,
      "learning_rate": 8.38440196116935e-05,
      "loss": 2.3003,
      "step": 82410
    },
    {
      "epoch": 1.69555825848143,
      "grad_norm": 0.4757576584815979,
      "learning_rate": 8.382210264217476e-05,
      "loss": 2.4216,
      "step": 82420
    },
    {
      "epoch": 1.6957639779982978,
      "grad_norm": 0.44907575845718384,
      "learning_rate": 8.380018647062349e-05,
      "loss": 2.3661,
      "step": 82430
    },
    {
      "epoch": 1.6959696975151655,
      "grad_norm": 0.45126643776893616,
      "learning_rate": 8.377827109812076e-05,
      "loss": 2.4362,
      "step": 82440
    },
    {
      "epoch": 1.6961754170320331,
      "grad_norm": 0.44167473912239075,
      "learning_rate": 8.375635652574754e-05,
      "loss": 2.4348,
      "step": 82450
    },
    {
      "epoch": 1.6963811365489008,
      "grad_norm": 0.44766074419021606,
      "learning_rate": 8.373444275458469e-05,
      "loss": 2.36,
      "step": 82460
    },
    {
      "epoch": 1.6965868560657684,
      "grad_norm": 0.4897756576538086,
      "learning_rate": 8.371252978571318e-05,
      "loss": 2.3955,
      "step": 82470
    },
    {
      "epoch": 1.6967925755826363,
      "grad_norm": 0.45941898226737976,
      "learning_rate": 8.369061762021382e-05,
      "loss": 2.4029,
      "step": 82480
    },
    {
      "epoch": 1.696998295099504,
      "grad_norm": 0.44747042655944824,
      "learning_rate": 8.366870625916739e-05,
      "loss": 2.3931,
      "step": 82490
    },
    {
      "epoch": 1.6972040146163718,
      "grad_norm": 0.4377357065677643,
      "learning_rate": 8.364679570365472e-05,
      "loss": 2.4132,
      "step": 82500
    },
    {
      "epoch": 1.6974097341332395,
      "grad_norm": 0.46499934792518616,
      "learning_rate": 8.362488595475648e-05,
      "loss": 2.4104,
      "step": 82510
    },
    {
      "epoch": 1.6976154536501071,
      "grad_norm": 0.43817251920700073,
      "learning_rate": 8.360297701355336e-05,
      "loss": 2.4034,
      "step": 82520
    },
    {
      "epoch": 1.6978211731669748,
      "grad_norm": 0.46955767273902893,
      "learning_rate": 8.358106888112608e-05,
      "loss": 2.3774,
      "step": 82530
    },
    {
      "epoch": 1.6980268926838424,
      "grad_norm": 0.42464596033096313,
      "learning_rate": 8.355916155855518e-05,
      "loss": 2.409,
      "step": 82540
    },
    {
      "epoch": 1.69823261220071,
      "grad_norm": 0.43895837664604187,
      "learning_rate": 8.353725504692127e-05,
      "loss": 2.3658,
      "step": 82550
    },
    {
      "epoch": 1.698438331717578,
      "grad_norm": 0.49620482325553894,
      "learning_rate": 8.35153493473048e-05,
      "loss": 2.4225,
      "step": 82560
    },
    {
      "epoch": 1.6986440512344458,
      "grad_norm": 0.4520701467990875,
      "learning_rate": 8.349344446078635e-05,
      "loss": 2.3951,
      "step": 82570
    },
    {
      "epoch": 1.6988497707513135,
      "grad_norm": 0.423006147146225,
      "learning_rate": 8.347154038844635e-05,
      "loss": 2.418,
      "step": 82580
    },
    {
      "epoch": 1.6990554902681811,
      "grad_norm": 0.4105711579322815,
      "learning_rate": 8.344963713136517e-05,
      "loss": 2.3741,
      "step": 82590
    },
    {
      "epoch": 1.6992612097850488,
      "grad_norm": 0.4489949345588684,
      "learning_rate": 8.342773469062321e-05,
      "loss": 2.3734,
      "step": 82600
    },
    {
      "epoch": 1.6994669293019165,
      "grad_norm": 0.43348774313926697,
      "learning_rate": 8.34058330673008e-05,
      "loss": 2.3729,
      "step": 82610
    },
    {
      "epoch": 1.699672648818784,
      "grad_norm": 0.44280171394348145,
      "learning_rate": 8.338393226247818e-05,
      "loss": 2.4234,
      "step": 82620
    },
    {
      "epoch": 1.699878368335652,
      "grad_norm": 0.3975193202495575,
      "learning_rate": 8.336203227723569e-05,
      "loss": 2.3606,
      "step": 82630
    },
    {
      "epoch": 1.7000840878525196,
      "grad_norm": 0.46691808104515076,
      "learning_rate": 8.334013311265345e-05,
      "loss": 2.3936,
      "step": 82640
    },
    {
      "epoch": 1.7002898073693875,
      "grad_norm": 0.46426495909690857,
      "learning_rate": 8.331823476981164e-05,
      "loss": 2.4517,
      "step": 82650
    },
    {
      "epoch": 1.7004955268862552,
      "grad_norm": 0.4472394287586212,
      "learning_rate": 8.329633724979044e-05,
      "loss": 2.3439,
      "step": 82660
    },
    {
      "epoch": 1.7007012464031228,
      "grad_norm": 0.4462488889694214,
      "learning_rate": 8.327444055366988e-05,
      "loss": 2.3896,
      "step": 82670
    },
    {
      "epoch": 1.7009069659199905,
      "grad_norm": 0.3956027328968048,
      "learning_rate": 8.325254468253e-05,
      "loss": 2.3494,
      "step": 82680
    },
    {
      "epoch": 1.7011126854368581,
      "grad_norm": 0.43187326192855835,
      "learning_rate": 8.32306496374509e-05,
      "loss": 2.4074,
      "step": 82690
    },
    {
      "epoch": 1.701318404953726,
      "grad_norm": 0.41447633504867554,
      "learning_rate": 8.320875541951242e-05,
      "loss": 2.4232,
      "step": 82700
    },
    {
      "epoch": 1.7015241244705936,
      "grad_norm": 0.549603283405304,
      "learning_rate": 8.318686202979456e-05,
      "loss": 2.4087,
      "step": 82710
    },
    {
      "epoch": 1.7017298439874615,
      "grad_norm": 0.4705141484737396,
      "learning_rate": 8.316496946937714e-05,
      "loss": 2.3629,
      "step": 82720
    },
    {
      "epoch": 1.7019355635043292,
      "grad_norm": 0.4358627200126648,
      "learning_rate": 8.314307773934004e-05,
      "loss": 2.43,
      "step": 82730
    },
    {
      "epoch": 1.7021412830211968,
      "grad_norm": 0.4127799868583679,
      "learning_rate": 8.31211868407631e-05,
      "loss": 2.4167,
      "step": 82740
    },
    {
      "epoch": 1.7023470025380645,
      "grad_norm": 0.52483731508255,
      "learning_rate": 8.309929677472598e-05,
      "loss": 2.3733,
      "step": 82750
    },
    {
      "epoch": 1.7025527220549321,
      "grad_norm": 0.5458113551139832,
      "learning_rate": 8.307740754230848e-05,
      "loss": 2.3334,
      "step": 82760
    },
    {
      "epoch": 1.7027584415718,
      "grad_norm": 0.4716968834400177,
      "learning_rate": 8.305551914459027e-05,
      "loss": 2.3792,
      "step": 82770
    },
    {
      "epoch": 1.7029641610886677,
      "grad_norm": 0.4799245595932007,
      "learning_rate": 8.303363158265091e-05,
      "loss": 2.3573,
      "step": 82780
    },
    {
      "epoch": 1.7031698806055355,
      "grad_norm": 0.4811381995677948,
      "learning_rate": 8.301174485757011e-05,
      "loss": 2.4464,
      "step": 82790
    },
    {
      "epoch": 1.7033756001224032,
      "grad_norm": 0.43762946128845215,
      "learning_rate": 8.298985897042733e-05,
      "loss": 2.4428,
      "step": 82800
    },
    {
      "epoch": 1.7035813196392708,
      "grad_norm": 0.4733099639415741,
      "learning_rate": 8.296797392230208e-05,
      "loss": 2.3632,
      "step": 82810
    },
    {
      "epoch": 1.7037870391561385,
      "grad_norm": 0.47047242522239685,
      "learning_rate": 8.294608971427391e-05,
      "loss": 2.3868,
      "step": 82820
    },
    {
      "epoch": 1.7039927586730061,
      "grad_norm": 0.4660894274711609,
      "learning_rate": 8.292420634742219e-05,
      "loss": 2.4714,
      "step": 82830
    },
    {
      "epoch": 1.704198478189874,
      "grad_norm": 0.4355912506580353,
      "learning_rate": 8.29023238228263e-05,
      "loss": 2.3375,
      "step": 82840
    },
    {
      "epoch": 1.7044041977067417,
      "grad_norm": 0.3983702063560486,
      "learning_rate": 8.288044214156563e-05,
      "loss": 2.4099,
      "step": 82850
    },
    {
      "epoch": 1.7046099172236095,
      "grad_norm": 0.4188264310359955,
      "learning_rate": 8.285856130471945e-05,
      "loss": 2.3987,
      "step": 82860
    },
    {
      "epoch": 1.7048156367404772,
      "grad_norm": 0.4455377459526062,
      "learning_rate": 8.283668131336706e-05,
      "loss": 2.3681,
      "step": 82870
    },
    {
      "epoch": 1.7050213562573449,
      "grad_norm": 0.41972729563713074,
      "learning_rate": 8.281480216858759e-05,
      "loss": 2.4091,
      "step": 82880
    },
    {
      "epoch": 1.7052270757742125,
      "grad_norm": 0.40721288323402405,
      "learning_rate": 8.279292387146032e-05,
      "loss": 2.3963,
      "step": 82890
    },
    {
      "epoch": 1.7054327952910802,
      "grad_norm": 0.45742273330688477,
      "learning_rate": 8.277104642306435e-05,
      "loss": 2.3971,
      "step": 82900
    },
    {
      "epoch": 1.7056385148079478,
      "grad_norm": 0.44766291975975037,
      "learning_rate": 8.274916982447876e-05,
      "loss": 2.3997,
      "step": 82910
    },
    {
      "epoch": 1.7058442343248157,
      "grad_norm": 0.428812175989151,
      "learning_rate": 8.272729407678262e-05,
      "loss": 2.4069,
      "step": 82920
    },
    {
      "epoch": 1.7060499538416836,
      "grad_norm": 0.45835527777671814,
      "learning_rate": 8.270541918105495e-05,
      "loss": 2.4029,
      "step": 82930
    },
    {
      "epoch": 1.7062556733585512,
      "grad_norm": 0.4828820824623108,
      "learning_rate": 8.268354513837469e-05,
      "loss": 2.4141,
      "step": 82940
    },
    {
      "epoch": 1.7064613928754189,
      "grad_norm": 0.4325438439846039,
      "learning_rate": 8.26616719498208e-05,
      "loss": 2.4393,
      "step": 82950
    },
    {
      "epoch": 1.7066671123922865,
      "grad_norm": 0.4340989291667938,
      "learning_rate": 8.263979961647215e-05,
      "loss": 2.4283,
      "step": 82960
    },
    {
      "epoch": 1.7068728319091542,
      "grad_norm": 0.4194823205471039,
      "learning_rate": 8.261792813940755e-05,
      "loss": 2.3614,
      "step": 82970
    },
    {
      "epoch": 1.7070785514260218,
      "grad_norm": 0.4426863491535187,
      "learning_rate": 8.259605751970588e-05,
      "loss": 2.45,
      "step": 82980
    },
    {
      "epoch": 1.7072842709428897,
      "grad_norm": 0.4479263424873352,
      "learning_rate": 8.257418775844583e-05,
      "loss": 2.4028,
      "step": 82990
    },
    {
      "epoch": 1.7074899904597574,
      "grad_norm": 0.4849853813648224,
      "learning_rate": 8.255231885670615e-05,
      "loss": 2.3967,
      "step": 83000
    },
    {
      "epoch": 1.7076957099766252,
      "grad_norm": 0.4434913694858551,
      "learning_rate": 8.253045081556549e-05,
      "loss": 2.3881,
      "step": 83010
    },
    {
      "epoch": 1.7079014294934929,
      "grad_norm": 0.42966488003730774,
      "learning_rate": 8.250858363610249e-05,
      "loss": 2.424,
      "step": 83020
    },
    {
      "epoch": 1.7081071490103605,
      "grad_norm": 0.4320562183856964,
      "learning_rate": 8.248671731939577e-05,
      "loss": 2.3907,
      "step": 83030
    },
    {
      "epoch": 1.7083128685272282,
      "grad_norm": 0.4237298369407654,
      "learning_rate": 8.24648518665238e-05,
      "loss": 2.3951,
      "step": 83040
    },
    {
      "epoch": 1.7085185880440958,
      "grad_norm": 0.4521685540676117,
      "learning_rate": 8.244298727856517e-05,
      "loss": 2.3389,
      "step": 83050
    },
    {
      "epoch": 1.7087243075609637,
      "grad_norm": 0.43601495027542114,
      "learning_rate": 8.242112355659832e-05,
      "loss": 2.3682,
      "step": 83060
    },
    {
      "epoch": 1.7089300270778314,
      "grad_norm": 0.4332275986671448,
      "learning_rate": 8.239926070170158e-05,
      "loss": 2.3597,
      "step": 83070
    },
    {
      "epoch": 1.7091357465946992,
      "grad_norm": 0.41807711124420166,
      "learning_rate": 8.237739871495347e-05,
      "loss": 2.4412,
      "step": 83080
    },
    {
      "epoch": 1.709341466111567,
      "grad_norm": 0.4471418857574463,
      "learning_rate": 8.235553759743222e-05,
      "loss": 2.3939,
      "step": 83090
    },
    {
      "epoch": 1.7095471856284346,
      "grad_norm": 0.43754467368125916,
      "learning_rate": 8.233367735021613e-05,
      "loss": 2.3756,
      "step": 83100
    },
    {
      "epoch": 1.7097529051453022,
      "grad_norm": 0.4794556200504303,
      "learning_rate": 8.23118179743835e-05,
      "loss": 2.3993,
      "step": 83110
    },
    {
      "epoch": 1.7099586246621699,
      "grad_norm": 0.4133966863155365,
      "learning_rate": 8.22899594710125e-05,
      "loss": 2.3509,
      "step": 83120
    },
    {
      "epoch": 1.7101643441790377,
      "grad_norm": 0.48153257369995117,
      "learning_rate": 8.226810184118126e-05,
      "loss": 2.3298,
      "step": 83130
    },
    {
      "epoch": 1.7103700636959054,
      "grad_norm": 0.42990222573280334,
      "learning_rate": 8.224624508596797e-05,
      "loss": 2.4395,
      "step": 83140
    },
    {
      "epoch": 1.7105757832127733,
      "grad_norm": 0.4740448594093323,
      "learning_rate": 8.222438920645064e-05,
      "loss": 2.3684,
      "step": 83150
    },
    {
      "epoch": 1.710781502729641,
      "grad_norm": 0.45204243063926697,
      "learning_rate": 8.220253420370734e-05,
      "loss": 2.3595,
      "step": 83160
    },
    {
      "epoch": 1.7109872222465086,
      "grad_norm": 0.4908026456832886,
      "learning_rate": 8.2180680078816e-05,
      "loss": 2.383,
      "step": 83170
    },
    {
      "epoch": 1.7111929417633762,
      "grad_norm": 0.4324021339416504,
      "learning_rate": 8.215882683285465e-05,
      "loss": 2.4043,
      "step": 83180
    },
    {
      "epoch": 1.7113986612802439,
      "grad_norm": 0.43131205439567566,
      "learning_rate": 8.213697446690116e-05,
      "loss": 2.432,
      "step": 83190
    },
    {
      "epoch": 1.7116043807971117,
      "grad_norm": 0.4457530081272125,
      "learning_rate": 8.211512298203334e-05,
      "loss": 2.3955,
      "step": 83200
    },
    {
      "epoch": 1.7118101003139794,
      "grad_norm": 0.4747178256511688,
      "learning_rate": 8.209327237932904e-05,
      "loss": 2.4228,
      "step": 83210
    },
    {
      "epoch": 1.7120158198308473,
      "grad_norm": 0.42759135365486145,
      "learning_rate": 8.207142265986608e-05,
      "loss": 2.4009,
      "step": 83220
    },
    {
      "epoch": 1.712221539347715,
      "grad_norm": 0.38935574889183044,
      "learning_rate": 8.204957382472209e-05,
      "loss": 2.4045,
      "step": 83230
    },
    {
      "epoch": 1.7124272588645826,
      "grad_norm": 0.47499045729637146,
      "learning_rate": 8.202772587497485e-05,
      "loss": 2.4241,
      "step": 83240
    },
    {
      "epoch": 1.7126329783814502,
      "grad_norm": 0.47659608721733093,
      "learning_rate": 8.200587881170193e-05,
      "loss": 2.3839,
      "step": 83250
    },
    {
      "epoch": 1.7128386978983179,
      "grad_norm": 0.4612823724746704,
      "learning_rate": 8.198403263598093e-05,
      "loss": 2.3771,
      "step": 83260
    },
    {
      "epoch": 1.7130444174151855,
      "grad_norm": 0.4345954358577728,
      "learning_rate": 8.196218734888948e-05,
      "loss": 2.412,
      "step": 83270
    },
    {
      "epoch": 1.7132501369320534,
      "grad_norm": 0.46205586194992065,
      "learning_rate": 8.1940342951505e-05,
      "loss": 2.4046,
      "step": 83280
    },
    {
      "epoch": 1.7134558564489213,
      "grad_norm": 0.4700497090816498,
      "learning_rate": 8.191849944490496e-05,
      "loss": 2.415,
      "step": 83290
    },
    {
      "epoch": 1.713661575965789,
      "grad_norm": 0.4584689140319824,
      "learning_rate": 8.189665683016688e-05,
      "loss": 2.4194,
      "step": 83300
    },
    {
      "epoch": 1.7138672954826566,
      "grad_norm": 0.44892561435699463,
      "learning_rate": 8.187481510836802e-05,
      "loss": 2.362,
      "step": 83310
    },
    {
      "epoch": 1.7140730149995242,
      "grad_norm": 0.5523364543914795,
      "learning_rate": 8.185297428058579e-05,
      "loss": 2.4268,
      "step": 83320
    },
    {
      "epoch": 1.714278734516392,
      "grad_norm": 0.4174414575099945,
      "learning_rate": 8.183113434789741e-05,
      "loss": 2.3544,
      "step": 83330
    },
    {
      "epoch": 1.7144844540332596,
      "grad_norm": 0.43130117654800415,
      "learning_rate": 8.180929531138017e-05,
      "loss": 2.4028,
      "step": 83340
    },
    {
      "epoch": 1.7146901735501274,
      "grad_norm": 0.4336974620819092,
      "learning_rate": 8.17874571721113e-05,
      "loss": 2.3801,
      "step": 83350
    },
    {
      "epoch": 1.714895893066995,
      "grad_norm": 0.4151400327682495,
      "learning_rate": 8.176561993116788e-05,
      "loss": 2.3175,
      "step": 83360
    },
    {
      "epoch": 1.715101612583863,
      "grad_norm": 0.42658430337905884,
      "learning_rate": 8.174378358962708e-05,
      "loss": 2.4547,
      "step": 83370
    },
    {
      "epoch": 1.7153073321007306,
      "grad_norm": 0.48035115003585815,
      "learning_rate": 8.172194814856598e-05,
      "loss": 2.3816,
      "step": 83380
    },
    {
      "epoch": 1.7155130516175983,
      "grad_norm": 0.4224866032600403,
      "learning_rate": 8.170011360906152e-05,
      "loss": 2.3971,
      "step": 83390
    },
    {
      "epoch": 1.715718771134466,
      "grad_norm": 0.43131208419799805,
      "learning_rate": 8.167827997219079e-05,
      "loss": 2.4197,
      "step": 83400
    },
    {
      "epoch": 1.7159244906513336,
      "grad_norm": 0.4820678234100342,
      "learning_rate": 8.165644723903063e-05,
      "loss": 2.4024,
      "step": 83410
    },
    {
      "epoch": 1.7161302101682014,
      "grad_norm": 0.4479643404483795,
      "learning_rate": 8.163461541065795e-05,
      "loss": 2.4089,
      "step": 83420
    },
    {
      "epoch": 1.716335929685069,
      "grad_norm": 0.43467772006988525,
      "learning_rate": 8.161278448814967e-05,
      "loss": 2.3801,
      "step": 83430
    },
    {
      "epoch": 1.716541649201937,
      "grad_norm": 0.43038368225097656,
      "learning_rate": 8.159095447258251e-05,
      "loss": 2.3827,
      "step": 83440
    },
    {
      "epoch": 1.7167473687188046,
      "grad_norm": 0.4671328663825989,
      "learning_rate": 8.15691253650332e-05,
      "loss": 2.4126,
      "step": 83450
    },
    {
      "epoch": 1.7169530882356723,
      "grad_norm": 0.481638103723526,
      "learning_rate": 8.154729716657857e-05,
      "loss": 2.3962,
      "step": 83460
    },
    {
      "epoch": 1.71715880775254,
      "grad_norm": 0.4718368649482727,
      "learning_rate": 8.152546987829518e-05,
      "loss": 2.4255,
      "step": 83470
    },
    {
      "epoch": 1.7173645272694076,
      "grad_norm": 0.4693675637245178,
      "learning_rate": 8.150364350125972e-05,
      "loss": 2.4393,
      "step": 83480
    },
    {
      "epoch": 1.7175702467862755,
      "grad_norm": 0.3956795334815979,
      "learning_rate": 8.148181803654868e-05,
      "loss": 2.3794,
      "step": 83490
    },
    {
      "epoch": 1.717775966303143,
      "grad_norm": 0.4381468594074249,
      "learning_rate": 8.145999348523868e-05,
      "loss": 2.3719,
      "step": 83500
    },
    {
      "epoch": 1.717981685820011,
      "grad_norm": 0.4580976366996765,
      "learning_rate": 8.143816984840617e-05,
      "loss": 2.3456,
      "step": 83510
    },
    {
      "epoch": 1.7181874053368786,
      "grad_norm": 0.43825581669807434,
      "learning_rate": 8.141634712712756e-05,
      "loss": 2.3905,
      "step": 83520
    },
    {
      "epoch": 1.7183931248537463,
      "grad_norm": 0.42369264364242554,
      "learning_rate": 8.13945253224793e-05,
      "loss": 2.4123,
      "step": 83530
    },
    {
      "epoch": 1.718598844370614,
      "grad_norm": 0.4269445538520813,
      "learning_rate": 8.137270443553774e-05,
      "loss": 2.3561,
      "step": 83540
    },
    {
      "epoch": 1.7188045638874816,
      "grad_norm": 0.45471426844596863,
      "learning_rate": 8.135088446737912e-05,
      "loss": 2.4931,
      "step": 83550
    },
    {
      "epoch": 1.7190102834043495,
      "grad_norm": 0.5582037568092346,
      "learning_rate": 8.132906541907979e-05,
      "loss": 2.3746,
      "step": 83560
    },
    {
      "epoch": 1.7192160029212171,
      "grad_norm": 0.47221776843070984,
      "learning_rate": 8.130724729171589e-05,
      "loss": 2.4194,
      "step": 83570
    },
    {
      "epoch": 1.719421722438085,
      "grad_norm": 0.44427284598350525,
      "learning_rate": 8.12854300863636e-05,
      "loss": 2.4331,
      "step": 83580
    },
    {
      "epoch": 1.7196274419549527,
      "grad_norm": 0.43041813373565674,
      "learning_rate": 8.12636138040991e-05,
      "loss": 2.3905,
      "step": 83590
    },
    {
      "epoch": 1.7198331614718203,
      "grad_norm": 0.4330960810184479,
      "learning_rate": 8.124179844599841e-05,
      "loss": 2.3703,
      "step": 83600
    },
    {
      "epoch": 1.720038880988688,
      "grad_norm": 0.41213443875312805,
      "learning_rate": 8.121998401313756e-05,
      "loss": 2.3562,
      "step": 83610
    },
    {
      "epoch": 1.7202446005055556,
      "grad_norm": 0.4829946458339691,
      "learning_rate": 8.119817050659261e-05,
      "loss": 2.3886,
      "step": 83620
    },
    {
      "epoch": 1.7204503200224233,
      "grad_norm": 0.42860010266304016,
      "learning_rate": 8.117635792743942e-05,
      "loss": 2.3575,
      "step": 83630
    },
    {
      "epoch": 1.7206560395392911,
      "grad_norm": 0.4435446262359619,
      "learning_rate": 8.115454627675394e-05,
      "loss": 2.3762,
      "step": 83640
    },
    {
      "epoch": 1.720861759056159,
      "grad_norm": 0.4568440020084381,
      "learning_rate": 8.113273555561197e-05,
      "loss": 2.3725,
      "step": 83650
    },
    {
      "epoch": 1.7210674785730267,
      "grad_norm": 0.44276246428489685,
      "learning_rate": 8.111092576508934e-05,
      "loss": 2.4579,
      "step": 83660
    },
    {
      "epoch": 1.7212731980898943,
      "grad_norm": 0.45389053225517273,
      "learning_rate": 8.108911690626185e-05,
      "loss": 2.3914,
      "step": 83670
    },
    {
      "epoch": 1.721478917606762,
      "grad_norm": 0.46469220519065857,
      "learning_rate": 8.106730898020511e-05,
      "loss": 2.366,
      "step": 83680
    },
    {
      "epoch": 1.7216846371236296,
      "grad_norm": 0.4316825270652771,
      "learning_rate": 8.104550198799488e-05,
      "loss": 2.4154,
      "step": 83690
    },
    {
      "epoch": 1.7218903566404973,
      "grad_norm": 0.437628835439682,
      "learning_rate": 8.102369593070677e-05,
      "loss": 2.3996,
      "step": 83700
    },
    {
      "epoch": 1.7220960761573652,
      "grad_norm": 0.447009414434433,
      "learning_rate": 8.100189080941628e-05,
      "loss": 2.3704,
      "step": 83710
    },
    {
      "epoch": 1.7223017956742328,
      "grad_norm": 0.4210056662559509,
      "learning_rate": 8.098008662519904e-05,
      "loss": 2.3932,
      "step": 83720
    },
    {
      "epoch": 1.7225075151911007,
      "grad_norm": 0.44867971539497375,
      "learning_rate": 8.095828337913046e-05,
      "loss": 2.3498,
      "step": 83730
    },
    {
      "epoch": 1.7227132347079683,
      "grad_norm": 0.42540889978408813,
      "learning_rate": 8.093648107228597e-05,
      "loss": 2.3351,
      "step": 83740
    },
    {
      "epoch": 1.722918954224836,
      "grad_norm": 0.43507930636405945,
      "learning_rate": 8.091467970574101e-05,
      "loss": 2.3949,
      "step": 83750
    },
    {
      "epoch": 1.7231246737417036,
      "grad_norm": 0.4514622986316681,
      "learning_rate": 8.08928792805709e-05,
      "loss": 2.3867,
      "step": 83760
    },
    {
      "epoch": 1.7233303932585713,
      "grad_norm": 0.45417383313179016,
      "learning_rate": 8.087107979785091e-05,
      "loss": 2.3977,
      "step": 83770
    },
    {
      "epoch": 1.7235361127754392,
      "grad_norm": 0.4604424238204956,
      "learning_rate": 8.084928125865635e-05,
      "loss": 2.425,
      "step": 83780
    },
    {
      "epoch": 1.7237418322923068,
      "grad_norm": 0.4421471655368805,
      "learning_rate": 8.082748366406236e-05,
      "loss": 2.4113,
      "step": 83790
    },
    {
      "epoch": 1.7239475518091747,
      "grad_norm": 0.4609163701534271,
      "learning_rate": 8.080568701514417e-05,
      "loss": 2.3768,
      "step": 83800
    },
    {
      "epoch": 1.7241532713260423,
      "grad_norm": 0.4476712942123413,
      "learning_rate": 8.078389131297677e-05,
      "loss": 2.4105,
      "step": 83810
    },
    {
      "epoch": 1.72435899084291,
      "grad_norm": 0.43802502751350403,
      "learning_rate": 8.076209655863534e-05,
      "loss": 2.3676,
      "step": 83820
    },
    {
      "epoch": 1.7245647103597777,
      "grad_norm": 0.4689338207244873,
      "learning_rate": 8.074030275319486e-05,
      "loss": 2.321,
      "step": 83830
    },
    {
      "epoch": 1.7247704298766453,
      "grad_norm": 0.4356488287448883,
      "learning_rate": 8.071850989773025e-05,
      "loss": 2.4133,
      "step": 83840
    },
    {
      "epoch": 1.7249761493935132,
      "grad_norm": 0.5009682774543762,
      "learning_rate": 8.069671799331651e-05,
      "loss": 2.3919,
      "step": 83850
    },
    {
      "epoch": 1.7251818689103808,
      "grad_norm": 0.44970858097076416,
      "learning_rate": 8.067492704102848e-05,
      "loss": 2.4668,
      "step": 83860
    },
    {
      "epoch": 1.7253875884272487,
      "grad_norm": 0.41991570591926575,
      "learning_rate": 8.065313704194098e-05,
      "loss": 2.3656,
      "step": 83870
    },
    {
      "epoch": 1.7255933079441164,
      "grad_norm": 0.453226238489151,
      "learning_rate": 8.063134799712882e-05,
      "loss": 2.4263,
      "step": 83880
    },
    {
      "epoch": 1.725799027460984,
      "grad_norm": 0.4482271075248718,
      "learning_rate": 8.06095599076667e-05,
      "loss": 2.3722,
      "step": 83890
    },
    {
      "epoch": 1.7260047469778517,
      "grad_norm": 0.45192280411720276,
      "learning_rate": 8.05877727746293e-05,
      "loss": 2.3916,
      "step": 83900
    },
    {
      "epoch": 1.7262104664947193,
      "grad_norm": 0.4798516631126404,
      "learning_rate": 8.056598659909134e-05,
      "loss": 2.4262,
      "step": 83910
    },
    {
      "epoch": 1.7264161860115872,
      "grad_norm": 0.4272024631500244,
      "learning_rate": 8.054420138212732e-05,
      "loss": 2.3751,
      "step": 83920
    },
    {
      "epoch": 1.7266219055284548,
      "grad_norm": 0.42685550451278687,
      "learning_rate": 8.052241712481181e-05,
      "loss": 2.4313,
      "step": 83930
    },
    {
      "epoch": 1.7268276250453227,
      "grad_norm": 0.4579620659351349,
      "learning_rate": 8.050063382821938e-05,
      "loss": 2.3686,
      "step": 83940
    },
    {
      "epoch": 1.7270333445621904,
      "grad_norm": 0.452043741941452,
      "learning_rate": 8.047885149342438e-05,
      "loss": 2.4009,
      "step": 83950
    },
    {
      "epoch": 1.727239064079058,
      "grad_norm": 0.47937750816345215,
      "learning_rate": 8.045707012150129e-05,
      "loss": 2.366,
      "step": 83960
    },
    {
      "epoch": 1.7274447835959257,
      "grad_norm": 0.4623757004737854,
      "learning_rate": 8.043528971352438e-05,
      "loss": 2.3807,
      "step": 83970
    },
    {
      "epoch": 1.7276505031127933,
      "grad_norm": 0.44588714838027954,
      "learning_rate": 8.041351027056806e-05,
      "loss": 2.4256,
      "step": 83980
    },
    {
      "epoch": 1.7278562226296612,
      "grad_norm": 0.47379225492477417,
      "learning_rate": 8.039173179370656e-05,
      "loss": 2.4387,
      "step": 83990
    },
    {
      "epoch": 1.7280619421465289,
      "grad_norm": 0.4592808187007904,
      "learning_rate": 8.036995428401401e-05,
      "loss": 2.3461,
      "step": 84000
    },
    {
      "epoch": 1.7282676616633967,
      "grad_norm": 0.44548454880714417,
      "learning_rate": 8.03481777425647e-05,
      "loss": 2.3654,
      "step": 84010
    },
    {
      "epoch": 1.7284733811802644,
      "grad_norm": 0.43899616599082947,
      "learning_rate": 8.03264021704327e-05,
      "loss": 2.3649,
      "step": 84020
    },
    {
      "epoch": 1.728679100697132,
      "grad_norm": 0.495125412940979,
      "learning_rate": 8.030462756869202e-05,
      "loss": 2.3952,
      "step": 84030
    },
    {
      "epoch": 1.7288848202139997,
      "grad_norm": 0.40847131609916687,
      "learning_rate": 8.028285393841679e-05,
      "loss": 2.3433,
      "step": 84040
    },
    {
      "epoch": 1.7290905397308673,
      "grad_norm": 0.41257819533348083,
      "learning_rate": 8.02610812806809e-05,
      "loss": 2.384,
      "step": 84050
    },
    {
      "epoch": 1.729296259247735,
      "grad_norm": 0.41110602021217346,
      "learning_rate": 8.02393095965583e-05,
      "loss": 2.4383,
      "step": 84060
    },
    {
      "epoch": 1.7295019787646029,
      "grad_norm": 0.424853652715683,
      "learning_rate": 8.02175388871229e-05,
      "loss": 2.3561,
      "step": 84070
    },
    {
      "epoch": 1.7297076982814708,
      "grad_norm": 0.44289958477020264,
      "learning_rate": 8.019576915344848e-05,
      "loss": 2.3113,
      "step": 84080
    },
    {
      "epoch": 1.7299134177983384,
      "grad_norm": 0.43555980920791626,
      "learning_rate": 8.017400039660885e-05,
      "loss": 2.3808,
      "step": 84090
    },
    {
      "epoch": 1.730119137315206,
      "grad_norm": 0.43702396750450134,
      "learning_rate": 8.015223261767777e-05,
      "loss": 2.3922,
      "step": 84100
    },
    {
      "epoch": 1.7303248568320737,
      "grad_norm": 0.4549087584018707,
      "learning_rate": 8.013046581772888e-05,
      "loss": 2.3744,
      "step": 84110
    },
    {
      "epoch": 1.7305305763489414,
      "grad_norm": 0.45563945174217224,
      "learning_rate": 8.010869999783586e-05,
      "loss": 2.428,
      "step": 84120
    },
    {
      "epoch": 1.730736295865809,
      "grad_norm": 0.4532777667045593,
      "learning_rate": 8.008693515907221e-05,
      "loss": 2.3721,
      "step": 84130
    },
    {
      "epoch": 1.730942015382677,
      "grad_norm": 0.43283316493034363,
      "learning_rate": 8.006517130251158e-05,
      "loss": 2.3815,
      "step": 84140
    },
    {
      "epoch": 1.7311477348995445,
      "grad_norm": 0.44887009263038635,
      "learning_rate": 8.004340842922743e-05,
      "loss": 2.4009,
      "step": 84150
    },
    {
      "epoch": 1.7313534544164124,
      "grad_norm": 0.44652295112609863,
      "learning_rate": 8.002164654029314e-05,
      "loss": 2.4019,
      "step": 84160
    },
    {
      "epoch": 1.73155917393328,
      "grad_norm": 0.4390377402305603,
      "learning_rate": 7.999988563678222e-05,
      "loss": 2.4213,
      "step": 84170
    },
    {
      "epoch": 1.7317648934501477,
      "grad_norm": 0.42287540435791016,
      "learning_rate": 7.997812571976793e-05,
      "loss": 2.3864,
      "step": 84180
    },
    {
      "epoch": 1.7319706129670154,
      "grad_norm": 0.5344187617301941,
      "learning_rate": 7.995636679032357e-05,
      "loss": 2.4077,
      "step": 84190
    },
    {
      "epoch": 1.732176332483883,
      "grad_norm": 0.5138290524482727,
      "learning_rate": 7.993460884952245e-05,
      "loss": 2.4021,
      "step": 84200
    },
    {
      "epoch": 1.732382052000751,
      "grad_norm": 0.5032148957252502,
      "learning_rate": 7.99128518984377e-05,
      "loss": 2.4118,
      "step": 84210
    },
    {
      "epoch": 1.7325877715176186,
      "grad_norm": 0.4777332842350006,
      "learning_rate": 7.989109593814249e-05,
      "loss": 2.4032,
      "step": 84220
    },
    {
      "epoch": 1.7327934910344864,
      "grad_norm": 0.4223076403141022,
      "learning_rate": 7.986934096970998e-05,
      "loss": 2.3949,
      "step": 84230
    },
    {
      "epoch": 1.732999210551354,
      "grad_norm": 0.43588119745254517,
      "learning_rate": 7.984758699421316e-05,
      "loss": 2.4159,
      "step": 84240
    },
    {
      "epoch": 1.7332049300682217,
      "grad_norm": 0.45360395312309265,
      "learning_rate": 7.982583401272508e-05,
      "loss": 2.4442,
      "step": 84250
    },
    {
      "epoch": 1.7334106495850894,
      "grad_norm": 0.4170217514038086,
      "learning_rate": 7.980408202631864e-05,
      "loss": 2.4067,
      "step": 84260
    },
    {
      "epoch": 1.733616369101957,
      "grad_norm": 0.43371841311454773,
      "learning_rate": 7.978233103606678e-05,
      "loss": 2.3657,
      "step": 84270
    },
    {
      "epoch": 1.733822088618825,
      "grad_norm": 0.4260849952697754,
      "learning_rate": 7.97605810430424e-05,
      "loss": 2.3989,
      "step": 84280
    },
    {
      "epoch": 1.7340278081356926,
      "grad_norm": 0.4602851867675781,
      "learning_rate": 7.97388320483182e-05,
      "loss": 2.3917,
      "step": 84290
    },
    {
      "epoch": 1.7342335276525604,
      "grad_norm": 0.4213187098503113,
      "learning_rate": 7.971708405296704e-05,
      "loss": 2.4139,
      "step": 84300
    },
    {
      "epoch": 1.734439247169428,
      "grad_norm": 0.42438483238220215,
      "learning_rate": 7.96953370580616e-05,
      "loss": 2.3893,
      "step": 84310
    },
    {
      "epoch": 1.7346449666862958,
      "grad_norm": 0.5118441581726074,
      "learning_rate": 7.967359106467451e-05,
      "loss": 2.3565,
      "step": 84320
    },
    {
      "epoch": 1.7348506862031634,
      "grad_norm": 0.4453502893447876,
      "learning_rate": 7.965184607387845e-05,
      "loss": 2.3649,
      "step": 84330
    },
    {
      "epoch": 1.735056405720031,
      "grad_norm": 0.4086341857910156,
      "learning_rate": 7.963010208674591e-05,
      "loss": 2.3613,
      "step": 84340
    },
    {
      "epoch": 1.735262125236899,
      "grad_norm": 0.454350084066391,
      "learning_rate": 7.960835910434942e-05,
      "loss": 2.4574,
      "step": 84350
    },
    {
      "epoch": 1.7354678447537666,
      "grad_norm": 0.45131373405456543,
      "learning_rate": 7.958661712776148e-05,
      "loss": 2.3154,
      "step": 84360
    },
    {
      "epoch": 1.7356735642706345,
      "grad_norm": 0.42543548345565796,
      "learning_rate": 7.956487615805447e-05,
      "loss": 2.3706,
      "step": 84370
    },
    {
      "epoch": 1.7358792837875021,
      "grad_norm": 0.441907674074173,
      "learning_rate": 7.954313619630073e-05,
      "loss": 2.3837,
      "step": 84380
    },
    {
      "epoch": 1.7360850033043698,
      "grad_norm": 0.4410327970981598,
      "learning_rate": 7.952139724357266e-05,
      "loss": 2.3349,
      "step": 84390
    },
    {
      "epoch": 1.7362907228212374,
      "grad_norm": 0.45810213685035706,
      "learning_rate": 7.949965930094245e-05,
      "loss": 2.3984,
      "step": 84400
    },
    {
      "epoch": 1.736496442338105,
      "grad_norm": 0.46733227372169495,
      "learning_rate": 7.947792236948234e-05,
      "loss": 2.3819,
      "step": 84410
    },
    {
      "epoch": 1.7367021618549727,
      "grad_norm": 0.5443389415740967,
      "learning_rate": 7.945618645026444e-05,
      "loss": 2.3804,
      "step": 84420
    },
    {
      "epoch": 1.7369078813718406,
      "grad_norm": 0.40745311975479126,
      "learning_rate": 7.943445154436096e-05,
      "loss": 2.4279,
      "step": 84430
    },
    {
      "epoch": 1.7371136008887085,
      "grad_norm": 0.4227984547615051,
      "learning_rate": 7.941271765284392e-05,
      "loss": 2.3655,
      "step": 84440
    },
    {
      "epoch": 1.7373193204055761,
      "grad_norm": 0.4443696439266205,
      "learning_rate": 7.939098477678529e-05,
      "loss": 2.38,
      "step": 84450
    },
    {
      "epoch": 1.7375250399224438,
      "grad_norm": 0.4590812623500824,
      "learning_rate": 7.93692529172571e-05,
      "loss": 2.3824,
      "step": 84460
    },
    {
      "epoch": 1.7377307594393114,
      "grad_norm": 0.4727169871330261,
      "learning_rate": 7.934752207533125e-05,
      "loss": 2.3673,
      "step": 84470
    },
    {
      "epoch": 1.737936478956179,
      "grad_norm": 0.45448899269104004,
      "learning_rate": 7.932579225207955e-05,
      "loss": 2.4023,
      "step": 84480
    },
    {
      "epoch": 1.7381421984730467,
      "grad_norm": 0.4327588677406311,
      "learning_rate": 7.930406344857392e-05,
      "loss": 2.3881,
      "step": 84490
    },
    {
      "epoch": 1.7383479179899146,
      "grad_norm": 0.42622390389442444,
      "learning_rate": 7.928233566588603e-05,
      "loss": 2.3847,
      "step": 84500
    },
    {
      "epoch": 1.7385536375067823,
      "grad_norm": 0.44089093804359436,
      "learning_rate": 7.92606089050876e-05,
      "loss": 2.3946,
      "step": 84510
    },
    {
      "epoch": 1.7387593570236501,
      "grad_norm": 0.6020872592926025,
      "learning_rate": 7.923888316725035e-05,
      "loss": 2.4021,
      "step": 84520
    },
    {
      "epoch": 1.7389650765405178,
      "grad_norm": 0.41088905930519104,
      "learning_rate": 7.921715845344586e-05,
      "loss": 2.3947,
      "step": 84530
    },
    {
      "epoch": 1.7391707960573854,
      "grad_norm": 0.401806503534317,
      "learning_rate": 7.919543476474565e-05,
      "loss": 2.4204,
      "step": 84540
    },
    {
      "epoch": 1.739376515574253,
      "grad_norm": 0.41118428111076355,
      "learning_rate": 7.917371210222132e-05,
      "loss": 2.3952,
      "step": 84550
    },
    {
      "epoch": 1.7395822350911208,
      "grad_norm": 0.5048223733901978,
      "learning_rate": 7.915199046694426e-05,
      "loss": 2.4432,
      "step": 84560
    },
    {
      "epoch": 1.7397879546079886,
      "grad_norm": 0.5065500736236572,
      "learning_rate": 7.913026985998593e-05,
      "loss": 2.4162,
      "step": 84570
    },
    {
      "epoch": 1.7399936741248563,
      "grad_norm": 0.4524672031402588,
      "learning_rate": 7.910855028241762e-05,
      "loss": 2.3918,
      "step": 84580
    },
    {
      "epoch": 1.7401993936417242,
      "grad_norm": 0.45630085468292236,
      "learning_rate": 7.908683173531068e-05,
      "loss": 2.3996,
      "step": 84590
    },
    {
      "epoch": 1.7404051131585918,
      "grad_norm": 0.42080676555633545,
      "learning_rate": 7.906511421973641e-05,
      "loss": 2.3487,
      "step": 84600
    },
    {
      "epoch": 1.7406108326754595,
      "grad_norm": 0.42824679613113403,
      "learning_rate": 7.904339773676592e-05,
      "loss": 2.3917,
      "step": 84610
    },
    {
      "epoch": 1.7408165521923271,
      "grad_norm": 0.48557814955711365,
      "learning_rate": 7.902168228747046e-05,
      "loss": 2.4255,
      "step": 84620
    },
    {
      "epoch": 1.7410222717091948,
      "grad_norm": 0.4560944437980652,
      "learning_rate": 7.89999678729211e-05,
      "loss": 2.3715,
      "step": 84630
    },
    {
      "epoch": 1.7412279912260626,
      "grad_norm": 0.4337458312511444,
      "learning_rate": 7.897825449418885e-05,
      "loss": 2.3828,
      "step": 84640
    },
    {
      "epoch": 1.7414337107429303,
      "grad_norm": 0.46823763847351074,
      "learning_rate": 7.89565421523448e-05,
      "loss": 2.408,
      "step": 84650
    },
    {
      "epoch": 1.7416394302597982,
      "grad_norm": 0.45372095704078674,
      "learning_rate": 7.89348308484598e-05,
      "loss": 2.4057,
      "step": 84660
    },
    {
      "epoch": 1.7418451497766658,
      "grad_norm": 0.4516531825065613,
      "learning_rate": 7.891312058360481e-05,
      "loss": 2.3809,
      "step": 84670
    },
    {
      "epoch": 1.7420508692935335,
      "grad_norm": 0.49130314588546753,
      "learning_rate": 7.889141135885072e-05,
      "loss": 2.3687,
      "step": 84680
    },
    {
      "epoch": 1.7422565888104011,
      "grad_norm": 0.4325880706310272,
      "learning_rate": 7.886970317526825e-05,
      "loss": 2.3667,
      "step": 84690
    },
    {
      "epoch": 1.7424623083272688,
      "grad_norm": 0.4699268341064453,
      "learning_rate": 7.884799603392815e-05,
      "loss": 2.3556,
      "step": 84700
    },
    {
      "epoch": 1.7426680278441367,
      "grad_norm": 0.45842245221138,
      "learning_rate": 7.882628993590118e-05,
      "loss": 2.4658,
      "step": 84710
    },
    {
      "epoch": 1.7428737473610043,
      "grad_norm": 0.45360374450683594,
      "learning_rate": 7.880458488225793e-05,
      "loss": 2.4559,
      "step": 84720
    },
    {
      "epoch": 1.7430794668778722,
      "grad_norm": 0.4265424907207489,
      "learning_rate": 7.878288087406903e-05,
      "loss": 2.4507,
      "step": 84730
    },
    {
      "epoch": 1.7432851863947398,
      "grad_norm": 0.4415615200996399,
      "learning_rate": 7.876117791240494e-05,
      "loss": 2.3698,
      "step": 84740
    },
    {
      "epoch": 1.7434909059116075,
      "grad_norm": 0.41336044669151306,
      "learning_rate": 7.873947599833625e-05,
      "loss": 2.3852,
      "step": 84750
    },
    {
      "epoch": 1.7436966254284751,
      "grad_norm": 0.4638722538948059,
      "learning_rate": 7.871777513293337e-05,
      "loss": 2.4414,
      "step": 84760
    },
    {
      "epoch": 1.7439023449453428,
      "grad_norm": 0.46189185976982117,
      "learning_rate": 7.869607531726662e-05,
      "loss": 2.4161,
      "step": 84770
    },
    {
      "epoch": 1.7441080644622105,
      "grad_norm": 0.41763490438461304,
      "learning_rate": 7.867437655240639e-05,
      "loss": 2.432,
      "step": 84780
    },
    {
      "epoch": 1.7443137839790783,
      "grad_norm": 0.42009684443473816,
      "learning_rate": 7.865267883942299e-05,
      "loss": 2.3966,
      "step": 84790
    },
    {
      "epoch": 1.7445195034959462,
      "grad_norm": 0.460078626871109,
      "learning_rate": 7.863098217938657e-05,
      "loss": 2.4117,
      "step": 84800
    },
    {
      "epoch": 1.7447252230128139,
      "grad_norm": 0.45180314779281616,
      "learning_rate": 7.860928657336739e-05,
      "loss": 2.404,
      "step": 84810
    },
    {
      "epoch": 1.7449309425296815,
      "grad_norm": 0.4133144021034241,
      "learning_rate": 7.858759202243554e-05,
      "loss": 2.341,
      "step": 84820
    },
    {
      "epoch": 1.7451366620465492,
      "grad_norm": 0.5695521235466003,
      "learning_rate": 7.856589852766104e-05,
      "loss": 2.4688,
      "step": 84830
    },
    {
      "epoch": 1.7453423815634168,
      "grad_norm": 0.4089541733264923,
      "learning_rate": 7.854420609011401e-05,
      "loss": 2.4047,
      "step": 84840
    },
    {
      "epoch": 1.7455481010802845,
      "grad_norm": 0.5841715931892395,
      "learning_rate": 7.852251471086436e-05,
      "loss": 2.3126,
      "step": 84850
    },
    {
      "epoch": 1.7457538205971523,
      "grad_norm": 0.45708051323890686,
      "learning_rate": 7.8500824390982e-05,
      "loss": 2.398,
      "step": 84860
    },
    {
      "epoch": 1.74595954011402,
      "grad_norm": 0.44249364733695984,
      "learning_rate": 7.847913513153687e-05,
      "loss": 2.4538,
      "step": 84870
    },
    {
      "epoch": 1.7461652596308879,
      "grad_norm": 0.5536108613014221,
      "learning_rate": 7.845744693359869e-05,
      "loss": 2.4238,
      "step": 84880
    },
    {
      "epoch": 1.7463709791477555,
      "grad_norm": 0.4478224813938141,
      "learning_rate": 7.843575979823728e-05,
      "loss": 2.36,
      "step": 84890
    },
    {
      "epoch": 1.7465766986646232,
      "grad_norm": 0.4533942937850952,
      "learning_rate": 7.84140737265223e-05,
      "loss": 2.3274,
      "step": 84900
    },
    {
      "epoch": 1.7467824181814908,
      "grad_norm": 0.44882485270500183,
      "learning_rate": 7.839238871952345e-05,
      "loss": 2.399,
      "step": 84910
    },
    {
      "epoch": 1.7469881376983585,
      "grad_norm": 0.4180455803871155,
      "learning_rate": 7.837070477831033e-05,
      "loss": 2.4098,
      "step": 84920
    },
    {
      "epoch": 1.7471938572152264,
      "grad_norm": 0.41526487469673157,
      "learning_rate": 7.834902190395245e-05,
      "loss": 2.3831,
      "step": 84930
    },
    {
      "epoch": 1.747399576732094,
      "grad_norm": 0.45951375365257263,
      "learning_rate": 7.832734009751934e-05,
      "loss": 2.4118,
      "step": 84940
    },
    {
      "epoch": 1.7476052962489619,
      "grad_norm": 0.4746122360229492,
      "learning_rate": 7.830565936008047e-05,
      "loss": 2.4704,
      "step": 84950
    },
    {
      "epoch": 1.7478110157658295,
      "grad_norm": 0.43981531262397766,
      "learning_rate": 7.828397969270517e-05,
      "loss": 2.4174,
      "step": 84960
    },
    {
      "epoch": 1.7480167352826972,
      "grad_norm": 0.41745898127555847,
      "learning_rate": 7.826230109646284e-05,
      "loss": 2.4537,
      "step": 84970
    },
    {
      "epoch": 1.7482224547995648,
      "grad_norm": 0.4144018590450287,
      "learning_rate": 7.824062357242274e-05,
      "loss": 2.3758,
      "step": 84980
    },
    {
      "epoch": 1.7484281743164325,
      "grad_norm": 0.4272163510322571,
      "learning_rate": 7.821894712165407e-05,
      "loss": 2.3879,
      "step": 84990
    },
    {
      "epoch": 1.7486338938333004,
      "grad_norm": 0.45270559191703796,
      "learning_rate": 7.819727174522609e-05,
      "loss": 2.3746,
      "step": 85000
    },
    {
      "epoch": 1.748839613350168,
      "grad_norm": 0.4284489154815674,
      "learning_rate": 7.817559744420786e-05,
      "loss": 2.3531,
      "step": 85010
    },
    {
      "epoch": 1.749045332867036,
      "grad_norm": 0.3973748981952667,
      "learning_rate": 7.815392421966846e-05,
      "loss": 2.3509,
      "step": 85020
    },
    {
      "epoch": 1.7492510523839035,
      "grad_norm": 0.4857757091522217,
      "learning_rate": 7.813225207267698e-05,
      "loss": 2.4184,
      "step": 85030
    },
    {
      "epoch": 1.7494567719007712,
      "grad_norm": 0.41459816694259644,
      "learning_rate": 7.811058100430231e-05,
      "loss": 2.3837,
      "step": 85040
    },
    {
      "epoch": 1.7496624914176389,
      "grad_norm": 0.43962204456329346,
      "learning_rate": 7.808891101561341e-05,
      "loss": 2.391,
      "step": 85050
    },
    {
      "epoch": 1.7498682109345065,
      "grad_norm": 0.4254012703895569,
      "learning_rate": 7.80672421076791e-05,
      "loss": 2.3731,
      "step": 85060
    },
    {
      "epoch": 1.7500739304513744,
      "grad_norm": 0.4899907112121582,
      "learning_rate": 7.804557428156825e-05,
      "loss": 2.3773,
      "step": 85070
    },
    {
      "epoch": 1.750279649968242,
      "grad_norm": 0.42596277594566345,
      "learning_rate": 7.802390753834959e-05,
      "loss": 2.3891,
      "step": 85080
    },
    {
      "epoch": 1.75048536948511,
      "grad_norm": 0.45535337924957275,
      "learning_rate": 7.800224187909178e-05,
      "loss": 2.4103,
      "step": 85090
    },
    {
      "epoch": 1.7506910890019776,
      "grad_norm": 0.4328348934650421,
      "learning_rate": 7.798057730486351e-05,
      "loss": 2.4318,
      "step": 85100
    },
    {
      "epoch": 1.7508968085188452,
      "grad_norm": 0.5030125975608826,
      "learning_rate": 7.79589138167334e-05,
      "loss": 2.3788,
      "step": 85110
    },
    {
      "epoch": 1.7511025280357129,
      "grad_norm": 0.45372456312179565,
      "learning_rate": 7.793725141576993e-05,
      "loss": 2.3704,
      "step": 85120
    },
    {
      "epoch": 1.7513082475525805,
      "grad_norm": 0.40988102555274963,
      "learning_rate": 7.791559010304164e-05,
      "loss": 2.3782,
      "step": 85130
    },
    {
      "epoch": 1.7515139670694482,
      "grad_norm": 0.44230005145072937,
      "learning_rate": 7.789392987961695e-05,
      "loss": 2.4592,
      "step": 85140
    },
    {
      "epoch": 1.751719686586316,
      "grad_norm": 0.4334588348865509,
      "learning_rate": 7.78722707465642e-05,
      "loss": 2.4083,
      "step": 85150
    },
    {
      "epoch": 1.751925406103184,
      "grad_norm": 0.4617113769054413,
      "learning_rate": 7.785061270495179e-05,
      "loss": 2.3667,
      "step": 85160
    },
    {
      "epoch": 1.7521311256200516,
      "grad_norm": 0.4697672724723816,
      "learning_rate": 7.782895575584795e-05,
      "loss": 2.4047,
      "step": 85170
    },
    {
      "epoch": 1.7523368451369192,
      "grad_norm": 0.41039636731147766,
      "learning_rate": 7.780729990032087e-05,
      "loss": 2.3459,
      "step": 85180
    },
    {
      "epoch": 1.7525425646537869,
      "grad_norm": 0.46029630303382874,
      "learning_rate": 7.77856451394388e-05,
      "loss": 2.4213,
      "step": 85190
    },
    {
      "epoch": 1.7527482841706545,
      "grad_norm": 0.4403567612171173,
      "learning_rate": 7.776399147426977e-05,
      "loss": 2.3855,
      "step": 85200
    },
    {
      "epoch": 1.7529540036875222,
      "grad_norm": 0.4373306334018707,
      "learning_rate": 7.774233890588189e-05,
      "loss": 2.3817,
      "step": 85210
    },
    {
      "epoch": 1.75315972320439,
      "grad_norm": 0.43841272592544556,
      "learning_rate": 7.772068743534311e-05,
      "loss": 2.4435,
      "step": 85220
    },
    {
      "epoch": 1.7533654427212577,
      "grad_norm": 0.44666188955307007,
      "learning_rate": 7.769903706372144e-05,
      "loss": 2.333,
      "step": 85230
    },
    {
      "epoch": 1.7535711622381256,
      "grad_norm": 0.4709462821483612,
      "learning_rate": 7.767738779208475e-05,
      "loss": 2.3917,
      "step": 85240
    },
    {
      "epoch": 1.7537768817549932,
      "grad_norm": 0.4161392152309418,
      "learning_rate": 7.765573962150083e-05,
      "loss": 2.3811,
      "step": 85250
    },
    {
      "epoch": 1.753982601271861,
      "grad_norm": 0.44329333305358887,
      "learning_rate": 7.763409255303755e-05,
      "loss": 2.416,
      "step": 85260
    },
    {
      "epoch": 1.7541883207887286,
      "grad_norm": 0.51023930311203,
      "learning_rate": 7.761244658776262e-05,
      "loss": 2.4288,
      "step": 85270
    },
    {
      "epoch": 1.7543940403055962,
      "grad_norm": 0.5237686634063721,
      "learning_rate": 7.759080172674366e-05,
      "loss": 2.4441,
      "step": 85280
    },
    {
      "epoch": 1.754599759822464,
      "grad_norm": 0.457133024930954,
      "learning_rate": 7.756915797104838e-05,
      "loss": 2.4032,
      "step": 85290
    },
    {
      "epoch": 1.7548054793393317,
      "grad_norm": 0.4373343586921692,
      "learning_rate": 7.754751532174428e-05,
      "loss": 2.3318,
      "step": 85300
    },
    {
      "epoch": 1.7550111988561996,
      "grad_norm": 0.5959612727165222,
      "learning_rate": 7.752587377989889e-05,
      "loss": 2.4004,
      "step": 85310
    },
    {
      "epoch": 1.7552169183730673,
      "grad_norm": 0.416753888130188,
      "learning_rate": 7.75042333465797e-05,
      "loss": 2.3584,
      "step": 85320
    },
    {
      "epoch": 1.755422637889935,
      "grad_norm": 0.4409542977809906,
      "learning_rate": 7.748259402285407e-05,
      "loss": 2.4652,
      "step": 85330
    },
    {
      "epoch": 1.7556283574068026,
      "grad_norm": 0.4479043781757355,
      "learning_rate": 7.746095580978936e-05,
      "loss": 2.4533,
      "step": 85340
    },
    {
      "epoch": 1.7558340769236702,
      "grad_norm": 0.42812487483024597,
      "learning_rate": 7.743931870845291e-05,
      "loss": 2.3848,
      "step": 85350
    },
    {
      "epoch": 1.756039796440538,
      "grad_norm": 0.41991350054740906,
      "learning_rate": 7.741768271991192e-05,
      "loss": 2.3773,
      "step": 85360
    },
    {
      "epoch": 1.7562455159574057,
      "grad_norm": 0.4266620874404907,
      "learning_rate": 7.73960478452336e-05,
      "loss": 2.3905,
      "step": 85370
    },
    {
      "epoch": 1.7564512354742736,
      "grad_norm": 0.4291110634803772,
      "learning_rate": 7.737441408548501e-05,
      "loss": 2.4093,
      "step": 85380
    },
    {
      "epoch": 1.7566569549911413,
      "grad_norm": 0.44095006585121155,
      "learning_rate": 7.735278144173332e-05,
      "loss": 2.3777,
      "step": 85390
    },
    {
      "epoch": 1.756862674508009,
      "grad_norm": 0.4508112072944641,
      "learning_rate": 7.733114991504552e-05,
      "loss": 2.4189,
      "step": 85400
    },
    {
      "epoch": 1.7570683940248766,
      "grad_norm": 0.505436360836029,
      "learning_rate": 7.73095195064885e-05,
      "loss": 2.4603,
      "step": 85410
    },
    {
      "epoch": 1.7572741135417442,
      "grad_norm": 0.4043220281600952,
      "learning_rate": 7.72878902171293e-05,
      "loss": 2.3885,
      "step": 85420
    },
    {
      "epoch": 1.757479833058612,
      "grad_norm": 0.45151486992836,
      "learning_rate": 7.726626204803468e-05,
      "loss": 2.4295,
      "step": 85430
    },
    {
      "epoch": 1.7576855525754798,
      "grad_norm": 0.4457261562347412,
      "learning_rate": 7.724463500027146e-05,
      "loss": 2.372,
      "step": 85440
    },
    {
      "epoch": 1.7578912720923476,
      "grad_norm": 0.42415881156921387,
      "learning_rate": 7.722300907490642e-05,
      "loss": 2.361,
      "step": 85450
    },
    {
      "epoch": 1.7580969916092153,
      "grad_norm": 0.43281105160713196,
      "learning_rate": 7.720138427300622e-05,
      "loss": 2.3981,
      "step": 85460
    },
    {
      "epoch": 1.758302711126083,
      "grad_norm": 0.42285895347595215,
      "learning_rate": 7.717976059563746e-05,
      "loss": 2.3331,
      "step": 85470
    },
    {
      "epoch": 1.7585084306429506,
      "grad_norm": 0.4130406379699707,
      "learning_rate": 7.71581380438668e-05,
      "loss": 2.4227,
      "step": 85480
    },
    {
      "epoch": 1.7587141501598182,
      "grad_norm": 0.4341704547405243,
      "learning_rate": 7.71365166187607e-05,
      "loss": 2.3932,
      "step": 85490
    },
    {
      "epoch": 1.758919869676686,
      "grad_norm": 0.43181362748146057,
      "learning_rate": 7.711489632138566e-05,
      "loss": 2.395,
      "step": 85500
    },
    {
      "epoch": 1.7591255891935538,
      "grad_norm": 0.44655516743659973,
      "learning_rate": 7.709327715280804e-05,
      "loss": 2.4183,
      "step": 85510
    },
    {
      "epoch": 1.7593313087104216,
      "grad_norm": 0.45312491059303284,
      "learning_rate": 7.707165911409426e-05,
      "loss": 2.4305,
      "step": 85520
    },
    {
      "epoch": 1.7595370282272893,
      "grad_norm": 0.4139205813407898,
      "learning_rate": 7.705004220631059e-05,
      "loss": 2.4308,
      "step": 85530
    },
    {
      "epoch": 1.759742747744157,
      "grad_norm": 0.42665934562683105,
      "learning_rate": 7.702842643052325e-05,
      "loss": 2.3239,
      "step": 85540
    },
    {
      "epoch": 1.7599484672610246,
      "grad_norm": 0.48071160912513733,
      "learning_rate": 7.700681178779846e-05,
      "loss": 2.4118,
      "step": 85550
    },
    {
      "epoch": 1.7601541867778923,
      "grad_norm": 0.4519008696079254,
      "learning_rate": 7.698519827920239e-05,
      "loss": 2.3951,
      "step": 85560
    },
    {
      "epoch": 1.76035990629476,
      "grad_norm": 0.42787477374076843,
      "learning_rate": 7.696358590580102e-05,
      "loss": 2.4304,
      "step": 85570
    },
    {
      "epoch": 1.7605656258116278,
      "grad_norm": 0.4687090218067169,
      "learning_rate": 7.694197466866046e-05,
      "loss": 2.4059,
      "step": 85580
    },
    {
      "epoch": 1.7607713453284954,
      "grad_norm": 0.4469675123691559,
      "learning_rate": 7.692036456884662e-05,
      "loss": 2.4495,
      "step": 85590
    },
    {
      "epoch": 1.7609770648453633,
      "grad_norm": 0.43398430943489075,
      "learning_rate": 7.68987556074254e-05,
      "loss": 2.4106,
      "step": 85600
    },
    {
      "epoch": 1.761182784362231,
      "grad_norm": 0.4052842855453491,
      "learning_rate": 7.687714778546273e-05,
      "loss": 2.4347,
      "step": 85610
    },
    {
      "epoch": 1.7613885038790986,
      "grad_norm": 0.4016619026660919,
      "learning_rate": 7.685554110402433e-05,
      "loss": 2.3966,
      "step": 85620
    },
    {
      "epoch": 1.7615942233959663,
      "grad_norm": 0.45212322473526,
      "learning_rate": 7.683393556417595e-05,
      "loss": 2.3856,
      "step": 85630
    },
    {
      "epoch": 1.761799942912834,
      "grad_norm": 0.43816080689430237,
      "learning_rate": 7.681233116698333e-05,
      "loss": 2.3707,
      "step": 85640
    },
    {
      "epoch": 1.7620056624297018,
      "grad_norm": 0.44577956199645996,
      "learning_rate": 7.679072791351203e-05,
      "loss": 2.351,
      "step": 85650
    },
    {
      "epoch": 1.7622113819465695,
      "grad_norm": 0.4846135973930359,
      "learning_rate": 7.676912580482768e-05,
      "loss": 2.4045,
      "step": 85660
    },
    {
      "epoch": 1.7624171014634373,
      "grad_norm": 0.4220319986343384,
      "learning_rate": 7.674752484199571e-05,
      "loss": 2.3783,
      "step": 85670
    },
    {
      "epoch": 1.762622820980305,
      "grad_norm": 0.41524791717529297,
      "learning_rate": 7.672592502608165e-05,
      "loss": 2.4235,
      "step": 85680
    },
    {
      "epoch": 1.7628285404971726,
      "grad_norm": 0.4172911047935486,
      "learning_rate": 7.67043263581509e-05,
      "loss": 2.4641,
      "step": 85690
    },
    {
      "epoch": 1.7630342600140403,
      "grad_norm": 0.4122879207134247,
      "learning_rate": 7.668272883926873e-05,
      "loss": 2.4014,
      "step": 85700
    },
    {
      "epoch": 1.763239979530908,
      "grad_norm": 0.4256889224052429,
      "learning_rate": 7.666113247050053e-05,
      "loss": 2.4183,
      "step": 85710
    },
    {
      "epoch": 1.7634456990477758,
      "grad_norm": 0.42490944266319275,
      "learning_rate": 7.663953725291148e-05,
      "loss": 2.3705,
      "step": 85720
    },
    {
      "epoch": 1.7636514185646435,
      "grad_norm": 0.457849383354187,
      "learning_rate": 7.661794318756673e-05,
      "loss": 2.4155,
      "step": 85730
    },
    {
      "epoch": 1.7638571380815113,
      "grad_norm": 0.46039023995399475,
      "learning_rate": 7.659635027553146e-05,
      "loss": 2.3777,
      "step": 85740
    },
    {
      "epoch": 1.764062857598379,
      "grad_norm": 0.4266442358493805,
      "learning_rate": 7.65747585178707e-05,
      "loss": 2.3731,
      "step": 85750
    },
    {
      "epoch": 1.7642685771152467,
      "grad_norm": 0.46528661251068115,
      "learning_rate": 7.65531679156494e-05,
      "loss": 2.3806,
      "step": 85760
    },
    {
      "epoch": 1.7644742966321143,
      "grad_norm": 0.4381188452243805,
      "learning_rate": 7.65315784699326e-05,
      "loss": 2.4083,
      "step": 85770
    },
    {
      "epoch": 1.764680016148982,
      "grad_norm": 0.4427553713321686,
      "learning_rate": 7.650999018178514e-05,
      "loss": 2.371,
      "step": 85780
    },
    {
      "epoch": 1.7648857356658498,
      "grad_norm": 0.4412307143211365,
      "learning_rate": 7.648840305227183e-05,
      "loss": 2.4177,
      "step": 85790
    },
    {
      "epoch": 1.7650914551827175,
      "grad_norm": 0.4231175482273102,
      "learning_rate": 7.646681708245752e-05,
      "loss": 2.3667,
      "step": 85800
    },
    {
      "epoch": 1.7652971746995854,
      "grad_norm": 0.4704482853412628,
      "learning_rate": 7.644523227340687e-05,
      "loss": 2.369,
      "step": 85810
    },
    {
      "epoch": 1.765502894216453,
      "grad_norm": 0.418850302696228,
      "learning_rate": 7.642364862618455e-05,
      "loss": 2.4597,
      "step": 85820
    },
    {
      "epoch": 1.7657086137333207,
      "grad_norm": 0.45451638102531433,
      "learning_rate": 7.640206614185514e-05,
      "loss": 2.4174,
      "step": 85830
    },
    {
      "epoch": 1.7659143332501883,
      "grad_norm": 0.5022757053375244,
      "learning_rate": 7.638048482148325e-05,
      "loss": 2.3622,
      "step": 85840
    },
    {
      "epoch": 1.766120052767056,
      "grad_norm": 0.4042636752128601,
      "learning_rate": 7.635890466613334e-05,
      "loss": 2.3996,
      "step": 85850
    },
    {
      "epoch": 1.7663257722839238,
      "grad_norm": 0.4426240622997284,
      "learning_rate": 7.633732567686977e-05,
      "loss": 2.335,
      "step": 85860
    },
    {
      "epoch": 1.7665314918007915,
      "grad_norm": 0.4467671513557434,
      "learning_rate": 7.631574785475703e-05,
      "loss": 2.4052,
      "step": 85870
    },
    {
      "epoch": 1.7667372113176594,
      "grad_norm": 0.45918774604797363,
      "learning_rate": 7.62941712008594e-05,
      "loss": 2.3822,
      "step": 85880
    },
    {
      "epoch": 1.766942930834527,
      "grad_norm": 0.4114369750022888,
      "learning_rate": 7.627259571624108e-05,
      "loss": 2.3851,
      "step": 85890
    },
    {
      "epoch": 1.7671486503513947,
      "grad_norm": 0.4594259262084961,
      "learning_rate": 7.625102140196635e-05,
      "loss": 2.4601,
      "step": 85900
    },
    {
      "epoch": 1.7673543698682623,
      "grad_norm": 0.45606428384780884,
      "learning_rate": 7.62294482590993e-05,
      "loss": 2.4394,
      "step": 85910
    },
    {
      "epoch": 1.76756008938513,
      "grad_norm": 0.492109090089798,
      "learning_rate": 7.620787628870402e-05,
      "loss": 2.384,
      "step": 85920
    },
    {
      "epoch": 1.7677658089019976,
      "grad_norm": 0.415907621383667,
      "learning_rate": 7.618630549184458e-05,
      "loss": 2.4189,
      "step": 85930
    },
    {
      "epoch": 1.7679715284188655,
      "grad_norm": 0.44171860814094543,
      "learning_rate": 7.61647358695849e-05,
      "loss": 2.3979,
      "step": 85940
    },
    {
      "epoch": 1.7681772479357334,
      "grad_norm": 0.47623685002326965,
      "learning_rate": 7.61431674229889e-05,
      "loss": 2.3481,
      "step": 85950
    },
    {
      "epoch": 1.768382967452601,
      "grad_norm": 0.4766377806663513,
      "learning_rate": 7.61216001531205e-05,
      "loss": 2.3276,
      "step": 85960
    },
    {
      "epoch": 1.7685886869694687,
      "grad_norm": 0.4700416326522827,
      "learning_rate": 7.610003406104341e-05,
      "loss": 2.3714,
      "step": 85970
    },
    {
      "epoch": 1.7687944064863363,
      "grad_norm": 0.4156726598739624,
      "learning_rate": 7.607846914782142e-05,
      "loss": 2.3426,
      "step": 85980
    },
    {
      "epoch": 1.769000126003204,
      "grad_norm": 0.42036062479019165,
      "learning_rate": 7.605690541451815e-05,
      "loss": 2.3821,
      "step": 85990
    },
    {
      "epoch": 1.7692058455200717,
      "grad_norm": 0.7286950349807739,
      "learning_rate": 7.603534286219728e-05,
      "loss": 2.3661,
      "step": 86000
    },
    {
      "epoch": 1.7694115650369395,
      "grad_norm": 0.4404364824295044,
      "learning_rate": 7.601378149192238e-05,
      "loss": 2.3961,
      "step": 86010
    },
    {
      "epoch": 1.7696172845538072,
      "grad_norm": 0.42135676741600037,
      "learning_rate": 7.599222130475688e-05,
      "loss": 2.3679,
      "step": 86020
    },
    {
      "epoch": 1.769823004070675,
      "grad_norm": 0.432188481092453,
      "learning_rate": 7.59706623017643e-05,
      "loss": 2.3903,
      "step": 86030
    },
    {
      "epoch": 1.7700287235875427,
      "grad_norm": 0.42402660846710205,
      "learning_rate": 7.594910448400802e-05,
      "loss": 2.3198,
      "step": 86040
    },
    {
      "epoch": 1.7702344431044104,
      "grad_norm": 0.40553900599479675,
      "learning_rate": 7.592754785255132e-05,
      "loss": 2.3408,
      "step": 86050
    },
    {
      "epoch": 1.770440162621278,
      "grad_norm": 0.4688425064086914,
      "learning_rate": 7.590599240845756e-05,
      "loss": 2.3659,
      "step": 86060
    },
    {
      "epoch": 1.7706458821381457,
      "grad_norm": 0.40307360887527466,
      "learning_rate": 7.588443815278985e-05,
      "loss": 2.3838,
      "step": 86070
    },
    {
      "epoch": 1.7708516016550135,
      "grad_norm": 0.46418359875679016,
      "learning_rate": 7.58628850866114e-05,
      "loss": 2.4339,
      "step": 86080
    },
    {
      "epoch": 1.7710573211718812,
      "grad_norm": 0.42762085795402527,
      "learning_rate": 7.584133321098533e-05,
      "loss": 2.3922,
      "step": 86090
    },
    {
      "epoch": 1.771263040688749,
      "grad_norm": 0.44523701071739197,
      "learning_rate": 7.581978252697462e-05,
      "loss": 2.4782,
      "step": 86100
    },
    {
      "epoch": 1.7714687602056167,
      "grad_norm": 0.43930891156196594,
      "learning_rate": 7.579823303564224e-05,
      "loss": 2.4016,
      "step": 86110
    },
    {
      "epoch": 1.7716744797224844,
      "grad_norm": 0.4144197702407837,
      "learning_rate": 7.57766847380512e-05,
      "loss": 2.4049,
      "step": 86120
    },
    {
      "epoch": 1.771880199239352,
      "grad_norm": 0.4301709532737732,
      "learning_rate": 7.575513763526427e-05,
      "loss": 2.3668,
      "step": 86130
    },
    {
      "epoch": 1.7720859187562197,
      "grad_norm": 0.4167874753475189,
      "learning_rate": 7.573359172834429e-05,
      "loss": 2.4003,
      "step": 86140
    },
    {
      "epoch": 1.7722916382730876,
      "grad_norm": 0.4405995011329651,
      "learning_rate": 7.571204701835397e-05,
      "loss": 2.4381,
      "step": 86150
    },
    {
      "epoch": 1.7724973577899552,
      "grad_norm": 0.4661315977573395,
      "learning_rate": 7.569050350635602e-05,
      "loss": 2.4538,
      "step": 86160
    },
    {
      "epoch": 1.772703077306823,
      "grad_norm": 0.473124235868454,
      "learning_rate": 7.56689611934131e-05,
      "loss": 2.3243,
      "step": 86170
    },
    {
      "epoch": 1.7729087968236907,
      "grad_norm": 0.48484575748443604,
      "learning_rate": 7.56474200805877e-05,
      "loss": 2.4328,
      "step": 86180
    },
    {
      "epoch": 1.7731145163405584,
      "grad_norm": 0.4299454689025879,
      "learning_rate": 7.562588016894236e-05,
      "loss": 2.4228,
      "step": 86190
    },
    {
      "epoch": 1.773320235857426,
      "grad_norm": 0.5084789395332336,
      "learning_rate": 7.560434145953956e-05,
      "loss": 2.3903,
      "step": 86200
    },
    {
      "epoch": 1.7735259553742937,
      "grad_norm": 0.516292154788971,
      "learning_rate": 7.558280395344162e-05,
      "loss": 2.4817,
      "step": 86210
    },
    {
      "epoch": 1.7737316748911616,
      "grad_norm": 0.4932178258895874,
      "learning_rate": 7.556126765171094e-05,
      "loss": 2.3968,
      "step": 86220
    },
    {
      "epoch": 1.7739373944080292,
      "grad_norm": 0.41668078303337097,
      "learning_rate": 7.553973255540973e-05,
      "loss": 2.3243,
      "step": 86230
    },
    {
      "epoch": 1.774143113924897,
      "grad_norm": 0.4388616681098938,
      "learning_rate": 7.551819866560021e-05,
      "loss": 2.4539,
      "step": 86240
    },
    {
      "epoch": 1.7743488334417648,
      "grad_norm": 0.4995209276676178,
      "learning_rate": 7.549666598334456e-05,
      "loss": 2.4101,
      "step": 86250
    },
    {
      "epoch": 1.7745545529586324,
      "grad_norm": 0.5033845901489258,
      "learning_rate": 7.547513450970485e-05,
      "loss": 2.4367,
      "step": 86260
    },
    {
      "epoch": 1.7747602724755,
      "grad_norm": 0.3894732594490051,
      "learning_rate": 7.545360424574308e-05,
      "loss": 2.4052,
      "step": 86270
    },
    {
      "epoch": 1.7749659919923677,
      "grad_norm": 0.49393707513809204,
      "learning_rate": 7.54320751925213e-05,
      "loss": 2.3414,
      "step": 86280
    },
    {
      "epoch": 1.7751717115092354,
      "grad_norm": 0.4474453926086426,
      "learning_rate": 7.541054735110133e-05,
      "loss": 2.3875,
      "step": 86290
    },
    {
      "epoch": 1.7753774310261032,
      "grad_norm": 0.4336961805820465,
      "learning_rate": 7.53890207225451e-05,
      "loss": 2.4193,
      "step": 86300
    },
    {
      "epoch": 1.7755831505429711,
      "grad_norm": 0.512824296951294,
      "learning_rate": 7.536749530791431e-05,
      "loss": 2.4264,
      "step": 86310
    },
    {
      "epoch": 1.7757888700598388,
      "grad_norm": 0.4468260705471039,
      "learning_rate": 7.534597110827079e-05,
      "loss": 2.3652,
      "step": 86320
    },
    {
      "epoch": 1.7759945895767064,
      "grad_norm": 0.43755242228507996,
      "learning_rate": 7.532444812467616e-05,
      "loss": 2.3324,
      "step": 86330
    },
    {
      "epoch": 1.776200309093574,
      "grad_norm": 0.4110073745250702,
      "learning_rate": 7.530292635819201e-05,
      "loss": 2.3933,
      "step": 86340
    },
    {
      "epoch": 1.7764060286104417,
      "grad_norm": 0.40789201855659485,
      "learning_rate": 7.528140580987995e-05,
      "loss": 2.3932,
      "step": 86350
    },
    {
      "epoch": 1.7766117481273094,
      "grad_norm": 0.42816004157066345,
      "learning_rate": 7.525988648080145e-05,
      "loss": 2.3925,
      "step": 86360
    },
    {
      "epoch": 1.7768174676441773,
      "grad_norm": 0.4019165635108948,
      "learning_rate": 7.52383683720179e-05,
      "loss": 2.4247,
      "step": 86370
    },
    {
      "epoch": 1.777023187161045,
      "grad_norm": 0.4412391781806946,
      "learning_rate": 7.521685148459073e-05,
      "loss": 2.3681,
      "step": 86380
    },
    {
      "epoch": 1.7772289066779128,
      "grad_norm": 0.4668397903442383,
      "learning_rate": 7.519533581958121e-05,
      "loss": 2.3427,
      "step": 86390
    },
    {
      "epoch": 1.7774346261947804,
      "grad_norm": 0.41123533248901367,
      "learning_rate": 7.51738213780506e-05,
      "loss": 2.3919,
      "step": 86400
    },
    {
      "epoch": 1.777640345711648,
      "grad_norm": 0.5162162184715271,
      "learning_rate": 7.515230816106012e-05,
      "loss": 2.3855,
      "step": 86410
    },
    {
      "epoch": 1.7778460652285157,
      "grad_norm": 0.437638521194458,
      "learning_rate": 7.513079616967085e-05,
      "loss": 2.4143,
      "step": 86420
    },
    {
      "epoch": 1.7780517847453834,
      "grad_norm": 0.41675737500190735,
      "learning_rate": 7.510928540494386e-05,
      "loss": 2.4639,
      "step": 86430
    },
    {
      "epoch": 1.7782575042622513,
      "grad_norm": 0.40595710277557373,
      "learning_rate": 7.508777586794024e-05,
      "loss": 2.3508,
      "step": 86440
    },
    {
      "epoch": 1.778463223779119,
      "grad_norm": 0.4366699159145355,
      "learning_rate": 7.506626755972086e-05,
      "loss": 2.4102,
      "step": 86450
    },
    {
      "epoch": 1.7786689432959868,
      "grad_norm": 0.445646733045578,
      "learning_rate": 7.504476048134665e-05,
      "loss": 2.3878,
      "step": 86460
    },
    {
      "epoch": 1.7788746628128544,
      "grad_norm": 0.411729097366333,
      "learning_rate": 7.502325463387836e-05,
      "loss": 2.3587,
      "step": 86470
    },
    {
      "epoch": 1.779080382329722,
      "grad_norm": 0.39432233572006226,
      "learning_rate": 7.500175001837684e-05,
      "loss": 2.3921,
      "step": 86480
    },
    {
      "epoch": 1.7792861018465898,
      "grad_norm": 0.46078595519065857,
      "learning_rate": 7.49802466359028e-05,
      "loss": 2.3716,
      "step": 86490
    },
    {
      "epoch": 1.7794918213634574,
      "grad_norm": 0.4397335946559906,
      "learning_rate": 7.495874448751678e-05,
      "loss": 2.3421,
      "step": 86500
    },
    {
      "epoch": 1.7796975408803253,
      "grad_norm": 0.46544310450553894,
      "learning_rate": 7.49372435742795e-05,
      "loss": 2.3881,
      "step": 86510
    },
    {
      "epoch": 1.779903260397193,
      "grad_norm": 0.43576502799987793,
      "learning_rate": 7.49157438972514e-05,
      "loss": 2.3479,
      "step": 86520
    },
    {
      "epoch": 1.7801089799140608,
      "grad_norm": 0.43119367957115173,
      "learning_rate": 7.489424545749295e-05,
      "loss": 2.4068,
      "step": 86530
    },
    {
      "epoch": 1.7803146994309285,
      "grad_norm": 0.45415934920310974,
      "learning_rate": 7.487274825606458e-05,
      "loss": 2.4199,
      "step": 86540
    },
    {
      "epoch": 1.7805204189477961,
      "grad_norm": 0.4986957311630249,
      "learning_rate": 7.485125229402659e-05,
      "loss": 2.4086,
      "step": 86550
    },
    {
      "epoch": 1.7807261384646638,
      "grad_norm": 0.49851199984550476,
      "learning_rate": 7.482975757243928e-05,
      "loss": 2.3704,
      "step": 86560
    },
    {
      "epoch": 1.7809318579815314,
      "grad_norm": 0.48665550351142883,
      "learning_rate": 7.480826409236291e-05,
      "loss": 2.3883,
      "step": 86570
    },
    {
      "epoch": 1.7811375774983993,
      "grad_norm": 0.4013282358646393,
      "learning_rate": 7.478677185485753e-05,
      "loss": 2.4213,
      "step": 86580
    },
    {
      "epoch": 1.781343297015267,
      "grad_norm": 0.41094282269477844,
      "learning_rate": 7.476528086098336e-05,
      "loss": 2.4169,
      "step": 86590
    },
    {
      "epoch": 1.7815490165321348,
      "grad_norm": 0.44734126329421997,
      "learning_rate": 7.474379111180032e-05,
      "loss": 2.4377,
      "step": 86600
    },
    {
      "epoch": 1.7817547360490025,
      "grad_norm": 0.4391474723815918,
      "learning_rate": 7.472230260836846e-05,
      "loss": 2.4261,
      "step": 86610
    },
    {
      "epoch": 1.7819604555658701,
      "grad_norm": 0.4500791132450104,
      "learning_rate": 7.470081535174769e-05,
      "loss": 2.3736,
      "step": 86620
    },
    {
      "epoch": 1.7821661750827378,
      "grad_norm": 0.5096790790557861,
      "learning_rate": 7.467932934299779e-05,
      "loss": 2.3479,
      "step": 86630
    },
    {
      "epoch": 1.7823718945996054,
      "grad_norm": 0.4426034688949585,
      "learning_rate": 7.465784458317861e-05,
      "loss": 2.3916,
      "step": 86640
    },
    {
      "epoch": 1.782577614116473,
      "grad_norm": 0.4147184193134308,
      "learning_rate": 7.463636107334987e-05,
      "loss": 2.4304,
      "step": 86650
    },
    {
      "epoch": 1.782783333633341,
      "grad_norm": 0.4095345139503479,
      "learning_rate": 7.461487881457119e-05,
      "loss": 2.387,
      "step": 86660
    },
    {
      "epoch": 1.7829890531502088,
      "grad_norm": 0.4738003611564636,
      "learning_rate": 7.459339780790224e-05,
      "loss": 2.3624,
      "step": 86670
    },
    {
      "epoch": 1.7831947726670765,
      "grad_norm": 0.4735930263996124,
      "learning_rate": 7.457191805440252e-05,
      "loss": 2.363,
      "step": 86680
    },
    {
      "epoch": 1.7834004921839441,
      "grad_norm": 0.44366195797920227,
      "learning_rate": 7.455043955513149e-05,
      "loss": 2.4007,
      "step": 86690
    },
    {
      "epoch": 1.7836062117008118,
      "grad_norm": 0.44958069920539856,
      "learning_rate": 7.452896231114863e-05,
      "loss": 2.3746,
      "step": 86700
    },
    {
      "epoch": 1.7838119312176794,
      "grad_norm": 0.4644606113433838,
      "learning_rate": 7.450748632351324e-05,
      "loss": 2.3669,
      "step": 86710
    },
    {
      "epoch": 1.784017650734547,
      "grad_norm": 0.47219324111938477,
      "learning_rate": 7.44860115932846e-05,
      "loss": 2.317,
      "step": 86720
    },
    {
      "epoch": 1.784223370251415,
      "grad_norm": 0.46211540699005127,
      "learning_rate": 7.446453812152204e-05,
      "loss": 2.3852,
      "step": 86730
    },
    {
      "epoch": 1.7844290897682826,
      "grad_norm": 0.4728487730026245,
      "learning_rate": 7.444306590928463e-05,
      "loss": 2.4175,
      "step": 86740
    },
    {
      "epoch": 1.7846348092851505,
      "grad_norm": 0.46010759472846985,
      "learning_rate": 7.442159495763154e-05,
      "loss": 2.3873,
      "step": 86750
    },
    {
      "epoch": 1.7848405288020182,
      "grad_norm": 0.45170024037361145,
      "learning_rate": 7.440012526762172e-05,
      "loss": 2.3889,
      "step": 86760
    },
    {
      "epoch": 1.7850462483188858,
      "grad_norm": 0.4211936593055725,
      "learning_rate": 7.437865684031426e-05,
      "loss": 2.4111,
      "step": 86770
    },
    {
      "epoch": 1.7852519678357535,
      "grad_norm": 0.45427295565605164,
      "learning_rate": 7.435718967676806e-05,
      "loss": 2.3955,
      "step": 86780
    },
    {
      "epoch": 1.7854576873526211,
      "grad_norm": 0.41865184903144836,
      "learning_rate": 7.43357237780419e-05,
      "loss": 2.3893,
      "step": 86790
    },
    {
      "epoch": 1.785663406869489,
      "grad_norm": 0.4390268921852112,
      "learning_rate": 7.431425914519465e-05,
      "loss": 2.4208,
      "step": 86800
    },
    {
      "epoch": 1.7858691263863566,
      "grad_norm": 0.4240015149116516,
      "learning_rate": 7.429279577928508e-05,
      "loss": 2.4475,
      "step": 86810
    },
    {
      "epoch": 1.7860748459032245,
      "grad_norm": 0.41921311616897583,
      "learning_rate": 7.427133368137174e-05,
      "loss": 2.3358,
      "step": 86820
    },
    {
      "epoch": 1.7862805654200922,
      "grad_norm": 0.42268866300582886,
      "learning_rate": 7.424987285251333e-05,
      "loss": 2.3932,
      "step": 86830
    },
    {
      "epoch": 1.7864862849369598,
      "grad_norm": 0.4954097270965576,
      "learning_rate": 7.422841329376837e-05,
      "loss": 2.3736,
      "step": 86840
    },
    {
      "epoch": 1.7866920044538275,
      "grad_norm": 0.4579169452190399,
      "learning_rate": 7.420695500619533e-05,
      "loss": 2.3975,
      "step": 86850
    },
    {
      "epoch": 1.7868977239706951,
      "grad_norm": 0.46185004711151123,
      "learning_rate": 7.418549799085268e-05,
      "loss": 2.4391,
      "step": 86860
    },
    {
      "epoch": 1.787103443487563,
      "grad_norm": 0.4381997287273407,
      "learning_rate": 7.416404224879871e-05,
      "loss": 2.4594,
      "step": 86870
    },
    {
      "epoch": 1.7873091630044307,
      "grad_norm": 0.4270161986351013,
      "learning_rate": 7.414258778109174e-05,
      "loss": 2.3803,
      "step": 86880
    },
    {
      "epoch": 1.7875148825212985,
      "grad_norm": 0.4226657748222351,
      "learning_rate": 7.412113458879004e-05,
      "loss": 2.4936,
      "step": 86890
    },
    {
      "epoch": 1.7877206020381662,
      "grad_norm": 0.44874143600463867,
      "learning_rate": 7.409968267295173e-05,
      "loss": 2.3938,
      "step": 86900
    },
    {
      "epoch": 1.7879263215550338,
      "grad_norm": 0.43707334995269775,
      "learning_rate": 7.407823203463496e-05,
      "loss": 2.4321,
      "step": 86910
    },
    {
      "epoch": 1.7881320410719015,
      "grad_norm": 0.4448072016239166,
      "learning_rate": 7.405678267489771e-05,
      "loss": 2.348,
      "step": 86920
    },
    {
      "epoch": 1.7883377605887691,
      "grad_norm": 0.5026451945304871,
      "learning_rate": 7.403533459479801e-05,
      "loss": 2.4179,
      "step": 86930
    },
    {
      "epoch": 1.788543480105637,
      "grad_norm": 0.6521521210670471,
      "learning_rate": 7.401388779539377e-05,
      "loss": 2.4077,
      "step": 86940
    },
    {
      "epoch": 1.7887491996225047,
      "grad_norm": 0.4445689916610718,
      "learning_rate": 7.399244227774282e-05,
      "loss": 2.3841,
      "step": 86950
    },
    {
      "epoch": 1.7889549191393725,
      "grad_norm": 0.43553221225738525,
      "learning_rate": 7.3970998042903e-05,
      "loss": 2.4415,
      "step": 86960
    },
    {
      "epoch": 1.7891606386562402,
      "grad_norm": 0.4436565637588501,
      "learning_rate": 7.3949555091932e-05,
      "loss": 2.3025,
      "step": 86970
    },
    {
      "epoch": 1.7893663581731079,
      "grad_norm": 0.47567978501319885,
      "learning_rate": 7.392811342588744e-05,
      "loss": 2.3699,
      "step": 86980
    },
    {
      "epoch": 1.7895720776899755,
      "grad_norm": 0.4700219929218292,
      "learning_rate": 7.390667304582703e-05,
      "loss": 2.4146,
      "step": 86990
    },
    {
      "epoch": 1.7897777972068432,
      "grad_norm": 0.4652997851371765,
      "learning_rate": 7.388523395280822e-05,
      "loss": 2.3451,
      "step": 87000
    },
    {
      "epoch": 1.7899835167237108,
      "grad_norm": 0.47420668601989746,
      "learning_rate": 7.38637961478885e-05,
      "loss": 2.4009,
      "step": 87010
    },
    {
      "epoch": 1.7901892362405787,
      "grad_norm": 0.45197394490242004,
      "learning_rate": 7.384235963212532e-05,
      "loss": 2.3983,
      "step": 87020
    },
    {
      "epoch": 1.7903949557574466,
      "grad_norm": 0.4467615783214569,
      "learning_rate": 7.382092440657599e-05,
      "loss": 2.4246,
      "step": 87030
    },
    {
      "epoch": 1.7906006752743142,
      "grad_norm": 0.521242082118988,
      "learning_rate": 7.379949047229778e-05,
      "loss": 2.4044,
      "step": 87040
    },
    {
      "epoch": 1.7908063947911819,
      "grad_norm": 0.46947747468948364,
      "learning_rate": 7.377805783034794e-05,
      "loss": 2.3818,
      "step": 87050
    },
    {
      "epoch": 1.7910121143080495,
      "grad_norm": 0.4552009403705597,
      "learning_rate": 7.375662648178364e-05,
      "loss": 2.4399,
      "step": 87060
    },
    {
      "epoch": 1.7912178338249172,
      "grad_norm": 0.4522486925125122,
      "learning_rate": 7.373519642766192e-05,
      "loss": 2.3538,
      "step": 87070
    },
    {
      "epoch": 1.7914235533417848,
      "grad_norm": 0.431203693151474,
      "learning_rate": 7.371376766903982e-05,
      "loss": 2.3997,
      "step": 87080
    },
    {
      "epoch": 1.7916292728586527,
      "grad_norm": 0.41608864068984985,
      "learning_rate": 7.369234020697433e-05,
      "loss": 2.4331,
      "step": 87090
    },
    {
      "epoch": 1.7918349923755204,
      "grad_norm": 0.4437103569507599,
      "learning_rate": 7.367091404252238e-05,
      "loss": 2.3809,
      "step": 87100
    },
    {
      "epoch": 1.7920407118923882,
      "grad_norm": 0.45496252179145813,
      "learning_rate": 7.36494891767407e-05,
      "loss": 2.4027,
      "step": 87110
    },
    {
      "epoch": 1.7922464314092559,
      "grad_norm": 0.42131948471069336,
      "learning_rate": 7.362806561068615e-05,
      "loss": 2.3722,
      "step": 87120
    },
    {
      "epoch": 1.7924521509261235,
      "grad_norm": 0.4132010340690613,
      "learning_rate": 7.360664334541543e-05,
      "loss": 2.3357,
      "step": 87130
    },
    {
      "epoch": 1.7926578704429912,
      "grad_norm": 0.42713427543640137,
      "learning_rate": 7.358522238198514e-05,
      "loss": 2.4294,
      "step": 87140
    },
    {
      "epoch": 1.7928635899598588,
      "grad_norm": 0.4194192886352539,
      "learning_rate": 7.356380272145192e-05,
      "loss": 2.3501,
      "step": 87150
    },
    {
      "epoch": 1.7930693094767267,
      "grad_norm": 0.4285983741283417,
      "learning_rate": 7.354238436487221e-05,
      "loss": 2.3936,
      "step": 87160
    },
    {
      "epoch": 1.7932750289935944,
      "grad_norm": 0.5264949202537537,
      "learning_rate": 7.352096731330251e-05,
      "loss": 2.4077,
      "step": 87170
    },
    {
      "epoch": 1.7934807485104622,
      "grad_norm": 0.4569384753704071,
      "learning_rate": 7.349955156779924e-05,
      "loss": 2.3279,
      "step": 87180
    },
    {
      "epoch": 1.79368646802733,
      "grad_norm": 0.4438920319080353,
      "learning_rate": 7.347813712941865e-05,
      "loss": 2.3455,
      "step": 87190
    },
    {
      "epoch": 1.7938921875441975,
      "grad_norm": 0.45350536704063416,
      "learning_rate": 7.345672399921702e-05,
      "loss": 2.4037,
      "step": 87200
    },
    {
      "epoch": 1.7940979070610652,
      "grad_norm": 0.46031898260116577,
      "learning_rate": 7.343531217825058e-05,
      "loss": 2.3304,
      "step": 87210
    },
    {
      "epoch": 1.7943036265779329,
      "grad_norm": 0.4098503589630127,
      "learning_rate": 7.341390166757543e-05,
      "loss": 2.364,
      "step": 87220
    },
    {
      "epoch": 1.7945093460948007,
      "grad_norm": 0.46045246720314026,
      "learning_rate": 7.339249246824762e-05,
      "loss": 2.4234,
      "step": 87230
    },
    {
      "epoch": 1.7947150656116684,
      "grad_norm": 0.5689665675163269,
      "learning_rate": 7.337108458132315e-05,
      "loss": 2.3915,
      "step": 87240
    },
    {
      "epoch": 1.7949207851285363,
      "grad_norm": 0.4259543716907501,
      "learning_rate": 7.334967800785797e-05,
      "loss": 2.3578,
      "step": 87250
    },
    {
      "epoch": 1.795126504645404,
      "grad_norm": 0.41921737790107727,
      "learning_rate": 7.332827274890799e-05,
      "loss": 2.3407,
      "step": 87260
    },
    {
      "epoch": 1.7953322241622716,
      "grad_norm": 0.44092482328414917,
      "learning_rate": 7.330686880552891e-05,
      "loss": 2.4713,
      "step": 87270
    },
    {
      "epoch": 1.7955379436791392,
      "grad_norm": 0.43570199608802795,
      "learning_rate": 7.328546617877656e-05,
      "loss": 2.3277,
      "step": 87280
    },
    {
      "epoch": 1.7957436631960069,
      "grad_norm": 0.44297441840171814,
      "learning_rate": 7.32640648697066e-05,
      "loss": 2.3828,
      "step": 87290
    },
    {
      "epoch": 1.7959493827128747,
      "grad_norm": 0.4635559916496277,
      "learning_rate": 7.32426648793746e-05,
      "loss": 2.3899,
      "step": 87300
    },
    {
      "epoch": 1.7961551022297424,
      "grad_norm": 0.4377078115940094,
      "learning_rate": 7.322126620883615e-05,
      "loss": 2.3898,
      "step": 87310
    },
    {
      "epoch": 1.7963608217466103,
      "grad_norm": 0.4123496115207672,
      "learning_rate": 7.319986885914671e-05,
      "loss": 2.417,
      "step": 87320
    },
    {
      "epoch": 1.796566541263478,
      "grad_norm": 0.39957112073898315,
      "learning_rate": 7.317847283136168e-05,
      "loss": 2.4207,
      "step": 87330
    },
    {
      "epoch": 1.7967722607803456,
      "grad_norm": 0.42711907625198364,
      "learning_rate": 7.315707812653645e-05,
      "loss": 2.4193,
      "step": 87340
    },
    {
      "epoch": 1.7969779802972132,
      "grad_norm": 0.4457653760910034,
      "learning_rate": 7.313568474572628e-05,
      "loss": 2.3813,
      "step": 87350
    },
    {
      "epoch": 1.7971836998140809,
      "grad_norm": 0.41829434037208557,
      "learning_rate": 7.311429268998635e-05,
      "loss": 2.3865,
      "step": 87360
    },
    {
      "epoch": 1.7973894193309488,
      "grad_norm": 0.4098914563655853,
      "learning_rate": 7.309290196037188e-05,
      "loss": 2.4358,
      "step": 87370
    },
    {
      "epoch": 1.7975951388478164,
      "grad_norm": 0.40930137038230896,
      "learning_rate": 7.307151255793794e-05,
      "loss": 2.3663,
      "step": 87380
    },
    {
      "epoch": 1.7978008583646843,
      "grad_norm": 0.45319342613220215,
      "learning_rate": 7.305012448373955e-05,
      "loss": 2.4717,
      "step": 87390
    },
    {
      "epoch": 1.798006577881552,
      "grad_norm": 0.42873719334602356,
      "learning_rate": 7.30287377388316e-05,
      "loss": 2.4355,
      "step": 87400
    },
    {
      "epoch": 1.7982122973984196,
      "grad_norm": 0.4267268180847168,
      "learning_rate": 7.30073523242691e-05,
      "loss": 2.4003,
      "step": 87410
    },
    {
      "epoch": 1.7984180169152872,
      "grad_norm": 0.41937318444252014,
      "learning_rate": 7.298596824110682e-05,
      "loss": 2.3984,
      "step": 87420
    },
    {
      "epoch": 1.798623736432155,
      "grad_norm": 0.40609362721443176,
      "learning_rate": 7.296458549039948e-05,
      "loss": 2.4099,
      "step": 87430
    },
    {
      "epoch": 1.7988294559490225,
      "grad_norm": 0.4393142759799957,
      "learning_rate": 7.294320407320181e-05,
      "loss": 2.3823,
      "step": 87440
    },
    {
      "epoch": 1.7990351754658904,
      "grad_norm": 0.441752552986145,
      "learning_rate": 7.292182399056849e-05,
      "loss": 2.4054,
      "step": 87450
    },
    {
      "epoch": 1.7992408949827583,
      "grad_norm": 0.47390979528427124,
      "learning_rate": 7.290044524355399e-05,
      "loss": 2.3667,
      "step": 87460
    },
    {
      "epoch": 1.799446614499626,
      "grad_norm": 0.4339698255062103,
      "learning_rate": 7.287906783321289e-05,
      "loss": 2.3796,
      "step": 87470
    },
    {
      "epoch": 1.7996523340164936,
      "grad_norm": 0.44871655106544495,
      "learning_rate": 7.285769176059955e-05,
      "loss": 2.3952,
      "step": 87480
    },
    {
      "epoch": 1.7998580535333613,
      "grad_norm": 0.5111786723136902,
      "learning_rate": 7.283631702676837e-05,
      "loss": 2.399,
      "step": 87490
    },
    {
      "epoch": 1.800063773050229,
      "grad_norm": 0.45961105823516846,
      "learning_rate": 7.281494363277366e-05,
      "loss": 2.3807,
      "step": 87500
    },
    {
      "epoch": 1.8002694925670966,
      "grad_norm": 0.4432114064693451,
      "learning_rate": 7.279357157966964e-05,
      "loss": 2.4025,
      "step": 87510
    },
    {
      "epoch": 1.8004752120839644,
      "grad_norm": 0.45521000027656555,
      "learning_rate": 7.277220086851044e-05,
      "loss": 2.3617,
      "step": 87520
    },
    {
      "epoch": 1.800680931600832,
      "grad_norm": 0.4573354125022888,
      "learning_rate": 7.275083150035024e-05,
      "loss": 2.3471,
      "step": 87530
    },
    {
      "epoch": 1.8008866511177,
      "grad_norm": 0.44723260402679443,
      "learning_rate": 7.272946347624304e-05,
      "loss": 2.3912,
      "step": 87540
    },
    {
      "epoch": 1.8010923706345676,
      "grad_norm": 0.47154274582862854,
      "learning_rate": 7.27080967972428e-05,
      "loss": 2.3718,
      "step": 87550
    },
    {
      "epoch": 1.8012980901514353,
      "grad_norm": 0.4712962210178375,
      "learning_rate": 7.268673146440337e-05,
      "loss": 2.3632,
      "step": 87560
    },
    {
      "epoch": 1.801503809668303,
      "grad_norm": 0.4607312083244324,
      "learning_rate": 7.266536747877867e-05,
      "loss": 2.4375,
      "step": 87570
    },
    {
      "epoch": 1.8017095291851706,
      "grad_norm": 0.4270971715450287,
      "learning_rate": 7.264400484142245e-05,
      "loss": 2.4197,
      "step": 87580
    },
    {
      "epoch": 1.8019152487020385,
      "grad_norm": 0.45342689752578735,
      "learning_rate": 7.262264355338837e-05,
      "loss": 2.4019,
      "step": 87590
    },
    {
      "epoch": 1.802120968218906,
      "grad_norm": 0.4950704574584961,
      "learning_rate": 7.260128361573011e-05,
      "loss": 2.3629,
      "step": 87600
    },
    {
      "epoch": 1.802326687735774,
      "grad_norm": 0.4105512499809265,
      "learning_rate": 7.257992502950124e-05,
      "loss": 2.3734,
      "step": 87610
    },
    {
      "epoch": 1.8025324072526416,
      "grad_norm": 0.4683164358139038,
      "learning_rate": 7.25585677957552e-05,
      "loss": 2.3916,
      "step": 87620
    },
    {
      "epoch": 1.8027381267695093,
      "grad_norm": 0.430323988199234,
      "learning_rate": 7.253721191554552e-05,
      "loss": 2.4459,
      "step": 87630
    },
    {
      "epoch": 1.802943846286377,
      "grad_norm": 0.5162807703018188,
      "learning_rate": 7.251585738992552e-05,
      "loss": 2.4062,
      "step": 87640
    },
    {
      "epoch": 1.8031495658032446,
      "grad_norm": 0.5862606763839722,
      "learning_rate": 7.249450421994846e-05,
      "loss": 2.4387,
      "step": 87650
    },
    {
      "epoch": 1.8033552853201125,
      "grad_norm": 0.41558411717414856,
      "learning_rate": 7.247315240666767e-05,
      "loss": 2.4433,
      "step": 87660
    },
    {
      "epoch": 1.8035610048369801,
      "grad_norm": 0.49300310015678406,
      "learning_rate": 7.245180195113625e-05,
      "loss": 2.3671,
      "step": 87670
    },
    {
      "epoch": 1.803766724353848,
      "grad_norm": 0.4914306402206421,
      "learning_rate": 7.243045285440728e-05,
      "loss": 2.4027,
      "step": 87680
    },
    {
      "epoch": 1.8039724438707156,
      "grad_norm": 0.445344477891922,
      "learning_rate": 7.240910511753389e-05,
      "loss": 2.3665,
      "step": 87690
    },
    {
      "epoch": 1.8041781633875833,
      "grad_norm": 0.41936004161834717,
      "learning_rate": 7.238775874156897e-05,
      "loss": 2.3977,
      "step": 87700
    },
    {
      "epoch": 1.804383882904451,
      "grad_norm": 0.44150015711784363,
      "learning_rate": 7.236641372756548e-05,
      "loss": 2.4315,
      "step": 87710
    },
    {
      "epoch": 1.8045896024213186,
      "grad_norm": 0.4501068592071533,
      "learning_rate": 7.234507007657616e-05,
      "loss": 2.374,
      "step": 87720
    },
    {
      "epoch": 1.8047953219381865,
      "grad_norm": 0.4466899633407593,
      "learning_rate": 7.232372778965387e-05,
      "loss": 2.4315,
      "step": 87730
    },
    {
      "epoch": 1.8050010414550541,
      "grad_norm": 0.44908031821250916,
      "learning_rate": 7.230238686785127e-05,
      "loss": 2.3768,
      "step": 87740
    },
    {
      "epoch": 1.805206760971922,
      "grad_norm": 0.4199247360229492,
      "learning_rate": 7.228104731222094e-05,
      "loss": 2.3896,
      "step": 87750
    },
    {
      "epoch": 1.8054124804887897,
      "grad_norm": 0.4000377655029297,
      "learning_rate": 7.225970912381556e-05,
      "loss": 2.4239,
      "step": 87760
    },
    {
      "epoch": 1.8056182000056573,
      "grad_norm": 0.45663148164749146,
      "learning_rate": 7.223837230368754e-05,
      "loss": 2.3712,
      "step": 87770
    },
    {
      "epoch": 1.805823919522525,
      "grad_norm": 0.43622127175331116,
      "learning_rate": 7.22170368528893e-05,
      "loss": 2.3913,
      "step": 87780
    },
    {
      "epoch": 1.8060296390393926,
      "grad_norm": 0.45760735869407654,
      "learning_rate": 7.219570277247327e-05,
      "loss": 2.4196,
      "step": 87790
    },
    {
      "epoch": 1.8062353585562603,
      "grad_norm": 0.44626957178115845,
      "learning_rate": 7.217437006349171e-05,
      "loss": 2.3433,
      "step": 87800
    },
    {
      "epoch": 1.8064410780731281,
      "grad_norm": 0.4071710407733917,
      "learning_rate": 7.21530387269968e-05,
      "loss": 2.3731,
      "step": 87810
    },
    {
      "epoch": 1.806646797589996,
      "grad_norm": 0.4189799129962921,
      "learning_rate": 7.213170876404079e-05,
      "loss": 2.4265,
      "step": 87820
    },
    {
      "epoch": 1.8068525171068637,
      "grad_norm": 0.5088493227958679,
      "learning_rate": 7.21103801756757e-05,
      "loss": 2.4158,
      "step": 87830
    },
    {
      "epoch": 1.8070582366237313,
      "grad_norm": 0.5478640198707581,
      "learning_rate": 7.208905296295361e-05,
      "loss": 2.4071,
      "step": 87840
    },
    {
      "epoch": 1.807263956140599,
      "grad_norm": 0.43782562017440796,
      "learning_rate": 7.206772712692639e-05,
      "loss": 2.3819,
      "step": 87850
    },
    {
      "epoch": 1.8074696756574666,
      "grad_norm": 0.4316660463809967,
      "learning_rate": 7.2046402668646e-05,
      "loss": 2.3808,
      "step": 87860
    },
    {
      "epoch": 1.8076753951743343,
      "grad_norm": 0.4756116569042206,
      "learning_rate": 7.202507958916425e-05,
      "loss": 2.4137,
      "step": 87870
    },
    {
      "epoch": 1.8078811146912022,
      "grad_norm": 0.4465528726577759,
      "learning_rate": 7.200375788953285e-05,
      "loss": 2.3475,
      "step": 87880
    },
    {
      "epoch": 1.8080868342080698,
      "grad_norm": 0.4826544225215912,
      "learning_rate": 7.198243757080353e-05,
      "loss": 2.4284,
      "step": 87890
    },
    {
      "epoch": 1.8082925537249377,
      "grad_norm": 0.4955427348613739,
      "learning_rate": 7.19611186340279e-05,
      "loss": 2.4105,
      "step": 87900
    },
    {
      "epoch": 1.8084982732418053,
      "grad_norm": 0.44242092967033386,
      "learning_rate": 7.193980108025746e-05,
      "loss": 2.3776,
      "step": 87910
    },
    {
      "epoch": 1.808703992758673,
      "grad_norm": 0.41321730613708496,
      "learning_rate": 7.191848491054376e-05,
      "loss": 2.3404,
      "step": 87920
    },
    {
      "epoch": 1.8089097122755406,
      "grad_norm": 0.42862462997436523,
      "learning_rate": 7.189717012593815e-05,
      "loss": 2.4054,
      "step": 87930
    },
    {
      "epoch": 1.8091154317924083,
      "grad_norm": 0.4066656231880188,
      "learning_rate": 7.187585672749198e-05,
      "loss": 2.3733,
      "step": 87940
    },
    {
      "epoch": 1.8093211513092762,
      "grad_norm": 0.44845375418663025,
      "learning_rate": 7.185454471625659e-05,
      "loss": 2.4121,
      "step": 87950
    },
    {
      "epoch": 1.8095268708261438,
      "grad_norm": 0.43195247650146484,
      "learning_rate": 7.18332340932831e-05,
      "loss": 2.3988,
      "step": 87960
    },
    {
      "epoch": 1.8097325903430117,
      "grad_norm": 0.42115718126296997,
      "learning_rate": 7.181192485962266e-05,
      "loss": 2.3609,
      "step": 87970
    },
    {
      "epoch": 1.8099383098598794,
      "grad_norm": 0.49602195620536804,
      "learning_rate": 7.179061701632641e-05,
      "loss": 2.3683,
      "step": 87980
    },
    {
      "epoch": 1.810144029376747,
      "grad_norm": 0.44280311465263367,
      "learning_rate": 7.176931056444529e-05,
      "loss": 2.3594,
      "step": 87990
    },
    {
      "epoch": 1.8103497488936147,
      "grad_norm": 0.45998287200927734,
      "learning_rate": 7.174800550503025e-05,
      "loss": 2.4444,
      "step": 88000
    },
    {
      "epoch": 1.8105554684104823,
      "grad_norm": 0.4532979726791382,
      "learning_rate": 7.17267018391321e-05,
      "loss": 2.4115,
      "step": 88010
    },
    {
      "epoch": 1.8107611879273502,
      "grad_norm": 0.4232354164123535,
      "learning_rate": 7.170539956780169e-05,
      "loss": 2.4278,
      "step": 88020
    },
    {
      "epoch": 1.8109669074442178,
      "grad_norm": 0.44530853629112244,
      "learning_rate": 7.168409869208977e-05,
      "loss": 2.4196,
      "step": 88030
    },
    {
      "epoch": 1.8111726269610857,
      "grad_norm": 0.42012232542037964,
      "learning_rate": 7.16627992130469e-05,
      "loss": 2.3993,
      "step": 88040
    },
    {
      "epoch": 1.8113783464779534,
      "grad_norm": 0.4424354135990143,
      "learning_rate": 7.164150113172375e-05,
      "loss": 2.3652,
      "step": 88050
    },
    {
      "epoch": 1.811584065994821,
      "grad_norm": 0.4411959648132324,
      "learning_rate": 7.162020444917085e-05,
      "loss": 2.3399,
      "step": 88060
    },
    {
      "epoch": 1.8117897855116887,
      "grad_norm": 0.4072721302509308,
      "learning_rate": 7.159890916643856e-05,
      "loss": 2.3786,
      "step": 88070
    },
    {
      "epoch": 1.8119955050285563,
      "grad_norm": 0.4846374988555908,
      "learning_rate": 7.157761528457737e-05,
      "loss": 2.4044,
      "step": 88080
    },
    {
      "epoch": 1.8122012245454242,
      "grad_norm": 0.456398606300354,
      "learning_rate": 7.155632280463749e-05,
      "loss": 2.3598,
      "step": 88090
    },
    {
      "epoch": 1.8124069440622919,
      "grad_norm": 0.43855810165405273,
      "learning_rate": 7.153503172766918e-05,
      "loss": 2.4028,
      "step": 88100
    },
    {
      "epoch": 1.8126126635791597,
      "grad_norm": 0.4063069820404053,
      "learning_rate": 7.15137420547227e-05,
      "loss": 2.4645,
      "step": 88110
    },
    {
      "epoch": 1.8128183830960274,
      "grad_norm": 0.4244294762611389,
      "learning_rate": 7.149245378684808e-05,
      "loss": 2.408,
      "step": 88120
    },
    {
      "epoch": 1.813024102612895,
      "grad_norm": 0.4640359580516815,
      "learning_rate": 7.147116692509532e-05,
      "loss": 2.4057,
      "step": 88130
    },
    {
      "epoch": 1.8132298221297627,
      "grad_norm": 0.47526097297668457,
      "learning_rate": 7.14498814705145e-05,
      "loss": 2.3564,
      "step": 88140
    },
    {
      "epoch": 1.8134355416466303,
      "grad_norm": 0.406154602766037,
      "learning_rate": 7.142859742415542e-05,
      "loss": 2.3473,
      "step": 88150
    },
    {
      "epoch": 1.813641261163498,
      "grad_norm": 0.4679902493953705,
      "learning_rate": 7.140731478706794e-05,
      "loss": 2.4022,
      "step": 88160
    },
    {
      "epoch": 1.8138469806803659,
      "grad_norm": 0.469527006149292,
      "learning_rate": 7.138603356030178e-05,
      "loss": 2.4558,
      "step": 88170
    },
    {
      "epoch": 1.8140527001972337,
      "grad_norm": 0.46030959486961365,
      "learning_rate": 7.136475374490668e-05,
      "loss": 2.3649,
      "step": 88180
    },
    {
      "epoch": 1.8142584197141014,
      "grad_norm": 0.44938036799430847,
      "learning_rate": 7.134347534193224e-05,
      "loss": 2.3806,
      "step": 88190
    },
    {
      "epoch": 1.814464139230969,
      "grad_norm": 0.48397502303123474,
      "learning_rate": 7.132219835242797e-05,
      "loss": 2.3959,
      "step": 88200
    },
    {
      "epoch": 1.8146698587478367,
      "grad_norm": 0.4660329222679138,
      "learning_rate": 7.130092277744342e-05,
      "loss": 2.4165,
      "step": 88210
    },
    {
      "epoch": 1.8148755782647044,
      "grad_norm": 0.44464364647865295,
      "learning_rate": 7.127964861802796e-05,
      "loss": 2.3266,
      "step": 88220
    },
    {
      "epoch": 1.815081297781572,
      "grad_norm": 0.46266409754753113,
      "learning_rate": 7.125837587523087e-05,
      "loss": 2.3621,
      "step": 88230
    },
    {
      "epoch": 1.8152870172984399,
      "grad_norm": 0.4285942316055298,
      "learning_rate": 7.123710455010155e-05,
      "loss": 2.4418,
      "step": 88240
    },
    {
      "epoch": 1.8154927368153075,
      "grad_norm": 0.4210164248943329,
      "learning_rate": 7.121583464368908e-05,
      "loss": 2.3599,
      "step": 88250
    },
    {
      "epoch": 1.8156984563321754,
      "grad_norm": 0.46055203676223755,
      "learning_rate": 7.119456615704262e-05,
      "loss": 2.3786,
      "step": 88260
    },
    {
      "epoch": 1.815904175849043,
      "grad_norm": 0.42519256472587585,
      "learning_rate": 7.117329909121127e-05,
      "loss": 2.4142,
      "step": 88270
    },
    {
      "epoch": 1.8161098953659107,
      "grad_norm": 0.42178234457969666,
      "learning_rate": 7.1152033447244e-05,
      "loss": 2.4042,
      "step": 88280
    },
    {
      "epoch": 1.8163156148827784,
      "grad_norm": 0.45439472794532776,
      "learning_rate": 7.113076922618967e-05,
      "loss": 2.3773,
      "step": 88290
    },
    {
      "epoch": 1.816521334399646,
      "grad_norm": 0.4306556284427643,
      "learning_rate": 7.110950642909721e-05,
      "loss": 2.3985,
      "step": 88300
    },
    {
      "epoch": 1.816727053916514,
      "grad_norm": 0.47691258788108826,
      "learning_rate": 7.108824505701535e-05,
      "loss": 2.4221,
      "step": 88310
    },
    {
      "epoch": 1.8169327734333816,
      "grad_norm": 0.44130510091781616,
      "learning_rate": 7.106698511099285e-05,
      "loss": 2.4219,
      "step": 88320
    },
    {
      "epoch": 1.8171384929502494,
      "grad_norm": 0.46017342805862427,
      "learning_rate": 7.104572659207825e-05,
      "loss": 2.3553,
      "step": 88330
    },
    {
      "epoch": 1.817344212467117,
      "grad_norm": 0.47651225328445435,
      "learning_rate": 7.10244695013202e-05,
      "loss": 2.4433,
      "step": 88340
    },
    {
      "epoch": 1.8175499319839847,
      "grad_norm": 0.4380127191543579,
      "learning_rate": 7.10032138397672e-05,
      "loss": 2.4335,
      "step": 88350
    },
    {
      "epoch": 1.8177556515008524,
      "grad_norm": 0.3866240680217743,
      "learning_rate": 7.098195960846759e-05,
      "loss": 2.3947,
      "step": 88360
    },
    {
      "epoch": 1.81796137101772,
      "grad_norm": 0.44754302501678467,
      "learning_rate": 7.09607068084698e-05,
      "loss": 2.3744,
      "step": 88370
    },
    {
      "epoch": 1.818167090534588,
      "grad_norm": 0.5041853189468384,
      "learning_rate": 7.093945544082215e-05,
      "loss": 2.4068,
      "step": 88380
    },
    {
      "epoch": 1.8183728100514556,
      "grad_norm": 0.4303123354911804,
      "learning_rate": 7.091820550657274e-05,
      "loss": 2.392,
      "step": 88390
    },
    {
      "epoch": 1.8185785295683234,
      "grad_norm": 0.5651243329048157,
      "learning_rate": 7.089695700676982e-05,
      "loss": 2.4242,
      "step": 88400
    },
    {
      "epoch": 1.818784249085191,
      "grad_norm": 0.480204313993454,
      "learning_rate": 7.087570994246139e-05,
      "loss": 2.3999,
      "step": 88410
    },
    {
      "epoch": 1.8189899686020587,
      "grad_norm": 0.43117639422416687,
      "learning_rate": 7.085446431469547e-05,
      "loss": 2.4065,
      "step": 88420
    },
    {
      "epoch": 1.8191956881189264,
      "grad_norm": 0.4264771342277527,
      "learning_rate": 7.083322012452003e-05,
      "loss": 2.4229,
      "step": 88430
    },
    {
      "epoch": 1.819401407635794,
      "grad_norm": 0.44422078132629395,
      "learning_rate": 7.081197737298289e-05,
      "loss": 2.375,
      "step": 88440
    },
    {
      "epoch": 1.819607127152662,
      "grad_norm": 0.46643680334091187,
      "learning_rate": 7.079073606113182e-05,
      "loss": 2.4634,
      "step": 88450
    },
    {
      "epoch": 1.8198128466695296,
      "grad_norm": 0.43385690450668335,
      "learning_rate": 7.076949619001463e-05,
      "loss": 2.3966,
      "step": 88460
    },
    {
      "epoch": 1.8200185661863975,
      "grad_norm": 0.4163321256637573,
      "learning_rate": 7.074825776067887e-05,
      "loss": 2.3893,
      "step": 88470
    },
    {
      "epoch": 1.820224285703265,
      "grad_norm": 0.6110782623291016,
      "learning_rate": 7.072702077417218e-05,
      "loss": 2.361,
      "step": 88480
    },
    {
      "epoch": 1.8204300052201328,
      "grad_norm": 0.4552111327648163,
      "learning_rate": 7.070578523154199e-05,
      "loss": 2.415,
      "step": 88490
    },
    {
      "epoch": 1.8206357247370004,
      "grad_norm": 0.41845703125,
      "learning_rate": 7.068455113383581e-05,
      "loss": 2.3887,
      "step": 88500
    },
    {
      "epoch": 1.820841444253868,
      "grad_norm": 0.4442366361618042,
      "learning_rate": 7.066331848210099e-05,
      "loss": 2.3995,
      "step": 88510
    },
    {
      "epoch": 1.8210471637707357,
      "grad_norm": 0.42948204278945923,
      "learning_rate": 7.064208727738475e-05,
      "loss": 2.4234,
      "step": 88520
    },
    {
      "epoch": 1.8212528832876036,
      "grad_norm": 0.43673211336135864,
      "learning_rate": 7.062085752073439e-05,
      "loss": 2.4136,
      "step": 88530
    },
    {
      "epoch": 1.8214586028044715,
      "grad_norm": 0.49408024549484253,
      "learning_rate": 7.059962921319705e-05,
      "loss": 2.361,
      "step": 88540
    },
    {
      "epoch": 1.8216643223213391,
      "grad_norm": 0.5012186765670776,
      "learning_rate": 7.057840235581976e-05,
      "loss": 2.3441,
      "step": 88550
    },
    {
      "epoch": 1.8218700418382068,
      "grad_norm": 0.4034097492694855,
      "learning_rate": 7.055717694964959e-05,
      "loss": 2.4027,
      "step": 88560
    },
    {
      "epoch": 1.8220757613550744,
      "grad_norm": 0.4304440915584564,
      "learning_rate": 7.05359529957334e-05,
      "loss": 2.388,
      "step": 88570
    },
    {
      "epoch": 1.822281480871942,
      "grad_norm": 0.43236738443374634,
      "learning_rate": 7.051473049511807e-05,
      "loss": 2.4133,
      "step": 88580
    },
    {
      "epoch": 1.8224872003888097,
      "grad_norm": 0.5776839256286621,
      "learning_rate": 7.049350944885047e-05,
      "loss": 2.4261,
      "step": 88590
    },
    {
      "epoch": 1.8226929199056776,
      "grad_norm": 0.4488838016986847,
      "learning_rate": 7.047228985797722e-05,
      "loss": 2.3915,
      "step": 88600
    },
    {
      "epoch": 1.8228986394225453,
      "grad_norm": 0.44355669617652893,
      "learning_rate": 7.0451071723545e-05,
      "loss": 2.4425,
      "step": 88610
    },
    {
      "epoch": 1.8231043589394131,
      "grad_norm": 0.4808800220489502,
      "learning_rate": 7.042985504660042e-05,
      "loss": 2.3961,
      "step": 88620
    },
    {
      "epoch": 1.8233100784562808,
      "grad_norm": 0.3999197483062744,
      "learning_rate": 7.040863982818994e-05,
      "loss": 2.4617,
      "step": 88630
    },
    {
      "epoch": 1.8235157979731484,
      "grad_norm": 0.4185711443424225,
      "learning_rate": 7.038742606936001e-05,
      "loss": 2.3584,
      "step": 88640
    },
    {
      "epoch": 1.823721517490016,
      "grad_norm": 0.40493521094322205,
      "learning_rate": 7.036621377115696e-05,
      "loss": 2.3748,
      "step": 88650
    },
    {
      "epoch": 1.8239272370068838,
      "grad_norm": 0.4477185904979706,
      "learning_rate": 7.034500293462711e-05,
      "loss": 2.3891,
      "step": 88660
    },
    {
      "epoch": 1.8241329565237516,
      "grad_norm": 0.42937374114990234,
      "learning_rate": 7.032379356081669e-05,
      "loss": 2.3518,
      "step": 88670
    },
    {
      "epoch": 1.8243386760406193,
      "grad_norm": 0.4564887583255768,
      "learning_rate": 7.030258565077177e-05,
      "loss": 2.4166,
      "step": 88680
    },
    {
      "epoch": 1.8245443955574872,
      "grad_norm": 0.42525848746299744,
      "learning_rate": 7.028137920553848e-05,
      "loss": 2.418,
      "step": 88690
    },
    {
      "epoch": 1.8247501150743548,
      "grad_norm": 0.46306854486465454,
      "learning_rate": 7.026017422616284e-05,
      "loss": 2.3829,
      "step": 88700
    },
    {
      "epoch": 1.8249558345912225,
      "grad_norm": 0.4210415482521057,
      "learning_rate": 7.02389707136907e-05,
      "loss": 2.3798,
      "step": 88710
    },
    {
      "epoch": 1.8251615541080901,
      "grad_norm": 0.45775148272514343,
      "learning_rate": 7.0217768669168e-05,
      "loss": 2.4293,
      "step": 88720
    },
    {
      "epoch": 1.8253672736249578,
      "grad_norm": 0.42965078353881836,
      "learning_rate": 7.019656809364045e-05,
      "loss": 2.3754,
      "step": 88730
    },
    {
      "epoch": 1.8255729931418256,
      "grad_norm": 0.4299543797969818,
      "learning_rate": 7.017536898815378e-05,
      "loss": 2.3974,
      "step": 88740
    },
    {
      "epoch": 1.8257787126586933,
      "grad_norm": 0.42328113317489624,
      "learning_rate": 7.015417135375366e-05,
      "loss": 2.405,
      "step": 88750
    },
    {
      "epoch": 1.8259844321755612,
      "grad_norm": 0.4232867658138275,
      "learning_rate": 7.013297519148563e-05,
      "loss": 2.359,
      "step": 88760
    },
    {
      "epoch": 1.8261901516924288,
      "grad_norm": 0.48747867345809937,
      "learning_rate": 7.011178050239513e-05,
      "loss": 2.3148,
      "step": 88770
    },
    {
      "epoch": 1.8263958712092965,
      "grad_norm": 0.4251510798931122,
      "learning_rate": 7.009058728752768e-05,
      "loss": 2.3571,
      "step": 88780
    },
    {
      "epoch": 1.8266015907261641,
      "grad_norm": 0.4308842122554779,
      "learning_rate": 7.006939554792855e-05,
      "loss": 2.393,
      "step": 88790
    },
    {
      "epoch": 1.8268073102430318,
      "grad_norm": 0.46020686626434326,
      "learning_rate": 7.004820528464305e-05,
      "loss": 2.4051,
      "step": 88800
    },
    {
      "epoch": 1.8270130297598997,
      "grad_norm": 0.5208209753036499,
      "learning_rate": 7.002701649871632e-05,
      "loss": 2.418,
      "step": 88810
    },
    {
      "epoch": 1.8272187492767673,
      "grad_norm": 0.4971392750740051,
      "learning_rate": 7.000582919119358e-05,
      "loss": 2.3633,
      "step": 88820
    },
    {
      "epoch": 1.8274244687936352,
      "grad_norm": 0.4169262647628784,
      "learning_rate": 6.998464336311984e-05,
      "loss": 2.3851,
      "step": 88830
    },
    {
      "epoch": 1.8276301883105028,
      "grad_norm": 0.4302755296230316,
      "learning_rate": 6.996345901554004e-05,
      "loss": 2.4041,
      "step": 88840
    },
    {
      "epoch": 1.8278359078273705,
      "grad_norm": 0.44739410281181335,
      "learning_rate": 6.994227614949914e-05,
      "loss": 2.4146,
      "step": 88850
    },
    {
      "epoch": 1.8280416273442381,
      "grad_norm": 0.42848917841911316,
      "learning_rate": 6.992109476604199e-05,
      "loss": 2.3845,
      "step": 88860
    },
    {
      "epoch": 1.8282473468611058,
      "grad_norm": 0.4462565779685974,
      "learning_rate": 6.989991486621328e-05,
      "loss": 2.3366,
      "step": 88870
    },
    {
      "epoch": 1.8284530663779734,
      "grad_norm": 0.45646920800209045,
      "learning_rate": 6.98787364510578e-05,
      "loss": 2.3895,
      "step": 88880
    },
    {
      "epoch": 1.8286587858948413,
      "grad_norm": 0.4626648724079132,
      "learning_rate": 6.985755952162008e-05,
      "loss": 2.4322,
      "step": 88890
    },
    {
      "epoch": 1.8288645054117092,
      "grad_norm": 0.4509754478931427,
      "learning_rate": 6.983638407894466e-05,
      "loss": 2.3593,
      "step": 88900
    },
    {
      "epoch": 1.8290702249285768,
      "grad_norm": 0.4679182171821594,
      "learning_rate": 6.981521012407609e-05,
      "loss": 2.3765,
      "step": 88910
    },
    {
      "epoch": 1.8292759444454445,
      "grad_norm": 0.4337368607521057,
      "learning_rate": 6.97940376580587e-05,
      "loss": 2.3867,
      "step": 88920
    },
    {
      "epoch": 1.8294816639623122,
      "grad_norm": 0.5000893473625183,
      "learning_rate": 6.977286668193685e-05,
      "loss": 2.4284,
      "step": 88930
    },
    {
      "epoch": 1.8296873834791798,
      "grad_norm": 0.4310499131679535,
      "learning_rate": 6.975169719675472e-05,
      "loss": 2.3734,
      "step": 88940
    },
    {
      "epoch": 1.8298931029960475,
      "grad_norm": 0.4607592225074768,
      "learning_rate": 6.973052920355656e-05,
      "loss": 2.4012,
      "step": 88950
    },
    {
      "epoch": 1.8300988225129153,
      "grad_norm": 0.4362430274486542,
      "learning_rate": 6.970936270338647e-05,
      "loss": 2.4057,
      "step": 88960
    },
    {
      "epoch": 1.830304542029783,
      "grad_norm": 0.4318399727344513,
      "learning_rate": 6.96881976972884e-05,
      "loss": 2.3843,
      "step": 88970
    },
    {
      "epoch": 1.8305102615466509,
      "grad_norm": 0.4320755898952484,
      "learning_rate": 6.96670341863064e-05,
      "loss": 2.3725,
      "step": 88980
    },
    {
      "epoch": 1.8307159810635185,
      "grad_norm": 0.47531619668006897,
      "learning_rate": 6.964587217148431e-05,
      "loss": 2.3699,
      "step": 88990
    },
    {
      "epoch": 1.8309217005803862,
      "grad_norm": 0.43141183257102966,
      "learning_rate": 6.96247116538659e-05,
      "loss": 2.4992,
      "step": 89000
    },
    {
      "epoch": 1.8311274200972538,
      "grad_norm": 0.4048047661781311,
      "learning_rate": 6.960355263449499e-05,
      "loss": 2.3885,
      "step": 89010
    },
    {
      "epoch": 1.8313331396141215,
      "grad_norm": 0.4620484411716461,
      "learning_rate": 6.958239511441516e-05,
      "loss": 2.4149,
      "step": 89020
    },
    {
      "epoch": 1.8315388591309893,
      "grad_norm": 0.48519259691238403,
      "learning_rate": 6.956123909467e-05,
      "loss": 2.3641,
      "step": 89030
    },
    {
      "epoch": 1.831744578647857,
      "grad_norm": 0.4931681156158447,
      "learning_rate": 6.954008457630308e-05,
      "loss": 2.404,
      "step": 89040
    },
    {
      "epoch": 1.8319502981647249,
      "grad_norm": 0.4352180063724518,
      "learning_rate": 6.951893156035778e-05,
      "loss": 2.3316,
      "step": 89050
    },
    {
      "epoch": 1.8321560176815925,
      "grad_norm": 0.5100112557411194,
      "learning_rate": 6.949778004787748e-05,
      "loss": 2.4566,
      "step": 89060
    },
    {
      "epoch": 1.8323617371984602,
      "grad_norm": 0.46778762340545654,
      "learning_rate": 6.94766300399055e-05,
      "loss": 2.3796,
      "step": 89070
    },
    {
      "epoch": 1.8325674567153278,
      "grad_norm": 0.47681036591529846,
      "learning_rate": 6.945548153748501e-05,
      "loss": 2.4756,
      "step": 89080
    },
    {
      "epoch": 1.8327731762321955,
      "grad_norm": 0.4129255414009094,
      "learning_rate": 6.94343345416592e-05,
      "loss": 2.3811,
      "step": 89090
    },
    {
      "epoch": 1.8329788957490634,
      "grad_norm": 0.47518932819366455,
      "learning_rate": 6.941318905347105e-05,
      "loss": 2.4722,
      "step": 89100
    },
    {
      "epoch": 1.833184615265931,
      "grad_norm": 0.5203112959861755,
      "learning_rate": 6.939204507396362e-05,
      "loss": 2.4343,
      "step": 89110
    },
    {
      "epoch": 1.833390334782799,
      "grad_norm": 0.5260732173919678,
      "learning_rate": 6.937090260417984e-05,
      "loss": 2.3986,
      "step": 89120
    },
    {
      "epoch": 1.8335960542996665,
      "grad_norm": 0.4333263635635376,
      "learning_rate": 6.93497616451625e-05,
      "loss": 2.3471,
      "step": 89130
    },
    {
      "epoch": 1.8338017738165342,
      "grad_norm": 0.4590888023376465,
      "learning_rate": 6.93286221979544e-05,
      "loss": 2.4307,
      "step": 89140
    },
    {
      "epoch": 1.8340074933334019,
      "grad_norm": 0.43931904435157776,
      "learning_rate": 6.930748426359825e-05,
      "loss": 2.3955,
      "step": 89150
    },
    {
      "epoch": 1.8342132128502695,
      "grad_norm": 0.4244077205657959,
      "learning_rate": 6.92863478431366e-05,
      "loss": 2.367,
      "step": 89160
    },
    {
      "epoch": 1.8344189323671374,
      "grad_norm": 0.4299137592315674,
      "learning_rate": 6.92652129376121e-05,
      "loss": 2.3995,
      "step": 89170
    },
    {
      "epoch": 1.834624651884005,
      "grad_norm": 0.40495672821998596,
      "learning_rate": 6.924407954806712e-05,
      "loss": 2.3388,
      "step": 89180
    },
    {
      "epoch": 1.834830371400873,
      "grad_norm": 0.4817443788051605,
      "learning_rate": 6.922294767554409e-05,
      "loss": 2.4212,
      "step": 89190
    },
    {
      "epoch": 1.8350360909177406,
      "grad_norm": 0.4375305771827698,
      "learning_rate": 6.920181732108538e-05,
      "loss": 2.3924,
      "step": 89200
    },
    {
      "epoch": 1.8352418104346082,
      "grad_norm": 0.3947596251964569,
      "learning_rate": 6.918068848573316e-05,
      "loss": 2.4007,
      "step": 89210
    },
    {
      "epoch": 1.8354475299514759,
      "grad_norm": 0.5410540699958801,
      "learning_rate": 6.915956117052962e-05,
      "loss": 2.4175,
      "step": 89220
    },
    {
      "epoch": 1.8356532494683435,
      "grad_norm": 0.47442394495010376,
      "learning_rate": 6.91384353765169e-05,
      "loss": 2.4075,
      "step": 89230
    },
    {
      "epoch": 1.8358589689852114,
      "grad_norm": 0.43450435996055603,
      "learning_rate": 6.911731110473698e-05,
      "loss": 2.4409,
      "step": 89240
    },
    {
      "epoch": 1.836064688502079,
      "grad_norm": 0.45585671067237854,
      "learning_rate": 6.909618835623182e-05,
      "loss": 2.3825,
      "step": 89250
    },
    {
      "epoch": 1.836270408018947,
      "grad_norm": 0.4101056754589081,
      "learning_rate": 6.907506713204326e-05,
      "loss": 2.3688,
      "step": 89260
    },
    {
      "epoch": 1.8364761275358146,
      "grad_norm": 0.4729311466217041,
      "learning_rate": 6.905394743321312e-05,
      "loss": 2.4256,
      "step": 89270
    },
    {
      "epoch": 1.8366818470526822,
      "grad_norm": 0.4300859272480011,
      "learning_rate": 6.903282926078314e-05,
      "loss": 2.3917,
      "step": 89280
    },
    {
      "epoch": 1.8368875665695499,
      "grad_norm": 0.4431162178516388,
      "learning_rate": 6.901171261579491e-05,
      "loss": 2.4106,
      "step": 89290
    },
    {
      "epoch": 1.8370932860864175,
      "grad_norm": 0.48601871728897095,
      "learning_rate": 6.899059749929005e-05,
      "loss": 2.4114,
      "step": 89300
    },
    {
      "epoch": 1.8372990056032852,
      "grad_norm": 0.42932990193367004,
      "learning_rate": 6.896948391231004e-05,
      "loss": 2.3875,
      "step": 89310
    },
    {
      "epoch": 1.837504725120153,
      "grad_norm": 0.49239248037338257,
      "learning_rate": 6.894837185589626e-05,
      "loss": 2.4383,
      "step": 89320
    },
    {
      "epoch": 1.837710444637021,
      "grad_norm": 0.47709429264068604,
      "learning_rate": 6.892726133109014e-05,
      "loss": 2.3602,
      "step": 89330
    },
    {
      "epoch": 1.8379161641538886,
      "grad_norm": 0.39858728647232056,
      "learning_rate": 6.890615233893284e-05,
      "loss": 2.3617,
      "step": 89340
    },
    {
      "epoch": 1.8381218836707562,
      "grad_norm": 0.4525034427642822,
      "learning_rate": 6.888504488046561e-05,
      "loss": 2.4224,
      "step": 89350
    },
    {
      "epoch": 1.838327603187624,
      "grad_norm": 0.43297502398490906,
      "learning_rate": 6.88639389567296e-05,
      "loss": 2.3989,
      "step": 89360
    },
    {
      "epoch": 1.8385333227044915,
      "grad_norm": 0.4229336082935333,
      "learning_rate": 6.884283456876577e-05,
      "loss": 2.3341,
      "step": 89370
    },
    {
      "epoch": 1.8387390422213592,
      "grad_norm": 0.4279748201370239,
      "learning_rate": 6.882173171761513e-05,
      "loss": 2.3484,
      "step": 89380
    },
    {
      "epoch": 1.838944761738227,
      "grad_norm": 0.4641672372817993,
      "learning_rate": 6.880063040431858e-05,
      "loss": 2.3619,
      "step": 89390
    },
    {
      "epoch": 1.8391504812550947,
      "grad_norm": 0.4572499990463257,
      "learning_rate": 6.87795306299169e-05,
      "loss": 2.3893,
      "step": 89400
    },
    {
      "epoch": 1.8393562007719626,
      "grad_norm": 0.4357408583164215,
      "learning_rate": 6.875843239545087e-05,
      "loss": 2.3428,
      "step": 89410
    },
    {
      "epoch": 1.8395619202888303,
      "grad_norm": 0.5095717906951904,
      "learning_rate": 6.87373357019611e-05,
      "loss": 2.3629,
      "step": 89420
    },
    {
      "epoch": 1.839767639805698,
      "grad_norm": 0.4414459466934204,
      "learning_rate": 6.87162405504882e-05,
      "loss": 2.3592,
      "step": 89430
    },
    {
      "epoch": 1.8399733593225656,
      "grad_norm": 0.4631339907646179,
      "learning_rate": 6.869514694207269e-05,
      "loss": 2.4052,
      "step": 89440
    },
    {
      "epoch": 1.8401790788394332,
      "grad_norm": 0.4300084412097931,
      "learning_rate": 6.867405487775497e-05,
      "loss": 2.3242,
      "step": 89450
    },
    {
      "epoch": 1.840384798356301,
      "grad_norm": 0.4459637701511383,
      "learning_rate": 6.865296435857542e-05,
      "loss": 2.412,
      "step": 89460
    },
    {
      "epoch": 1.8405905178731687,
      "grad_norm": 0.4356177747249603,
      "learning_rate": 6.863187538557435e-05,
      "loss": 2.4131,
      "step": 89470
    },
    {
      "epoch": 1.8407962373900366,
      "grad_norm": 0.4810592532157898,
      "learning_rate": 6.861078795979189e-05,
      "loss": 2.4568,
      "step": 89480
    },
    {
      "epoch": 1.8410019569069043,
      "grad_norm": 0.4242947995662689,
      "learning_rate": 6.858970208226825e-05,
      "loss": 2.4204,
      "step": 89490
    },
    {
      "epoch": 1.841207676423772,
      "grad_norm": 0.46208852529525757,
      "learning_rate": 6.85686177540434e-05,
      "loss": 2.3797,
      "step": 89500
    },
    {
      "epoch": 1.8414133959406396,
      "grad_norm": 0.44510942697525024,
      "learning_rate": 6.854753497615736e-05,
      "loss": 2.4108,
      "step": 89510
    },
    {
      "epoch": 1.8416191154575072,
      "grad_norm": 0.45149660110473633,
      "learning_rate": 6.852645374965005e-05,
      "loss": 2.3913,
      "step": 89520
    },
    {
      "epoch": 1.841824834974375,
      "grad_norm": 0.41496431827545166,
      "learning_rate": 6.850537407556125e-05,
      "loss": 2.3845,
      "step": 89530
    },
    {
      "epoch": 1.8420305544912428,
      "grad_norm": 0.4887220561504364,
      "learning_rate": 6.84842959549307e-05,
      "loss": 2.3833,
      "step": 89540
    },
    {
      "epoch": 1.8422362740081106,
      "grad_norm": 0.41897034645080566,
      "learning_rate": 6.846321938879816e-05,
      "loss": 2.3785,
      "step": 89550
    },
    {
      "epoch": 1.8424419935249783,
      "grad_norm": 0.4196394085884094,
      "learning_rate": 6.844214437820308e-05,
      "loss": 2.4427,
      "step": 89560
    },
    {
      "epoch": 1.842647713041846,
      "grad_norm": 0.44958508014678955,
      "learning_rate": 6.84210709241851e-05,
      "loss": 2.403,
      "step": 89570
    },
    {
      "epoch": 1.8428534325587136,
      "grad_norm": 0.43117576837539673,
      "learning_rate": 6.839999902778357e-05,
      "loss": 2.4242,
      "step": 89580
    },
    {
      "epoch": 1.8430591520755812,
      "grad_norm": 0.4169330894947052,
      "learning_rate": 6.83789286900379e-05,
      "loss": 2.3987,
      "step": 89590
    },
    {
      "epoch": 1.8432648715924491,
      "grad_norm": 0.4272821545600891,
      "learning_rate": 6.835785991198738e-05,
      "loss": 2.3005,
      "step": 89600
    },
    {
      "epoch": 1.8434705911093168,
      "grad_norm": 0.43814682960510254,
      "learning_rate": 6.833679269467117e-05,
      "loss": 2.3834,
      "step": 89610
    },
    {
      "epoch": 1.8436763106261846,
      "grad_norm": 0.4963216781616211,
      "learning_rate": 6.831572703912846e-05,
      "loss": 2.397,
      "step": 89620
    },
    {
      "epoch": 1.8438820301430523,
      "grad_norm": 0.43276238441467285,
      "learning_rate": 6.829466294639828e-05,
      "loss": 2.3926,
      "step": 89630
    },
    {
      "epoch": 1.84408774965992,
      "grad_norm": 0.4191025197505951,
      "learning_rate": 6.827360041751956e-05,
      "loss": 2.4059,
      "step": 89640
    },
    {
      "epoch": 1.8442934691767876,
      "grad_norm": 0.41180741786956787,
      "learning_rate": 6.82525394535313e-05,
      "loss": 2.4444,
      "step": 89650
    },
    {
      "epoch": 1.8444991886936553,
      "grad_norm": 0.43837642669677734,
      "learning_rate": 6.823148005547224e-05,
      "loss": 2.3934,
      "step": 89660
    },
    {
      "epoch": 1.844704908210523,
      "grad_norm": 0.46027708053588867,
      "learning_rate": 6.821042222438112e-05,
      "loss": 2.3768,
      "step": 89670
    },
    {
      "epoch": 1.8449106277273908,
      "grad_norm": 0.5161380171775818,
      "learning_rate": 6.818936596129669e-05,
      "loss": 2.4239,
      "step": 89680
    },
    {
      "epoch": 1.8451163472442587,
      "grad_norm": 0.44482824206352234,
      "learning_rate": 6.816831126725747e-05,
      "loss": 2.3698,
      "step": 89690
    },
    {
      "epoch": 1.8453220667611263,
      "grad_norm": 0.43470871448516846,
      "learning_rate": 6.814725814330198e-05,
      "loss": 2.3529,
      "step": 89700
    },
    {
      "epoch": 1.845527786277994,
      "grad_norm": 0.4874163269996643,
      "learning_rate": 6.81262065904687e-05,
      "loss": 2.4097,
      "step": 89710
    },
    {
      "epoch": 1.8457335057948616,
      "grad_norm": 0.4705725908279419,
      "learning_rate": 6.810515660979593e-05,
      "loss": 2.3992,
      "step": 89720
    },
    {
      "epoch": 1.8459392253117293,
      "grad_norm": 0.4224875569343567,
      "learning_rate": 6.8084108202322e-05,
      "loss": 2.4105,
      "step": 89730
    },
    {
      "epoch": 1.846144944828597,
      "grad_norm": 0.8488872647285461,
      "learning_rate": 6.806306136908507e-05,
      "loss": 2.3987,
      "step": 89740
    },
    {
      "epoch": 1.8463506643454648,
      "grad_norm": 0.4629446268081665,
      "learning_rate": 6.804201611112328e-05,
      "loss": 2.3598,
      "step": 89750
    },
    {
      "epoch": 1.8465563838623325,
      "grad_norm": 0.46334096789360046,
      "learning_rate": 6.802097242947474e-05,
      "loss": 2.3752,
      "step": 89760
    },
    {
      "epoch": 1.8467621033792003,
      "grad_norm": 0.46707984805107117,
      "learning_rate": 6.79999303251773e-05,
      "loss": 2.3696,
      "step": 89770
    },
    {
      "epoch": 1.846967822896068,
      "grad_norm": 0.47050297260284424,
      "learning_rate": 6.797888979926894e-05,
      "loss": 2.4269,
      "step": 89780
    },
    {
      "epoch": 1.8471735424129356,
      "grad_norm": 0.4943868815898895,
      "learning_rate": 6.795785085278748e-05,
      "loss": 2.3765,
      "step": 89790
    },
    {
      "epoch": 1.8473792619298033,
      "grad_norm": 0.4217175841331482,
      "learning_rate": 6.793681348677058e-05,
      "loss": 2.4212,
      "step": 89800
    },
    {
      "epoch": 1.847584981446671,
      "grad_norm": 0.41837507486343384,
      "learning_rate": 6.791577770225599e-05,
      "loss": 2.3526,
      "step": 89810
    },
    {
      "epoch": 1.8477907009635388,
      "grad_norm": 0.4704955816268921,
      "learning_rate": 6.789474350028124e-05,
      "loss": 2.385,
      "step": 89820
    },
    {
      "epoch": 1.8479964204804065,
      "grad_norm": 0.4255530834197998,
      "learning_rate": 6.78737108818838e-05,
      "loss": 2.426,
      "step": 89830
    },
    {
      "epoch": 1.8482021399972743,
      "grad_norm": 0.46005144715309143,
      "learning_rate": 6.785267984810118e-05,
      "loss": 2.3831,
      "step": 89840
    },
    {
      "epoch": 1.848407859514142,
      "grad_norm": 0.38606905937194824,
      "learning_rate": 6.783165039997066e-05,
      "loss": 2.3696,
      "step": 89850
    },
    {
      "epoch": 1.8486135790310096,
      "grad_norm": 0.4556218683719635,
      "learning_rate": 6.781062253852952e-05,
      "loss": 2.3882,
      "step": 89860
    },
    {
      "epoch": 1.8488192985478773,
      "grad_norm": 0.43143710494041443,
      "learning_rate": 6.7789596264815e-05,
      "loss": 2.3548,
      "step": 89870
    },
    {
      "epoch": 1.849025018064745,
      "grad_norm": 0.41824331879615784,
      "learning_rate": 6.776857157986415e-05,
      "loss": 2.3486,
      "step": 89880
    },
    {
      "epoch": 1.8492307375816128,
      "grad_norm": 0.4383297860622406,
      "learning_rate": 6.774754848471403e-05,
      "loss": 2.433,
      "step": 89890
    },
    {
      "epoch": 1.8494364570984805,
      "grad_norm": 0.47132793068885803,
      "learning_rate": 6.772652698040156e-05,
      "loss": 2.3915,
      "step": 89900
    },
    {
      "epoch": 1.8496421766153484,
      "grad_norm": 0.45683860778808594,
      "learning_rate": 6.770550706796366e-05,
      "loss": 2.3393,
      "step": 89910
    },
    {
      "epoch": 1.849847896132216,
      "grad_norm": 0.5972456932067871,
      "learning_rate": 6.768448874843713e-05,
      "loss": 2.3406,
      "step": 89920
    },
    {
      "epoch": 1.8500536156490837,
      "grad_norm": 0.46604955196380615,
      "learning_rate": 6.766347202285864e-05,
      "loss": 2.4459,
      "step": 89930
    },
    {
      "epoch": 1.8502593351659513,
      "grad_norm": 0.44655948877334595,
      "learning_rate": 6.764245689226488e-05,
      "loss": 2.4118,
      "step": 89940
    },
    {
      "epoch": 1.850465054682819,
      "grad_norm": 0.42995622754096985,
      "learning_rate": 6.762144335769242e-05,
      "loss": 2.4256,
      "step": 89950
    },
    {
      "epoch": 1.8506707741996868,
      "grad_norm": 0.4350866675376892,
      "learning_rate": 6.760043142017766e-05,
      "loss": 2.3797,
      "step": 89960
    },
    {
      "epoch": 1.8508764937165545,
      "grad_norm": 0.448342889547348,
      "learning_rate": 6.757942108075712e-05,
      "loss": 2.4081,
      "step": 89970
    },
    {
      "epoch": 1.8510822132334224,
      "grad_norm": 0.45947107672691345,
      "learning_rate": 6.755841234046702e-05,
      "loss": 2.4206,
      "step": 89980
    },
    {
      "epoch": 1.85128793275029,
      "grad_norm": 0.4604204297065735,
      "learning_rate": 6.753740520034366e-05,
      "loss": 2.4124,
      "step": 89990
    },
    {
      "epoch": 1.8514936522671577,
      "grad_norm": 0.4269692301750183,
      "learning_rate": 6.751639966142324e-05,
      "loss": 2.4141,
      "step": 90000
    },
    {
      "epoch": 1.8516993717840253,
      "grad_norm": 0.42400965094566345,
      "learning_rate": 6.749539572474178e-05,
      "loss": 2.4146,
      "step": 90010
    },
    {
      "epoch": 1.851905091300893,
      "grad_norm": 0.4302583634853363,
      "learning_rate": 6.74743933913353e-05,
      "loss": 2.3961,
      "step": 90020
    },
    {
      "epoch": 1.8521108108177606,
      "grad_norm": 0.45451119542121887,
      "learning_rate": 6.74533926622398e-05,
      "loss": 2.3904,
      "step": 90030
    },
    {
      "epoch": 1.8523165303346285,
      "grad_norm": 0.4358251392841339,
      "learning_rate": 6.743239353849106e-05,
      "loss": 2.3941,
      "step": 90040
    },
    {
      "epoch": 1.8525222498514964,
      "grad_norm": 0.4330974519252777,
      "learning_rate": 6.741139602112489e-05,
      "loss": 2.3931,
      "step": 90050
    },
    {
      "epoch": 1.852727969368364,
      "grad_norm": 0.43600067496299744,
      "learning_rate": 6.739040011117693e-05,
      "loss": 2.4036,
      "step": 90060
    },
    {
      "epoch": 1.8529336888852317,
      "grad_norm": 0.45042893290519714,
      "learning_rate": 6.736940580968285e-05,
      "loss": 2.4127,
      "step": 90070
    },
    {
      "epoch": 1.8531394084020993,
      "grad_norm": 0.4268648326396942,
      "learning_rate": 6.734841311767819e-05,
      "loss": 2.3301,
      "step": 90080
    },
    {
      "epoch": 1.853345127918967,
      "grad_norm": 0.48867133259773254,
      "learning_rate": 6.732742203619833e-05,
      "loss": 2.367,
      "step": 90090
    },
    {
      "epoch": 1.8535508474358346,
      "grad_norm": 0.45869627594947815,
      "learning_rate": 6.730643256627874e-05,
      "loss": 2.3387,
      "step": 90100
    },
    {
      "epoch": 1.8537565669527025,
      "grad_norm": 0.40781188011169434,
      "learning_rate": 6.728544470895466e-05,
      "loss": 2.3921,
      "step": 90110
    },
    {
      "epoch": 1.8539622864695702,
      "grad_norm": 0.41421595215797424,
      "learning_rate": 6.72644584652613e-05,
      "loss": 2.4247,
      "step": 90120
    },
    {
      "epoch": 1.854168005986438,
      "grad_norm": 0.5754795670509338,
      "learning_rate": 6.724347383623385e-05,
      "loss": 2.2758,
      "step": 90130
    },
    {
      "epoch": 1.8543737255033057,
      "grad_norm": 0.43957260251045227,
      "learning_rate": 6.722249082290733e-05,
      "loss": 2.3971,
      "step": 90140
    },
    {
      "epoch": 1.8545794450201734,
      "grad_norm": 0.4545102119445801,
      "learning_rate": 6.720150942631668e-05,
      "loss": 2.4115,
      "step": 90150
    },
    {
      "epoch": 1.854785164537041,
      "grad_norm": 0.45308250188827515,
      "learning_rate": 6.718052964749689e-05,
      "loss": 2.3661,
      "step": 90160
    },
    {
      "epoch": 1.8549908840539087,
      "grad_norm": 0.43385186791419983,
      "learning_rate": 6.71595514874827e-05,
      "loss": 2.4117,
      "step": 90170
    },
    {
      "epoch": 1.8551966035707765,
      "grad_norm": 0.4005761444568634,
      "learning_rate": 6.713857494730891e-05,
      "loss": 2.3796,
      "step": 90180
    },
    {
      "epoch": 1.8554023230876442,
      "grad_norm": 0.43642619252204895,
      "learning_rate": 6.711760002801008e-05,
      "loss": 2.3475,
      "step": 90190
    },
    {
      "epoch": 1.855608042604512,
      "grad_norm": 0.445588618516922,
      "learning_rate": 6.70966267306209e-05,
      "loss": 2.3662,
      "step": 90200
    },
    {
      "epoch": 1.8558137621213797,
      "grad_norm": 0.4387073516845703,
      "learning_rate": 6.707565505617584e-05,
      "loss": 2.4468,
      "step": 90210
    },
    {
      "epoch": 1.8560194816382474,
      "grad_norm": 0.4224250018596649,
      "learning_rate": 6.705468500570925e-05,
      "loss": 2.3899,
      "step": 90220
    },
    {
      "epoch": 1.856225201155115,
      "grad_norm": 0.4436706304550171,
      "learning_rate": 6.703371658025553e-05,
      "loss": 2.3512,
      "step": 90230
    },
    {
      "epoch": 1.8564309206719827,
      "grad_norm": 0.4655897319316864,
      "learning_rate": 6.701274978084895e-05,
      "loss": 2.4258,
      "step": 90240
    },
    {
      "epoch": 1.8566366401888506,
      "grad_norm": 0.48048296570777893,
      "learning_rate": 6.699178460852364e-05,
      "loss": 2.4136,
      "step": 90250
    },
    {
      "epoch": 1.8568423597057182,
      "grad_norm": 0.41441890597343445,
      "learning_rate": 6.697082106431376e-05,
      "loss": 2.3469,
      "step": 90260
    },
    {
      "epoch": 1.857048079222586,
      "grad_norm": 0.4544997215270996,
      "learning_rate": 6.694985914925326e-05,
      "loss": 2.4244,
      "step": 90270
    },
    {
      "epoch": 1.8572537987394537,
      "grad_norm": 0.47472232580184937,
      "learning_rate": 6.692889886437607e-05,
      "loss": 2.3901,
      "step": 90280
    },
    {
      "epoch": 1.8574595182563214,
      "grad_norm": 0.46058914065361023,
      "learning_rate": 6.690794021071616e-05,
      "loss": 2.4237,
      "step": 90290
    },
    {
      "epoch": 1.857665237773189,
      "grad_norm": 0.4202367663383484,
      "learning_rate": 6.68869831893072e-05,
      "loss": 2.3321,
      "step": 90300
    },
    {
      "epoch": 1.8578709572900567,
      "grad_norm": 0.5118812322616577,
      "learning_rate": 6.686602780118287e-05,
      "loss": 2.3992,
      "step": 90310
    },
    {
      "epoch": 1.8580766768069246,
      "grad_norm": 0.4476131200790405,
      "learning_rate": 6.684507404737689e-05,
      "loss": 2.4001,
      "step": 90320
    },
    {
      "epoch": 1.8582823963237922,
      "grad_norm": 0.43168023228645325,
      "learning_rate": 6.682412192892271e-05,
      "loss": 2.3896,
      "step": 90330
    },
    {
      "epoch": 1.85848811584066,
      "grad_norm": 0.41892382502555847,
      "learning_rate": 6.680317144685383e-05,
      "loss": 2.3726,
      "step": 90340
    },
    {
      "epoch": 1.8586938353575277,
      "grad_norm": 0.44078901410102844,
      "learning_rate": 6.678222260220356e-05,
      "loss": 2.3627,
      "step": 90350
    },
    {
      "epoch": 1.8588995548743954,
      "grad_norm": 0.4600977897644043,
      "learning_rate": 6.676127539600526e-05,
      "loss": 2.3609,
      "step": 90360
    },
    {
      "epoch": 1.859105274391263,
      "grad_norm": 0.4241754412651062,
      "learning_rate": 6.674032982929212e-05,
      "loss": 2.3332,
      "step": 90370
    },
    {
      "epoch": 1.8593109939081307,
      "grad_norm": 0.47781306505203247,
      "learning_rate": 6.671938590309725e-05,
      "loss": 2.4022,
      "step": 90380
    },
    {
      "epoch": 1.8595167134249984,
      "grad_norm": 0.435665100812912,
      "learning_rate": 6.669844361845372e-05,
      "loss": 2.386,
      "step": 90390
    },
    {
      "epoch": 1.8597224329418662,
      "grad_norm": 0.5488210916519165,
      "learning_rate": 6.66775029763945e-05,
      "loss": 2.3861,
      "step": 90400
    },
    {
      "epoch": 1.859928152458734,
      "grad_norm": 0.42135173082351685,
      "learning_rate": 6.665656397795245e-05,
      "loss": 2.3958,
      "step": 90410
    },
    {
      "epoch": 1.8601338719756018,
      "grad_norm": 0.4365689158439636,
      "learning_rate": 6.663562662416045e-05,
      "loss": 2.4287,
      "step": 90420
    },
    {
      "epoch": 1.8603395914924694,
      "grad_norm": 0.48146024346351624,
      "learning_rate": 6.661469091605113e-05,
      "loss": 2.3998,
      "step": 90430
    },
    {
      "epoch": 1.860545311009337,
      "grad_norm": 0.39794450998306274,
      "learning_rate": 6.659375685465718e-05,
      "loss": 2.3448,
      "step": 90440
    },
    {
      "epoch": 1.8607510305262047,
      "grad_norm": 0.46959203481674194,
      "learning_rate": 6.657282444101119e-05,
      "loss": 2.3897,
      "step": 90450
    },
    {
      "epoch": 1.8609567500430724,
      "grad_norm": 0.41845107078552246,
      "learning_rate": 6.65518936761456e-05,
      "loss": 2.4093,
      "step": 90460
    },
    {
      "epoch": 1.8611624695599402,
      "grad_norm": 0.45155569911003113,
      "learning_rate": 6.65309645610928e-05,
      "loss": 2.3428,
      "step": 90470
    },
    {
      "epoch": 1.861368189076808,
      "grad_norm": 0.4127676486968994,
      "learning_rate": 6.651003709688517e-05,
      "loss": 2.3824,
      "step": 90480
    },
    {
      "epoch": 1.8615739085936758,
      "grad_norm": 0.44402116537094116,
      "learning_rate": 6.64891112845549e-05,
      "loss": 2.3413,
      "step": 90490
    },
    {
      "epoch": 1.8617796281105434,
      "grad_norm": 0.428641140460968,
      "learning_rate": 6.646818712513418e-05,
      "loss": 2.4157,
      "step": 90500
    },
    {
      "epoch": 1.861985347627411,
      "grad_norm": 0.4202735424041748,
      "learning_rate": 6.644726461965503e-05,
      "loss": 2.4511,
      "step": 90510
    },
    {
      "epoch": 1.8621910671442787,
      "grad_norm": 0.505913496017456,
      "learning_rate": 6.642634376914946e-05,
      "loss": 2.3877,
      "step": 90520
    },
    {
      "epoch": 1.8623967866611464,
      "grad_norm": 0.4314071536064148,
      "learning_rate": 6.640542457464946e-05,
      "loss": 2.4439,
      "step": 90530
    },
    {
      "epoch": 1.8626025061780143,
      "grad_norm": 0.4419718384742737,
      "learning_rate": 6.638450703718673e-05,
      "loss": 2.3851,
      "step": 90540
    },
    {
      "epoch": 1.862808225694882,
      "grad_norm": 0.5194011926651001,
      "learning_rate": 6.636359115779312e-05,
      "loss": 2.3813,
      "step": 90550
    },
    {
      "epoch": 1.8630139452117498,
      "grad_norm": 0.45022034645080566,
      "learning_rate": 6.634267693750027e-05,
      "loss": 2.4161,
      "step": 90560
    },
    {
      "epoch": 1.8632196647286174,
      "grad_norm": 0.44785916805267334,
      "learning_rate": 6.632176437733972e-05,
      "loss": 2.3686,
      "step": 90570
    },
    {
      "epoch": 1.863425384245485,
      "grad_norm": 0.44341689348220825,
      "learning_rate": 6.630085347834308e-05,
      "loss": 2.3618,
      "step": 90580
    },
    {
      "epoch": 1.8636311037623527,
      "grad_norm": 0.4184902310371399,
      "learning_rate": 6.627994424154163e-05,
      "loss": 2.4207,
      "step": 90590
    },
    {
      "epoch": 1.8638368232792204,
      "grad_norm": 0.47492197155952454,
      "learning_rate": 6.625903666796678e-05,
      "loss": 2.4177,
      "step": 90600
    },
    {
      "epoch": 1.8640425427960883,
      "grad_norm": 0.5022614598274231,
      "learning_rate": 6.623813075864982e-05,
      "loss": 2.3625,
      "step": 90610
    },
    {
      "epoch": 1.864248262312956,
      "grad_norm": 0.48158666491508484,
      "learning_rate": 6.621722651462187e-05,
      "loss": 2.4426,
      "step": 90620
    },
    {
      "epoch": 1.8644539818298238,
      "grad_norm": 0.4447362422943115,
      "learning_rate": 6.619632393691402e-05,
      "loss": 2.3941,
      "step": 90630
    },
    {
      "epoch": 1.8646597013466915,
      "grad_norm": 0.3852933645248413,
      "learning_rate": 6.617542302655734e-05,
      "loss": 2.3939,
      "step": 90640
    },
    {
      "epoch": 1.864865420863559,
      "grad_norm": 0.40845927596092224,
      "learning_rate": 6.615452378458271e-05,
      "loss": 2.4251,
      "step": 90650
    },
    {
      "epoch": 1.8650711403804268,
      "grad_norm": 0.4585970640182495,
      "learning_rate": 6.613362621202101e-05,
      "loss": 2.4078,
      "step": 90660
    },
    {
      "epoch": 1.8652768598972944,
      "grad_norm": 0.4116969108581543,
      "learning_rate": 6.611273030990293e-05,
      "loss": 2.4066,
      "step": 90670
    },
    {
      "epoch": 1.8654825794141623,
      "grad_norm": 0.4246051609516144,
      "learning_rate": 6.609183607925923e-05,
      "loss": 2.3779,
      "step": 90680
    },
    {
      "epoch": 1.86568829893103,
      "grad_norm": 0.4175414741039276,
      "learning_rate": 6.607094352112048e-05,
      "loss": 2.4411,
      "step": 90690
    },
    {
      "epoch": 1.8658940184478978,
      "grad_norm": 0.42926478385925293,
      "learning_rate": 6.605005263651718e-05,
      "loss": 2.4066,
      "step": 90700
    },
    {
      "epoch": 1.8660997379647655,
      "grad_norm": 0.41879966855049133,
      "learning_rate": 6.602916342647977e-05,
      "loss": 2.3837,
      "step": 90710
    },
    {
      "epoch": 1.8663054574816331,
      "grad_norm": 0.44260063767433167,
      "learning_rate": 6.600827589203866e-05,
      "loss": 2.3694,
      "step": 90720
    },
    {
      "epoch": 1.8665111769985008,
      "grad_norm": 0.4177769720554352,
      "learning_rate": 6.598739003422401e-05,
      "loss": 2.3894,
      "step": 90730
    },
    {
      "epoch": 1.8667168965153684,
      "grad_norm": 0.46344155073165894,
      "learning_rate": 6.596650585406613e-05,
      "loss": 2.4249,
      "step": 90740
    },
    {
      "epoch": 1.866922616032236,
      "grad_norm": 0.3946082293987274,
      "learning_rate": 6.594562335259498e-05,
      "loss": 2.4207,
      "step": 90750
    },
    {
      "epoch": 1.867128335549104,
      "grad_norm": 0.4220229685306549,
      "learning_rate": 6.592474253084068e-05,
      "loss": 2.3795,
      "step": 90760
    },
    {
      "epoch": 1.8673340550659718,
      "grad_norm": 0.4296412765979767,
      "learning_rate": 6.590386338983319e-05,
      "loss": 2.4628,
      "step": 90770
    },
    {
      "epoch": 1.8675397745828395,
      "grad_norm": 0.44302594661712646,
      "learning_rate": 6.588298593060225e-05,
      "loss": 2.4207,
      "step": 90780
    },
    {
      "epoch": 1.8677454940997071,
      "grad_norm": 0.4856652021408081,
      "learning_rate": 6.586211015417772e-05,
      "loss": 2.3874,
      "step": 90790
    },
    {
      "epoch": 1.8679512136165748,
      "grad_norm": 0.4309452176094055,
      "learning_rate": 6.584123606158928e-05,
      "loss": 2.3925,
      "step": 90800
    },
    {
      "epoch": 1.8681569331334424,
      "grad_norm": 0.45921310782432556,
      "learning_rate": 6.582036365386652e-05,
      "loss": 2.3899,
      "step": 90810
    },
    {
      "epoch": 1.86836265265031,
      "grad_norm": 0.4397883713245392,
      "learning_rate": 6.579949293203898e-05,
      "loss": 2.3646,
      "step": 90820
    },
    {
      "epoch": 1.868568372167178,
      "grad_norm": 0.4320881962776184,
      "learning_rate": 6.577862389713605e-05,
      "loss": 2.4451,
      "step": 90830
    },
    {
      "epoch": 1.8687740916840458,
      "grad_norm": 0.4434238374233246,
      "learning_rate": 6.575775655018712e-05,
      "loss": 2.3868,
      "step": 90840
    },
    {
      "epoch": 1.8689798112009135,
      "grad_norm": 0.438994824886322,
      "learning_rate": 6.573689089222151e-05,
      "loss": 2.3749,
      "step": 90850
    },
    {
      "epoch": 1.8691855307177812,
      "grad_norm": 0.4417699873447418,
      "learning_rate": 6.57160269242683e-05,
      "loss": 2.388,
      "step": 90860
    },
    {
      "epoch": 1.8693912502346488,
      "grad_norm": 0.45111995935440063,
      "learning_rate": 6.569516464735668e-05,
      "loss": 2.3883,
      "step": 90870
    },
    {
      "epoch": 1.8695969697515165,
      "grad_norm": 0.4293932616710663,
      "learning_rate": 6.567430406251567e-05,
      "loss": 2.4451,
      "step": 90880
    },
    {
      "epoch": 1.869802689268384,
      "grad_norm": 0.40787842869758606,
      "learning_rate": 6.565344517077415e-05,
      "loss": 2.3419,
      "step": 90890
    },
    {
      "epoch": 1.870008408785252,
      "grad_norm": 0.4772271513938904,
      "learning_rate": 6.563258797316107e-05,
      "loss": 2.4037,
      "step": 90900
    },
    {
      "epoch": 1.8702141283021196,
      "grad_norm": 0.3976544141769409,
      "learning_rate": 6.561173247070508e-05,
      "loss": 2.3607,
      "step": 90910
    },
    {
      "epoch": 1.8704198478189875,
      "grad_norm": 0.4330104887485504,
      "learning_rate": 6.559087866443494e-05,
      "loss": 2.4036,
      "step": 90920
    },
    {
      "epoch": 1.8706255673358552,
      "grad_norm": 0.4385615289211273,
      "learning_rate": 6.557002655537931e-05,
      "loss": 2.434,
      "step": 90930
    },
    {
      "epoch": 1.8708312868527228,
      "grad_norm": 0.49091780185699463,
      "learning_rate": 6.554917614456659e-05,
      "loss": 2.4616,
      "step": 90940
    },
    {
      "epoch": 1.8710370063695905,
      "grad_norm": 0.41058170795440674,
      "learning_rate": 6.552832743302526e-05,
      "loss": 2.4058,
      "step": 90950
    },
    {
      "epoch": 1.8712427258864581,
      "grad_norm": 0.4438905119895935,
      "learning_rate": 6.550748042178377e-05,
      "loss": 2.3848,
      "step": 90960
    },
    {
      "epoch": 1.871448445403326,
      "grad_norm": 0.4404224753379822,
      "learning_rate": 6.548663511187026e-05,
      "loss": 2.4333,
      "step": 90970
    },
    {
      "epoch": 1.8716541649201937,
      "grad_norm": 0.4457189738750458,
      "learning_rate": 6.546579150431298e-05,
      "loss": 2.4558,
      "step": 90980
    },
    {
      "epoch": 1.8718598844370615,
      "grad_norm": 0.4457888901233673,
      "learning_rate": 6.544494960013998e-05,
      "loss": 2.3775,
      "step": 90990
    },
    {
      "epoch": 1.8720656039539292,
      "grad_norm": 0.4284377992153168,
      "learning_rate": 6.542410940037933e-05,
      "loss": 2.3814,
      "step": 91000
    },
    {
      "epoch": 1.8722713234707968,
      "grad_norm": 0.4361388385295868,
      "learning_rate": 6.540327090605898e-05,
      "loss": 2.3783,
      "step": 91010
    },
    {
      "epoch": 1.8724770429876645,
      "grad_norm": 0.4340449273586273,
      "learning_rate": 6.53824341182067e-05,
      "loss": 2.4269,
      "step": 91020
    },
    {
      "epoch": 1.8726827625045321,
      "grad_norm": 0.42956748604774475,
      "learning_rate": 6.536159903785033e-05,
      "loss": 2.4013,
      "step": 91030
    },
    {
      "epoch": 1.8728884820214,
      "grad_norm": 0.4323524534702301,
      "learning_rate": 6.534076566601752e-05,
      "loss": 2.3829,
      "step": 91040
    },
    {
      "epoch": 1.8730942015382677,
      "grad_norm": 0.40111008286476135,
      "learning_rate": 6.531993400373586e-05,
      "loss": 2.3231,
      "step": 91050
    },
    {
      "epoch": 1.8732999210551355,
      "grad_norm": 0.4015694856643677,
      "learning_rate": 6.529910405203291e-05,
      "loss": 2.4182,
      "step": 91060
    },
    {
      "epoch": 1.8735056405720032,
      "grad_norm": 0.4012575149536133,
      "learning_rate": 6.5278275811936e-05,
      "loss": 2.408,
      "step": 91070
    },
    {
      "epoch": 1.8737113600888708,
      "grad_norm": 0.6398689150810242,
      "learning_rate": 6.525744928447254e-05,
      "loss": 2.3647,
      "step": 91080
    },
    {
      "epoch": 1.8739170796057385,
      "grad_norm": 0.47409161925315857,
      "learning_rate": 6.523662447066985e-05,
      "loss": 2.389,
      "step": 91090
    },
    {
      "epoch": 1.8741227991226062,
      "grad_norm": 0.4349336624145508,
      "learning_rate": 6.521580137155498e-05,
      "loss": 2.3301,
      "step": 91100
    },
    {
      "epoch": 1.874328518639474,
      "grad_norm": 0.4153061807155609,
      "learning_rate": 6.519497998815506e-05,
      "loss": 2.3635,
      "step": 91110
    },
    {
      "epoch": 1.8745342381563417,
      "grad_norm": 0.4494875371456146,
      "learning_rate": 6.517416032149719e-05,
      "loss": 2.4122,
      "step": 91120
    },
    {
      "epoch": 1.8747399576732096,
      "grad_norm": 0.5035796165466309,
      "learning_rate": 6.515334237260813e-05,
      "loss": 2.3548,
      "step": 91130
    },
    {
      "epoch": 1.8749456771900772,
      "grad_norm": 0.43417543172836304,
      "learning_rate": 6.513252614251486e-05,
      "loss": 2.3699,
      "step": 91140
    },
    {
      "epoch": 1.8751513967069449,
      "grad_norm": 0.4357742667198181,
      "learning_rate": 6.511171163224404e-05,
      "loss": 2.4151,
      "step": 91150
    },
    {
      "epoch": 1.8753571162238125,
      "grad_norm": 0.44539669156074524,
      "learning_rate": 6.509089884282236e-05,
      "loss": 2.3934,
      "step": 91160
    },
    {
      "epoch": 1.8755628357406802,
      "grad_norm": 0.41425973176956177,
      "learning_rate": 6.507008777527644e-05,
      "loss": 2.3956,
      "step": 91170
    },
    {
      "epoch": 1.8757685552575478,
      "grad_norm": 0.4361231029033661,
      "learning_rate": 6.504927843063268e-05,
      "loss": 2.3754,
      "step": 91180
    },
    {
      "epoch": 1.8759742747744157,
      "grad_norm": 0.4790736138820648,
      "learning_rate": 6.502847080991761e-05,
      "loss": 2.4122,
      "step": 91190
    },
    {
      "epoch": 1.8761799942912836,
      "grad_norm": 0.47834375500679016,
      "learning_rate": 6.500766491415752e-05,
      "loss": 2.3592,
      "step": 91200
    },
    {
      "epoch": 1.8763857138081512,
      "grad_norm": 0.4373825192451477,
      "learning_rate": 6.498686074437858e-05,
      "loss": 2.4405,
      "step": 91210
    },
    {
      "epoch": 1.8765914333250189,
      "grad_norm": 0.43286794424057007,
      "learning_rate": 6.496605830160707e-05,
      "loss": 2.3622,
      "step": 91220
    },
    {
      "epoch": 1.8767971528418865,
      "grad_norm": 0.46466922760009766,
      "learning_rate": 6.494525758686893e-05,
      "loss": 2.4092,
      "step": 91230
    },
    {
      "epoch": 1.8770028723587542,
      "grad_norm": 0.46718698740005493,
      "learning_rate": 6.49244586011902e-05,
      "loss": 2.4651,
      "step": 91240
    },
    {
      "epoch": 1.8772085918756218,
      "grad_norm": 0.4220433831214905,
      "learning_rate": 6.490366134559687e-05,
      "loss": 2.3599,
      "step": 91250
    },
    {
      "epoch": 1.8774143113924897,
      "grad_norm": 0.4329603314399719,
      "learning_rate": 6.48828658211146e-05,
      "loss": 2.3959,
      "step": 91260
    },
    {
      "epoch": 1.8776200309093574,
      "grad_norm": 0.42117607593536377,
      "learning_rate": 6.486207202876922e-05,
      "loss": 2.3834,
      "step": 91270
    },
    {
      "epoch": 1.8778257504262252,
      "grad_norm": 0.40976348519325256,
      "learning_rate": 6.484127996958636e-05,
      "loss": 2.3629,
      "step": 91280
    },
    {
      "epoch": 1.878031469943093,
      "grad_norm": 0.5383798480033875,
      "learning_rate": 6.482048964459151e-05,
      "loss": 2.4159,
      "step": 91290
    },
    {
      "epoch": 1.8782371894599605,
      "grad_norm": 0.45883697271347046,
      "learning_rate": 6.479970105481027e-05,
      "loss": 2.3149,
      "step": 91300
    },
    {
      "epoch": 1.8784429089768282,
      "grad_norm": 0.41599881649017334,
      "learning_rate": 6.477891420126793e-05,
      "loss": 2.3701,
      "step": 91310
    },
    {
      "epoch": 1.8786486284936958,
      "grad_norm": 0.4287915527820587,
      "learning_rate": 6.47581290849898e-05,
      "loss": 2.4001,
      "step": 91320
    },
    {
      "epoch": 1.8788543480105637,
      "grad_norm": 0.46515315771102905,
      "learning_rate": 6.473734570700114e-05,
      "loss": 2.3785,
      "step": 91330
    },
    {
      "epoch": 1.8790600675274314,
      "grad_norm": 0.5024645328521729,
      "learning_rate": 6.471656406832706e-05,
      "loss": 2.4044,
      "step": 91340
    },
    {
      "epoch": 1.8792657870442993,
      "grad_norm": 0.4660324454307556,
      "learning_rate": 6.469578416999263e-05,
      "loss": 2.4397,
      "step": 91350
    },
    {
      "epoch": 1.879471506561167,
      "grad_norm": 0.4393770098686218,
      "learning_rate": 6.467500601302271e-05,
      "loss": 2.3773,
      "step": 91360
    },
    {
      "epoch": 1.8796772260780346,
      "grad_norm": 0.4204711616039276,
      "learning_rate": 6.465422959844226e-05,
      "loss": 2.3838,
      "step": 91370
    },
    {
      "epoch": 1.8798829455949022,
      "grad_norm": 0.4599703848361969,
      "learning_rate": 6.463345492727612e-05,
      "loss": 2.3642,
      "step": 91380
    },
    {
      "epoch": 1.8800886651117699,
      "grad_norm": 0.4311610162258148,
      "learning_rate": 6.461268200054886e-05,
      "loss": 2.3545,
      "step": 91390
    },
    {
      "epoch": 1.8802943846286377,
      "grad_norm": 0.4790462553501129,
      "learning_rate": 6.459191081928515e-05,
      "loss": 2.4537,
      "step": 91400
    },
    {
      "epoch": 1.8805001041455054,
      "grad_norm": 0.46498367190361023,
      "learning_rate": 6.45711413845096e-05,
      "loss": 2.3551,
      "step": 91410
    },
    {
      "epoch": 1.8807058236623733,
      "grad_norm": 0.43564221262931824,
      "learning_rate": 6.455037369724652e-05,
      "loss": 2.4132,
      "step": 91420
    },
    {
      "epoch": 1.880911543179241,
      "grad_norm": 0.4883398711681366,
      "learning_rate": 6.452960775852037e-05,
      "loss": 2.3852,
      "step": 91430
    },
    {
      "epoch": 1.8811172626961086,
      "grad_norm": 0.5414177775382996,
      "learning_rate": 6.450884356935536e-05,
      "loss": 2.391,
      "step": 91440
    },
    {
      "epoch": 1.8813229822129762,
      "grad_norm": 0.46829184889793396,
      "learning_rate": 6.448808113077566e-05,
      "loss": 2.4128,
      "step": 91450
    },
    {
      "epoch": 1.8815287017298439,
      "grad_norm": 0.42966538667678833,
      "learning_rate": 6.446732044380546e-05,
      "loss": 2.3821,
      "step": 91460
    },
    {
      "epoch": 1.8817344212467118,
      "grad_norm": 0.44019562005996704,
      "learning_rate": 6.444656150946867e-05,
      "loss": 2.4118,
      "step": 91470
    },
    {
      "epoch": 1.8819401407635794,
      "grad_norm": 0.4391895830631256,
      "learning_rate": 6.442580432878925e-05,
      "loss": 2.3814,
      "step": 91480
    },
    {
      "epoch": 1.8821458602804473,
      "grad_norm": 0.4313350021839142,
      "learning_rate": 6.440504890279108e-05,
      "loss": 2.4034,
      "step": 91490
    },
    {
      "epoch": 1.882351579797315,
      "grad_norm": 0.43395715951919556,
      "learning_rate": 6.438429523249787e-05,
      "loss": 2.4129,
      "step": 91500
    },
    {
      "epoch": 1.8825572993141826,
      "grad_norm": 0.4507271647453308,
      "learning_rate": 6.436354331893331e-05,
      "loss": 2.357,
      "step": 91510
    },
    {
      "epoch": 1.8827630188310502,
      "grad_norm": 0.42682135105133057,
      "learning_rate": 6.434279316312093e-05,
      "loss": 2.3397,
      "step": 91520
    },
    {
      "epoch": 1.882968738347918,
      "grad_norm": 0.46134135127067566,
      "learning_rate": 6.432204476608426e-05,
      "loss": 2.363,
      "step": 91530
    },
    {
      "epoch": 1.8831744578647855,
      "grad_norm": 0.4678691029548645,
      "learning_rate": 6.430129812884673e-05,
      "loss": 2.3828,
      "step": 91540
    },
    {
      "epoch": 1.8833801773816534,
      "grad_norm": 0.47198930382728577,
      "learning_rate": 6.428055325243158e-05,
      "loss": 2.3888,
      "step": 91550
    },
    {
      "epoch": 1.8835858968985213,
      "grad_norm": 0.4186001420021057,
      "learning_rate": 6.425981013786209e-05,
      "loss": 2.4086,
      "step": 91560
    },
    {
      "epoch": 1.883791616415389,
      "grad_norm": 0.4567194879055023,
      "learning_rate": 6.423906878616148e-05,
      "loss": 2.4159,
      "step": 91570
    },
    {
      "epoch": 1.8839973359322566,
      "grad_norm": 0.4593752324581146,
      "learning_rate": 6.421832919835266e-05,
      "loss": 2.3306,
      "step": 91580
    },
    {
      "epoch": 1.8842030554491243,
      "grad_norm": 0.4600312411785126,
      "learning_rate": 6.419759137545872e-05,
      "loss": 2.3561,
      "step": 91590
    },
    {
      "epoch": 1.884408774965992,
      "grad_norm": 0.44204092025756836,
      "learning_rate": 6.417685531850247e-05,
      "loss": 2.3366,
      "step": 91600
    },
    {
      "epoch": 1.8846144944828596,
      "grad_norm": 0.421246200799942,
      "learning_rate": 6.415612102850672e-05,
      "loss": 2.394,
      "step": 91610
    },
    {
      "epoch": 1.8848202139997274,
      "grad_norm": 0.48245176672935486,
      "learning_rate": 6.413538850649424e-05,
      "loss": 2.3772,
      "step": 91620
    },
    {
      "epoch": 1.885025933516595,
      "grad_norm": 0.4082530736923218,
      "learning_rate": 6.411465775348757e-05,
      "loss": 2.3886,
      "step": 91630
    },
    {
      "epoch": 1.885231653033463,
      "grad_norm": 0.5015794634819031,
      "learning_rate": 6.409392877050927e-05,
      "loss": 2.4294,
      "step": 91640
    },
    {
      "epoch": 1.8854373725503306,
      "grad_norm": 0.4394964575767517,
      "learning_rate": 6.407320155858186e-05,
      "loss": 2.4096,
      "step": 91650
    },
    {
      "epoch": 1.8856430920671983,
      "grad_norm": 0.4689101278781891,
      "learning_rate": 6.405247611872758e-05,
      "loss": 2.3243,
      "step": 91660
    },
    {
      "epoch": 1.885848811584066,
      "grad_norm": 0.4231984317302704,
      "learning_rate": 6.403175245196881e-05,
      "loss": 2.3687,
      "step": 91670
    },
    {
      "epoch": 1.8860545311009336,
      "grad_norm": 0.5039031505584717,
      "learning_rate": 6.401103055932763e-05,
      "loss": 2.429,
      "step": 91680
    },
    {
      "epoch": 1.8862602506178014,
      "grad_norm": 0.46682479977607727,
      "learning_rate": 6.399031044182625e-05,
      "loss": 2.3929,
      "step": 91690
    },
    {
      "epoch": 1.886465970134669,
      "grad_norm": 0.4143178164958954,
      "learning_rate": 6.396959210048661e-05,
      "loss": 2.4244,
      "step": 91700
    },
    {
      "epoch": 1.886671689651537,
      "grad_norm": 0.4846198260784149,
      "learning_rate": 6.394887553633062e-05,
      "loss": 2.4389,
      "step": 91710
    },
    {
      "epoch": 1.8868774091684046,
      "grad_norm": 0.40929970145225525,
      "learning_rate": 6.392816075038017e-05,
      "loss": 2.3934,
      "step": 91720
    },
    {
      "epoch": 1.8870831286852723,
      "grad_norm": 0.4424014985561371,
      "learning_rate": 6.390744774365701e-05,
      "loss": 2.3361,
      "step": 91730
    },
    {
      "epoch": 1.88728884820214,
      "grad_norm": 0.4422971308231354,
      "learning_rate": 6.388673651718271e-05,
      "loss": 2.4,
      "step": 91740
    },
    {
      "epoch": 1.8874945677190076,
      "grad_norm": 0.5142861604690552,
      "learning_rate": 6.386602707197895e-05,
      "loss": 2.4308,
      "step": 91750
    },
    {
      "epoch": 1.8877002872358755,
      "grad_norm": 0.4673389196395874,
      "learning_rate": 6.384531940906714e-05,
      "loss": 2.3966,
      "step": 91760
    },
    {
      "epoch": 1.8879060067527431,
      "grad_norm": 0.4705176055431366,
      "learning_rate": 6.382461352946872e-05,
      "loss": 2.3929,
      "step": 91770
    },
    {
      "epoch": 1.888111726269611,
      "grad_norm": 0.41606467962265015,
      "learning_rate": 6.3803909434205e-05,
      "loss": 2.4294,
      "step": 91780
    },
    {
      "epoch": 1.8883174457864786,
      "grad_norm": 0.43911415338516235,
      "learning_rate": 6.378320712429715e-05,
      "loss": 2.3822,
      "step": 91790
    },
    {
      "epoch": 1.8885231653033463,
      "grad_norm": 0.4222176671028137,
      "learning_rate": 6.376250660076633e-05,
      "loss": 2.4637,
      "step": 91800
    },
    {
      "epoch": 1.888728884820214,
      "grad_norm": 0.44886720180511475,
      "learning_rate": 6.374180786463363e-05,
      "loss": 2.3805,
      "step": 91810
    },
    {
      "epoch": 1.8889346043370816,
      "grad_norm": 0.4613570272922516,
      "learning_rate": 6.372111091691994e-05,
      "loss": 2.3953,
      "step": 91820
    },
    {
      "epoch": 1.8891403238539495,
      "grad_norm": 0.49130991101264954,
      "learning_rate": 6.370041575864617e-05,
      "loss": 2.414,
      "step": 91830
    },
    {
      "epoch": 1.8893460433708171,
      "grad_norm": 0.4520902633666992,
      "learning_rate": 6.367972239083304e-05,
      "loss": 2.4013,
      "step": 91840
    },
    {
      "epoch": 1.889551762887685,
      "grad_norm": 0.4574175775051117,
      "learning_rate": 6.365903081450131e-05,
      "loss": 2.4663,
      "step": 91850
    },
    {
      "epoch": 1.8897574824045527,
      "grad_norm": 0.4638931155204773,
      "learning_rate": 6.363834103067157e-05,
      "loss": 2.3703,
      "step": 91860
    },
    {
      "epoch": 1.8899632019214203,
      "grad_norm": 0.5487886071205139,
      "learning_rate": 6.36176530403643e-05,
      "loss": 2.382,
      "step": 91870
    },
    {
      "epoch": 1.890168921438288,
      "grad_norm": 0.40999364852905273,
      "learning_rate": 6.359696684459994e-05,
      "loss": 2.3442,
      "step": 91880
    },
    {
      "epoch": 1.8903746409551556,
      "grad_norm": 0.4885186553001404,
      "learning_rate": 6.357628244439886e-05,
      "loss": 2.3763,
      "step": 91890
    },
    {
      "epoch": 1.8905803604720233,
      "grad_norm": 0.43331998586654663,
      "learning_rate": 6.355559984078124e-05,
      "loss": 2.3882,
      "step": 91900
    },
    {
      "epoch": 1.8907860799888911,
      "grad_norm": 0.4662948548793793,
      "learning_rate": 6.353491903476732e-05,
      "loss": 2.3693,
      "step": 91910
    },
    {
      "epoch": 1.890991799505759,
      "grad_norm": 0.42357149720191956,
      "learning_rate": 6.351424002737711e-05,
      "loss": 2.3521,
      "step": 91920
    },
    {
      "epoch": 1.8911975190226267,
      "grad_norm": 0.4562366008758545,
      "learning_rate": 6.349356281963062e-05,
      "loss": 2.367,
      "step": 91930
    },
    {
      "epoch": 1.8914032385394943,
      "grad_norm": 0.41763612627983093,
      "learning_rate": 6.347288741254776e-05,
      "loss": 2.4202,
      "step": 91940
    },
    {
      "epoch": 1.891608958056362,
      "grad_norm": 0.44091567397117615,
      "learning_rate": 6.34522138071483e-05,
      "loss": 2.435,
      "step": 91950
    },
    {
      "epoch": 1.8918146775732296,
      "grad_norm": 0.41990649700164795,
      "learning_rate": 6.343154200445194e-05,
      "loss": 2.359,
      "step": 91960
    },
    {
      "epoch": 1.8920203970900973,
      "grad_norm": 0.44799157977104187,
      "learning_rate": 6.34108720054784e-05,
      "loss": 2.3983,
      "step": 91970
    },
    {
      "epoch": 1.8922261166069652,
      "grad_norm": 0.450824111700058,
      "learning_rate": 6.339020381124712e-05,
      "loss": 2.4244,
      "step": 91980
    },
    {
      "epoch": 1.8924318361238328,
      "grad_norm": 0.4181632399559021,
      "learning_rate": 6.336953742277761e-05,
      "loss": 2.3761,
      "step": 91990
    },
    {
      "epoch": 1.8926375556407007,
      "grad_norm": 0.43394526839256287,
      "learning_rate": 6.334887284108916e-05,
      "loss": 2.3686,
      "step": 92000
    },
    {
      "epoch": 1.8928432751575683,
      "grad_norm": 0.439258337020874,
      "learning_rate": 6.332821006720112e-05,
      "loss": 2.4111,
      "step": 92010
    },
    {
      "epoch": 1.893048994674436,
      "grad_norm": 0.44943201541900635,
      "learning_rate": 6.330754910213266e-05,
      "loss": 2.3865,
      "step": 92020
    },
    {
      "epoch": 1.8932547141913036,
      "grad_norm": 0.44110405445098877,
      "learning_rate": 6.328688994690281e-05,
      "loss": 2.3377,
      "step": 92030
    },
    {
      "epoch": 1.8934604337081713,
      "grad_norm": 0.4377257525920868,
      "learning_rate": 6.326623260253063e-05,
      "loss": 2.4136,
      "step": 92040
    },
    {
      "epoch": 1.8936661532250392,
      "grad_norm": 0.4283754825592041,
      "learning_rate": 6.324557707003505e-05,
      "loss": 2.4409,
      "step": 92050
    },
    {
      "epoch": 1.8938718727419068,
      "grad_norm": 0.4022914469242096,
      "learning_rate": 6.322492335043481e-05,
      "loss": 2.4317,
      "step": 92060
    },
    {
      "epoch": 1.8940775922587747,
      "grad_norm": 0.44512879848480225,
      "learning_rate": 6.320427144474876e-05,
      "loss": 2.4172,
      "step": 92070
    },
    {
      "epoch": 1.8942833117756424,
      "grad_norm": 0.4548310339450836,
      "learning_rate": 6.318362135399547e-05,
      "loss": 2.412,
      "step": 92080
    },
    {
      "epoch": 1.89448903129251,
      "grad_norm": 0.3964283764362335,
      "learning_rate": 6.316297307919348e-05,
      "loss": 2.3592,
      "step": 92090
    },
    {
      "epoch": 1.8946947508093777,
      "grad_norm": 0.5043025612831116,
      "learning_rate": 6.314232662136133e-05,
      "loss": 2.4273,
      "step": 92100
    },
    {
      "epoch": 1.8949004703262453,
      "grad_norm": 0.44228312373161316,
      "learning_rate": 6.312168198151734e-05,
      "loss": 2.3561,
      "step": 92110
    },
    {
      "epoch": 1.8951061898431132,
      "grad_norm": 0.43930599093437195,
      "learning_rate": 6.31010391606798e-05,
      "loss": 2.3782,
      "step": 92120
    },
    {
      "epoch": 1.8953119093599808,
      "grad_norm": 0.4853486716747284,
      "learning_rate": 6.308039815986696e-05,
      "loss": 2.4112,
      "step": 92130
    },
    {
      "epoch": 1.8955176288768487,
      "grad_norm": 0.499393492937088,
      "learning_rate": 6.305975898009686e-05,
      "loss": 2.362,
      "step": 92140
    },
    {
      "epoch": 1.8957233483937164,
      "grad_norm": 0.4390840232372284,
      "learning_rate": 6.303912162238758e-05,
      "loss": 2.3944,
      "step": 92150
    },
    {
      "epoch": 1.895929067910584,
      "grad_norm": 0.44686955213546753,
      "learning_rate": 6.301848608775697e-05,
      "loss": 2.3844,
      "step": 92160
    },
    {
      "epoch": 1.8961347874274517,
      "grad_norm": 0.4164400100708008,
      "learning_rate": 6.299785237722296e-05,
      "loss": 2.3696,
      "step": 92170
    },
    {
      "epoch": 1.8963405069443193,
      "grad_norm": 0.48164817690849304,
      "learning_rate": 6.297722049180326e-05,
      "loss": 2.3731,
      "step": 92180
    },
    {
      "epoch": 1.8965462264611872,
      "grad_norm": 0.4354551434516907,
      "learning_rate": 6.295659043251547e-05,
      "loss": 2.3311,
      "step": 92190
    },
    {
      "epoch": 1.8967519459780549,
      "grad_norm": 0.46195316314697266,
      "learning_rate": 6.293596220037726e-05,
      "loss": 2.4106,
      "step": 92200
    },
    {
      "epoch": 1.8969576654949227,
      "grad_norm": 0.38244426250457764,
      "learning_rate": 6.291533579640605e-05,
      "loss": 2.3316,
      "step": 92210
    },
    {
      "epoch": 1.8971633850117904,
      "grad_norm": 0.45740175247192383,
      "learning_rate": 6.289471122161922e-05,
      "loss": 2.3559,
      "step": 92220
    },
    {
      "epoch": 1.897369104528658,
      "grad_norm": 0.40897876024246216,
      "learning_rate": 6.287408847703411e-05,
      "loss": 2.353,
      "step": 92230
    },
    {
      "epoch": 1.8975748240455257,
      "grad_norm": 0.4330233037471771,
      "learning_rate": 6.28534675636679e-05,
      "loss": 2.4139,
      "step": 92240
    },
    {
      "epoch": 1.8977805435623933,
      "grad_norm": 0.429643839597702,
      "learning_rate": 6.283284848253767e-05,
      "loss": 2.3579,
      "step": 92250
    },
    {
      "epoch": 1.897986263079261,
      "grad_norm": 0.4219549298286438,
      "learning_rate": 6.281223123466055e-05,
      "loss": 2.4041,
      "step": 92260
    },
    {
      "epoch": 1.8981919825961289,
      "grad_norm": 0.4816249907016754,
      "learning_rate": 6.279161582105337e-05,
      "loss": 2.4271,
      "step": 92270
    },
    {
      "epoch": 1.8983977021129967,
      "grad_norm": 0.4609975814819336,
      "learning_rate": 6.277100224273303e-05,
      "loss": 2.3736,
      "step": 92280
    },
    {
      "epoch": 1.8986034216298644,
      "grad_norm": 0.43966975808143616,
      "learning_rate": 6.27503905007163e-05,
      "loss": 2.4276,
      "step": 92290
    },
    {
      "epoch": 1.898809141146732,
      "grad_norm": 0.44271528720855713,
      "learning_rate": 6.272978059601979e-05,
      "loss": 2.3998,
      "step": 92300
    },
    {
      "epoch": 1.8990148606635997,
      "grad_norm": 0.4484920799732208,
      "learning_rate": 6.270917252966013e-05,
      "loss": 2.4272,
      "step": 92310
    },
    {
      "epoch": 1.8992205801804674,
      "grad_norm": 0.5879501700401306,
      "learning_rate": 6.268856630265374e-05,
      "loss": 2.352,
      "step": 92320
    },
    {
      "epoch": 1.899426299697335,
      "grad_norm": 0.4305966794490814,
      "learning_rate": 6.266796191601708e-05,
      "loss": 2.3568,
      "step": 92330
    },
    {
      "epoch": 1.8996320192142029,
      "grad_norm": 0.4894067943096161,
      "learning_rate": 6.264735937076645e-05,
      "loss": 2.3935,
      "step": 92340
    },
    {
      "epoch": 1.8998377387310705,
      "grad_norm": 0.5082672238349915,
      "learning_rate": 6.262675866791796e-05,
      "loss": 2.4136,
      "step": 92350
    },
    {
      "epoch": 1.9000434582479384,
      "grad_norm": 0.4658523499965668,
      "learning_rate": 6.260615980848786e-05,
      "loss": 2.3897,
      "step": 92360
    },
    {
      "epoch": 1.900249177764806,
      "grad_norm": 0.4256356954574585,
      "learning_rate": 6.258556279349212e-05,
      "loss": 2.3944,
      "step": 92370
    },
    {
      "epoch": 1.9004548972816737,
      "grad_norm": 0.45223546028137207,
      "learning_rate": 6.256496762394666e-05,
      "loss": 2.3308,
      "step": 92380
    },
    {
      "epoch": 1.9006606167985414,
      "grad_norm": 0.4851524531841278,
      "learning_rate": 6.254437430086739e-05,
      "loss": 2.3784,
      "step": 92390
    },
    {
      "epoch": 1.900866336315409,
      "grad_norm": 0.431471049785614,
      "learning_rate": 6.252378282527e-05,
      "loss": 2.4045,
      "step": 92400
    },
    {
      "epoch": 1.901072055832277,
      "grad_norm": 0.5196780562400818,
      "learning_rate": 6.250319319817017e-05,
      "loss": 2.3721,
      "step": 92410
    },
    {
      "epoch": 1.9012777753491445,
      "grad_norm": 0.42470163106918335,
      "learning_rate": 6.248260542058351e-05,
      "loss": 2.3691,
      "step": 92420
    },
    {
      "epoch": 1.9014834948660124,
      "grad_norm": 0.46167242527008057,
      "learning_rate": 6.246201949352549e-05,
      "loss": 2.3955,
      "step": 92430
    },
    {
      "epoch": 1.90168921438288,
      "grad_norm": 0.421928733587265,
      "learning_rate": 6.244143541801144e-05,
      "loss": 2.4223,
      "step": 92440
    },
    {
      "epoch": 1.9018949338997477,
      "grad_norm": 0.45759621262550354,
      "learning_rate": 6.242085319505678e-05,
      "loss": 2.4165,
      "step": 92450
    },
    {
      "epoch": 1.9021006534166154,
      "grad_norm": 0.45072734355926514,
      "learning_rate": 6.24002728256766e-05,
      "loss": 2.4298,
      "step": 92460
    },
    {
      "epoch": 1.902306372933483,
      "grad_norm": 0.4282219409942627,
      "learning_rate": 6.237969431088611e-05,
      "loss": 2.3924,
      "step": 92470
    },
    {
      "epoch": 1.902512092450351,
      "grad_norm": 0.4686240553855896,
      "learning_rate": 6.235911765170023e-05,
      "loss": 2.3528,
      "step": 92480
    },
    {
      "epoch": 1.9027178119672186,
      "grad_norm": 0.430103600025177,
      "learning_rate": 6.2338542849134e-05,
      "loss": 2.3985,
      "step": 92490
    },
    {
      "epoch": 1.9029235314840864,
      "grad_norm": 0.47396159172058105,
      "learning_rate": 6.231796990420223e-05,
      "loss": 2.3685,
      "step": 92500
    },
    {
      "epoch": 1.903129251000954,
      "grad_norm": 0.4374691843986511,
      "learning_rate": 6.229739881791962e-05,
      "loss": 2.3866,
      "step": 92510
    },
    {
      "epoch": 1.9033349705178217,
      "grad_norm": 0.49969354271888733,
      "learning_rate": 6.227682959130093e-05,
      "loss": 2.3736,
      "step": 92520
    },
    {
      "epoch": 1.9035406900346894,
      "grad_norm": 0.46369218826293945,
      "learning_rate": 6.225626222536063e-05,
      "loss": 2.4317,
      "step": 92530
    },
    {
      "epoch": 1.903746409551557,
      "grad_norm": 0.40460139513015747,
      "learning_rate": 6.223569672111322e-05,
      "loss": 2.3876,
      "step": 92540
    },
    {
      "epoch": 1.903952129068425,
      "grad_norm": 0.47263848781585693,
      "learning_rate": 6.221513307957315e-05,
      "loss": 2.3749,
      "step": 92550
    },
    {
      "epoch": 1.9041578485852926,
      "grad_norm": 0.48301205039024353,
      "learning_rate": 6.219457130175464e-05,
      "loss": 2.4113,
      "step": 92560
    },
    {
      "epoch": 1.9043635681021605,
      "grad_norm": 0.410327285528183,
      "learning_rate": 6.217401138867187e-05,
      "loss": 2.3992,
      "step": 92570
    },
    {
      "epoch": 1.904569287619028,
      "grad_norm": 0.4608479142189026,
      "learning_rate": 6.215345334133906e-05,
      "loss": 2.3763,
      "step": 92580
    },
    {
      "epoch": 1.9047750071358958,
      "grad_norm": 0.4331522583961487,
      "learning_rate": 6.213289716077013e-05,
      "loss": 2.4023,
      "step": 92590
    },
    {
      "epoch": 1.9049807266527634,
      "grad_norm": 0.4645753800868988,
      "learning_rate": 6.211234284797904e-05,
      "loss": 2.3828,
      "step": 92600
    },
    {
      "epoch": 1.905186446169631,
      "grad_norm": 0.4361638128757477,
      "learning_rate": 6.209179040397958e-05,
      "loss": 2.3939,
      "step": 92610
    },
    {
      "epoch": 1.905392165686499,
      "grad_norm": 0.4427531063556671,
      "learning_rate": 6.207123982978556e-05,
      "loss": 2.4062,
      "step": 92620
    },
    {
      "epoch": 1.9055978852033666,
      "grad_norm": 0.4669228494167328,
      "learning_rate": 6.20506911264106e-05,
      "loss": 2.3526,
      "step": 92630
    },
    {
      "epoch": 1.9058036047202345,
      "grad_norm": 0.4526941478252411,
      "learning_rate": 6.20301442948682e-05,
      "loss": 2.3855,
      "step": 92640
    },
    {
      "epoch": 1.9060093242371021,
      "grad_norm": 0.45724135637283325,
      "learning_rate": 6.20095993361719e-05,
      "loss": 2.4015,
      "step": 92650
    },
    {
      "epoch": 1.9062150437539698,
      "grad_norm": 0.44504591822624207,
      "learning_rate": 6.198905625133507e-05,
      "loss": 2.4034,
      "step": 92660
    },
    {
      "epoch": 1.9064207632708374,
      "grad_norm": 0.44531869888305664,
      "learning_rate": 6.19685150413709e-05,
      "loss": 2.4282,
      "step": 92670
    },
    {
      "epoch": 1.906626482787705,
      "grad_norm": 0.4441043436527252,
      "learning_rate": 6.194797570729269e-05,
      "loss": 2.3612,
      "step": 92680
    },
    {
      "epoch": 1.9068322023045727,
      "grad_norm": 0.471211701631546,
      "learning_rate": 6.192743825011345e-05,
      "loss": 2.3945,
      "step": 92690
    },
    {
      "epoch": 1.9070379218214406,
      "grad_norm": 0.46706897020339966,
      "learning_rate": 6.19069026708462e-05,
      "loss": 2.4242,
      "step": 92700
    },
    {
      "epoch": 1.9072436413383085,
      "grad_norm": 0.4366188943386078,
      "learning_rate": 6.18863689705039e-05,
      "loss": 2.4007,
      "step": 92710
    },
    {
      "epoch": 1.9074493608551761,
      "grad_norm": 0.46734559535980225,
      "learning_rate": 6.18658371500993e-05,
      "loss": 2.3807,
      "step": 92720
    },
    {
      "epoch": 1.9076550803720438,
      "grad_norm": 0.47976866364479065,
      "learning_rate": 6.184530721064514e-05,
      "loss": 2.3951,
      "step": 92730
    },
    {
      "epoch": 1.9078607998889114,
      "grad_norm": 0.4599480628967285,
      "learning_rate": 6.182477915315407e-05,
      "loss": 2.4332,
      "step": 92740
    },
    {
      "epoch": 1.908066519405779,
      "grad_norm": 0.40828850865364075,
      "learning_rate": 6.180425297863861e-05,
      "loss": 2.3798,
      "step": 92750
    },
    {
      "epoch": 1.9082722389226467,
      "grad_norm": 0.41424208879470825,
      "learning_rate": 6.178372868811124e-05,
      "loss": 2.3759,
      "step": 92760
    },
    {
      "epoch": 1.9084779584395146,
      "grad_norm": 0.4546986222267151,
      "learning_rate": 6.176320628258422e-05,
      "loss": 2.3689,
      "step": 92770
    },
    {
      "epoch": 1.9086836779563823,
      "grad_norm": 0.4100381135940552,
      "learning_rate": 6.17426857630699e-05,
      "loss": 2.3569,
      "step": 92780
    },
    {
      "epoch": 1.9088893974732501,
      "grad_norm": 0.4544045031070709,
      "learning_rate": 6.172216713058043e-05,
      "loss": 2.4431,
      "step": 92790
    },
    {
      "epoch": 1.9090951169901178,
      "grad_norm": 0.43481552600860596,
      "learning_rate": 6.170165038612783e-05,
      "loss": 2.3919,
      "step": 92800
    },
    {
      "epoch": 1.9093008365069855,
      "grad_norm": 0.4383656680583954,
      "learning_rate": 6.168113553072412e-05,
      "loss": 2.394,
      "step": 92810
    },
    {
      "epoch": 1.909506556023853,
      "grad_norm": 0.4506937861442566,
      "learning_rate": 6.166062256538122e-05,
      "loss": 2.3498,
      "step": 92820
    },
    {
      "epoch": 1.9097122755407208,
      "grad_norm": 0.4311995506286621,
      "learning_rate": 6.164011149111084e-05,
      "loss": 2.4042,
      "step": 92830
    },
    {
      "epoch": 1.9099179950575886,
      "grad_norm": 0.41508227586746216,
      "learning_rate": 6.161960230892476e-05,
      "loss": 2.3924,
      "step": 92840
    },
    {
      "epoch": 1.9101237145744563,
      "grad_norm": 0.43548449873924255,
      "learning_rate": 6.159909501983453e-05,
      "loss": 2.3977,
      "step": 92850
    },
    {
      "epoch": 1.9103294340913242,
      "grad_norm": 0.4877195358276367,
      "learning_rate": 6.157858962485167e-05,
      "loss": 2.3933,
      "step": 92860
    },
    {
      "epoch": 1.9105351536081918,
      "grad_norm": 0.4487294554710388,
      "learning_rate": 6.155808612498763e-05,
      "loss": 2.4718,
      "step": 92870
    },
    {
      "epoch": 1.9107408731250595,
      "grad_norm": 0.4495629072189331,
      "learning_rate": 6.153758452125374e-05,
      "loss": 2.4249,
      "step": 92880
    },
    {
      "epoch": 1.9109465926419271,
      "grad_norm": 0.4776647388935089,
      "learning_rate": 6.151708481466115e-05,
      "loss": 2.419,
      "step": 92890
    },
    {
      "epoch": 1.9111523121587948,
      "grad_norm": 0.46406036615371704,
      "learning_rate": 6.149658700622112e-05,
      "loss": 2.4219,
      "step": 92900
    },
    {
      "epoch": 1.9113580316756626,
      "grad_norm": 0.47144266963005066,
      "learning_rate": 6.147609109694461e-05,
      "loss": 2.4298,
      "step": 92910
    },
    {
      "epoch": 1.9115637511925303,
      "grad_norm": 0.45075613260269165,
      "learning_rate": 6.145559708784262e-05,
      "loss": 2.388,
      "step": 92920
    },
    {
      "epoch": 1.9117694707093982,
      "grad_norm": 0.4467519223690033,
      "learning_rate": 6.143510497992593e-05,
      "loss": 2.3599,
      "step": 92930
    },
    {
      "epoch": 1.9119751902262658,
      "grad_norm": 0.43934303522109985,
      "learning_rate": 6.14146147742054e-05,
      "loss": 2.362,
      "step": 92940
    },
    {
      "epoch": 1.9121809097431335,
      "grad_norm": 0.39301908016204834,
      "learning_rate": 6.139412647169164e-05,
      "loss": 2.4087,
      "step": 92950
    },
    {
      "epoch": 1.9123866292600011,
      "grad_norm": 0.44537073373794556,
      "learning_rate": 6.137364007339524e-05,
      "loss": 2.3657,
      "step": 92960
    },
    {
      "epoch": 1.9125923487768688,
      "grad_norm": 0.4484299123287201,
      "learning_rate": 6.13531555803267e-05,
      "loss": 2.3977,
      "step": 92970
    },
    {
      "epoch": 1.9127980682937367,
      "grad_norm": 0.40137094259262085,
      "learning_rate": 6.133267299349641e-05,
      "loss": 2.3879,
      "step": 92980
    },
    {
      "epoch": 1.9130037878106043,
      "grad_norm": 0.42138057947158813,
      "learning_rate": 6.13121923139146e-05,
      "loss": 2.4,
      "step": 92990
    },
    {
      "epoch": 1.9132095073274722,
      "grad_norm": 0.4227756857872009,
      "learning_rate": 6.129171354259158e-05,
      "loss": 2.3795,
      "step": 93000
    },
    {
      "epoch": 1.9134152268443398,
      "grad_norm": 0.465200811624527,
      "learning_rate": 6.127123668053738e-05,
      "loss": 2.4467,
      "step": 93010
    },
    {
      "epoch": 1.9136209463612075,
      "grad_norm": 0.43188348412513733,
      "learning_rate": 6.125076172876199e-05,
      "loss": 2.3983,
      "step": 93020
    },
    {
      "epoch": 1.9138266658780752,
      "grad_norm": 0.4377543032169342,
      "learning_rate": 6.12302886882754e-05,
      "loss": 2.4543,
      "step": 93030
    },
    {
      "epoch": 1.9140323853949428,
      "grad_norm": 0.44892561435699463,
      "learning_rate": 6.12098175600874e-05,
      "loss": 2.4007,
      "step": 93040
    },
    {
      "epoch": 1.9142381049118105,
      "grad_norm": 0.4289973974227905,
      "learning_rate": 6.118934834520769e-05,
      "loss": 2.3451,
      "step": 93050
    },
    {
      "epoch": 1.9144438244286783,
      "grad_norm": 0.41844233870506287,
      "learning_rate": 6.116888104464598e-05,
      "loss": 2.3594,
      "step": 93060
    },
    {
      "epoch": 1.9146495439455462,
      "grad_norm": 0.5472850799560547,
      "learning_rate": 6.114841565941173e-05,
      "loss": 2.4958,
      "step": 93070
    },
    {
      "epoch": 1.9148552634624139,
      "grad_norm": 0.5080177187919617,
      "learning_rate": 6.112795219051445e-05,
      "loss": 2.4126,
      "step": 93080
    },
    {
      "epoch": 1.9150609829792815,
      "grad_norm": 0.4691077470779419,
      "learning_rate": 6.110749063896341e-05,
      "loss": 2.4193,
      "step": 93090
    },
    {
      "epoch": 1.9152667024961492,
      "grad_norm": 0.41710731387138367,
      "learning_rate": 6.108703100576795e-05,
      "loss": 2.2697,
      "step": 93100
    },
    {
      "epoch": 1.9154724220130168,
      "grad_norm": 0.4350225031375885,
      "learning_rate": 6.106657329193721e-05,
      "loss": 2.3567,
      "step": 93110
    },
    {
      "epoch": 1.9156781415298845,
      "grad_norm": 0.4195058345794678,
      "learning_rate": 6.104611749848021e-05,
      "loss": 2.4077,
      "step": 93120
    },
    {
      "epoch": 1.9158838610467523,
      "grad_norm": 0.4720888137817383,
      "learning_rate": 6.102566362640599e-05,
      "loss": 2.3553,
      "step": 93130
    },
    {
      "epoch": 1.91608958056362,
      "grad_norm": 0.4504825174808502,
      "learning_rate": 6.1005211676723414e-05,
      "loss": 2.3779,
      "step": 93140
    },
    {
      "epoch": 1.9162953000804879,
      "grad_norm": 0.4769377112388611,
      "learning_rate": 6.09847616504412e-05,
      "loss": 2.3967,
      "step": 93150
    },
    {
      "epoch": 1.9165010195973555,
      "grad_norm": 0.425535649061203,
      "learning_rate": 6.096431354856814e-05,
      "loss": 2.427,
      "step": 93160
    },
    {
      "epoch": 1.9167067391142232,
      "grad_norm": 0.43521177768707275,
      "learning_rate": 6.094386737211275e-05,
      "loss": 2.4026,
      "step": 93170
    },
    {
      "epoch": 1.9169124586310908,
      "grad_norm": 0.43225806951522827,
      "learning_rate": 6.092342312208353e-05,
      "loss": 2.3425,
      "step": 93180
    },
    {
      "epoch": 1.9171181781479585,
      "grad_norm": 0.4013378620147705,
      "learning_rate": 6.090298079948895e-05,
      "loss": 2.3537,
      "step": 93190
    },
    {
      "epoch": 1.9173238976648264,
      "grad_norm": 0.40910428762435913,
      "learning_rate": 6.088254040533725e-05,
      "loss": 2.3547,
      "step": 93200
    },
    {
      "epoch": 1.917529617181694,
      "grad_norm": 0.43571361899375916,
      "learning_rate": 6.086210194063664e-05,
      "loss": 2.3874,
      "step": 93210
    },
    {
      "epoch": 1.9177353366985619,
      "grad_norm": 0.42859792709350586,
      "learning_rate": 6.0841665406395324e-05,
      "loss": 2.4096,
      "step": 93220
    },
    {
      "epoch": 1.9179410562154295,
      "grad_norm": 0.45164409279823303,
      "learning_rate": 6.082123080362123e-05,
      "loss": 2.4301,
      "step": 93230
    },
    {
      "epoch": 1.9181467757322972,
      "grad_norm": 0.4801684021949768,
      "learning_rate": 6.0800798133322346e-05,
      "loss": 2.4377,
      "step": 93240
    },
    {
      "epoch": 1.9183524952491648,
      "grad_norm": 0.45162028074264526,
      "learning_rate": 6.0780367396506444e-05,
      "loss": 2.4425,
      "step": 93250
    },
    {
      "epoch": 1.9185582147660325,
      "grad_norm": 0.42389988899230957,
      "learning_rate": 6.075993859418131e-05,
      "loss": 2.3761,
      "step": 93260
    },
    {
      "epoch": 1.9187639342829004,
      "grad_norm": 0.3848464787006378,
      "learning_rate": 6.073951172735459e-05,
      "loss": 2.4379,
      "step": 93270
    },
    {
      "epoch": 1.918969653799768,
      "grad_norm": 0.4716414213180542,
      "learning_rate": 6.0719086797033786e-05,
      "loss": 2.4021,
      "step": 93280
    },
    {
      "epoch": 1.919175373316636,
      "grad_norm": 0.4115830361843109,
      "learning_rate": 6.069866380422638e-05,
      "loss": 2.4096,
      "step": 93290
    },
    {
      "epoch": 1.9193810928335036,
      "grad_norm": 0.4221152663230896,
      "learning_rate": 6.067824274993976e-05,
      "loss": 2.3841,
      "step": 93300
    },
    {
      "epoch": 1.9195868123503712,
      "grad_norm": 0.46289747953414917,
      "learning_rate": 6.0657823635181086e-05,
      "loss": 2.3609,
      "step": 93310
    },
    {
      "epoch": 1.9197925318672389,
      "grad_norm": 0.47023168206214905,
      "learning_rate": 6.0637406460957633e-05,
      "loss": 2.4371,
      "step": 93320
    },
    {
      "epoch": 1.9199982513841065,
      "grad_norm": 0.4258280396461487,
      "learning_rate": 6.061699122827641e-05,
      "loss": 2.4177,
      "step": 93330
    },
    {
      "epoch": 1.9202039709009744,
      "grad_norm": 0.5202220678329468,
      "learning_rate": 6.059657793814437e-05,
      "loss": 2.3307,
      "step": 93340
    },
    {
      "epoch": 1.920409690417842,
      "grad_norm": 0.41358450055122375,
      "learning_rate": 6.057616659156845e-05,
      "loss": 2.4014,
      "step": 93350
    },
    {
      "epoch": 1.92061540993471,
      "grad_norm": 0.599728524684906,
      "learning_rate": 6.0555757189555385e-05,
      "loss": 2.4209,
      "step": 93360
    },
    {
      "epoch": 1.9208211294515776,
      "grad_norm": 0.4449940621852875,
      "learning_rate": 6.053534973311186e-05,
      "loss": 2.3592,
      "step": 93370
    },
    {
      "epoch": 1.9210268489684452,
      "grad_norm": 0.4234105944633484,
      "learning_rate": 6.05149442232445e-05,
      "loss": 2.3758,
      "step": 93380
    },
    {
      "epoch": 1.9212325684853129,
      "grad_norm": 0.6112462282180786,
      "learning_rate": 6.049454066095976e-05,
      "loss": 2.362,
      "step": 93390
    },
    {
      "epoch": 1.9214382880021805,
      "grad_norm": 0.5002745389938354,
      "learning_rate": 6.047413904726407e-05,
      "loss": 2.373,
      "step": 93400
    },
    {
      "epoch": 1.9216440075190482,
      "grad_norm": 0.422374427318573,
      "learning_rate": 6.0453739383163675e-05,
      "loss": 2.3551,
      "step": 93410
    },
    {
      "epoch": 1.921849727035916,
      "grad_norm": 0.43827253580093384,
      "learning_rate": 6.0433341669664826e-05,
      "loss": 2.4072,
      "step": 93420
    },
    {
      "epoch": 1.922055446552784,
      "grad_norm": 0.4237130582332611,
      "learning_rate": 6.041294590777364e-05,
      "loss": 2.382,
      "step": 93430
    },
    {
      "epoch": 1.9222611660696516,
      "grad_norm": 0.42748165130615234,
      "learning_rate": 6.039255209849608e-05,
      "loss": 2.4281,
      "step": 93440
    },
    {
      "epoch": 1.9224668855865192,
      "grad_norm": 0.4493885934352875,
      "learning_rate": 6.037216024283811e-05,
      "loss": 2.3977,
      "step": 93450
    },
    {
      "epoch": 1.922672605103387,
      "grad_norm": 0.4225637912750244,
      "learning_rate": 6.0351770341805535e-05,
      "loss": 2.3683,
      "step": 93460
    },
    {
      "epoch": 1.9228783246202545,
      "grad_norm": 0.42976802587509155,
      "learning_rate": 6.0331382396404034e-05,
      "loss": 2.3368,
      "step": 93470
    },
    {
      "epoch": 1.9230840441371222,
      "grad_norm": 0.41769641637802124,
      "learning_rate": 6.031099640763933e-05,
      "loss": 2.3881,
      "step": 93480
    },
    {
      "epoch": 1.92328976365399,
      "grad_norm": 0.4777204096317291,
      "learning_rate": 6.029061237651685e-05,
      "loss": 2.3731,
      "step": 93490
    },
    {
      "epoch": 1.9234954831708577,
      "grad_norm": 0.4660399258136749,
      "learning_rate": 6.027023030404206e-05,
      "loss": 2.436,
      "step": 93500
    },
    {
      "epoch": 1.9237012026877256,
      "grad_norm": 0.5112795829772949,
      "learning_rate": 6.024985019122036e-05,
      "loss": 2.3854,
      "step": 93510
    },
    {
      "epoch": 1.9239069222045933,
      "grad_norm": 0.4051155149936676,
      "learning_rate": 6.022947203905691e-05,
      "loss": 2.3712,
      "step": 93520
    },
    {
      "epoch": 1.924112641721461,
      "grad_norm": 0.449108749628067,
      "learning_rate": 6.020909584855686e-05,
      "loss": 2.3864,
      "step": 93530
    },
    {
      "epoch": 1.9243183612383286,
      "grad_norm": 0.4275805950164795,
      "learning_rate": 6.018872162072532e-05,
      "loss": 2.4198,
      "step": 93540
    },
    {
      "epoch": 1.9245240807551962,
      "grad_norm": 0.4423309564590454,
      "learning_rate": 6.0168349356567164e-05,
      "loss": 2.4049,
      "step": 93550
    },
    {
      "epoch": 1.924729800272064,
      "grad_norm": 0.44739553332328796,
      "learning_rate": 6.0147979057087314e-05,
      "loss": 2.429,
      "step": 93560
    },
    {
      "epoch": 1.9249355197889317,
      "grad_norm": 0.5003595352172852,
      "learning_rate": 6.012761072329045e-05,
      "loss": 2.4005,
      "step": 93570
    },
    {
      "epoch": 1.9251412393057996,
      "grad_norm": 0.43352973461151123,
      "learning_rate": 6.010724435618128e-05,
      "loss": 2.3443,
      "step": 93580
    },
    {
      "epoch": 1.9253469588226673,
      "grad_norm": 0.43409428000450134,
      "learning_rate": 6.008687995676438e-05,
      "loss": 2.3489,
      "step": 93590
    },
    {
      "epoch": 1.925552678339535,
      "grad_norm": 0.4522683620452881,
      "learning_rate": 6.006651752604415e-05,
      "loss": 2.3861,
      "step": 93600
    },
    {
      "epoch": 1.9257583978564026,
      "grad_norm": 0.4398570656776428,
      "learning_rate": 6.0046157065025036e-05,
      "loss": 2.3905,
      "step": 93610
    },
    {
      "epoch": 1.9259641173732702,
      "grad_norm": 0.44097182154655457,
      "learning_rate": 6.002579857471127e-05,
      "loss": 2.3885,
      "step": 93620
    },
    {
      "epoch": 1.926169836890138,
      "grad_norm": 0.4806622564792633,
      "learning_rate": 6.000544205610699e-05,
      "loss": 2.4004,
      "step": 93630
    },
    {
      "epoch": 1.9263755564070058,
      "grad_norm": 0.40359416604042053,
      "learning_rate": 5.9985087510216354e-05,
      "loss": 2.3351,
      "step": 93640
    },
    {
      "epoch": 1.9265812759238736,
      "grad_norm": 0.39793917536735535,
      "learning_rate": 5.9964734938043275e-05,
      "loss": 2.3883,
      "step": 93650
    },
    {
      "epoch": 1.9267869954407413,
      "grad_norm": 0.4068503677845001,
      "learning_rate": 5.994438434059164e-05,
      "loss": 2.3272,
      "step": 93660
    },
    {
      "epoch": 1.926992714957609,
      "grad_norm": 0.46698904037475586,
      "learning_rate": 5.992403571886528e-05,
      "loss": 2.3908,
      "step": 93670
    },
    {
      "epoch": 1.9271984344744766,
      "grad_norm": 0.5026007890701294,
      "learning_rate": 5.990368907386783e-05,
      "loss": 2.3634,
      "step": 93680
    },
    {
      "epoch": 1.9274041539913442,
      "grad_norm": 0.43234163522720337,
      "learning_rate": 5.988334440660292e-05,
      "loss": 2.3627,
      "step": 93690
    },
    {
      "epoch": 1.9276098735082121,
      "grad_norm": 0.4561239778995514,
      "learning_rate": 5.9863001718074e-05,
      "loss": 2.3945,
      "step": 93700
    },
    {
      "epoch": 1.9278155930250798,
      "grad_norm": 0.45340806245803833,
      "learning_rate": 5.9842661009284486e-05,
      "loss": 2.4011,
      "step": 93710
    },
    {
      "epoch": 1.9280213125419476,
      "grad_norm": 0.4288921058177948,
      "learning_rate": 5.98223222812377e-05,
      "loss": 2.3696,
      "step": 93720
    },
    {
      "epoch": 1.9282270320588153,
      "grad_norm": 0.6656550765037537,
      "learning_rate": 5.9801985534936766e-05,
      "loss": 2.3597,
      "step": 93730
    },
    {
      "epoch": 1.928432751575683,
      "grad_norm": 0.48433718085289,
      "learning_rate": 5.978165077138484e-05,
      "loss": 2.3693,
      "step": 93740
    },
    {
      "epoch": 1.9286384710925506,
      "grad_norm": 0.4242750108242035,
      "learning_rate": 5.976131799158497e-05,
      "loss": 2.3721,
      "step": 93750
    },
    {
      "epoch": 1.9288441906094183,
      "grad_norm": 0.45822668075561523,
      "learning_rate": 5.9740987196539956e-05,
      "loss": 2.3449,
      "step": 93760
    },
    {
      "epoch": 1.929049910126286,
      "grad_norm": 0.44525012373924255,
      "learning_rate": 5.972065838725269e-05,
      "loss": 2.3506,
      "step": 93770
    },
    {
      "epoch": 1.9292556296431538,
      "grad_norm": 0.4375591278076172,
      "learning_rate": 5.970033156472583e-05,
      "loss": 2.3708,
      "step": 93780
    },
    {
      "epoch": 1.9294613491600217,
      "grad_norm": 0.4194212555885315,
      "learning_rate": 5.968000672996199e-05,
      "loss": 2.4015,
      "step": 93790
    },
    {
      "epoch": 1.9296670686768893,
      "grad_norm": 0.43875765800476074,
      "learning_rate": 5.965968388396375e-05,
      "loss": 2.3697,
      "step": 93800
    },
    {
      "epoch": 1.929872788193757,
      "grad_norm": 0.45176002383232117,
      "learning_rate": 5.9639363027733454e-05,
      "loss": 2.3721,
      "step": 93810
    },
    {
      "epoch": 1.9300785077106246,
      "grad_norm": 0.4798277020454407,
      "learning_rate": 5.9619044162273406e-05,
      "loss": 2.3436,
      "step": 93820
    },
    {
      "epoch": 1.9302842272274923,
      "grad_norm": 0.46898606419563293,
      "learning_rate": 5.959872728858591e-05,
      "loss": 2.3837,
      "step": 93830
    },
    {
      "epoch": 1.93048994674436,
      "grad_norm": 0.4609358608722687,
      "learning_rate": 5.957841240767301e-05,
      "loss": 2.3574,
      "step": 93840
    },
    {
      "epoch": 1.9306956662612278,
      "grad_norm": 0.48163118958473206,
      "learning_rate": 5.955809952053679e-05,
      "loss": 2.3876,
      "step": 93850
    },
    {
      "epoch": 1.9309013857780954,
      "grad_norm": 0.4202282130718231,
      "learning_rate": 5.953778862817909e-05,
      "loss": 2.4165,
      "step": 93860
    },
    {
      "epoch": 1.9311071052949633,
      "grad_norm": 0.4389393627643585,
      "learning_rate": 5.95174797316018e-05,
      "loss": 2.4022,
      "step": 93870
    },
    {
      "epoch": 1.931312824811831,
      "grad_norm": 0.44825315475463867,
      "learning_rate": 5.949717283180665e-05,
      "loss": 2.3964,
      "step": 93880
    },
    {
      "epoch": 1.9315185443286986,
      "grad_norm": 0.41952821612358093,
      "learning_rate": 5.947686792979521e-05,
      "loss": 2.3898,
      "step": 93890
    },
    {
      "epoch": 1.9317242638455663,
      "grad_norm": 0.4307079613208771,
      "learning_rate": 5.9456565026569086e-05,
      "loss": 2.3561,
      "step": 93900
    },
    {
      "epoch": 1.931929983362434,
      "grad_norm": 0.4565310776233673,
      "learning_rate": 5.943626412312968e-05,
      "loss": 2.3753,
      "step": 93910
    },
    {
      "epoch": 1.9321357028793018,
      "grad_norm": 0.4597446322441101,
      "learning_rate": 5.941596522047828e-05,
      "loss": 2.4184,
      "step": 93920
    },
    {
      "epoch": 1.9323414223961695,
      "grad_norm": 0.43448126316070557,
      "learning_rate": 5.9395668319616204e-05,
      "loss": 2.3655,
      "step": 93930
    },
    {
      "epoch": 1.9325471419130373,
      "grad_norm": 0.4613983631134033,
      "learning_rate": 5.937537342154452e-05,
      "loss": 2.3537,
      "step": 93940
    },
    {
      "epoch": 1.932752861429905,
      "grad_norm": 0.46736258268356323,
      "learning_rate": 5.935508052726426e-05,
      "loss": 2.4248,
      "step": 93950
    },
    {
      "epoch": 1.9329585809467726,
      "grad_norm": 0.45526671409606934,
      "learning_rate": 5.933478963777644e-05,
      "loss": 2.4481,
      "step": 93960
    },
    {
      "epoch": 1.9331643004636403,
      "grad_norm": 0.4436907470226288,
      "learning_rate": 5.9314500754081824e-05,
      "loss": 2.3889,
      "step": 93970
    },
    {
      "epoch": 1.933370019980508,
      "grad_norm": 0.5255995392799377,
      "learning_rate": 5.929421387718117e-05,
      "loss": 2.4371,
      "step": 93980
    },
    {
      "epoch": 1.9335757394973758,
      "grad_norm": 0.4283786714076996,
      "learning_rate": 5.9273929008075146e-05,
      "loss": 2.3937,
      "step": 93990
    },
    {
      "epoch": 1.9337814590142435,
      "grad_norm": 0.45985767245292664,
      "learning_rate": 5.9253646147764265e-05,
      "loss": 2.3548,
      "step": 94000
    },
    {
      "epoch": 1.9339871785311114,
      "grad_norm": 0.40851977467536926,
      "learning_rate": 5.9233365297248986e-05,
      "loss": 2.332,
      "step": 94010
    },
    {
      "epoch": 1.934192898047979,
      "grad_norm": 0.6347037553787231,
      "learning_rate": 5.921308645752962e-05,
      "loss": 2.3866,
      "step": 94020
    },
    {
      "epoch": 1.9343986175648467,
      "grad_norm": 0.44978058338165283,
      "learning_rate": 5.919280962960645e-05,
      "loss": 2.3647,
      "step": 94030
    },
    {
      "epoch": 1.9346043370817143,
      "grad_norm": 0.4593315124511719,
      "learning_rate": 5.917253481447963e-05,
      "loss": 2.4204,
      "step": 94040
    },
    {
      "epoch": 1.934810056598582,
      "grad_norm": 0.4258660078048706,
      "learning_rate": 5.915226201314914e-05,
      "loss": 2.3875,
      "step": 94050
    },
    {
      "epoch": 1.9350157761154498,
      "grad_norm": 0.44066205620765686,
      "learning_rate": 5.913199122661499e-05,
      "loss": 2.3744,
      "step": 94060
    },
    {
      "epoch": 1.9352214956323175,
      "grad_norm": 0.45923230051994324,
      "learning_rate": 5.911172245587704e-05,
      "loss": 2.3914,
      "step": 94070
    },
    {
      "epoch": 1.9354272151491854,
      "grad_norm": 0.44899463653564453,
      "learning_rate": 5.909145570193496e-05,
      "loss": 2.3705,
      "step": 94080
    },
    {
      "epoch": 1.935632934666053,
      "grad_norm": 0.5085963010787964,
      "learning_rate": 5.9071190965788495e-05,
      "loss": 2.3779,
      "step": 94090
    },
    {
      "epoch": 1.9358386541829207,
      "grad_norm": 0.4561370611190796,
      "learning_rate": 5.905092824843711e-05,
      "loss": 2.343,
      "step": 94100
    },
    {
      "epoch": 1.9360443736997883,
      "grad_norm": 0.41266486048698425,
      "learning_rate": 5.903066755088027e-05,
      "loss": 2.4039,
      "step": 94110
    },
    {
      "epoch": 1.936250093216656,
      "grad_norm": 0.44019246101379395,
      "learning_rate": 5.90104088741174e-05,
      "loss": 2.4394,
      "step": 94120
    },
    {
      "epoch": 1.9364558127335236,
      "grad_norm": 0.4439091980457306,
      "learning_rate": 5.8990152219147656e-05,
      "loss": 2.4019,
      "step": 94130
    },
    {
      "epoch": 1.9366615322503915,
      "grad_norm": 0.4560917317867279,
      "learning_rate": 5.8969897586970215e-05,
      "loss": 2.4241,
      "step": 94140
    },
    {
      "epoch": 1.9368672517672594,
      "grad_norm": 0.40432143211364746,
      "learning_rate": 5.894964497858417e-05,
      "loss": 2.4092,
      "step": 94150
    },
    {
      "epoch": 1.937072971284127,
      "grad_norm": 0.42234987020492554,
      "learning_rate": 5.892939439498843e-05,
      "loss": 2.4028,
      "step": 94160
    },
    {
      "epoch": 1.9372786908009947,
      "grad_norm": 0.3970158100128174,
      "learning_rate": 5.890914583718188e-05,
      "loss": 2.4401,
      "step": 94170
    },
    {
      "epoch": 1.9374844103178623,
      "grad_norm": 0.40879836678504944,
      "learning_rate": 5.888889930616319e-05,
      "loss": 2.401,
      "step": 94180
    },
    {
      "epoch": 1.93769012983473,
      "grad_norm": 0.5435805916786194,
      "learning_rate": 5.88686548029311e-05,
      "loss": 2.3903,
      "step": 94190
    },
    {
      "epoch": 1.9378958493515976,
      "grad_norm": 0.4391219913959503,
      "learning_rate": 5.884841232848415e-05,
      "loss": 2.3756,
      "step": 94200
    },
    {
      "epoch": 1.9381015688684655,
      "grad_norm": 0.44616276025772095,
      "learning_rate": 5.882817188382073e-05,
      "loss": 2.4017,
      "step": 94210
    },
    {
      "epoch": 1.9383072883853334,
      "grad_norm": 0.4440360963344574,
      "learning_rate": 5.8807933469939246e-05,
      "loss": 2.3906,
      "step": 94220
    },
    {
      "epoch": 1.938513007902201,
      "grad_norm": 0.4673629403114319,
      "learning_rate": 5.878769708783796e-05,
      "loss": 2.3912,
      "step": 94230
    },
    {
      "epoch": 1.9387187274190687,
      "grad_norm": 0.46713143587112427,
      "learning_rate": 5.8767462738514954e-05,
      "loss": 2.3402,
      "step": 94240
    },
    {
      "epoch": 1.9389244469359364,
      "grad_norm": 0.4389398694038391,
      "learning_rate": 5.874723042296837e-05,
      "loss": 2.3934,
      "step": 94250
    },
    {
      "epoch": 1.939130166452804,
      "grad_norm": 0.429811030626297,
      "learning_rate": 5.872700014219608e-05,
      "loss": 2.3671,
      "step": 94260
    },
    {
      "epoch": 1.9393358859696717,
      "grad_norm": 0.42365047335624695,
      "learning_rate": 5.870677189719593e-05,
      "loss": 2.4009,
      "step": 94270
    },
    {
      "epoch": 1.9395416054865395,
      "grad_norm": 0.44553154706954956,
      "learning_rate": 5.868654568896577e-05,
      "loss": 2.3903,
      "step": 94280
    },
    {
      "epoch": 1.9397473250034072,
      "grad_norm": 0.4298798143863678,
      "learning_rate": 5.866632151850313e-05,
      "loss": 2.4038,
      "step": 94290
    },
    {
      "epoch": 1.939953044520275,
      "grad_norm": 0.4240160882472992,
      "learning_rate": 5.864609938680561e-05,
      "loss": 2.3748,
      "step": 94300
    },
    {
      "epoch": 1.9401587640371427,
      "grad_norm": 0.4312121272087097,
      "learning_rate": 5.862587929487069e-05,
      "loss": 2.3959,
      "step": 94310
    },
    {
      "epoch": 1.9403644835540104,
      "grad_norm": 0.45494309067726135,
      "learning_rate": 5.8605661243695655e-05,
      "loss": 2.4415,
      "step": 94320
    },
    {
      "epoch": 1.940570203070878,
      "grad_norm": 0.4494026005268097,
      "learning_rate": 5.85854452342778e-05,
      "loss": 2.356,
      "step": 94330
    },
    {
      "epoch": 1.9407759225877457,
      "grad_norm": 0.41958367824554443,
      "learning_rate": 5.8565231267614216e-05,
      "loss": 2.3667,
      "step": 94340
    },
    {
      "epoch": 1.9409816421046135,
      "grad_norm": 0.42415279150009155,
      "learning_rate": 5.8545019344702004e-05,
      "loss": 2.3857,
      "step": 94350
    },
    {
      "epoch": 1.9411873616214812,
      "grad_norm": 0.4403563439846039,
      "learning_rate": 5.852480946653809e-05,
      "loss": 2.3698,
      "step": 94360
    },
    {
      "epoch": 1.941393081138349,
      "grad_norm": 0.593808650970459,
      "learning_rate": 5.850460163411928e-05,
      "loss": 2.4207,
      "step": 94370
    },
    {
      "epoch": 1.9415988006552167,
      "grad_norm": 0.45863664150238037,
      "learning_rate": 5.8484395848442374e-05,
      "loss": 2.346,
      "step": 94380
    },
    {
      "epoch": 1.9418045201720844,
      "grad_norm": 0.5064952969551086,
      "learning_rate": 5.8464192110504e-05,
      "loss": 2.3739,
      "step": 94390
    },
    {
      "epoch": 1.942010239688952,
      "grad_norm": 0.44243550300598145,
      "learning_rate": 5.844399042130065e-05,
      "loss": 2.3113,
      "step": 94400
    },
    {
      "epoch": 1.9422159592058197,
      "grad_norm": 0.3957783579826355,
      "learning_rate": 5.8423790781828846e-05,
      "loss": 2.3158,
      "step": 94410
    },
    {
      "epoch": 1.9424216787226876,
      "grad_norm": 0.4603268504142761,
      "learning_rate": 5.840359319308486e-05,
      "loss": 2.3658,
      "step": 94420
    },
    {
      "epoch": 1.9426273982395552,
      "grad_norm": 0.464133083820343,
      "learning_rate": 5.838339765606491e-05,
      "loss": 2.3117,
      "step": 94430
    },
    {
      "epoch": 1.942833117756423,
      "grad_norm": 0.4556775689125061,
      "learning_rate": 5.836320417176524e-05,
      "loss": 2.4075,
      "step": 94440
    },
    {
      "epoch": 1.9430388372732907,
      "grad_norm": 0.41572025418281555,
      "learning_rate": 5.834301274118175e-05,
      "loss": 2.3895,
      "step": 94450
    },
    {
      "epoch": 1.9432445567901584,
      "grad_norm": 0.4427511692047119,
      "learning_rate": 5.832282336531045e-05,
      "loss": 2.4264,
      "step": 94460
    },
    {
      "epoch": 1.943450276307026,
      "grad_norm": 0.43214643001556396,
      "learning_rate": 5.830263604514723e-05,
      "loss": 2.4149,
      "step": 94470
    },
    {
      "epoch": 1.9436559958238937,
      "grad_norm": 0.44992145895957947,
      "learning_rate": 5.828245078168765e-05,
      "loss": 2.3719,
      "step": 94480
    },
    {
      "epoch": 1.9438617153407616,
      "grad_norm": 0.41674548387527466,
      "learning_rate": 5.826226757592752e-05,
      "loss": 2.3949,
      "step": 94490
    },
    {
      "epoch": 1.9440674348576292,
      "grad_norm": 0.47731760144233704,
      "learning_rate": 5.8242086428862266e-05,
      "loss": 2.407,
      "step": 94500
    },
    {
      "epoch": 1.944273154374497,
      "grad_norm": 0.47421029210090637,
      "learning_rate": 5.822190734148727e-05,
      "loss": 2.3928,
      "step": 94510
    },
    {
      "epoch": 1.9444788738913648,
      "grad_norm": 0.4499848484992981,
      "learning_rate": 5.820173031479801e-05,
      "loss": 2.3954,
      "step": 94520
    },
    {
      "epoch": 1.9446845934082324,
      "grad_norm": 0.3978174328804016,
      "learning_rate": 5.81815553497896e-05,
      "loss": 2.4196,
      "step": 94530
    },
    {
      "epoch": 1.9448903129251,
      "grad_norm": 0.47471457719802856,
      "learning_rate": 5.8161382447457114e-05,
      "loss": 2.4122,
      "step": 94540
    },
    {
      "epoch": 1.9450960324419677,
      "grad_norm": 0.48141661286354065,
      "learning_rate": 5.814121160879573e-05,
      "loss": 2.3606,
      "step": 94550
    },
    {
      "epoch": 1.9453017519588354,
      "grad_norm": 0.4278329312801361,
      "learning_rate": 5.812104283480023e-05,
      "loss": 2.388,
      "step": 94560
    },
    {
      "epoch": 1.9455074714757032,
      "grad_norm": 0.43384644389152527,
      "learning_rate": 5.810087612646549e-05,
      "loss": 2.3211,
      "step": 94570
    },
    {
      "epoch": 1.9457131909925711,
      "grad_norm": 0.42799246311187744,
      "learning_rate": 5.8080711484786186e-05,
      "loss": 2.378,
      "step": 94580
    },
    {
      "epoch": 1.9459189105094388,
      "grad_norm": 0.4134562611579895,
      "learning_rate": 5.806054891075696e-05,
      "loss": 2.3828,
      "step": 94590
    },
    {
      "epoch": 1.9461246300263064,
      "grad_norm": 0.44845113158226013,
      "learning_rate": 5.804038840537232e-05,
      "loss": 2.3874,
      "step": 94600
    },
    {
      "epoch": 1.946330349543174,
      "grad_norm": 0.42741525173187256,
      "learning_rate": 5.802022996962666e-05,
      "loss": 2.3715,
      "step": 94610
    },
    {
      "epoch": 1.9465360690600417,
      "grad_norm": 0.4701380729675293,
      "learning_rate": 5.8000073604514296e-05,
      "loss": 2.4287,
      "step": 94620
    },
    {
      "epoch": 1.9467417885769094,
      "grad_norm": 0.44249677658081055,
      "learning_rate": 5.7979919311029416e-05,
      "loss": 2.394,
      "step": 94630
    },
    {
      "epoch": 1.9469475080937773,
      "grad_norm": 0.4800536036491394,
      "learning_rate": 5.795976709016615e-05,
      "loss": 2.3791,
      "step": 94640
    },
    {
      "epoch": 1.947153227610645,
      "grad_norm": 0.4325028359889984,
      "learning_rate": 5.793961694291849e-05,
      "loss": 2.3947,
      "step": 94650
    },
    {
      "epoch": 1.9473589471275128,
      "grad_norm": 0.46949833631515503,
      "learning_rate": 5.79194688702803e-05,
      "loss": 2.3941,
      "step": 94660
    },
    {
      "epoch": 1.9475646666443804,
      "grad_norm": 0.453691303730011,
      "learning_rate": 5.789932287324542e-05,
      "loss": 2.3483,
      "step": 94670
    },
    {
      "epoch": 1.947770386161248,
      "grad_norm": 0.4694453179836273,
      "learning_rate": 5.787917895280752e-05,
      "loss": 2.4032,
      "step": 94680
    },
    {
      "epoch": 1.9479761056781157,
      "grad_norm": 0.4289671778678894,
      "learning_rate": 5.785903710996018e-05,
      "loss": 2.4246,
      "step": 94690
    },
    {
      "epoch": 1.9481818251949834,
      "grad_norm": 0.42368993163108826,
      "learning_rate": 5.7838897345696905e-05,
      "loss": 2.3831,
      "step": 94700
    },
    {
      "epoch": 1.9483875447118513,
      "grad_norm": 0.4293564260005951,
      "learning_rate": 5.7818759661011065e-05,
      "loss": 2.4211,
      "step": 94710
    },
    {
      "epoch": 1.948593264228719,
      "grad_norm": 0.44670313596725464,
      "learning_rate": 5.779862405689597e-05,
      "loss": 2.3734,
      "step": 94720
    },
    {
      "epoch": 1.9487989837455868,
      "grad_norm": 0.43115106225013733,
      "learning_rate": 5.777849053434481e-05,
      "loss": 2.3693,
      "step": 94730
    },
    {
      "epoch": 1.9490047032624545,
      "grad_norm": 0.4818108081817627,
      "learning_rate": 5.7758359094350546e-05,
      "loss": 2.4124,
      "step": 94740
    },
    {
      "epoch": 1.949210422779322,
      "grad_norm": 0.5084360837936401,
      "learning_rate": 5.773822973790628e-05,
      "loss": 2.3777,
      "step": 94750
    },
    {
      "epoch": 1.9494161422961898,
      "grad_norm": 0.4198831021785736,
      "learning_rate": 5.7718102466004884e-05,
      "loss": 2.3697,
      "step": 94760
    },
    {
      "epoch": 1.9496218618130574,
      "grad_norm": 0.41473832726478577,
      "learning_rate": 5.7697977279639016e-05,
      "loss": 2.4633,
      "step": 94770
    },
    {
      "epoch": 1.9498275813299253,
      "grad_norm": 0.44242390990257263,
      "learning_rate": 5.7677854179801425e-05,
      "loss": 2.4017,
      "step": 94780
    },
    {
      "epoch": 1.950033300846793,
      "grad_norm": 0.4007618725299835,
      "learning_rate": 5.765773316748473e-05,
      "loss": 2.408,
      "step": 94790
    },
    {
      "epoch": 1.9502390203636608,
      "grad_norm": 0.43258610367774963,
      "learning_rate": 5.763761424368122e-05,
      "loss": 2.4418,
      "step": 94800
    },
    {
      "epoch": 1.9504447398805285,
      "grad_norm": 0.46833527088165283,
      "learning_rate": 5.761749740938344e-05,
      "loss": 2.3341,
      "step": 94810
    },
    {
      "epoch": 1.9506504593973961,
      "grad_norm": 0.4372949004173279,
      "learning_rate": 5.759738266558351e-05,
      "loss": 2.4398,
      "step": 94820
    },
    {
      "epoch": 1.9508561789142638,
      "grad_norm": 0.4302500784397125,
      "learning_rate": 5.757727001327359e-05,
      "loss": 2.323,
      "step": 94830
    },
    {
      "epoch": 1.9510618984311314,
      "grad_norm": 0.47326338291168213,
      "learning_rate": 5.755715945344584e-05,
      "loss": 2.4017,
      "step": 94840
    },
    {
      "epoch": 1.9512676179479993,
      "grad_norm": 0.4916745126247406,
      "learning_rate": 5.75370509870921e-05,
      "loss": 2.4192,
      "step": 94850
    },
    {
      "epoch": 1.951473337464867,
      "grad_norm": 0.5435112118721008,
      "learning_rate": 5.751694461520423e-05,
      "loss": 2.3908,
      "step": 94860
    },
    {
      "epoch": 1.9516790569817348,
      "grad_norm": 0.5069200992584229,
      "learning_rate": 5.749684033877397e-05,
      "loss": 2.4312,
      "step": 94870
    },
    {
      "epoch": 1.9518847764986025,
      "grad_norm": 0.5137089490890503,
      "learning_rate": 5.747673815879298e-05,
      "loss": 2.4619,
      "step": 94880
    },
    {
      "epoch": 1.9520904960154701,
      "grad_norm": 0.4511597752571106,
      "learning_rate": 5.745663807625276e-05,
      "loss": 2.3828,
      "step": 94890
    },
    {
      "epoch": 1.9522962155323378,
      "grad_norm": 0.4397105574607849,
      "learning_rate": 5.7436540092144744e-05,
      "loss": 2.3785,
      "step": 94900
    },
    {
      "epoch": 1.9525019350492054,
      "grad_norm": 0.6145091652870178,
      "learning_rate": 5.741644420746027e-05,
      "loss": 2.434,
      "step": 94910
    },
    {
      "epoch": 1.952707654566073,
      "grad_norm": 0.4736424684524536,
      "learning_rate": 5.739635042319056e-05,
      "loss": 2.4333,
      "step": 94920
    },
    {
      "epoch": 1.952913374082941,
      "grad_norm": 0.42532873153686523,
      "learning_rate": 5.7376258740326705e-05,
      "loss": 2.3783,
      "step": 94930
    },
    {
      "epoch": 1.9531190935998088,
      "grad_norm": 0.49066320061683655,
      "learning_rate": 5.735616915985975e-05,
      "loss": 2.3421,
      "step": 94940
    },
    {
      "epoch": 1.9533248131166765,
      "grad_norm": 0.48318079113960266,
      "learning_rate": 5.733608168278058e-05,
      "loss": 2.4094,
      "step": 94950
    },
    {
      "epoch": 1.9535305326335441,
      "grad_norm": 0.4276715815067291,
      "learning_rate": 5.731599631008002e-05,
      "loss": 2.4009,
      "step": 94960
    },
    {
      "epoch": 1.9537362521504118,
      "grad_norm": 0.45516109466552734,
      "learning_rate": 5.729591304274875e-05,
      "loss": 2.3831,
      "step": 94970
    },
    {
      "epoch": 1.9539419716672795,
      "grad_norm": 0.464826762676239,
      "learning_rate": 5.727583188177739e-05,
      "loss": 2.4281,
      "step": 94980
    },
    {
      "epoch": 1.954147691184147,
      "grad_norm": 0.41220423579216003,
      "learning_rate": 5.7255752828156415e-05,
      "loss": 2.3924,
      "step": 94990
    },
    {
      "epoch": 1.954353410701015,
      "grad_norm": 0.46382299065589905,
      "learning_rate": 5.723567588287623e-05,
      "loss": 2.4047,
      "step": 95000
    },
    {
      "epoch": 1.9545591302178826,
      "grad_norm": 0.4517529308795929,
      "learning_rate": 5.7215601046927114e-05,
      "loss": 2.4307,
      "step": 95010
    },
    {
      "epoch": 1.9547648497347505,
      "grad_norm": 0.45755329728126526,
      "learning_rate": 5.719552832129928e-05,
      "loss": 2.3405,
      "step": 95020
    },
    {
      "epoch": 1.9549705692516182,
      "grad_norm": 0.44516390562057495,
      "learning_rate": 5.7175457706982715e-05,
      "loss": 2.3943,
      "step": 95030
    },
    {
      "epoch": 1.9551762887684858,
      "grad_norm": 0.43490761518478394,
      "learning_rate": 5.71553892049675e-05,
      "loss": 2.3998,
      "step": 95040
    },
    {
      "epoch": 1.9553820082853535,
      "grad_norm": 0.3987610936164856,
      "learning_rate": 5.71353228162435e-05,
      "loss": 2.4084,
      "step": 95050
    },
    {
      "epoch": 1.9555877278022211,
      "grad_norm": 0.4599618911743164,
      "learning_rate": 5.7115258541800346e-05,
      "loss": 2.2896,
      "step": 95060
    },
    {
      "epoch": 1.955793447319089,
      "grad_norm": 0.4294920265674591,
      "learning_rate": 5.709519638262783e-05,
      "loss": 2.3607,
      "step": 95070
    },
    {
      "epoch": 1.9559991668359566,
      "grad_norm": 0.4058254063129425,
      "learning_rate": 5.707513633971553e-05,
      "loss": 2.3476,
      "step": 95080
    },
    {
      "epoch": 1.9562048863528245,
      "grad_norm": 0.46159929037094116,
      "learning_rate": 5.705507841405275e-05,
      "loss": 2.3935,
      "step": 95090
    },
    {
      "epoch": 1.9564106058696922,
      "grad_norm": 0.45147883892059326,
      "learning_rate": 5.703502260662903e-05,
      "loss": 2.3873,
      "step": 95100
    },
    {
      "epoch": 1.9566163253865598,
      "grad_norm": 0.4283185601234436,
      "learning_rate": 5.7014968918433455e-05,
      "loss": 2.3899,
      "step": 95110
    },
    {
      "epoch": 1.9568220449034275,
      "grad_norm": 0.42809394001960754,
      "learning_rate": 5.699491735045519e-05,
      "loss": 2.3873,
      "step": 95120
    },
    {
      "epoch": 1.9570277644202951,
      "grad_norm": 0.4438454508781433,
      "learning_rate": 5.6974867903683384e-05,
      "loss": 2.3557,
      "step": 95130
    },
    {
      "epoch": 1.957233483937163,
      "grad_norm": 0.43261846899986267,
      "learning_rate": 5.695482057910684e-05,
      "loss": 2.2984,
      "step": 95140
    },
    {
      "epoch": 1.9574392034540307,
      "grad_norm": 0.46665507555007935,
      "learning_rate": 5.693477537771439e-05,
      "loss": 2.3822,
      "step": 95150
    },
    {
      "epoch": 1.9576449229708985,
      "grad_norm": 0.5333294868469238,
      "learning_rate": 5.6914732300494865e-05,
      "loss": 2.374,
      "step": 95160
    },
    {
      "epoch": 1.9578506424877662,
      "grad_norm": 0.4302231967449188,
      "learning_rate": 5.689469134843678e-05,
      "loss": 2.4382,
      "step": 95170
    },
    {
      "epoch": 1.9580563620046338,
      "grad_norm": 0.45911362767219543,
      "learning_rate": 5.687465252252867e-05,
      "loss": 2.4455,
      "step": 95180
    },
    {
      "epoch": 1.9582620815215015,
      "grad_norm": 0.4546543061733246,
      "learning_rate": 5.685461582375895e-05,
      "loss": 2.4411,
      "step": 95190
    },
    {
      "epoch": 1.9584678010383691,
      "grad_norm": 0.4450639486312866,
      "learning_rate": 5.6834581253115916e-05,
      "loss": 2.3408,
      "step": 95200
    },
    {
      "epoch": 1.958673520555237,
      "grad_norm": 0.45010700821876526,
      "learning_rate": 5.6814548811587764e-05,
      "loss": 2.4535,
      "step": 95210
    },
    {
      "epoch": 1.9588792400721047,
      "grad_norm": 0.4488166868686676,
      "learning_rate": 5.67945185001626e-05,
      "loss": 2.376,
      "step": 95220
    },
    {
      "epoch": 1.9590849595889726,
      "grad_norm": 0.4341602623462677,
      "learning_rate": 5.6774490319828375e-05,
      "loss": 2.4691,
      "step": 95230
    },
    {
      "epoch": 1.9592906791058402,
      "grad_norm": 0.4375194013118744,
      "learning_rate": 5.6754464271573e-05,
      "loss": 2.3753,
      "step": 95240
    },
    {
      "epoch": 1.9594963986227079,
      "grad_norm": 0.4601644277572632,
      "learning_rate": 5.673444035638425e-05,
      "loss": 2.3865,
      "step": 95250
    },
    {
      "epoch": 1.9597021181395755,
      "grad_norm": 0.44089820981025696,
      "learning_rate": 5.671441857524977e-05,
      "loss": 2.3767,
      "step": 95260
    },
    {
      "epoch": 1.9599078376564432,
      "grad_norm": 0.42799803614616394,
      "learning_rate": 5.6694398929157154e-05,
      "loss": 2.3874,
      "step": 95270
    },
    {
      "epoch": 1.9601135571733108,
      "grad_norm": 0.43924954533576965,
      "learning_rate": 5.6674381419093866e-05,
      "loss": 2.4116,
      "step": 95280
    },
    {
      "epoch": 1.9603192766901787,
      "grad_norm": 0.47215160727500916,
      "learning_rate": 5.6654366046047236e-05,
      "loss": 2.3837,
      "step": 95290
    },
    {
      "epoch": 1.9605249962070466,
      "grad_norm": 0.4528011083602905,
      "learning_rate": 5.663435281100451e-05,
      "loss": 2.3548,
      "step": 95300
    },
    {
      "epoch": 1.9607307157239142,
      "grad_norm": 0.4391680359840393,
      "learning_rate": 5.661434171495285e-05,
      "loss": 2.4433,
      "step": 95310
    },
    {
      "epoch": 1.9609364352407819,
      "grad_norm": 0.41538265347480774,
      "learning_rate": 5.659433275887931e-05,
      "loss": 2.3837,
      "step": 95320
    },
    {
      "epoch": 1.9611421547576495,
      "grad_norm": 0.45934945344924927,
      "learning_rate": 5.657432594377079e-05,
      "loss": 2.3432,
      "step": 95330
    },
    {
      "epoch": 1.9613478742745172,
      "grad_norm": 0.4266291856765747,
      "learning_rate": 5.655432127061418e-05,
      "loss": 2.403,
      "step": 95340
    },
    {
      "epoch": 1.9615535937913848,
      "grad_norm": 0.4646095931529999,
      "learning_rate": 5.6534318740396054e-05,
      "loss": 2.3802,
      "step": 95350
    },
    {
      "epoch": 1.9617593133082527,
      "grad_norm": 0.44078779220581055,
      "learning_rate": 5.651431835410317e-05,
      "loss": 2.4019,
      "step": 95360
    },
    {
      "epoch": 1.9619650328251204,
      "grad_norm": 0.4475950598716736,
      "learning_rate": 5.649432011272202e-05,
      "loss": 2.4232,
      "step": 95370
    },
    {
      "epoch": 1.9621707523419882,
      "grad_norm": 0.44932475686073303,
      "learning_rate": 5.6474324017238924e-05,
      "loss": 2.3584,
      "step": 95380
    },
    {
      "epoch": 1.9623764718588559,
      "grad_norm": 0.4228662848472595,
      "learning_rate": 5.645433006864027e-05,
      "loss": 2.4079,
      "step": 95390
    },
    {
      "epoch": 1.9625821913757235,
      "grad_norm": 0.4658302664756775,
      "learning_rate": 5.643433826791226e-05,
      "loss": 2.3802,
      "step": 95400
    },
    {
      "epoch": 1.9627879108925912,
      "grad_norm": 0.4414978623390198,
      "learning_rate": 5.6414348616040846e-05,
      "loss": 2.435,
      "step": 95410
    },
    {
      "epoch": 1.9629936304094588,
      "grad_norm": 0.490329772233963,
      "learning_rate": 5.6394361114012197e-05,
      "loss": 2.4472,
      "step": 95420
    },
    {
      "epoch": 1.9631993499263267,
      "grad_norm": 0.45725560188293457,
      "learning_rate": 5.637437576281204e-05,
      "loss": 2.4068,
      "step": 95430
    },
    {
      "epoch": 1.9634050694431944,
      "grad_norm": 0.43534383177757263,
      "learning_rate": 5.635439256342616e-05,
      "loss": 2.3684,
      "step": 95440
    },
    {
      "epoch": 1.9636107889600622,
      "grad_norm": 0.4393675625324249,
      "learning_rate": 5.633441151684034e-05,
      "loss": 2.4175,
      "step": 95450
    },
    {
      "epoch": 1.96381650847693,
      "grad_norm": 0.5648490786552429,
      "learning_rate": 5.6314432624040016e-05,
      "loss": 2.3963,
      "step": 95460
    },
    {
      "epoch": 1.9640222279937976,
      "grad_norm": 0.42751094698905945,
      "learning_rate": 5.629445588601062e-05,
      "loss": 2.3719,
      "step": 95470
    },
    {
      "epoch": 1.9642279475106652,
      "grad_norm": 0.4491327404975891,
      "learning_rate": 5.6274481303737625e-05,
      "loss": 2.409,
      "step": 95480
    },
    {
      "epoch": 1.9644336670275329,
      "grad_norm": 0.47108566761016846,
      "learning_rate": 5.625450887820617e-05,
      "loss": 2.4129,
      "step": 95490
    },
    {
      "epoch": 1.9646393865444007,
      "grad_norm": 0.4269063174724579,
      "learning_rate": 5.62345386104014e-05,
      "loss": 2.3551,
      "step": 95500
    },
    {
      "epoch": 1.9648451060612684,
      "grad_norm": 0.4154456555843353,
      "learning_rate": 5.621457050130834e-05,
      "loss": 2.4348,
      "step": 95510
    },
    {
      "epoch": 1.9650508255781363,
      "grad_norm": 0.4790040850639343,
      "learning_rate": 5.619460455191192e-05,
      "loss": 2.4317,
      "step": 95520
    },
    {
      "epoch": 1.965256545095004,
      "grad_norm": 0.46107926964759827,
      "learning_rate": 5.617464076319694e-05,
      "loss": 2.3893,
      "step": 95530
    },
    {
      "epoch": 1.9654622646118716,
      "grad_norm": 0.4048466682434082,
      "learning_rate": 5.615467913614813e-05,
      "loss": 2.3389,
      "step": 95540
    },
    {
      "epoch": 1.9656679841287392,
      "grad_norm": 0.6725085973739624,
      "learning_rate": 5.613471967175006e-05,
      "loss": 2.3573,
      "step": 95550
    },
    {
      "epoch": 1.9658737036456069,
      "grad_norm": 0.43369755148887634,
      "learning_rate": 5.611476237098723e-05,
      "loss": 2.3952,
      "step": 95560
    },
    {
      "epoch": 1.9660794231624747,
      "grad_norm": 0.5303452014923096,
      "learning_rate": 5.6094807234844014e-05,
      "loss": 2.3939,
      "step": 95570
    },
    {
      "epoch": 1.9662851426793424,
      "grad_norm": 0.4796278178691864,
      "learning_rate": 5.607485426430472e-05,
      "loss": 2.3224,
      "step": 95580
    },
    {
      "epoch": 1.9664908621962103,
      "grad_norm": 0.4558713436126709,
      "learning_rate": 5.605490346035349e-05,
      "loss": 2.3427,
      "step": 95590
    },
    {
      "epoch": 1.966696581713078,
      "grad_norm": 0.43054574728012085,
      "learning_rate": 5.603495482397441e-05,
      "loss": 2.4107,
      "step": 95600
    },
    {
      "epoch": 1.9669023012299456,
      "grad_norm": 0.43085888028144836,
      "learning_rate": 5.6015008356151435e-05,
      "loss": 2.3947,
      "step": 95610
    },
    {
      "epoch": 1.9671080207468132,
      "grad_norm": 0.4463730752468109,
      "learning_rate": 5.5995064057868386e-05,
      "loss": 2.3245,
      "step": 95620
    },
    {
      "epoch": 1.9673137402636809,
      "grad_norm": 0.46302735805511475,
      "learning_rate": 5.597512193010904e-05,
      "loss": 2.4387,
      "step": 95630
    },
    {
      "epoch": 1.9675194597805485,
      "grad_norm": 0.41456231474876404,
      "learning_rate": 5.5955181973857026e-05,
      "loss": 2.3667,
      "step": 95640
    },
    {
      "epoch": 1.9677251792974164,
      "grad_norm": 0.42104658484458923,
      "learning_rate": 5.593524419009586e-05,
      "loss": 2.3627,
      "step": 95650
    },
    {
      "epoch": 1.9679308988142843,
      "grad_norm": 0.44275906682014465,
      "learning_rate": 5.591530857980902e-05,
      "loss": 2.3866,
      "step": 95660
    },
    {
      "epoch": 1.968136618331152,
      "grad_norm": 0.44664356112480164,
      "learning_rate": 5.58953751439797e-05,
      "loss": 2.3906,
      "step": 95670
    },
    {
      "epoch": 1.9683423378480196,
      "grad_norm": 0.42916110157966614,
      "learning_rate": 5.587544388359123e-05,
      "loss": 2.3904,
      "step": 95680
    },
    {
      "epoch": 1.9685480573648872,
      "grad_norm": 0.42860132455825806,
      "learning_rate": 5.5855514799626674e-05,
      "loss": 2.3858,
      "step": 95690
    },
    {
      "epoch": 1.968753776881755,
      "grad_norm": 0.45928528904914856,
      "learning_rate": 5.583558789306895e-05,
      "loss": 2.3303,
      "step": 95700
    },
    {
      "epoch": 1.9689594963986226,
      "grad_norm": 0.47087594866752625,
      "learning_rate": 5.581566316490105e-05,
      "loss": 2.3795,
      "step": 95710
    },
    {
      "epoch": 1.9691652159154904,
      "grad_norm": 0.4146174490451813,
      "learning_rate": 5.579574061610575e-05,
      "loss": 2.3539,
      "step": 95720
    },
    {
      "epoch": 1.969370935432358,
      "grad_norm": 0.4684527516365051,
      "learning_rate": 5.5775820247665614e-05,
      "loss": 2.3577,
      "step": 95730
    },
    {
      "epoch": 1.969576654949226,
      "grad_norm": 0.41875454783439636,
      "learning_rate": 5.5755902060563335e-05,
      "loss": 2.4315,
      "step": 95740
    },
    {
      "epoch": 1.9697823744660936,
      "grad_norm": 0.4328162372112274,
      "learning_rate": 5.573598605578128e-05,
      "loss": 2.3082,
      "step": 95750
    },
    {
      "epoch": 1.9699880939829613,
      "grad_norm": 0.41795945167541504,
      "learning_rate": 5.5716072234301765e-05,
      "loss": 2.422,
      "step": 95760
    },
    {
      "epoch": 1.970193813499829,
      "grad_norm": 0.44695961475372314,
      "learning_rate": 5.569616059710718e-05,
      "loss": 2.4066,
      "step": 95770
    },
    {
      "epoch": 1.9703995330166966,
      "grad_norm": 0.46108222007751465,
      "learning_rate": 5.5676251145179516e-05,
      "loss": 2.3986,
      "step": 95780
    },
    {
      "epoch": 1.9706052525335644,
      "grad_norm": 0.43871915340423584,
      "learning_rate": 5.565634387950081e-05,
      "loss": 2.3698,
      "step": 95790
    },
    {
      "epoch": 1.970810972050432,
      "grad_norm": 0.47062253952026367,
      "learning_rate": 5.563643880105312e-05,
      "loss": 2.3493,
      "step": 95800
    },
    {
      "epoch": 1.9710166915673,
      "grad_norm": 0.47543928027153015,
      "learning_rate": 5.561653591081808e-05,
      "loss": 2.3414,
      "step": 95810
    },
    {
      "epoch": 1.9712224110841676,
      "grad_norm": 0.425686776638031,
      "learning_rate": 5.559663520977747e-05,
      "loss": 2.4016,
      "step": 95820
    },
    {
      "epoch": 1.9714281306010353,
      "grad_norm": 0.4259553849697113,
      "learning_rate": 5.5576736698912876e-05,
      "loss": 2.4196,
      "step": 95830
    },
    {
      "epoch": 1.971633850117903,
      "grad_norm": 0.4318956136703491,
      "learning_rate": 5.555684037920579e-05,
      "loss": 2.3324,
      "step": 95840
    },
    {
      "epoch": 1.9718395696347706,
      "grad_norm": 0.46358275413513184,
      "learning_rate": 5.553694625163759e-05,
      "loss": 2.4453,
      "step": 95850
    },
    {
      "epoch": 1.9720452891516385,
      "grad_norm": 0.4312404990196228,
      "learning_rate": 5.5517054317189524e-05,
      "loss": 2.3992,
      "step": 95860
    },
    {
      "epoch": 1.9722510086685061,
      "grad_norm": 0.5457307696342468,
      "learning_rate": 5.549716457684276e-05,
      "loss": 2.3604,
      "step": 95870
    },
    {
      "epoch": 1.972456728185374,
      "grad_norm": 0.42162540555000305,
      "learning_rate": 5.5477277031578366e-05,
      "loss": 2.3825,
      "step": 95880
    },
    {
      "epoch": 1.9726624477022416,
      "grad_norm": 0.4745285212993622,
      "learning_rate": 5.545739168237727e-05,
      "loss": 2.3819,
      "step": 95890
    },
    {
      "epoch": 1.9728681672191093,
      "grad_norm": 0.43958425521850586,
      "learning_rate": 5.543750853022032e-05,
      "loss": 2.3557,
      "step": 95900
    },
    {
      "epoch": 1.973073886735977,
      "grad_norm": 0.4677988886833191,
      "learning_rate": 5.541762757608824e-05,
      "loss": 2.3939,
      "step": 95910
    },
    {
      "epoch": 1.9732796062528446,
      "grad_norm": 0.44803082942962646,
      "learning_rate": 5.539774882096165e-05,
      "loss": 2.3989,
      "step": 95920
    },
    {
      "epoch": 1.9734853257697125,
      "grad_norm": 0.47393110394477844,
      "learning_rate": 5.537787226582105e-05,
      "loss": 2.3087,
      "step": 95930
    },
    {
      "epoch": 1.9736910452865801,
      "grad_norm": 0.45153582096099854,
      "learning_rate": 5.535799791164684e-05,
      "loss": 2.4337,
      "step": 95940
    },
    {
      "epoch": 1.973896764803448,
      "grad_norm": 0.4591459631919861,
      "learning_rate": 5.5338125759419335e-05,
      "loss": 2.3822,
      "step": 95950
    },
    {
      "epoch": 1.9741024843203157,
      "grad_norm": 0.4357735216617584,
      "learning_rate": 5.5318255810118715e-05,
      "loss": 2.4273,
      "step": 95960
    },
    {
      "epoch": 1.9743082038371833,
      "grad_norm": 0.4259437918663025,
      "learning_rate": 5.529838806472504e-05,
      "loss": 2.3927,
      "step": 95970
    },
    {
      "epoch": 1.974513923354051,
      "grad_norm": 0.4298020005226135,
      "learning_rate": 5.5278522524218326e-05,
      "loss": 2.3473,
      "step": 95980
    },
    {
      "epoch": 1.9747196428709186,
      "grad_norm": 0.4367475211620331,
      "learning_rate": 5.525865918957831e-05,
      "loss": 2.3485,
      "step": 95990
    },
    {
      "epoch": 1.9749253623877865,
      "grad_norm": 0.4295291602611542,
      "learning_rate": 5.523879806178487e-05,
      "loss": 2.4059,
      "step": 96000
    },
    {
      "epoch": 1.9751310819046541,
      "grad_norm": 0.4568321704864502,
      "learning_rate": 5.5218939141817636e-05,
      "loss": 2.3789,
      "step": 96010
    },
    {
      "epoch": 1.975336801421522,
      "grad_norm": 0.42797723412513733,
      "learning_rate": 5.5199082430656035e-05,
      "loss": 2.4342,
      "step": 96020
    },
    {
      "epoch": 1.9755425209383897,
      "grad_norm": 0.39379775524139404,
      "learning_rate": 5.517922792927965e-05,
      "loss": 2.3808,
      "step": 96030
    },
    {
      "epoch": 1.9757482404552573,
      "grad_norm": 0.43000274896621704,
      "learning_rate": 5.515937563866765e-05,
      "loss": 2.3986,
      "step": 96040
    },
    {
      "epoch": 1.975953959972125,
      "grad_norm": 0.5343746542930603,
      "learning_rate": 5.513952555979927e-05,
      "loss": 2.3569,
      "step": 96050
    },
    {
      "epoch": 1.9761596794889926,
      "grad_norm": 0.4689468443393707,
      "learning_rate": 5.5119677693653716e-05,
      "loss": 2.3571,
      "step": 96060
    },
    {
      "epoch": 1.9763653990058603,
      "grad_norm": 0.42504581809043884,
      "learning_rate": 5.5099832041209845e-05,
      "loss": 2.3525,
      "step": 96070
    },
    {
      "epoch": 1.9765711185227282,
      "grad_norm": 0.44309884309768677,
      "learning_rate": 5.507998860344654e-05,
      "loss": 2.3206,
      "step": 96080
    },
    {
      "epoch": 1.976776838039596,
      "grad_norm": 0.42713576555252075,
      "learning_rate": 5.5060147381342697e-05,
      "loss": 2.3657,
      "step": 96090
    },
    {
      "epoch": 1.9769825575564637,
      "grad_norm": 0.4509729743003845,
      "learning_rate": 5.504030837587685e-05,
      "loss": 2.3609,
      "step": 96100
    },
    {
      "epoch": 1.9771882770733313,
      "grad_norm": 0.45044997334480286,
      "learning_rate": 5.502047158802759e-05,
      "loss": 2.3566,
      "step": 96110
    },
    {
      "epoch": 1.977393996590199,
      "grad_norm": 0.4371258318424225,
      "learning_rate": 5.500063701877336e-05,
      "loss": 2.3621,
      "step": 96120
    },
    {
      "epoch": 1.9775997161070666,
      "grad_norm": 0.47066888213157654,
      "learning_rate": 5.498080466909248e-05,
      "loss": 2.3991,
      "step": 96130
    },
    {
      "epoch": 1.9778054356239343,
      "grad_norm": 0.4616340100765228,
      "learning_rate": 5.496097453996318e-05,
      "loss": 2.4162,
      "step": 96140
    },
    {
      "epoch": 1.9780111551408022,
      "grad_norm": 0.43403592705726624,
      "learning_rate": 5.4941146632363594e-05,
      "loss": 2.4122,
      "step": 96150
    },
    {
      "epoch": 1.9782168746576698,
      "grad_norm": 0.4721321761608124,
      "learning_rate": 5.4921320947271694e-05,
      "loss": 2.4405,
      "step": 96160
    },
    {
      "epoch": 1.9784225941745377,
      "grad_norm": 0.45039597153663635,
      "learning_rate": 5.4901497485665376e-05,
      "loss": 2.4187,
      "step": 96170
    },
    {
      "epoch": 1.9786283136914053,
      "grad_norm": 0.43288928270339966,
      "learning_rate": 5.488167624852243e-05,
      "loss": 2.4232,
      "step": 96180
    },
    {
      "epoch": 1.978834033208273,
      "grad_norm": 0.4412643611431122,
      "learning_rate": 5.4861857236820525e-05,
      "loss": 2.3501,
      "step": 96190
    },
    {
      "epoch": 1.9790397527251407,
      "grad_norm": 0.45484504103660583,
      "learning_rate": 5.4842040451537235e-05,
      "loss": 2.3777,
      "step": 96200
    },
    {
      "epoch": 1.9792454722420083,
      "grad_norm": 0.4356164038181305,
      "learning_rate": 5.482222589365e-05,
      "loss": 2.4404,
      "step": 96210
    },
    {
      "epoch": 1.9794511917588762,
      "grad_norm": 0.4320377707481384,
      "learning_rate": 5.480241356413618e-05,
      "loss": 2.376,
      "step": 96220
    },
    {
      "epoch": 1.9796569112757438,
      "grad_norm": 0.42844146490097046,
      "learning_rate": 5.478260346397301e-05,
      "loss": 2.3514,
      "step": 96230
    },
    {
      "epoch": 1.9798626307926117,
      "grad_norm": 0.4599158763885498,
      "learning_rate": 5.4762795594137596e-05,
      "loss": 2.3675,
      "step": 96240
    },
    {
      "epoch": 1.9800683503094794,
      "grad_norm": 0.48742642998695374,
      "learning_rate": 5.474298995560696e-05,
      "loss": 2.3993,
      "step": 96250
    },
    {
      "epoch": 1.980274069826347,
      "grad_norm": 0.4151877462863922,
      "learning_rate": 5.472318654935802e-05,
      "loss": 2.3085,
      "step": 96260
    },
    {
      "epoch": 1.9804797893432147,
      "grad_norm": 0.42525163292884827,
      "learning_rate": 5.470338537636759e-05,
      "loss": 2.438,
      "step": 96270
    },
    {
      "epoch": 1.9806855088600823,
      "grad_norm": 0.5093091130256653,
      "learning_rate": 5.468358643761224e-05,
      "loss": 2.3432,
      "step": 96280
    },
    {
      "epoch": 1.9808912283769502,
      "grad_norm": 0.498644083738327,
      "learning_rate": 5.466378973406867e-05,
      "loss": 2.4225,
      "step": 96290
    },
    {
      "epoch": 1.9810969478938179,
      "grad_norm": 0.4654150903224945,
      "learning_rate": 5.4643995266713335e-05,
      "loss": 2.3769,
      "step": 96300
    },
    {
      "epoch": 1.9813026674106857,
      "grad_norm": 0.4651503562927246,
      "learning_rate": 5.462420303652247e-05,
      "loss": 2.3971,
      "step": 96310
    },
    {
      "epoch": 1.9815083869275534,
      "grad_norm": 0.46021416783332825,
      "learning_rate": 5.460441304447246e-05,
      "loss": 2.4332,
      "step": 96320
    },
    {
      "epoch": 1.981714106444421,
      "grad_norm": 0.5131248831748962,
      "learning_rate": 5.4584625291539404e-05,
      "loss": 2.3691,
      "step": 96330
    },
    {
      "epoch": 1.9819198259612887,
      "grad_norm": 0.4383140802383423,
      "learning_rate": 5.456483977869922e-05,
      "loss": 2.4234,
      "step": 96340
    },
    {
      "epoch": 1.9821255454781563,
      "grad_norm": 0.4512176811695099,
      "learning_rate": 5.454505650692797e-05,
      "loss": 2.4098,
      "step": 96350
    },
    {
      "epoch": 1.9823312649950242,
      "grad_norm": 0.4207127094268799,
      "learning_rate": 5.452527547720136e-05,
      "loss": 2.3095,
      "step": 96360
    },
    {
      "epoch": 1.9825369845118919,
      "grad_norm": 0.43787744641304016,
      "learning_rate": 5.450549669049505e-05,
      "loss": 2.3757,
      "step": 96370
    },
    {
      "epoch": 1.9827427040287597,
      "grad_norm": 0.4541587233543396,
      "learning_rate": 5.4485720147784746e-05,
      "loss": 2.3606,
      "step": 96380
    },
    {
      "epoch": 1.9829484235456274,
      "grad_norm": 0.5057007074356079,
      "learning_rate": 5.446594585004583e-05,
      "loss": 2.3862,
      "step": 96390
    },
    {
      "epoch": 1.983154143062495,
      "grad_norm": 0.4145667552947998,
      "learning_rate": 5.444617379825361e-05,
      "loss": 2.3704,
      "step": 96400
    },
    {
      "epoch": 1.9833598625793627,
      "grad_norm": 0.44510418176651,
      "learning_rate": 5.442640399338349e-05,
      "loss": 2.4376,
      "step": 96410
    },
    {
      "epoch": 1.9835655820962304,
      "grad_norm": 0.4192887246608734,
      "learning_rate": 5.4406636436410463e-05,
      "loss": 2.3942,
      "step": 96420
    },
    {
      "epoch": 1.983771301613098,
      "grad_norm": 0.5080559849739075,
      "learning_rate": 5.438687112830962e-05,
      "loss": 2.4165,
      "step": 96430
    },
    {
      "epoch": 1.9839770211299659,
      "grad_norm": 0.4256494343280792,
      "learning_rate": 5.4367108070055864e-05,
      "loss": 2.4111,
      "step": 96440
    },
    {
      "epoch": 1.9841827406468338,
      "grad_norm": 0.42102184891700745,
      "learning_rate": 5.4347347262623984e-05,
      "loss": 2.4356,
      "step": 96450
    },
    {
      "epoch": 1.9843884601637014,
      "grad_norm": 0.44955670833587646,
      "learning_rate": 5.43275887069887e-05,
      "loss": 2.3304,
      "step": 96460
    },
    {
      "epoch": 1.984594179680569,
      "grad_norm": 0.3992345631122589,
      "learning_rate": 5.430783240412457e-05,
      "loss": 2.3752,
      "step": 96470
    },
    {
      "epoch": 1.9847998991974367,
      "grad_norm": 0.4177442789077759,
      "learning_rate": 5.4288078355006076e-05,
      "loss": 2.334,
      "step": 96480
    },
    {
      "epoch": 1.9850056187143044,
      "grad_norm": 0.4230309724807739,
      "learning_rate": 5.4268326560607585e-05,
      "loss": 2.3446,
      "step": 96490
    },
    {
      "epoch": 1.985211338231172,
      "grad_norm": 0.49530625343322754,
      "learning_rate": 5.4248577021903335e-05,
      "loss": 2.3871,
      "step": 96500
    },
    {
      "epoch": 1.98541705774804,
      "grad_norm": 0.42562928795814514,
      "learning_rate": 5.422882973986745e-05,
      "loss": 2.3911,
      "step": 96510
    },
    {
      "epoch": 1.9856227772649075,
      "grad_norm": 0.4472818374633789,
      "learning_rate": 5.420908471547399e-05,
      "loss": 2.4082,
      "step": 96520
    },
    {
      "epoch": 1.9858284967817754,
      "grad_norm": 0.46447592973709106,
      "learning_rate": 5.418934194969685e-05,
      "loss": 2.4294,
      "step": 96530
    },
    {
      "epoch": 1.986034216298643,
      "grad_norm": 0.4593963027000427,
      "learning_rate": 5.416960144350982e-05,
      "loss": 2.4393,
      "step": 96540
    },
    {
      "epoch": 1.9862399358155107,
      "grad_norm": 0.4584764242172241,
      "learning_rate": 5.414986319788661e-05,
      "loss": 2.3951,
      "step": 96550
    },
    {
      "epoch": 1.9864456553323784,
      "grad_norm": 0.428136944770813,
      "learning_rate": 5.41301272138008e-05,
      "loss": 2.4147,
      "step": 96560
    },
    {
      "epoch": 1.986651374849246,
      "grad_norm": 0.4190935492515564,
      "learning_rate": 5.411039349222584e-05,
      "loss": 2.3636,
      "step": 96570
    },
    {
      "epoch": 1.986857094366114,
      "grad_norm": 0.5147709846496582,
      "learning_rate": 5.409066203413512e-05,
      "loss": 2.4088,
      "step": 96580
    },
    {
      "epoch": 1.9870628138829816,
      "grad_norm": 0.48836642503738403,
      "learning_rate": 5.407093284050189e-05,
      "loss": 2.3768,
      "step": 96590
    },
    {
      "epoch": 1.9872685333998494,
      "grad_norm": 0.45720377564430237,
      "learning_rate": 5.4051205912299196e-05,
      "loss": 2.4188,
      "step": 96600
    },
    {
      "epoch": 1.987474252916717,
      "grad_norm": 0.45606160163879395,
      "learning_rate": 5.403148125050015e-05,
      "loss": 2.3962,
      "step": 96610
    },
    {
      "epoch": 1.9876799724335847,
      "grad_norm": 0.42926231026649475,
      "learning_rate": 5.4011758856077675e-05,
      "loss": 2.4023,
      "step": 96620
    },
    {
      "epoch": 1.9878856919504524,
      "grad_norm": 0.41658687591552734,
      "learning_rate": 5.3992038730004454e-05,
      "loss": 2.4064,
      "step": 96630
    },
    {
      "epoch": 1.98809141146732,
      "grad_norm": 0.4433613121509552,
      "learning_rate": 5.39723208732533e-05,
      "loss": 2.392,
      "step": 96640
    },
    {
      "epoch": 1.988297130984188,
      "grad_norm": 0.41971173882484436,
      "learning_rate": 5.395260528679675e-05,
      "loss": 2.419,
      "step": 96650
    },
    {
      "epoch": 1.9885028505010556,
      "grad_norm": 0.44871601462364197,
      "learning_rate": 5.39328919716072e-05,
      "loss": 2.3857,
      "step": 96660
    },
    {
      "epoch": 1.9887085700179234,
      "grad_norm": 0.4782079756259918,
      "learning_rate": 5.391318092865712e-05,
      "loss": 2.4345,
      "step": 96670
    },
    {
      "epoch": 1.988914289534791,
      "grad_norm": 0.4310947358608246,
      "learning_rate": 5.3893472158918646e-05,
      "loss": 2.3788,
      "step": 96680
    },
    {
      "epoch": 1.9891200090516588,
      "grad_norm": 0.444161057472229,
      "learning_rate": 5.38737656633639e-05,
      "loss": 2.3936,
      "step": 96690
    },
    {
      "epoch": 1.9893257285685264,
      "grad_norm": 0.43832382559776306,
      "learning_rate": 5.385406144296501e-05,
      "loss": 2.3653,
      "step": 96700
    },
    {
      "epoch": 1.989531448085394,
      "grad_norm": 0.42150869965553284,
      "learning_rate": 5.3834359498693774e-05,
      "loss": 2.3872,
      "step": 96710
    },
    {
      "epoch": 1.989737167602262,
      "grad_norm": 0.43828389048576355,
      "learning_rate": 5.3814659831521964e-05,
      "loss": 2.3089,
      "step": 96720
    },
    {
      "epoch": 1.9899428871191296,
      "grad_norm": 0.4390542209148407,
      "learning_rate": 5.379496244242138e-05,
      "loss": 2.3605,
      "step": 96730
    },
    {
      "epoch": 1.9901486066359975,
      "grad_norm": 0.41975775361061096,
      "learning_rate": 5.377526733236349e-05,
      "loss": 2.4021,
      "step": 96740
    },
    {
      "epoch": 1.9903543261528651,
      "grad_norm": 0.41644877195358276,
      "learning_rate": 5.3755574502319736e-05,
      "loss": 2.4368,
      "step": 96750
    },
    {
      "epoch": 1.9905600456697328,
      "grad_norm": 0.4165025055408478,
      "learning_rate": 5.3735883953261526e-05,
      "loss": 2.4232,
      "step": 96760
    },
    {
      "epoch": 1.9907657651866004,
      "grad_norm": 0.460948646068573,
      "learning_rate": 5.371619568616002e-05,
      "loss": 2.3995,
      "step": 96770
    },
    {
      "epoch": 1.990971484703468,
      "grad_norm": 0.4683985710144043,
      "learning_rate": 5.369650970198638e-05,
      "loss": 2.4303,
      "step": 96780
    },
    {
      "epoch": 1.9911772042203357,
      "grad_norm": 0.5844603180885315,
      "learning_rate": 5.36768260017116e-05,
      "loss": 2.4389,
      "step": 96790
    },
    {
      "epoch": 1.9913829237372036,
      "grad_norm": 0.4184388816356659,
      "learning_rate": 5.365714458630655e-05,
      "loss": 2.3801,
      "step": 96800
    },
    {
      "epoch": 1.9915886432540715,
      "grad_norm": 0.4724671542644501,
      "learning_rate": 5.363746545674202e-05,
      "loss": 2.4317,
      "step": 96810
    },
    {
      "epoch": 1.9917943627709391,
      "grad_norm": 0.4534631371498108,
      "learning_rate": 5.3617788613988685e-05,
      "loss": 2.3206,
      "step": 96820
    },
    {
      "epoch": 1.9920000822878068,
      "grad_norm": 0.6006976366043091,
      "learning_rate": 5.359811405901707e-05,
      "loss": 2.4211,
      "step": 96830
    },
    {
      "epoch": 1.9922058018046744,
      "grad_norm": 0.42697280645370483,
      "learning_rate": 5.357844179279764e-05,
      "loss": 2.3725,
      "step": 96840
    },
    {
      "epoch": 1.992411521321542,
      "grad_norm": 0.41991183161735535,
      "learning_rate": 5.35587718163007e-05,
      "loss": 2.4194,
      "step": 96850
    },
    {
      "epoch": 1.9926172408384097,
      "grad_norm": 0.4171312153339386,
      "learning_rate": 5.353910413049648e-05,
      "loss": 2.4278,
      "step": 96860
    },
    {
      "epoch": 1.9928229603552776,
      "grad_norm": 0.47229161858558655,
      "learning_rate": 5.351943873635507e-05,
      "loss": 2.4017,
      "step": 96870
    },
    {
      "epoch": 1.9930286798721453,
      "grad_norm": 0.4383198320865631,
      "learning_rate": 5.349977563484645e-05,
      "loss": 2.3929,
      "step": 96880
    },
    {
      "epoch": 1.9932343993890131,
      "grad_norm": 0.482444167137146,
      "learning_rate": 5.348011482694051e-05,
      "loss": 2.3539,
      "step": 96890
    },
    {
      "epoch": 1.9934401189058808,
      "grad_norm": 0.4048670828342438,
      "learning_rate": 5.346045631360701e-05,
      "loss": 2.4204,
      "step": 96900
    },
    {
      "epoch": 1.9936458384227485,
      "grad_norm": 0.4302031099796295,
      "learning_rate": 5.344080009581561e-05,
      "loss": 2.4177,
      "step": 96910
    },
    {
      "epoch": 1.993851557939616,
      "grad_norm": 0.5590426325798035,
      "learning_rate": 5.3421146174535755e-05,
      "loss": 2.3628,
      "step": 96920
    },
    {
      "epoch": 1.9940572774564838,
      "grad_norm": 0.5241518020629883,
      "learning_rate": 5.340149455073698e-05,
      "loss": 2.401,
      "step": 96930
    },
    {
      "epoch": 1.9942629969733516,
      "grad_norm": 0.4623241126537323,
      "learning_rate": 5.3381845225388583e-05,
      "loss": 2.3631,
      "step": 96940
    },
    {
      "epoch": 1.9944687164902193,
      "grad_norm": 0.4371144771575928,
      "learning_rate": 5.336219819945965e-05,
      "loss": 2.3998,
      "step": 96950
    },
    {
      "epoch": 1.9946744360070872,
      "grad_norm": 0.5908966064453125,
      "learning_rate": 5.3342553473919364e-05,
      "loss": 2.3803,
      "step": 96960
    },
    {
      "epoch": 1.9948801555239548,
      "grad_norm": 0.4455196261405945,
      "learning_rate": 5.33229110497367e-05,
      "loss": 2.4231,
      "step": 96970
    },
    {
      "epoch": 1.9950858750408225,
      "grad_norm": 0.4889536201953888,
      "learning_rate": 5.33032709278804e-05,
      "loss": 2.3785,
      "step": 96980
    },
    {
      "epoch": 1.9952915945576901,
      "grad_norm": 0.43397584557533264,
      "learning_rate": 5.3283633109319356e-05,
      "loss": 2.3466,
      "step": 96990
    },
    {
      "epoch": 1.9954973140745578,
      "grad_norm": 0.44701841473579407,
      "learning_rate": 5.3263997595022074e-05,
      "loss": 2.4621,
      "step": 97000
    },
    {
      "epoch": 1.9957030335914256,
      "grad_norm": 0.45138123631477356,
      "learning_rate": 5.324436438595708e-05,
      "loss": 2.3795,
      "step": 97010
    },
    {
      "epoch": 1.9959087531082933,
      "grad_norm": 0.4577173888683319,
      "learning_rate": 5.322473348309287e-05,
      "loss": 2.4155,
      "step": 97020
    },
    {
      "epoch": 1.9961144726251612,
      "grad_norm": 0.4849832057952881,
      "learning_rate": 5.320510488739764e-05,
      "loss": 2.3621,
      "step": 97030
    },
    {
      "epoch": 1.9963201921420288,
      "grad_norm": 0.4293181002140045,
      "learning_rate": 5.3185478599839535e-05,
      "loss": 2.3574,
      "step": 97040
    },
    {
      "epoch": 1.9965259116588965,
      "grad_norm": 0.5187534093856812,
      "learning_rate": 5.3165854621386744e-05,
      "loss": 2.3591,
      "step": 97050
    },
    {
      "epoch": 1.9967316311757641,
      "grad_norm": 0.46480628848075867,
      "learning_rate": 5.314623295300708e-05,
      "loss": 2.4188,
      "step": 97060
    },
    {
      "epoch": 1.9969373506926318,
      "grad_norm": 0.44221484661102295,
      "learning_rate": 5.312661359566843e-05,
      "loss": 2.3984,
      "step": 97070
    },
    {
      "epoch": 1.9971430702094997,
      "grad_norm": 0.4363827705383301,
      "learning_rate": 5.31069965503385e-05,
      "loss": 2.3599,
      "step": 97080
    },
    {
      "epoch": 1.9973487897263673,
      "grad_norm": 0.4428638517856598,
      "learning_rate": 5.308738181798489e-05,
      "loss": 2.3899,
      "step": 97090
    },
    {
      "epoch": 1.9975545092432352,
      "grad_norm": 0.492196649312973,
      "learning_rate": 5.3067769399575096e-05,
      "loss": 2.4003,
      "step": 97100
    },
    {
      "epoch": 1.9977602287601028,
      "grad_norm": 0.4617409408092499,
      "learning_rate": 5.30481592960765e-05,
      "loss": 2.3797,
      "step": 97110
    },
    {
      "epoch": 1.9979659482769705,
      "grad_norm": 0.4282301366329193,
      "learning_rate": 5.3028551508456336e-05,
      "loss": 2.3606,
      "step": 97120
    },
    {
      "epoch": 1.9981716677938381,
      "grad_norm": 0.40384548902511597,
      "learning_rate": 5.3008946037681764e-05,
      "loss": 2.3463,
      "step": 97130
    },
    {
      "epoch": 1.9983773873107058,
      "grad_norm": 0.44568124413490295,
      "learning_rate": 5.2989342884719814e-05,
      "loss": 2.3552,
      "step": 97140
    },
    {
      "epoch": 1.9985831068275735,
      "grad_norm": 0.428893506526947,
      "learning_rate": 5.29697420505374e-05,
      "loss": 2.3857,
      "step": 97150
    },
    {
      "epoch": 1.9987888263444413,
      "grad_norm": 0.4816215932369232,
      "learning_rate": 5.2950143536101326e-05,
      "loss": 2.3362,
      "step": 97160
    },
    {
      "epoch": 1.9989945458613092,
      "grad_norm": 0.43628957867622375,
      "learning_rate": 5.293054734237828e-05,
      "loss": 2.3978,
      "step": 97170
    },
    {
      "epoch": 1.9992002653781769,
      "grad_norm": 0.45457401871681213,
      "learning_rate": 5.291095347033483e-05,
      "loss": 2.4017,
      "step": 97180
    },
    {
      "epoch": 1.9994059848950445,
      "grad_norm": 0.4543430805206299,
      "learning_rate": 5.2891361920937466e-05,
      "loss": 2.4031,
      "step": 97190
    },
    {
      "epoch": 1.9996117044119122,
      "grad_norm": 0.4964406192302704,
      "learning_rate": 5.287177269515252e-05,
      "loss": 2.3897,
      "step": 97200
    },
    {
      "epoch": 1.9998174239287798,
      "grad_norm": 0.42331165075302124,
      "learning_rate": 5.285218579394614e-05,
      "loss": 2.3787,
      "step": 97210
    },
    {
      "epoch": 2.0000205719516866,
      "grad_norm": 0.42579731345176697,
      "learning_rate": 5.283260121828455e-05,
      "loss": 2.4268,
      "step": 97220
    },
    {
      "epoch": 2.0002262914685547,
      "grad_norm": 0.44696101546287537,
      "learning_rate": 5.2813018969133756e-05,
      "loss": 2.3212,
      "step": 97230
    },
    {
      "epoch": 2.0004320109854223,
      "grad_norm": 0.4985596239566803,
      "learning_rate": 5.27934390474595e-05,
      "loss": 2.4048,
      "step": 97240
    },
    {
      "epoch": 2.00063773050229,
      "grad_norm": 0.4801066517829895,
      "learning_rate": 5.277386145422771e-05,
      "loss": 2.3183,
      "step": 97250
    },
    {
      "epoch": 2.0008434500191576,
      "grad_norm": 0.44672393798828125,
      "learning_rate": 5.2754286190404014e-05,
      "loss": 2.3592,
      "step": 97260
    },
    {
      "epoch": 2.0010491695360253,
      "grad_norm": 0.4434973895549774,
      "learning_rate": 5.273471325695384e-05,
      "loss": 2.4091,
      "step": 97270
    },
    {
      "epoch": 2.001254889052893,
      "grad_norm": 0.4351329803466797,
      "learning_rate": 5.2715142654842785e-05,
      "loss": 2.3276,
      "step": 97280
    },
    {
      "epoch": 2.0014606085697606,
      "grad_norm": 0.7624773383140564,
      "learning_rate": 5.269557438503602e-05,
      "loss": 2.3956,
      "step": 97290
    },
    {
      "epoch": 2.0016663280866287,
      "grad_norm": 0.4650985598564148,
      "learning_rate": 5.267600844849876e-05,
      "loss": 2.3817,
      "step": 97300
    },
    {
      "epoch": 2.0018720476034964,
      "grad_norm": 0.47971367835998535,
      "learning_rate": 5.2656444846196194e-05,
      "loss": 2.3944,
      "step": 97310
    },
    {
      "epoch": 2.002077767120364,
      "grad_norm": 0.41693711280822754,
      "learning_rate": 5.2636883579093174e-05,
      "loss": 2.3495,
      "step": 97320
    },
    {
      "epoch": 2.0022834866372317,
      "grad_norm": 0.468826025724411,
      "learning_rate": 5.2617324648154545e-05,
      "loss": 2.3676,
      "step": 97330
    },
    {
      "epoch": 2.0024892061540993,
      "grad_norm": 0.399565726518631,
      "learning_rate": 5.259776805434518e-05,
      "loss": 2.3954,
      "step": 97340
    },
    {
      "epoch": 2.002694925670967,
      "grad_norm": 0.43956366181373596,
      "learning_rate": 5.257821379862955e-05,
      "loss": 2.3693,
      "step": 97350
    },
    {
      "epoch": 2.0029006451878346,
      "grad_norm": 0.43180525302886963,
      "learning_rate": 5.2558661881972225e-05,
      "loss": 2.3805,
      "step": 97360
    },
    {
      "epoch": 2.0031063647047027,
      "grad_norm": 0.40557199716567993,
      "learning_rate": 5.2539112305337576e-05,
      "loss": 2.3347,
      "step": 97370
    },
    {
      "epoch": 2.0033120842215704,
      "grad_norm": 0.49148932099342346,
      "learning_rate": 5.251956506968989e-05,
      "loss": 2.4262,
      "step": 97380
    },
    {
      "epoch": 2.003517803738438,
      "grad_norm": 0.42819637060165405,
      "learning_rate": 5.250002017599334e-05,
      "loss": 2.3544,
      "step": 97390
    },
    {
      "epoch": 2.0037235232553057,
      "grad_norm": 0.4564547836780548,
      "learning_rate": 5.248047762521193e-05,
      "loss": 2.3692,
      "step": 97400
    },
    {
      "epoch": 2.0039292427721733,
      "grad_norm": 0.44288861751556396,
      "learning_rate": 5.2460937418309605e-05,
      "loss": 2.3616,
      "step": 97410
    },
    {
      "epoch": 2.004134962289041,
      "grad_norm": 0.42322060465812683,
      "learning_rate": 5.2441399556250195e-05,
      "loss": 2.3805,
      "step": 97420
    },
    {
      "epoch": 2.0043406818059086,
      "grad_norm": 0.43677061796188354,
      "learning_rate": 5.242186403999738e-05,
      "loss": 2.3422,
      "step": 97430
    },
    {
      "epoch": 2.0045464013227763,
      "grad_norm": 0.4748198091983795,
      "learning_rate": 5.240233087051474e-05,
      "loss": 2.3003,
      "step": 97440
    },
    {
      "epoch": 2.0047521208396444,
      "grad_norm": 0.4358004629611969,
      "learning_rate": 5.2382800048765736e-05,
      "loss": 2.384,
      "step": 97450
    },
    {
      "epoch": 2.004957840356512,
      "grad_norm": 0.4449768364429474,
      "learning_rate": 5.2363271575713727e-05,
      "loss": 2.3439,
      "step": 97460
    },
    {
      "epoch": 2.0051635598733797,
      "grad_norm": 0.47973883152008057,
      "learning_rate": 5.234374545232194e-05,
      "loss": 2.3899,
      "step": 97470
    },
    {
      "epoch": 2.0053692793902473,
      "grad_norm": 0.4290030598640442,
      "learning_rate": 5.232422167955349e-05,
      "loss": 2.4192,
      "step": 97480
    },
    {
      "epoch": 2.005574998907115,
      "grad_norm": 0.4579137861728668,
      "learning_rate": 5.2304700258371396e-05,
      "loss": 2.3834,
      "step": 97490
    },
    {
      "epoch": 2.0057807184239826,
      "grad_norm": 0.43647363781929016,
      "learning_rate": 5.228518118973852e-05,
      "loss": 2.3257,
      "step": 97500
    },
    {
      "epoch": 2.0059864379408503,
      "grad_norm": 0.4363526403903961,
      "learning_rate": 5.226566447461765e-05,
      "loss": 2.3779,
      "step": 97510
    },
    {
      "epoch": 2.0061921574577184,
      "grad_norm": 0.45595160126686096,
      "learning_rate": 5.224615011397146e-05,
      "loss": 2.3697,
      "step": 97520
    },
    {
      "epoch": 2.006397876974586,
      "grad_norm": 0.4198445677757263,
      "learning_rate": 5.222663810876238e-05,
      "loss": 2.3063,
      "step": 97530
    },
    {
      "epoch": 2.0066035964914537,
      "grad_norm": 0.4187558591365814,
      "learning_rate": 5.2207128459952945e-05,
      "loss": 2.3725,
      "step": 97540
    },
    {
      "epoch": 2.0068093160083214,
      "grad_norm": 0.45756351947784424,
      "learning_rate": 5.2187621168505474e-05,
      "loss": 2.4187,
      "step": 97550
    },
    {
      "epoch": 2.007015035525189,
      "grad_norm": 0.5024564266204834,
      "learning_rate": 5.216811623538202e-05,
      "loss": 2.423,
      "step": 97560
    },
    {
      "epoch": 2.0072207550420567,
      "grad_norm": 0.45336902141571045,
      "learning_rate": 5.214861366154477e-05,
      "loss": 2.3241,
      "step": 97570
    },
    {
      "epoch": 2.0074264745589243,
      "grad_norm": 0.4637250304222107,
      "learning_rate": 5.21291134479557e-05,
      "loss": 2.3298,
      "step": 97580
    },
    {
      "epoch": 2.0076321940757924,
      "grad_norm": 0.4520038962364197,
      "learning_rate": 5.210961559557652e-05,
      "loss": 2.3649,
      "step": 97590
    },
    {
      "epoch": 2.00783791359266,
      "grad_norm": 0.4636962115764618,
      "learning_rate": 5.209012010536911e-05,
      "loss": 2.3295,
      "step": 97600
    },
    {
      "epoch": 2.0080436331095277,
      "grad_norm": 0.4646149277687073,
      "learning_rate": 5.207062697829495e-05,
      "loss": 2.4059,
      "step": 97610
    },
    {
      "epoch": 2.0082493526263954,
      "grad_norm": 0.4371204376220703,
      "learning_rate": 5.205113621531556e-05,
      "loss": 2.3629,
      "step": 97620
    },
    {
      "epoch": 2.008455072143263,
      "grad_norm": 0.47356489300727844,
      "learning_rate": 5.2031647817392385e-05,
      "loss": 2.3944,
      "step": 97630
    },
    {
      "epoch": 2.0086607916601307,
      "grad_norm": 0.4186791479587555,
      "learning_rate": 5.20121617854866e-05,
      "loss": 2.3475,
      "step": 97640
    },
    {
      "epoch": 2.0088665111769983,
      "grad_norm": 0.4172382950782776,
      "learning_rate": 5.1992678120559325e-05,
      "loss": 2.305,
      "step": 97650
    },
    {
      "epoch": 2.0090722306938664,
      "grad_norm": 0.42093631625175476,
      "learning_rate": 5.1973196823571715e-05,
      "loss": 2.3281,
      "step": 97660
    },
    {
      "epoch": 2.009277950210734,
      "grad_norm": 0.43056389689445496,
      "learning_rate": 5.195371789548454e-05,
      "loss": 2.373,
      "step": 97670
    },
    {
      "epoch": 2.0094836697276017,
      "grad_norm": 0.45839324593544006,
      "learning_rate": 5.193424133725866e-05,
      "loss": 2.3646,
      "step": 97680
    },
    {
      "epoch": 2.0096893892444694,
      "grad_norm": 0.5037609338760376,
      "learning_rate": 5.191476714985471e-05,
      "loss": 2.4127,
      "step": 97690
    },
    {
      "epoch": 2.009895108761337,
      "grad_norm": 0.4356291592121124,
      "learning_rate": 5.1895295334233276e-05,
      "loss": 2.3867,
      "step": 97700
    },
    {
      "epoch": 2.0101008282782047,
      "grad_norm": 0.4449324607849121,
      "learning_rate": 5.187582589135478e-05,
      "loss": 2.4021,
      "step": 97710
    },
    {
      "epoch": 2.0103065477950723,
      "grad_norm": 0.47637611627578735,
      "learning_rate": 5.185635882217954e-05,
      "loss": 2.4107,
      "step": 97720
    },
    {
      "epoch": 2.0105122673119404,
      "grad_norm": 0.45472243428230286,
      "learning_rate": 5.183689412766777e-05,
      "loss": 2.3576,
      "step": 97730
    },
    {
      "epoch": 2.010717986828808,
      "grad_norm": 0.44347232580184937,
      "learning_rate": 5.181743180877956e-05,
      "loss": 2.3366,
      "step": 97740
    },
    {
      "epoch": 2.0109237063456757,
      "grad_norm": 0.454085111618042,
      "learning_rate": 5.179797186647488e-05,
      "loss": 2.4426,
      "step": 97750
    },
    {
      "epoch": 2.0111294258625434,
      "grad_norm": 0.4826352596282959,
      "learning_rate": 5.177851430171357e-05,
      "loss": 2.3476,
      "step": 97760
    },
    {
      "epoch": 2.011335145379411,
      "grad_norm": 0.4222069978713989,
      "learning_rate": 5.175905911545538e-05,
      "loss": 2.3017,
      "step": 97770
    },
    {
      "epoch": 2.0115408648962787,
      "grad_norm": 0.49212226271629333,
      "learning_rate": 5.17396063086599e-05,
      "loss": 2.2811,
      "step": 97780
    },
    {
      "epoch": 2.0117465844131464,
      "grad_norm": 0.4808942675590515,
      "learning_rate": 5.172015588228669e-05,
      "loss": 2.433,
      "step": 97790
    },
    {
      "epoch": 2.011952303930014,
      "grad_norm": 0.4364011585712433,
      "learning_rate": 5.170070783729507e-05,
      "loss": 2.3015,
      "step": 97800
    },
    {
      "epoch": 2.012158023446882,
      "grad_norm": 0.45468437671661377,
      "learning_rate": 5.168126217464435e-05,
      "loss": 2.36,
      "step": 97810
    },
    {
      "epoch": 2.0123637429637498,
      "grad_norm": 0.4346503019332886,
      "learning_rate": 5.166181889529365e-05,
      "loss": 2.3983,
      "step": 97820
    },
    {
      "epoch": 2.0125694624806174,
      "grad_norm": 0.4241114556789398,
      "learning_rate": 5.164237800020202e-05,
      "loss": 2.3801,
      "step": 97830
    },
    {
      "epoch": 2.012775181997485,
      "grad_norm": 0.43052443861961365,
      "learning_rate": 5.162293949032839e-05,
      "loss": 2.3539,
      "step": 97840
    },
    {
      "epoch": 2.0129809015143527,
      "grad_norm": 0.4112275540828705,
      "learning_rate": 5.1603503366631465e-05,
      "loss": 2.3477,
      "step": 97850
    },
    {
      "epoch": 2.0131866210312204,
      "grad_norm": 0.4423513412475586,
      "learning_rate": 5.158406963007003e-05,
      "loss": 2.3611,
      "step": 97860
    },
    {
      "epoch": 2.013392340548088,
      "grad_norm": 0.4482271671295166,
      "learning_rate": 5.156463828160264e-05,
      "loss": 2.4161,
      "step": 97870
    },
    {
      "epoch": 2.013598060064956,
      "grad_norm": 0.42245182394981384,
      "learning_rate": 5.154520932218762e-05,
      "loss": 2.3614,
      "step": 97880
    },
    {
      "epoch": 2.0138037795818238,
      "grad_norm": 0.4098612070083618,
      "learning_rate": 5.1525782752783427e-05,
      "loss": 2.3432,
      "step": 97890
    },
    {
      "epoch": 2.0140094990986914,
      "grad_norm": 0.43634819984436035,
      "learning_rate": 5.1506358574348244e-05,
      "loss": 2.3685,
      "step": 97900
    },
    {
      "epoch": 2.014215218615559,
      "grad_norm": 0.40594005584716797,
      "learning_rate": 5.148693678784007e-05,
      "loss": 2.3571,
      "step": 97910
    },
    {
      "epoch": 2.0144209381324267,
      "grad_norm": 0.47858938574790955,
      "learning_rate": 5.1467517394217016e-05,
      "loss": 2.3374,
      "step": 97920
    },
    {
      "epoch": 2.0146266576492944,
      "grad_norm": 0.4568404257297516,
      "learning_rate": 5.1448100394436816e-05,
      "loss": 2.3102,
      "step": 97930
    },
    {
      "epoch": 2.014832377166162,
      "grad_norm": 0.46205928921699524,
      "learning_rate": 5.142868578945721e-05,
      "loss": 2.3569,
      "step": 97940
    },
    {
      "epoch": 2.01503809668303,
      "grad_norm": 0.46074023842811584,
      "learning_rate": 5.1409273580235914e-05,
      "loss": 2.3826,
      "step": 97950
    },
    {
      "epoch": 2.015243816199898,
      "grad_norm": 0.4134630858898163,
      "learning_rate": 5.138986376773034e-05,
      "loss": 2.3952,
      "step": 97960
    },
    {
      "epoch": 2.0154495357167654,
      "grad_norm": 0.4497564435005188,
      "learning_rate": 5.137045635289786e-05,
      "loss": 2.3421,
      "step": 97970
    },
    {
      "epoch": 2.015655255233633,
      "grad_norm": 0.4673091173171997,
      "learning_rate": 5.1351051336695844e-05,
      "loss": 2.3317,
      "step": 97980
    },
    {
      "epoch": 2.0158609747505007,
      "grad_norm": 0.46047303080558777,
      "learning_rate": 5.1331648720081316e-05,
      "loss": 2.2805,
      "step": 97990
    },
    {
      "epoch": 2.0160666942673684,
      "grad_norm": 0.4142971932888031,
      "learning_rate": 5.131224850401134e-05,
      "loss": 2.3725,
      "step": 98000
    },
    {
      "epoch": 2.016272413784236,
      "grad_norm": 0.5080583095550537,
      "learning_rate": 5.129285068944284e-05,
      "loss": 2.3975,
      "step": 98010
    },
    {
      "epoch": 2.016478133301104,
      "grad_norm": 0.49727070331573486,
      "learning_rate": 5.127345527733258e-05,
      "loss": 2.3635,
      "step": 98020
    },
    {
      "epoch": 2.016683852817972,
      "grad_norm": 0.4547169804573059,
      "learning_rate": 5.125406226863726e-05,
      "loss": 2.3226,
      "step": 98030
    },
    {
      "epoch": 2.0168895723348395,
      "grad_norm": 0.4464908838272095,
      "learning_rate": 5.1234671664313405e-05,
      "loss": 2.338,
      "step": 98040
    },
    {
      "epoch": 2.017095291851707,
      "grad_norm": 0.46416598558425903,
      "learning_rate": 5.1215283465317466e-05,
      "loss": 2.3729,
      "step": 98050
    },
    {
      "epoch": 2.0173010113685748,
      "grad_norm": 0.51157146692276,
      "learning_rate": 5.1195897672605743e-05,
      "loss": 2.4056,
      "step": 98060
    },
    {
      "epoch": 2.0175067308854424,
      "grad_norm": 0.47828903794288635,
      "learning_rate": 5.117651428713444e-05,
      "loss": 2.3448,
      "step": 98070
    },
    {
      "epoch": 2.01771245040231,
      "grad_norm": 0.4451988935470581,
      "learning_rate": 5.115713330985963e-05,
      "loss": 2.3286,
      "step": 98080
    },
    {
      "epoch": 2.017918169919178,
      "grad_norm": 0.4286468029022217,
      "learning_rate": 5.113775474173726e-05,
      "loss": 2.416,
      "step": 98090
    },
    {
      "epoch": 2.018123889436046,
      "grad_norm": 0.4633089601993561,
      "learning_rate": 5.111837858372319e-05,
      "loss": 2.3696,
      "step": 98100
    },
    {
      "epoch": 2.0183296089529135,
      "grad_norm": 0.4103390872478485,
      "learning_rate": 5.1099004836773146e-05,
      "loss": 2.3965,
      "step": 98110
    },
    {
      "epoch": 2.018535328469781,
      "grad_norm": 0.42279982566833496,
      "learning_rate": 5.1079633501842696e-05,
      "loss": 2.352,
      "step": 98120
    },
    {
      "epoch": 2.0187410479866488,
      "grad_norm": 0.44089996814727783,
      "learning_rate": 5.106026457988735e-05,
      "loss": 2.3954,
      "step": 98130
    },
    {
      "epoch": 2.0189467675035164,
      "grad_norm": 0.4429146945476532,
      "learning_rate": 5.1040898071862454e-05,
      "loss": 2.3463,
      "step": 98140
    },
    {
      "epoch": 2.019152487020384,
      "grad_norm": 0.4852723777294159,
      "learning_rate": 5.102153397872327e-05,
      "loss": 2.3792,
      "step": 98150
    },
    {
      "epoch": 2.0193582065372517,
      "grad_norm": 0.5246065855026245,
      "learning_rate": 5.1002172301424956e-05,
      "loss": 2.3915,
      "step": 98160
    },
    {
      "epoch": 2.01956392605412,
      "grad_norm": 0.4439992308616638,
      "learning_rate": 5.098281304092238e-05,
      "loss": 2.3589,
      "step": 98170
    },
    {
      "epoch": 2.0197696455709875,
      "grad_norm": 0.4807276725769043,
      "learning_rate": 5.096345619817059e-05,
      "loss": 2.4081,
      "step": 98180
    },
    {
      "epoch": 2.019975365087855,
      "grad_norm": 0.42166367173194885,
      "learning_rate": 5.094410177412432e-05,
      "loss": 2.3723,
      "step": 98190
    },
    {
      "epoch": 2.020181084604723,
      "grad_norm": 0.4276028275489807,
      "learning_rate": 5.092474976973809e-05,
      "loss": 2.3839,
      "step": 98200
    },
    {
      "epoch": 2.0203868041215904,
      "grad_norm": 0.498276025056839,
      "learning_rate": 5.090540018596659e-05,
      "loss": 2.3963,
      "step": 98210
    },
    {
      "epoch": 2.020592523638458,
      "grad_norm": 0.42812252044677734,
      "learning_rate": 5.088605302376419e-05,
      "loss": 2.3405,
      "step": 98220
    },
    {
      "epoch": 2.0207982431553257,
      "grad_norm": 0.44764432311058044,
      "learning_rate": 5.0866708284085087e-05,
      "loss": 2.3377,
      "step": 98230
    },
    {
      "epoch": 2.021003962672194,
      "grad_norm": 0.5231209397315979,
      "learning_rate": 5.084736596788358e-05,
      "loss": 2.3279,
      "step": 98240
    },
    {
      "epoch": 2.0212096821890615,
      "grad_norm": 0.42488858103752136,
      "learning_rate": 5.0828026076113635e-05,
      "loss": 2.3904,
      "step": 98250
    },
    {
      "epoch": 2.021415401705929,
      "grad_norm": 0.5167943835258484,
      "learning_rate": 5.080868860972916e-05,
      "loss": 2.3374,
      "step": 98260
    },
    {
      "epoch": 2.021621121222797,
      "grad_norm": 0.4190918207168579,
      "learning_rate": 5.078935356968411e-05,
      "loss": 2.3981,
      "step": 98270
    },
    {
      "epoch": 2.0218268407396645,
      "grad_norm": 0.45640984177589417,
      "learning_rate": 5.077002095693203e-05,
      "loss": 2.3834,
      "step": 98280
    },
    {
      "epoch": 2.022032560256532,
      "grad_norm": 0.4688188433647156,
      "learning_rate": 5.075069077242651e-05,
      "loss": 2.4368,
      "step": 98290
    },
    {
      "epoch": 2.0222382797733998,
      "grad_norm": 0.45296382904052734,
      "learning_rate": 5.0731363017121117e-05,
      "loss": 2.3469,
      "step": 98300
    },
    {
      "epoch": 2.022443999290268,
      "grad_norm": 0.4964909553527832,
      "learning_rate": 5.071203769196906e-05,
      "loss": 2.4148,
      "step": 98310
    },
    {
      "epoch": 2.0226497188071355,
      "grad_norm": 0.49999120831489563,
      "learning_rate": 5.0692714797923605e-05,
      "loss": 2.4194,
      "step": 98320
    },
    {
      "epoch": 2.022855438324003,
      "grad_norm": 0.46829351782798767,
      "learning_rate": 5.067339433593783e-05,
      "loss": 2.3907,
      "step": 98330
    },
    {
      "epoch": 2.023061157840871,
      "grad_norm": 0.43694499135017395,
      "learning_rate": 5.0654076306964714e-05,
      "loss": 2.3429,
      "step": 98340
    },
    {
      "epoch": 2.0232668773577385,
      "grad_norm": 0.43118351697921753,
      "learning_rate": 5.063476071195712e-05,
      "loss": 2.3344,
      "step": 98350
    },
    {
      "epoch": 2.023472596874606,
      "grad_norm": 0.47542133927345276,
      "learning_rate": 5.0615447551867754e-05,
      "loss": 2.3859,
      "step": 98360
    },
    {
      "epoch": 2.0236783163914738,
      "grad_norm": 0.46859171986579895,
      "learning_rate": 5.0596136827649256e-05,
      "loss": 2.3492,
      "step": 98370
    },
    {
      "epoch": 2.023884035908342,
      "grad_norm": 0.4397693872451782,
      "learning_rate": 5.057682854025412e-05,
      "loss": 2.4169,
      "step": 98380
    },
    {
      "epoch": 2.0240897554252095,
      "grad_norm": 0.5126308798789978,
      "learning_rate": 5.0557522690634695e-05,
      "loss": 2.3634,
      "step": 98390
    },
    {
      "epoch": 2.024295474942077,
      "grad_norm": 0.4209919273853302,
      "learning_rate": 5.053821927974325e-05,
      "loss": 2.3909,
      "step": 98400
    },
    {
      "epoch": 2.024501194458945,
      "grad_norm": 0.49658507108688354,
      "learning_rate": 5.051891830853192e-05,
      "loss": 2.3493,
      "step": 98410
    },
    {
      "epoch": 2.0247069139758125,
      "grad_norm": 0.43722596764564514,
      "learning_rate": 5.04996197779527e-05,
      "loss": 2.4126,
      "step": 98420
    },
    {
      "epoch": 2.02491263349268,
      "grad_norm": 0.42255887389183044,
      "learning_rate": 5.04803236889575e-05,
      "loss": 2.2962,
      "step": 98430
    },
    {
      "epoch": 2.025118353009548,
      "grad_norm": 0.4544583559036255,
      "learning_rate": 5.046103004249807e-05,
      "loss": 2.3749,
      "step": 98440
    },
    {
      "epoch": 2.025324072526416,
      "grad_norm": 0.466582328081131,
      "learning_rate": 5.044173883952612e-05,
      "loss": 2.3849,
      "step": 98450
    },
    {
      "epoch": 2.0255297920432835,
      "grad_norm": 0.4235455095767975,
      "learning_rate": 5.0422450080993064e-05,
      "loss": 2.3682,
      "step": 98460
    },
    {
      "epoch": 2.025735511560151,
      "grad_norm": 0.45901575684547424,
      "learning_rate": 5.0403163767850414e-05,
      "loss": 2.397,
      "step": 98470
    },
    {
      "epoch": 2.025941231077019,
      "grad_norm": 0.4652213752269745,
      "learning_rate": 5.038387990104946e-05,
      "loss": 2.4182,
      "step": 98480
    },
    {
      "epoch": 2.0261469505938865,
      "grad_norm": 0.41566094756126404,
      "learning_rate": 5.0364598481541256e-05,
      "loss": 2.4125,
      "step": 98490
    },
    {
      "epoch": 2.026352670110754,
      "grad_norm": 0.4112749695777893,
      "learning_rate": 5.034531951027697e-05,
      "loss": 2.354,
      "step": 98500
    },
    {
      "epoch": 2.026558389627622,
      "grad_norm": 0.476509690284729,
      "learning_rate": 5.032604298820752e-05,
      "loss": 2.338,
      "step": 98510
    },
    {
      "epoch": 2.0267641091444895,
      "grad_norm": 0.4275241196155548,
      "learning_rate": 5.0306768916283606e-05,
      "loss": 2.3453,
      "step": 98520
    },
    {
      "epoch": 2.0269698286613576,
      "grad_norm": 0.43772658705711365,
      "learning_rate": 5.028749729545608e-05,
      "loss": 2.3212,
      "step": 98530
    },
    {
      "epoch": 2.027175548178225,
      "grad_norm": 0.44186803698539734,
      "learning_rate": 5.0268228126675354e-05,
      "loss": 2.3369,
      "step": 98540
    },
    {
      "epoch": 2.027381267695093,
      "grad_norm": 0.4163166582584381,
      "learning_rate": 5.024896141089188e-05,
      "loss": 2.3348,
      "step": 98550
    },
    {
      "epoch": 2.0275869872119605,
      "grad_norm": 0.4164360761642456,
      "learning_rate": 5.022969714905612e-05,
      "loss": 2.3298,
      "step": 98560
    },
    {
      "epoch": 2.027792706728828,
      "grad_norm": 0.45739686489105225,
      "learning_rate": 5.021043534211816e-05,
      "loss": 2.3296,
      "step": 98570
    },
    {
      "epoch": 2.027998426245696,
      "grad_norm": 0.5238650441169739,
      "learning_rate": 5.019117599102804e-05,
      "loss": 2.3667,
      "step": 98580
    },
    {
      "epoch": 2.0282041457625635,
      "grad_norm": 0.42169174551963806,
      "learning_rate": 5.0171919096735864e-05,
      "loss": 2.4072,
      "step": 98590
    },
    {
      "epoch": 2.0284098652794316,
      "grad_norm": 0.4180797040462494,
      "learning_rate": 5.0152664660191364e-05,
      "loss": 2.3296,
      "step": 98600
    },
    {
      "epoch": 2.028615584796299,
      "grad_norm": 0.47452977299690247,
      "learning_rate": 5.013341268234427e-05,
      "loss": 2.3874,
      "step": 98610
    },
    {
      "epoch": 2.028821304313167,
      "grad_norm": 0.44672125577926636,
      "learning_rate": 5.011416316414419e-05,
      "loss": 2.3925,
      "step": 98620
    },
    {
      "epoch": 2.0290270238300345,
      "grad_norm": 0.4547407627105713,
      "learning_rate": 5.00949161065406e-05,
      "loss": 2.3492,
      "step": 98630
    },
    {
      "epoch": 2.029232743346902,
      "grad_norm": 0.4474589228630066,
      "learning_rate": 5.0075671510482844e-05,
      "loss": 2.3856,
      "step": 98640
    },
    {
      "epoch": 2.02943846286377,
      "grad_norm": 0.48502224683761597,
      "learning_rate": 5.005642937692016e-05,
      "loss": 2.3978,
      "step": 98650
    },
    {
      "epoch": 2.0296441823806375,
      "grad_norm": 0.4532147943973541,
      "learning_rate": 5.003718970680165e-05,
      "loss": 2.3823,
      "step": 98660
    },
    {
      "epoch": 2.0298499018975056,
      "grad_norm": 0.40593618154525757,
      "learning_rate": 5.00179525010763e-05,
      "loss": 2.3434,
      "step": 98670
    },
    {
      "epoch": 2.0300556214143732,
      "grad_norm": 0.44688519835472107,
      "learning_rate": 4.9998717760693e-05,
      "loss": 2.4063,
      "step": 98680
    },
    {
      "epoch": 2.030261340931241,
      "grad_norm": 0.49706101417541504,
      "learning_rate": 4.9979485486600476e-05,
      "loss": 2.3306,
      "step": 98690
    },
    {
      "epoch": 2.0304670604481085,
      "grad_norm": 0.42907217144966125,
      "learning_rate": 4.9960255679747356e-05,
      "loss": 2.3831,
      "step": 98700
    },
    {
      "epoch": 2.030672779964976,
      "grad_norm": 0.4312041699886322,
      "learning_rate": 4.994102834108213e-05,
      "loss": 2.3383,
      "step": 98710
    },
    {
      "epoch": 2.030878499481844,
      "grad_norm": 0.43668991327285767,
      "learning_rate": 4.99218034715532e-05,
      "loss": 2.3664,
      "step": 98720
    },
    {
      "epoch": 2.0310842189987115,
      "grad_norm": 0.4504932165145874,
      "learning_rate": 4.99025810721088e-05,
      "loss": 2.3484,
      "step": 98730
    },
    {
      "epoch": 2.0312899385155796,
      "grad_norm": 0.44967740774154663,
      "learning_rate": 4.988336114369707e-05,
      "loss": 2.3534,
      "step": 98740
    },
    {
      "epoch": 2.0314956580324472,
      "grad_norm": 0.4599183201789856,
      "learning_rate": 4.9864143687266054e-05,
      "loss": 2.3365,
      "step": 98750
    },
    {
      "epoch": 2.031701377549315,
      "grad_norm": 0.45117199420928955,
      "learning_rate": 4.9844928703763603e-05,
      "loss": 2.3455,
      "step": 98760
    },
    {
      "epoch": 2.0319070970661826,
      "grad_norm": 0.44543543457984924,
      "learning_rate": 4.982571619413755e-05,
      "loss": 2.3149,
      "step": 98770
    },
    {
      "epoch": 2.03211281658305,
      "grad_norm": 0.4377196729183197,
      "learning_rate": 4.980650615933543e-05,
      "loss": 2.4323,
      "step": 98780
    },
    {
      "epoch": 2.032318536099918,
      "grad_norm": 0.4772215187549591,
      "learning_rate": 4.978729860030488e-05,
      "loss": 2.3215,
      "step": 98790
    },
    {
      "epoch": 2.0325242556167855,
      "grad_norm": 0.49496009945869446,
      "learning_rate": 4.9768093517993285e-05,
      "loss": 2.3643,
      "step": 98800
    },
    {
      "epoch": 2.0327299751336536,
      "grad_norm": 0.44907909631729126,
      "learning_rate": 4.974889091334785e-05,
      "loss": 2.3789,
      "step": 98810
    },
    {
      "epoch": 2.0329356946505213,
      "grad_norm": 0.5006933212280273,
      "learning_rate": 4.972969078731581e-05,
      "loss": 2.3518,
      "step": 98820
    },
    {
      "epoch": 2.033141414167389,
      "grad_norm": 0.46247243881225586,
      "learning_rate": 4.971049314084423e-05,
      "loss": 2.3201,
      "step": 98830
    },
    {
      "epoch": 2.0333471336842566,
      "grad_norm": 0.4690895080566406,
      "learning_rate": 4.96912979748799e-05,
      "loss": 2.4,
      "step": 98840
    },
    {
      "epoch": 2.033552853201124,
      "grad_norm": 0.4572330713272095,
      "learning_rate": 4.9672105290369774e-05,
      "loss": 2.4019,
      "step": 98850
    },
    {
      "epoch": 2.033758572717992,
      "grad_norm": 0.530848503112793,
      "learning_rate": 4.96529150882604e-05,
      "loss": 2.3252,
      "step": 98860
    },
    {
      "epoch": 2.0339642922348595,
      "grad_norm": 0.4535403847694397,
      "learning_rate": 4.9633727369498316e-05,
      "loss": 2.3787,
      "step": 98870
    },
    {
      "epoch": 2.034170011751727,
      "grad_norm": 0.4380919337272644,
      "learning_rate": 4.961454213503008e-05,
      "loss": 2.376,
      "step": 98880
    },
    {
      "epoch": 2.0343757312685953,
      "grad_norm": 0.4344138503074646,
      "learning_rate": 4.959535938580189e-05,
      "loss": 2.3909,
      "step": 98890
    },
    {
      "epoch": 2.034581450785463,
      "grad_norm": 0.43216755986213684,
      "learning_rate": 4.957617912275989e-05,
      "loss": 2.3697,
      "step": 98900
    },
    {
      "epoch": 2.0347871703023306,
      "grad_norm": 0.4417170286178589,
      "learning_rate": 4.9557001346850284e-05,
      "loss": 2.3202,
      "step": 98910
    },
    {
      "epoch": 2.0349928898191982,
      "grad_norm": 0.486188679933548,
      "learning_rate": 4.9537826059018874e-05,
      "loss": 2.3895,
      "step": 98920
    },
    {
      "epoch": 2.035198609336066,
      "grad_norm": 0.44623100757598877,
      "learning_rate": 4.951865326021153e-05,
      "loss": 2.3751,
      "step": 98930
    },
    {
      "epoch": 2.0354043288529335,
      "grad_norm": 0.41737788915634155,
      "learning_rate": 4.949948295137392e-05,
      "loss": 2.3896,
      "step": 98940
    },
    {
      "epoch": 2.035610048369801,
      "grad_norm": 0.46260732412338257,
      "learning_rate": 4.948031513345163e-05,
      "loss": 2.3524,
      "step": 98950
    },
    {
      "epoch": 2.0358157678866693,
      "grad_norm": 0.44233438372612,
      "learning_rate": 4.946114980739009e-05,
      "loss": 2.3592,
      "step": 98960
    },
    {
      "epoch": 2.036021487403537,
      "grad_norm": 0.47086286544799805,
      "learning_rate": 4.9441986974134626e-05,
      "loss": 2.3715,
      "step": 98970
    },
    {
      "epoch": 2.0362272069204046,
      "grad_norm": 0.4333277642726898,
      "learning_rate": 4.942282663463044e-05,
      "loss": 2.3656,
      "step": 98980
    },
    {
      "epoch": 2.0364329264372723,
      "grad_norm": 0.42921024560928345,
      "learning_rate": 4.94036687898226e-05,
      "loss": 2.3934,
      "step": 98990
    },
    {
      "epoch": 2.03663864595414,
      "grad_norm": 0.417276531457901,
      "learning_rate": 4.938451344065607e-05,
      "loss": 2.3751,
      "step": 99000
    },
    {
      "epoch": 2.0368443654710076,
      "grad_norm": 0.46446892619132996,
      "learning_rate": 4.936536058807566e-05,
      "loss": 2.3911,
      "step": 99010
    },
    {
      "epoch": 2.037050084987875,
      "grad_norm": 0.5019649267196655,
      "learning_rate": 4.934621023302609e-05,
      "loss": 2.3534,
      "step": 99020
    },
    {
      "epoch": 2.0372558045047433,
      "grad_norm": 0.5324623584747314,
      "learning_rate": 4.9327062376451935e-05,
      "loss": 2.3524,
      "step": 99030
    },
    {
      "epoch": 2.037461524021611,
      "grad_norm": 0.48205670714378357,
      "learning_rate": 4.930791701929767e-05,
      "loss": 2.3565,
      "step": 99040
    },
    {
      "epoch": 2.0376672435384786,
      "grad_norm": 0.4419535994529724,
      "learning_rate": 4.928877416250761e-05,
      "loss": 2.3556,
      "step": 99050
    },
    {
      "epoch": 2.0378729630553463,
      "grad_norm": 0.4770897924900055,
      "learning_rate": 4.9269633807025986e-05,
      "loss": 2.3531,
      "step": 99060
    },
    {
      "epoch": 2.038078682572214,
      "grad_norm": 0.4819726049900055,
      "learning_rate": 4.925049595379687e-05,
      "loss": 2.404,
      "step": 99070
    },
    {
      "epoch": 2.0382844020890816,
      "grad_norm": 0.4813310205936432,
      "learning_rate": 4.923136060376423e-05,
      "loss": 2.3294,
      "step": 99080
    },
    {
      "epoch": 2.0384901216059492,
      "grad_norm": 0.4597179889678955,
      "learning_rate": 4.921222775787198e-05,
      "loss": 2.3972,
      "step": 99090
    },
    {
      "epoch": 2.0386958411228173,
      "grad_norm": 0.4485006630420685,
      "learning_rate": 4.919309741706367e-05,
      "loss": 2.4006,
      "step": 99100
    },
    {
      "epoch": 2.038901560639685,
      "grad_norm": 0.43271973729133606,
      "learning_rate": 4.917396958228305e-05,
      "loss": 2.3294,
      "step": 99110
    },
    {
      "epoch": 2.0391072801565526,
      "grad_norm": 0.4528166949748993,
      "learning_rate": 4.9154844254473586e-05,
      "loss": 2.3214,
      "step": 99120
    },
    {
      "epoch": 2.0393129996734203,
      "grad_norm": 0.4309658706188202,
      "learning_rate": 4.9135721434578496e-05,
      "loss": 2.3144,
      "step": 99130
    },
    {
      "epoch": 2.039518719190288,
      "grad_norm": 0.4486859440803528,
      "learning_rate": 4.911660112354113e-05,
      "loss": 2.3589,
      "step": 99140
    },
    {
      "epoch": 2.0397244387071556,
      "grad_norm": 0.4564310312271118,
      "learning_rate": 4.90974833223046e-05,
      "loss": 2.4316,
      "step": 99150
    },
    {
      "epoch": 2.0399301582240232,
      "grad_norm": 0.498216837644577,
      "learning_rate": 4.907836803181174e-05,
      "loss": 2.3745,
      "step": 99160
    },
    {
      "epoch": 2.0401358777408913,
      "grad_norm": 0.44711679220199585,
      "learning_rate": 4.905925525300557e-05,
      "loss": 2.3709,
      "step": 99170
    },
    {
      "epoch": 2.040341597257759,
      "grad_norm": 0.43823832273483276,
      "learning_rate": 4.904014498682872e-05,
      "loss": 2.322,
      "step": 99180
    },
    {
      "epoch": 2.0405473167746266,
      "grad_norm": 0.4478078782558441,
      "learning_rate": 4.902103723422378e-05,
      "loss": 2.3833,
      "step": 99190
    },
    {
      "epoch": 2.0407530362914943,
      "grad_norm": 0.45701056718826294,
      "learning_rate": 4.9001931996133345e-05,
      "loss": 2.3547,
      "step": 99200
    },
    {
      "epoch": 2.040958755808362,
      "grad_norm": 0.43532437086105347,
      "learning_rate": 4.8982829273499655e-05,
      "loss": 2.4227,
      "step": 99210
    },
    {
      "epoch": 2.0411644753252296,
      "grad_norm": 0.46097368001937866,
      "learning_rate": 4.896372906726495e-05,
      "loss": 2.38,
      "step": 99220
    },
    {
      "epoch": 2.0413701948420973,
      "grad_norm": 0.456128865480423,
      "learning_rate": 4.894463137837145e-05,
      "loss": 2.3562,
      "step": 99230
    },
    {
      "epoch": 2.0415759143589653,
      "grad_norm": 0.42934733629226685,
      "learning_rate": 4.892553620776104e-05,
      "loss": 2.3842,
      "step": 99240
    },
    {
      "epoch": 2.041781633875833,
      "grad_norm": 0.4586297571659088,
      "learning_rate": 4.890644355637559e-05,
      "loss": 2.3783,
      "step": 99250
    },
    {
      "epoch": 2.0419873533927007,
      "grad_norm": 0.49849390983581543,
      "learning_rate": 4.8887353425156865e-05,
      "loss": 2.4088,
      "step": 99260
    },
    {
      "epoch": 2.0421930729095683,
      "grad_norm": 0.4535650610923767,
      "learning_rate": 4.886826581504647e-05,
      "loss": 2.3481,
      "step": 99270
    },
    {
      "epoch": 2.042398792426436,
      "grad_norm": 0.44492319226264954,
      "learning_rate": 4.884918072698588e-05,
      "loss": 2.3304,
      "step": 99280
    },
    {
      "epoch": 2.0426045119433036,
      "grad_norm": 0.46411094069480896,
      "learning_rate": 4.883009816191646e-05,
      "loss": 2.3466,
      "step": 99290
    },
    {
      "epoch": 2.0428102314601713,
      "grad_norm": 0.427791565656662,
      "learning_rate": 4.8811018120779465e-05,
      "loss": 2.3852,
      "step": 99300
    },
    {
      "epoch": 2.043015950977039,
      "grad_norm": 0.5209773778915405,
      "learning_rate": 4.879194060451598e-05,
      "loss": 2.355,
      "step": 99310
    },
    {
      "epoch": 2.043221670493907,
      "grad_norm": 0.434520959854126,
      "learning_rate": 4.8772865614067045e-05,
      "loss": 2.3597,
      "step": 99320
    },
    {
      "epoch": 2.0434273900107747,
      "grad_norm": 0.4569319188594818,
      "learning_rate": 4.875379315037347e-05,
      "loss": 2.3582,
      "step": 99330
    },
    {
      "epoch": 2.0436331095276423,
      "grad_norm": 0.46785879135131836,
      "learning_rate": 4.873472321437603e-05,
      "loss": 2.3923,
      "step": 99340
    },
    {
      "epoch": 2.04383882904451,
      "grad_norm": 0.4530865252017975,
      "learning_rate": 4.871565580701534e-05,
      "loss": 2.4171,
      "step": 99350
    },
    {
      "epoch": 2.0440445485613776,
      "grad_norm": 0.4812861382961273,
      "learning_rate": 4.869659092923187e-05,
      "loss": 2.3676,
      "step": 99360
    },
    {
      "epoch": 2.0442502680782453,
      "grad_norm": 0.44882842898368835,
      "learning_rate": 4.8677528581966e-05,
      "loss": 2.3781,
      "step": 99370
    },
    {
      "epoch": 2.044455987595113,
      "grad_norm": 0.46845847368240356,
      "learning_rate": 4.8658468766157985e-05,
      "loss": 2.3574,
      "step": 99380
    },
    {
      "epoch": 2.044661707111981,
      "grad_norm": 0.43020758032798767,
      "learning_rate": 4.863941148274791e-05,
      "loss": 2.3634,
      "step": 99390
    },
    {
      "epoch": 2.0448674266288487,
      "grad_norm": 0.4269103705883026,
      "learning_rate": 4.86203567326758e-05,
      "loss": 2.3755,
      "step": 99400
    },
    {
      "epoch": 2.0450731461457163,
      "grad_norm": 0.44348815083503723,
      "learning_rate": 4.860130451688153e-05,
      "loss": 2.3977,
      "step": 99410
    },
    {
      "epoch": 2.045278865662584,
      "grad_norm": 0.5803319811820984,
      "learning_rate": 4.858225483630475e-05,
      "loss": 2.3692,
      "step": 99420
    },
    {
      "epoch": 2.0454845851794516,
      "grad_norm": 0.4422992765903473,
      "learning_rate": 4.8563207691885184e-05,
      "loss": 2.3615,
      "step": 99430
    },
    {
      "epoch": 2.0456903046963193,
      "grad_norm": 0.4567500352859497,
      "learning_rate": 4.854416308456232e-05,
      "loss": 2.3717,
      "step": 99440
    },
    {
      "epoch": 2.045896024213187,
      "grad_norm": 0.41027840971946716,
      "learning_rate": 4.8525121015275424e-05,
      "loss": 2.3357,
      "step": 99450
    },
    {
      "epoch": 2.046101743730055,
      "grad_norm": 0.43519726395606995,
      "learning_rate": 4.8506081484963836e-05,
      "loss": 2.3526,
      "step": 99460
    },
    {
      "epoch": 2.0463074632469227,
      "grad_norm": 0.4242253601551056,
      "learning_rate": 4.848704449456668e-05,
      "loss": 2.3402,
      "step": 99470
    },
    {
      "epoch": 2.0465131827637904,
      "grad_norm": 0.4467620253562927,
      "learning_rate": 4.846801004502283e-05,
      "loss": 2.3598,
      "step": 99480
    },
    {
      "epoch": 2.046718902280658,
      "grad_norm": 0.4617164433002472,
      "learning_rate": 4.844897813727131e-05,
      "loss": 2.4064,
      "step": 99490
    },
    {
      "epoch": 2.0469246217975257,
      "grad_norm": 0.4630338251590729,
      "learning_rate": 4.842994877225073e-05,
      "loss": 2.3785,
      "step": 99500
    },
    {
      "epoch": 2.0471303413143933,
      "grad_norm": 0.4846068322658539,
      "learning_rate": 4.8410921950899724e-05,
      "loss": 2.3319,
      "step": 99510
    },
    {
      "epoch": 2.047336060831261,
      "grad_norm": 0.4590199291706085,
      "learning_rate": 4.839189767415688e-05,
      "loss": 2.3797,
      "step": 99520
    },
    {
      "epoch": 2.047541780348129,
      "grad_norm": 0.413078635931015,
      "learning_rate": 4.837287594296046e-05,
      "loss": 2.3651,
      "step": 99530
    },
    {
      "epoch": 2.0477474998649967,
      "grad_norm": 0.4269758462905884,
      "learning_rate": 4.835385675824869e-05,
      "loss": 2.3659,
      "step": 99540
    },
    {
      "epoch": 2.0479532193818644,
      "grad_norm": 0.4643554091453552,
      "learning_rate": 4.833484012095982e-05,
      "loss": 2.3343,
      "step": 99550
    },
    {
      "epoch": 2.048158938898732,
      "grad_norm": 0.4333425462245941,
      "learning_rate": 4.8315826032031696e-05,
      "loss": 2.3593,
      "step": 99560
    },
    {
      "epoch": 2.0483646584155997,
      "grad_norm": 0.48663967847824097,
      "learning_rate": 4.829681449240224e-05,
      "loss": 2.4385,
      "step": 99570
    },
    {
      "epoch": 2.0485703779324673,
      "grad_norm": 0.42585915327072144,
      "learning_rate": 4.827780550300917e-05,
      "loss": 2.3833,
      "step": 99580
    },
    {
      "epoch": 2.048776097449335,
      "grad_norm": 0.44054070115089417,
      "learning_rate": 4.8258799064790096e-05,
      "loss": 2.3232,
      "step": 99590
    },
    {
      "epoch": 2.048981816966203,
      "grad_norm": 0.4982989728450775,
      "learning_rate": 4.823979517868251e-05,
      "loss": 2.3689,
      "step": 99600
    },
    {
      "epoch": 2.0491875364830707,
      "grad_norm": 0.43486225605010986,
      "learning_rate": 4.822079384562378e-05,
      "loss": 2.4006,
      "step": 99610
    },
    {
      "epoch": 2.0493932559999384,
      "grad_norm": 0.4463408291339874,
      "learning_rate": 4.820179506655112e-05,
      "loss": 2.341,
      "step": 99620
    },
    {
      "epoch": 2.049598975516806,
      "grad_norm": 0.45119938254356384,
      "learning_rate": 4.8182798842401646e-05,
      "loss": 2.3822,
      "step": 99630
    },
    {
      "epoch": 2.0498046950336737,
      "grad_norm": 0.4654248058795929,
      "learning_rate": 4.8163805174112333e-05,
      "loss": 2.3672,
      "step": 99640
    },
    {
      "epoch": 2.0500104145505413,
      "grad_norm": 0.42975762486457825,
      "learning_rate": 4.8144814062620026e-05,
      "loss": 2.3394,
      "step": 99650
    },
    {
      "epoch": 2.050216134067409,
      "grad_norm": 0.43403780460357666,
      "learning_rate": 4.8125825508861475e-05,
      "loss": 2.4225,
      "step": 99660
    },
    {
      "epoch": 2.0504218535842766,
      "grad_norm": 0.45023953914642334,
      "learning_rate": 4.8106839513773274e-05,
      "loss": 2.3173,
      "step": 99670
    },
    {
      "epoch": 2.0506275731011447,
      "grad_norm": 0.4469382166862488,
      "learning_rate": 4.8087856078291896e-05,
      "loss": 2.3426,
      "step": 99680
    },
    {
      "epoch": 2.0508332926180124,
      "grad_norm": 0.5129194259643555,
      "learning_rate": 4.806887520335368e-05,
      "loss": 2.3798,
      "step": 99690
    },
    {
      "epoch": 2.05103901213488,
      "grad_norm": 0.45356521010398865,
      "learning_rate": 4.8049896889894906e-05,
      "loss": 2.3671,
      "step": 99700
    },
    {
      "epoch": 2.0512447316517477,
      "grad_norm": 0.4684801995754242,
      "learning_rate": 4.803092113885155e-05,
      "loss": 2.3616,
      "step": 99710
    },
    {
      "epoch": 2.0514504511686154,
      "grad_norm": 0.5196163058280945,
      "learning_rate": 4.8011947951159685e-05,
      "loss": 2.354,
      "step": 99720
    },
    {
      "epoch": 2.051656170685483,
      "grad_norm": 0.44296061992645264,
      "learning_rate": 4.7992977327755174e-05,
      "loss": 2.3554,
      "step": 99730
    },
    {
      "epoch": 2.0518618902023507,
      "grad_norm": 0.5359805226325989,
      "learning_rate": 4.7974009269573605e-05,
      "loss": 2.3924,
      "step": 99740
    },
    {
      "epoch": 2.0520676097192188,
      "grad_norm": 0.44953346252441406,
      "learning_rate": 4.79550437775507e-05,
      "loss": 2.368,
      "step": 99750
    },
    {
      "epoch": 2.0522733292360864,
      "grad_norm": 0.622281014919281,
      "learning_rate": 4.79360808526219e-05,
      "loss": 2.4062,
      "step": 99760
    },
    {
      "epoch": 2.052479048752954,
      "grad_norm": 0.41701146960258484,
      "learning_rate": 4.7917120495722445e-05,
      "loss": 2.3845,
      "step": 99770
    },
    {
      "epoch": 2.0526847682698217,
      "grad_norm": 0.4674203097820282,
      "learning_rate": 4.7898162707787694e-05,
      "loss": 2.2962,
      "step": 99780
    },
    {
      "epoch": 2.0528904877866894,
      "grad_norm": 0.4465137720108032,
      "learning_rate": 4.787920748975262e-05,
      "loss": 2.3809,
      "step": 99790
    },
    {
      "epoch": 2.053096207303557,
      "grad_norm": 0.4306848645210266,
      "learning_rate": 4.786025484255218e-05,
      "loss": 2.2851,
      "step": 99800
    },
    {
      "epoch": 2.053281354868738,
      "grad_norm": 0.3961649537086487,
      "learning_rate": 4.784130476712131e-05,
      "loss": 2.3588,
      "step": 99810
    },
    {
      "epoch": 2.0534870743856057,
      "grad_norm": 0.43716511130332947,
      "learning_rate": 4.782235726439461e-05,
      "loss": 2.3668,
      "step": 99820
    },
    {
      "epoch": 2.0536927939024734,
      "grad_norm": 0.46902996301651,
      "learning_rate": 4.780341233530666e-05,
      "loss": 2.3769,
      "step": 99830
    },
    {
      "epoch": 2.053898513419341,
      "grad_norm": 0.432475209236145,
      "learning_rate": 4.778446998079201e-05,
      "loss": 2.3426,
      "step": 99840
    },
    {
      "epoch": 2.054104232936209,
      "grad_norm": 0.46830013394355774,
      "learning_rate": 4.776553020178488e-05,
      "loss": 2.3918,
      "step": 99850
    },
    {
      "epoch": 2.054309952453077,
      "grad_norm": 0.4493865370750427,
      "learning_rate": 4.7746592999219505e-05,
      "loss": 2.3703,
      "step": 99860
    },
    {
      "epoch": 2.0545156719699444,
      "grad_norm": 0.44695305824279785,
      "learning_rate": 4.772765837402995e-05,
      "loss": 2.3808,
      "step": 99870
    },
    {
      "epoch": 2.054721391486812,
      "grad_norm": 0.5037286281585693,
      "learning_rate": 4.7708726327150156e-05,
      "loss": 2.3637,
      "step": 99880
    },
    {
      "epoch": 2.0549271110036798,
      "grad_norm": 0.5250405669212341,
      "learning_rate": 4.768979685951394e-05,
      "loss": 2.4085,
      "step": 99890
    },
    {
      "epoch": 2.0551328305205474,
      "grad_norm": 0.6436448693275452,
      "learning_rate": 4.767086997205499e-05,
      "loss": 2.3873,
      "step": 99900
    },
    {
      "epoch": 2.055338550037415,
      "grad_norm": 0.45538637042045593,
      "learning_rate": 4.7651945665706856e-05,
      "loss": 2.3949,
      "step": 99910
    },
    {
      "epoch": 2.055544269554283,
      "grad_norm": 0.43029025197029114,
      "learning_rate": 4.763302394140299e-05,
      "loss": 2.4102,
      "step": 99920
    },
    {
      "epoch": 2.055749989071151,
      "grad_norm": 0.48406967520713806,
      "learning_rate": 4.761410480007667e-05,
      "loss": 2.4061,
      "step": 99930
    },
    {
      "epoch": 2.0559557085880185,
      "grad_norm": 0.49729201197624207,
      "learning_rate": 4.7595188242661104e-05,
      "loss": 2.3788,
      "step": 99940
    },
    {
      "epoch": 2.056161428104886,
      "grad_norm": 0.4329087734222412,
      "learning_rate": 4.757627427008934e-05,
      "loss": 2.3838,
      "step": 99950
    },
    {
      "epoch": 2.0563671476217538,
      "grad_norm": 0.40738803148269653,
      "learning_rate": 4.7557362883294274e-05,
      "loss": 2.368,
      "step": 99960
    },
    {
      "epoch": 2.0565728671386214,
      "grad_norm": 0.4191805422306061,
      "learning_rate": 4.753845408320873e-05,
      "loss": 2.3408,
      "step": 99970
    },
    {
      "epoch": 2.056778586655489,
      "grad_norm": 0.46321335434913635,
      "learning_rate": 4.751954787076536e-05,
      "loss": 2.397,
      "step": 99980
    },
    {
      "epoch": 2.056984306172357,
      "grad_norm": 0.4524278938770294,
      "learning_rate": 4.7500644246896705e-05,
      "loss": 2.3412,
      "step": 99990
    },
    {
      "epoch": 2.057190025689225,
      "grad_norm": 0.44497764110565186,
      "learning_rate": 4.748174321253519e-05,
      "loss": 2.4193,
      "step": 100000
    },
    {
      "epoch": 2.0573957452060925,
      "grad_norm": 0.43774646520614624,
      "learning_rate": 4.746284476861309e-05,
      "loss": 2.4175,
      "step": 100010
    },
    {
      "epoch": 2.05760146472296,
      "grad_norm": 0.4378645718097687,
      "learning_rate": 4.7443948916062595e-05,
      "loss": 2.4089,
      "step": 100020
    },
    {
      "epoch": 2.057807184239828,
      "grad_norm": 0.4267254173755646,
      "learning_rate": 4.7425055655815644e-05,
      "loss": 2.3689,
      "step": 100030
    },
    {
      "epoch": 2.0580129037566954,
      "grad_norm": 0.44172534346580505,
      "learning_rate": 4.740616498880423e-05,
      "loss": 2.3281,
      "step": 100040
    },
    {
      "epoch": 2.058218623273563,
      "grad_norm": 0.47843241691589355,
      "learning_rate": 4.738727691596012e-05,
      "loss": 2.3467,
      "step": 100050
    },
    {
      "epoch": 2.0584243427904307,
      "grad_norm": 0.47548621892929077,
      "learning_rate": 4.736839143821487e-05,
      "loss": 2.3885,
      "step": 100060
    },
    {
      "epoch": 2.058630062307299,
      "grad_norm": 0.47458982467651367,
      "learning_rate": 4.73495085565001e-05,
      "loss": 2.3748,
      "step": 100070
    },
    {
      "epoch": 2.0588357818241665,
      "grad_norm": 0.47334742546081543,
      "learning_rate": 4.7330628271747204e-05,
      "loss": 2.376,
      "step": 100080
    },
    {
      "epoch": 2.059041501341034,
      "grad_norm": 0.4473048150539398,
      "learning_rate": 4.7311750584887316e-05,
      "loss": 2.3745,
      "step": 100090
    },
    {
      "epoch": 2.059247220857902,
      "grad_norm": 0.47086283564567566,
      "learning_rate": 4.7292875496851726e-05,
      "loss": 2.3477,
      "step": 100100
    },
    {
      "epoch": 2.0594529403747694,
      "grad_norm": 0.4073047637939453,
      "learning_rate": 4.727400300857135e-05,
      "loss": 2.4105,
      "step": 100110
    },
    {
      "epoch": 2.059658659891637,
      "grad_norm": 0.4562819004058838,
      "learning_rate": 4.725513312097704e-05,
      "loss": 2.4179,
      "step": 100120
    },
    {
      "epoch": 2.0598643794085048,
      "grad_norm": 0.43297410011291504,
      "learning_rate": 4.723626583499966e-05,
      "loss": 2.3577,
      "step": 100130
    },
    {
      "epoch": 2.060070098925373,
      "grad_norm": 0.49973830580711365,
      "learning_rate": 4.721740115156972e-05,
      "loss": 2.3603,
      "step": 100140
    },
    {
      "epoch": 2.0602758184422405,
      "grad_norm": 0.4901467561721802,
      "learning_rate": 4.719853907161771e-05,
      "loss": 2.3293,
      "step": 100150
    },
    {
      "epoch": 2.060481537959108,
      "grad_norm": 0.4113597273826599,
      "learning_rate": 4.717967959607411e-05,
      "loss": 2.3455,
      "step": 100160
    },
    {
      "epoch": 2.060687257475976,
      "grad_norm": 0.4324691891670227,
      "learning_rate": 4.716082272586905e-05,
      "loss": 2.3737,
      "step": 100170
    },
    {
      "epoch": 2.0608929769928435,
      "grad_norm": 0.47885775566101074,
      "learning_rate": 4.714196846193265e-05,
      "loss": 2.3518,
      "step": 100180
    },
    {
      "epoch": 2.061098696509711,
      "grad_norm": 0.45289093255996704,
      "learning_rate": 4.712311680519492e-05,
      "loss": 2.3626,
      "step": 100190
    },
    {
      "epoch": 2.0613044160265788,
      "grad_norm": 0.4811722934246063,
      "learning_rate": 4.7104267756585684e-05,
      "loss": 2.3596,
      "step": 100200
    },
    {
      "epoch": 2.061510135543447,
      "grad_norm": 0.477376252412796,
      "learning_rate": 4.708542131703469e-05,
      "loss": 2.4362,
      "step": 100210
    },
    {
      "epoch": 2.0617158550603145,
      "grad_norm": 0.4507252275943756,
      "learning_rate": 4.7066577487471484e-05,
      "loss": 2.399,
      "step": 100220
    },
    {
      "epoch": 2.061921574577182,
      "grad_norm": 0.4071862995624542,
      "learning_rate": 4.704773626882558e-05,
      "loss": 2.3933,
      "step": 100230
    },
    {
      "epoch": 2.06212729409405,
      "grad_norm": 0.43494075536727905,
      "learning_rate": 4.7028897662026286e-05,
      "loss": 2.3559,
      "step": 100240
    },
    {
      "epoch": 2.0623330136109175,
      "grad_norm": 0.4225088655948639,
      "learning_rate": 4.70100616680028e-05,
      "loss": 2.3962,
      "step": 100250
    },
    {
      "epoch": 2.062538733127785,
      "grad_norm": 0.4656294584274292,
      "learning_rate": 4.6991228287684266e-05,
      "loss": 2.4219,
      "step": 100260
    },
    {
      "epoch": 2.062744452644653,
      "grad_norm": 0.4673035740852356,
      "learning_rate": 4.6972397521999486e-05,
      "loss": 2.3446,
      "step": 100270
    },
    {
      "epoch": 2.062950172161521,
      "grad_norm": 0.48506951332092285,
      "learning_rate": 4.695356937187743e-05,
      "loss": 2.3724,
      "step": 100280
    },
    {
      "epoch": 2.0631558916783885,
      "grad_norm": 0.4619486331939697,
      "learning_rate": 4.693474383824674e-05,
      "loss": 2.371,
      "step": 100290
    },
    {
      "epoch": 2.063361611195256,
      "grad_norm": 0.4618074297904968,
      "learning_rate": 4.6915920922035916e-05,
      "loss": 2.3809,
      "step": 100300
    },
    {
      "epoch": 2.063567330712124,
      "grad_norm": 0.4654325842857361,
      "learning_rate": 4.6897100624173465e-05,
      "loss": 2.355,
      "step": 100310
    },
    {
      "epoch": 2.0637730502289915,
      "grad_norm": 0.45291751623153687,
      "learning_rate": 4.687828294558766e-05,
      "loss": 2.4138,
      "step": 100320
    },
    {
      "epoch": 2.063978769745859,
      "grad_norm": 0.4485359489917755,
      "learning_rate": 4.6859467887206674e-05,
      "loss": 2.3228,
      "step": 100330
    },
    {
      "epoch": 2.064184489262727,
      "grad_norm": 0.4887651205062866,
      "learning_rate": 4.68406554499586e-05,
      "loss": 2.3592,
      "step": 100340
    },
    {
      "epoch": 2.064390208779595,
      "grad_norm": 0.45897534489631653,
      "learning_rate": 4.6821845634771244e-05,
      "loss": 2.411,
      "step": 100350
    },
    {
      "epoch": 2.0645959282964625,
      "grad_norm": 0.4693050980567932,
      "learning_rate": 4.680303844257249e-05,
      "loss": 2.3635,
      "step": 100360
    },
    {
      "epoch": 2.06480164781333,
      "grad_norm": 0.43202799558639526,
      "learning_rate": 4.678423387428998e-05,
      "loss": 2.3892,
      "step": 100370
    },
    {
      "epoch": 2.065007367330198,
      "grad_norm": 0.4658796191215515,
      "learning_rate": 4.676543193085117e-05,
      "loss": 2.3662,
      "step": 100380
    },
    {
      "epoch": 2.0652130868470655,
      "grad_norm": 0.4753347337245941,
      "learning_rate": 4.674663261318354e-05,
      "loss": 2.4008,
      "step": 100390
    },
    {
      "epoch": 2.065418806363933,
      "grad_norm": 0.4894641637802124,
      "learning_rate": 4.672783592221436e-05,
      "loss": 2.3737,
      "step": 100400
    },
    {
      "epoch": 2.065624525880801,
      "grad_norm": 0.4746410846710205,
      "learning_rate": 4.670904185887066e-05,
      "loss": 2.3604,
      "step": 100410
    },
    {
      "epoch": 2.0658302453976685,
      "grad_norm": 0.4743834435939789,
      "learning_rate": 4.669025042407961e-05,
      "loss": 2.3687,
      "step": 100420
    },
    {
      "epoch": 2.0660359649145366,
      "grad_norm": 0.44309064745903015,
      "learning_rate": 4.667146161876795e-05,
      "loss": 2.3908,
      "step": 100430
    },
    {
      "epoch": 2.066241684431404,
      "grad_norm": 0.45879891514778137,
      "learning_rate": 4.665267544386246e-05,
      "loss": 2.3937,
      "step": 100440
    },
    {
      "epoch": 2.066447403948272,
      "grad_norm": 0.474469393491745,
      "learning_rate": 4.663389190028985e-05,
      "loss": 2.3896,
      "step": 100450
    },
    {
      "epoch": 2.0666531234651395,
      "grad_norm": 0.5140464305877686,
      "learning_rate": 4.661511098897651e-05,
      "loss": 2.3386,
      "step": 100460
    },
    {
      "epoch": 2.066858842982007,
      "grad_norm": 0.43162626028060913,
      "learning_rate": 4.659633271084878e-05,
      "loss": 2.3605,
      "step": 100470
    },
    {
      "epoch": 2.067064562498875,
      "grad_norm": 0.4660750925540924,
      "learning_rate": 4.6577557066833036e-05,
      "loss": 2.3318,
      "step": 100480
    },
    {
      "epoch": 2.0672702820157425,
      "grad_norm": 0.4649600088596344,
      "learning_rate": 4.655878405785523e-05,
      "loss": 2.3755,
      "step": 100490
    },
    {
      "epoch": 2.0674760015326106,
      "grad_norm": 0.44869711995124817,
      "learning_rate": 4.65400136848414e-05,
      "loss": 2.3557,
      "step": 100500
    },
    {
      "epoch": 2.0676817210494782,
      "grad_norm": 0.4333121180534363,
      "learning_rate": 4.652124594871736e-05,
      "loss": 2.3867,
      "step": 100510
    },
    {
      "epoch": 2.067887440566346,
      "grad_norm": 0.45157235860824585,
      "learning_rate": 4.6502480850408844e-05,
      "loss": 2.4118,
      "step": 100520
    },
    {
      "epoch": 2.0680931600832135,
      "grad_norm": 0.4343825876712799,
      "learning_rate": 4.6483718390841425e-05,
      "loss": 2.3778,
      "step": 100530
    },
    {
      "epoch": 2.068298879600081,
      "grad_norm": 0.45237502455711365,
      "learning_rate": 4.6464958570940534e-05,
      "loss": 2.3693,
      "step": 100540
    },
    {
      "epoch": 2.068504599116949,
      "grad_norm": 0.48721444606781006,
      "learning_rate": 4.6446201391631516e-05,
      "loss": 2.382,
      "step": 100550
    },
    {
      "epoch": 2.0687103186338165,
      "grad_norm": 0.4963269531726837,
      "learning_rate": 4.642744685383955e-05,
      "loss": 2.3875,
      "step": 100560
    },
    {
      "epoch": 2.0689160381506846,
      "grad_norm": 0.4859902858734131,
      "learning_rate": 4.6408694958489695e-05,
      "loss": 2.4308,
      "step": 100570
    },
    {
      "epoch": 2.0691217576675522,
      "grad_norm": 0.4290238320827484,
      "learning_rate": 4.638994570650691e-05,
      "loss": 2.3566,
      "step": 100580
    },
    {
      "epoch": 2.06932747718442,
      "grad_norm": 0.48368486762046814,
      "learning_rate": 4.63711990988159e-05,
      "loss": 2.4222,
      "step": 100590
    },
    {
      "epoch": 2.0695331967012875,
      "grad_norm": 0.4579751193523407,
      "learning_rate": 4.635245513634142e-05,
      "loss": 2.398,
      "step": 100600
    },
    {
      "epoch": 2.069738916218155,
      "grad_norm": 0.4148177206516266,
      "learning_rate": 4.633371382000803e-05,
      "loss": 2.4185,
      "step": 100610
    },
    {
      "epoch": 2.069944635735023,
      "grad_norm": 0.4803650379180908,
      "learning_rate": 4.6314975150740014e-05,
      "loss": 2.3796,
      "step": 100620
    },
    {
      "epoch": 2.0701503552518905,
      "grad_norm": 0.42958566546440125,
      "learning_rate": 4.629623912946177e-05,
      "loss": 2.3928,
      "step": 100630
    },
    {
      "epoch": 2.0703560747687586,
      "grad_norm": 0.4693281352519989,
      "learning_rate": 4.627750575709743e-05,
      "loss": 2.3541,
      "step": 100640
    },
    {
      "epoch": 2.0705617942856263,
      "grad_norm": 0.4726308286190033,
      "learning_rate": 4.6258775034570914e-05,
      "loss": 2.3961,
      "step": 100650
    },
    {
      "epoch": 2.070767513802494,
      "grad_norm": 0.5377638339996338,
      "learning_rate": 4.624004696280623e-05,
      "loss": 2.3898,
      "step": 100660
    },
    {
      "epoch": 2.0709732333193616,
      "grad_norm": 0.42878076434135437,
      "learning_rate": 4.6221321542727045e-05,
      "loss": 2.3329,
      "step": 100670
    },
    {
      "epoch": 2.071178952836229,
      "grad_norm": 0.4525192677974701,
      "learning_rate": 4.6202598775256966e-05,
      "loss": 2.3206,
      "step": 100680
    },
    {
      "epoch": 2.071384672353097,
      "grad_norm": 0.4621517062187195,
      "learning_rate": 4.6183878661319615e-05,
      "loss": 2.3323,
      "step": 100690
    },
    {
      "epoch": 2.0715903918699645,
      "grad_norm": 0.43873295187950134,
      "learning_rate": 4.616516120183818e-05,
      "loss": 2.3579,
      "step": 100700
    },
    {
      "epoch": 2.0717961113868326,
      "grad_norm": 0.46240296959877014,
      "learning_rate": 4.614644639773602e-05,
      "loss": 2.3692,
      "step": 100710
    },
    {
      "epoch": 2.0720018309037003,
      "grad_norm": 0.49652770161628723,
      "learning_rate": 4.612773424993623e-05,
      "loss": 2.3786,
      "step": 100720
    },
    {
      "epoch": 2.072207550420568,
      "grad_norm": 0.4289131462574005,
      "learning_rate": 4.6109024759361663e-05,
      "loss": 2.3592,
      "step": 100730
    },
    {
      "epoch": 2.0724132699374356,
      "grad_norm": 0.45898500084877014,
      "learning_rate": 4.609031792693531e-05,
      "loss": 2.4053,
      "step": 100740
    },
    {
      "epoch": 2.0726189894543032,
      "grad_norm": 0.43793344497680664,
      "learning_rate": 4.6071613753579765e-05,
      "loss": 2.4294,
      "step": 100750
    },
    {
      "epoch": 2.072824708971171,
      "grad_norm": 0.5062021613121033,
      "learning_rate": 4.60529122402176e-05,
      "loss": 2.3389,
      "step": 100760
    },
    {
      "epoch": 2.0730304284880385,
      "grad_norm": 0.4673271179199219,
      "learning_rate": 4.603421338777139e-05,
      "loss": 2.4043,
      "step": 100770
    },
    {
      "epoch": 2.0732361480049066,
      "grad_norm": 0.4605526328086853,
      "learning_rate": 4.60155171971633e-05,
      "loss": 2.3597,
      "step": 100780
    },
    {
      "epoch": 2.0734418675217743,
      "grad_norm": 0.4220367968082428,
      "learning_rate": 4.599682366931556e-05,
      "loss": 2.3467,
      "step": 100790
    },
    {
      "epoch": 2.073647587038642,
      "grad_norm": 0.4582441747188568,
      "learning_rate": 4.5978132805150245e-05,
      "loss": 2.3456,
      "step": 100800
    },
    {
      "epoch": 2.0738533065555096,
      "grad_norm": 0.48250389099121094,
      "learning_rate": 4.595944460558924e-05,
      "loss": 2.3467,
      "step": 100810
    },
    {
      "epoch": 2.0740590260723772,
      "grad_norm": 0.46254318952560425,
      "learning_rate": 4.594075907155435e-05,
      "loss": 2.3494,
      "step": 100820
    },
    {
      "epoch": 2.074264745589245,
      "grad_norm": 0.4353925287723541,
      "learning_rate": 4.5922076203967234e-05,
      "loss": 2.3435,
      "step": 100830
    },
    {
      "epoch": 2.0744704651061125,
      "grad_norm": 0.460591584444046,
      "learning_rate": 4.59033960037494e-05,
      "loss": 2.3293,
      "step": 100840
    },
    {
      "epoch": 2.07467618462298,
      "grad_norm": 0.4643067419528961,
      "learning_rate": 4.5884718471822255e-05,
      "loss": 2.3622,
      "step": 100850
    },
    {
      "epoch": 2.0748819041398483,
      "grad_norm": 0.398823082447052,
      "learning_rate": 4.586604360910706e-05,
      "loss": 2.3665,
      "step": 100860
    },
    {
      "epoch": 2.075087623656716,
      "grad_norm": 0.5018327832221985,
      "learning_rate": 4.584737141652497e-05,
      "loss": 2.3672,
      "step": 100870
    },
    {
      "epoch": 2.0752933431735836,
      "grad_norm": 0.43724653124809265,
      "learning_rate": 4.582870189499687e-05,
      "loss": 2.3885,
      "step": 100880
    },
    {
      "epoch": 2.0754990626904513,
      "grad_norm": 0.5289894938468933,
      "learning_rate": 4.581003504544376e-05,
      "loss": 2.3661,
      "step": 100890
    },
    {
      "epoch": 2.075704782207319,
      "grad_norm": 0.44785940647125244,
      "learning_rate": 4.5791370868786354e-05,
      "loss": 2.4048,
      "step": 100900
    },
    {
      "epoch": 2.0759105017241866,
      "grad_norm": 0.4815511405467987,
      "learning_rate": 4.577270936594514e-05,
      "loss": 2.3534,
      "step": 100910
    },
    {
      "epoch": 2.076116221241054,
      "grad_norm": 0.5141943097114563,
      "learning_rate": 4.5754050537840706e-05,
      "loss": 2.3427,
      "step": 100920
    },
    {
      "epoch": 2.0763219407579223,
      "grad_norm": 0.455561101436615,
      "learning_rate": 4.5735394385393404e-05,
      "loss": 2.3721,
      "step": 100930
    },
    {
      "epoch": 2.07652766027479,
      "grad_norm": 0.4726191759109497,
      "learning_rate": 4.5716740909523316e-05,
      "loss": 2.3123,
      "step": 100940
    },
    {
      "epoch": 2.0767333797916576,
      "grad_norm": 0.4350593388080597,
      "learning_rate": 4.5698090111150646e-05,
      "loss": 2.3845,
      "step": 100950
    },
    {
      "epoch": 2.0769390993085253,
      "grad_norm": 0.4452088475227356,
      "learning_rate": 4.567944199119526e-05,
      "loss": 2.3843,
      "step": 100960
    },
    {
      "epoch": 2.077144818825393,
      "grad_norm": 0.4538528323173523,
      "learning_rate": 4.566079655057694e-05,
      "loss": 2.3443,
      "step": 100970
    },
    {
      "epoch": 2.0773505383422606,
      "grad_norm": 0.420705646276474,
      "learning_rate": 4.56421537902155e-05,
      "loss": 2.3836,
      "step": 100980
    },
    {
      "epoch": 2.0775562578591282,
      "grad_norm": 0.45389315485954285,
      "learning_rate": 4.5623513711030366e-05,
      "loss": 2.3536,
      "step": 100990
    },
    {
      "epoch": 2.0777619773759963,
      "grad_norm": 0.47602006793022156,
      "learning_rate": 4.560487631394094e-05,
      "loss": 2.3478,
      "step": 101000
    },
    {
      "epoch": 2.077967696892864,
      "grad_norm": 0.430868536233902,
      "learning_rate": 4.5586241599866605e-05,
      "loss": 2.4081,
      "step": 101010
    },
    {
      "epoch": 2.0781734164097316,
      "grad_norm": 0.46498531103134155,
      "learning_rate": 4.5567609569726434e-05,
      "loss": 2.3362,
      "step": 101020
    },
    {
      "epoch": 2.0783791359265993,
      "grad_norm": 0.4284331798553467,
      "learning_rate": 4.5548980224439464e-05,
      "loss": 2.3718,
      "step": 101030
    },
    {
      "epoch": 2.078584855443467,
      "grad_norm": 0.5288864374160767,
      "learning_rate": 4.5530353564924564e-05,
      "loss": 2.4341,
      "step": 101040
    },
    {
      "epoch": 2.0787905749603346,
      "grad_norm": 0.44016993045806885,
      "learning_rate": 4.5511729592100505e-05,
      "loss": 2.3937,
      "step": 101050
    },
    {
      "epoch": 2.0789962944772022,
      "grad_norm": 0.41607406735420227,
      "learning_rate": 4.54931083068859e-05,
      "loss": 2.3473,
      "step": 101060
    },
    {
      "epoch": 2.0792020139940703,
      "grad_norm": 0.5086447596549988,
      "learning_rate": 4.547448971019924e-05,
      "loss": 2.3448,
      "step": 101070
    },
    {
      "epoch": 2.079407733510938,
      "grad_norm": 0.43977341055870056,
      "learning_rate": 4.545587380295883e-05,
      "loss": 2.3575,
      "step": 101080
    },
    {
      "epoch": 2.0796134530278056,
      "grad_norm": 0.43958526849746704,
      "learning_rate": 4.543726058608301e-05,
      "loss": 2.4115,
      "step": 101090
    },
    {
      "epoch": 2.0798191725446733,
      "grad_norm": 0.46054965257644653,
      "learning_rate": 4.541865006048976e-05,
      "loss": 2.3536,
      "step": 101100
    },
    {
      "epoch": 2.080024892061541,
      "grad_norm": 0.42697110772132874,
      "learning_rate": 4.5400042227097076e-05,
      "loss": 2.3701,
      "step": 101110
    },
    {
      "epoch": 2.0802306115784086,
      "grad_norm": 0.42645448446273804,
      "learning_rate": 4.538143708682277e-05,
      "loss": 2.3391,
      "step": 101120
    },
    {
      "epoch": 2.0804363310952763,
      "grad_norm": 0.4524151682853699,
      "learning_rate": 4.536283464058453e-05,
      "loss": 2.3545,
      "step": 101130
    },
    {
      "epoch": 2.080642050612144,
      "grad_norm": 0.44026774168014526,
      "learning_rate": 4.534423488929992e-05,
      "loss": 2.3687,
      "step": 101140
    },
    {
      "epoch": 2.080847770129012,
      "grad_norm": 0.4329189360141754,
      "learning_rate": 4.532563783388637e-05,
      "loss": 2.3428,
      "step": 101150
    },
    {
      "epoch": 2.0810534896458797,
      "grad_norm": 0.42650794982910156,
      "learning_rate": 4.530704347526117e-05,
      "loss": 2.369,
      "step": 101160
    },
    {
      "epoch": 2.0812592091627473,
      "grad_norm": 0.4538338780403137,
      "learning_rate": 4.528845181434147e-05,
      "loss": 2.3529,
      "step": 101170
    },
    {
      "epoch": 2.081464928679615,
      "grad_norm": 0.48013928532600403,
      "learning_rate": 4.52698628520443e-05,
      "loss": 2.3717,
      "step": 101180
    },
    {
      "epoch": 2.0816706481964826,
      "grad_norm": 0.46648186445236206,
      "learning_rate": 4.525127658928659e-05,
      "loss": 2.3442,
      "step": 101190
    },
    {
      "epoch": 2.0818763677133503,
      "grad_norm": 0.4405581057071686,
      "learning_rate": 4.5232693026984985e-05,
      "loss": 2.443,
      "step": 101200
    },
    {
      "epoch": 2.082082087230218,
      "grad_norm": 0.43826109170913696,
      "learning_rate": 4.521411216605622e-05,
      "loss": 2.3689,
      "step": 101210
    },
    {
      "epoch": 2.082287806747086,
      "grad_norm": 0.40997663140296936,
      "learning_rate": 4.519553400741681e-05,
      "loss": 2.3847,
      "step": 101220
    },
    {
      "epoch": 2.0824935262639537,
      "grad_norm": 0.4503615200519562,
      "learning_rate": 4.517695855198296e-05,
      "loss": 2.3632,
      "step": 101230
    },
    {
      "epoch": 2.0826992457808213,
      "grad_norm": 0.45860838890075684,
      "learning_rate": 4.515838580067104e-05,
      "loss": 2.3976,
      "step": 101240
    },
    {
      "epoch": 2.082904965297689,
      "grad_norm": 0.4369523525238037,
      "learning_rate": 4.513981575439714e-05,
      "loss": 2.368,
      "step": 101250
    },
    {
      "epoch": 2.0831106848145566,
      "grad_norm": 0.806318998336792,
      "learning_rate": 4.5121248414077114e-05,
      "loss": 2.4116,
      "step": 101260
    },
    {
      "epoch": 2.0833164043314243,
      "grad_norm": 0.47015631198883057,
      "learning_rate": 4.510268378062691e-05,
      "loss": 2.3192,
      "step": 101270
    },
    {
      "epoch": 2.083522123848292,
      "grad_norm": 0.4240927994251251,
      "learning_rate": 4.508412185496211e-05,
      "loss": 2.4255,
      "step": 101280
    },
    {
      "epoch": 2.08372784336516,
      "grad_norm": 0.4900381863117218,
      "learning_rate": 4.50655626379983e-05,
      "loss": 2.3433,
      "step": 101290
    },
    {
      "epoch": 2.0839335628820277,
      "grad_norm": 0.5095400810241699,
      "learning_rate": 4.504700613065102e-05,
      "loss": 2.3946,
      "step": 101300
    },
    {
      "epoch": 2.0841392823988953,
      "grad_norm": 0.4687415361404419,
      "learning_rate": 4.502845233383542e-05,
      "loss": 2.3944,
      "step": 101310
    },
    {
      "epoch": 2.084345001915763,
      "grad_norm": 0.5398086905479431,
      "learning_rate": 4.500990124846666e-05,
      "loss": 2.3714,
      "step": 101320
    },
    {
      "epoch": 2.0845507214326306,
      "grad_norm": 0.4868466854095459,
      "learning_rate": 4.4991352875459894e-05,
      "loss": 2.3265,
      "step": 101330
    },
    {
      "epoch": 2.0847564409494983,
      "grad_norm": 0.477655827999115,
      "learning_rate": 4.497280721572988e-05,
      "loss": 2.3611,
      "step": 101340
    },
    {
      "epoch": 2.084962160466366,
      "grad_norm": 0.45131343603134155,
      "learning_rate": 4.495426427019144e-05,
      "loss": 2.3324,
      "step": 101350
    },
    {
      "epoch": 2.085167879983234,
      "grad_norm": 0.4304748475551605,
      "learning_rate": 4.4935724039759174e-05,
      "loss": 2.401,
      "step": 101360
    },
    {
      "epoch": 2.0853735995001017,
      "grad_norm": 0.410949170589447,
      "learning_rate": 4.491718652534757e-05,
      "loss": 2.3633,
      "step": 101370
    },
    {
      "epoch": 2.0855793190169694,
      "grad_norm": 0.4714694023132324,
      "learning_rate": 4.4898651727871e-05,
      "loss": 2.3972,
      "step": 101380
    },
    {
      "epoch": 2.085785038533837,
      "grad_norm": 0.4595628082752228,
      "learning_rate": 4.488011964824367e-05,
      "loss": 2.3896,
      "step": 101390
    },
    {
      "epoch": 2.0859907580507047,
      "grad_norm": 0.4279443323612213,
      "learning_rate": 4.4861590287379675e-05,
      "loss": 2.3966,
      "step": 101400
    },
    {
      "epoch": 2.0861964775675723,
      "grad_norm": 0.43106746673583984,
      "learning_rate": 4.484306364619295e-05,
      "loss": 2.3983,
      "step": 101410
    },
    {
      "epoch": 2.08640219708444,
      "grad_norm": 0.4491201639175415,
      "learning_rate": 4.4824539725597334e-05,
      "loss": 2.3707,
      "step": 101420
    },
    {
      "epoch": 2.086607916601308,
      "grad_norm": 0.47879090905189514,
      "learning_rate": 4.480601852650651e-05,
      "loss": 2.3848,
      "step": 101430
    },
    {
      "epoch": 2.0868136361181757,
      "grad_norm": 0.4642570912837982,
      "learning_rate": 4.478750004983402e-05,
      "loss": 2.4084,
      "step": 101440
    },
    {
      "epoch": 2.0870193556350434,
      "grad_norm": 0.45134788751602173,
      "learning_rate": 4.4768984296493265e-05,
      "loss": 2.4146,
      "step": 101450
    },
    {
      "epoch": 2.087225075151911,
      "grad_norm": 0.4465351104736328,
      "learning_rate": 4.475047126739755e-05,
      "loss": 2.3669,
      "step": 101460
    },
    {
      "epoch": 2.0874307946687787,
      "grad_norm": 0.4580678939819336,
      "learning_rate": 4.4731960963460027e-05,
      "loss": 2.3508,
      "step": 101470
    },
    {
      "epoch": 2.0876365141856463,
      "grad_norm": 0.4296332597732544,
      "learning_rate": 4.4713453385593687e-05,
      "loss": 2.3835,
      "step": 101480
    },
    {
      "epoch": 2.087842233702514,
      "grad_norm": 0.4565941393375397,
      "learning_rate": 4.46949485347114e-05,
      "loss": 2.3726,
      "step": 101490
    },
    {
      "epoch": 2.088047953219382,
      "grad_norm": 0.556587278842926,
      "learning_rate": 4.467644641172595e-05,
      "loss": 2.3518,
      "step": 101500
    },
    {
      "epoch": 2.0882536727362497,
      "grad_norm": 0.513001561164856,
      "learning_rate": 4.465794701754994e-05,
      "loss": 2.3792,
      "step": 101510
    },
    {
      "epoch": 2.0884593922531174,
      "grad_norm": 0.46130189299583435,
      "learning_rate": 4.4639450353095755e-05,
      "loss": 2.4205,
      "step": 101520
    },
    {
      "epoch": 2.088665111769985,
      "grad_norm": 0.430005818605423,
      "learning_rate": 4.462095641927585e-05,
      "loss": 2.3669,
      "step": 101530
    },
    {
      "epoch": 2.0888708312868527,
      "grad_norm": 0.4304669201374054,
      "learning_rate": 4.460246521700242e-05,
      "loss": 2.3558,
      "step": 101540
    },
    {
      "epoch": 2.0890765508037203,
      "grad_norm": 0.455292284488678,
      "learning_rate": 4.4583976747187414e-05,
      "loss": 2.3477,
      "step": 101550
    },
    {
      "epoch": 2.089282270320588,
      "grad_norm": 0.40403950214385986,
      "learning_rate": 4.456549101074291e-05,
      "loss": 2.341,
      "step": 101560
    },
    {
      "epoch": 2.0894879898374557,
      "grad_norm": 0.4719444513320923,
      "learning_rate": 4.454700800858067e-05,
      "loss": 2.3546,
      "step": 101570
    },
    {
      "epoch": 2.0896937093543237,
      "grad_norm": 0.45371609926223755,
      "learning_rate": 4.452852774161228e-05,
      "loss": 2.3758,
      "step": 101580
    },
    {
      "epoch": 2.0898994288711914,
      "grad_norm": 0.46801888942718506,
      "learning_rate": 4.451005021074941e-05,
      "loss": 2.34,
      "step": 101590
    },
    {
      "epoch": 2.090105148388059,
      "grad_norm": 0.45142579078674316,
      "learning_rate": 4.449157541690334e-05,
      "loss": 2.3053,
      "step": 101600
    },
    {
      "epoch": 2.0903108679049267,
      "grad_norm": 0.4761373698711395,
      "learning_rate": 4.4473103360985324e-05,
      "loss": 2.3443,
      "step": 101610
    },
    {
      "epoch": 2.0905165874217944,
      "grad_norm": 0.42972517013549805,
      "learning_rate": 4.4454634043906616e-05,
      "loss": 2.3611,
      "step": 101620
    },
    {
      "epoch": 2.090722306938662,
      "grad_norm": 0.4386831223964691,
      "learning_rate": 4.4436167466578093e-05,
      "loss": 2.3955,
      "step": 101630
    },
    {
      "epoch": 2.0909280264555297,
      "grad_norm": 0.4175902009010315,
      "learning_rate": 4.44177036299106e-05,
      "loss": 2.3815,
      "step": 101640
    },
    {
      "epoch": 2.0911337459723978,
      "grad_norm": 0.5001564025878906,
      "learning_rate": 4.4399242534814965e-05,
      "loss": 2.3878,
      "step": 101650
    },
    {
      "epoch": 2.0913394654892654,
      "grad_norm": 0.5118852257728577,
      "learning_rate": 4.438078418220168e-05,
      "loss": 2.33,
      "step": 101660
    },
    {
      "epoch": 2.091545185006133,
      "grad_norm": 0.4347897171974182,
      "learning_rate": 4.436232857298123e-05,
      "loss": 2.3641,
      "step": 101670
    },
    {
      "epoch": 2.0917509045230007,
      "grad_norm": 0.48324647545814514,
      "learning_rate": 4.434387570806391e-05,
      "loss": 2.435,
      "step": 101680
    },
    {
      "epoch": 2.0919566240398684,
      "grad_norm": 0.4830058217048645,
      "learning_rate": 4.4325425588359916e-05,
      "loss": 2.3329,
      "step": 101690
    },
    {
      "epoch": 2.092162343556736,
      "grad_norm": 0.4252041280269623,
      "learning_rate": 4.430697821477928e-05,
      "loss": 2.4029,
      "step": 101700
    },
    {
      "epoch": 2.0923680630736037,
      "grad_norm": 0.452354371547699,
      "learning_rate": 4.428853358823193e-05,
      "loss": 2.3232,
      "step": 101710
    },
    {
      "epoch": 2.0925737825904718,
      "grad_norm": 0.43494221568107605,
      "learning_rate": 4.4270091709627625e-05,
      "loss": 2.3443,
      "step": 101720
    },
    {
      "epoch": 2.0927795021073394,
      "grad_norm": 0.4405861794948578,
      "learning_rate": 4.4251652579876e-05,
      "loss": 2.3574,
      "step": 101730
    },
    {
      "epoch": 2.092985221624207,
      "grad_norm": 0.44351130723953247,
      "learning_rate": 4.423321619988656e-05,
      "loss": 2.3712,
      "step": 101740
    },
    {
      "epoch": 2.0931909411410747,
      "grad_norm": 0.5155765414237976,
      "learning_rate": 4.421478257056868e-05,
      "loss": 2.4097,
      "step": 101750
    },
    {
      "epoch": 2.0933966606579424,
      "grad_norm": 0.45476049184799194,
      "learning_rate": 4.4196351692831584e-05,
      "loss": 2.3849,
      "step": 101760
    },
    {
      "epoch": 2.09360238017481,
      "grad_norm": 0.5142292976379395,
      "learning_rate": 4.4177923567584356e-05,
      "loss": 2.379,
      "step": 101770
    },
    {
      "epoch": 2.0938080996916777,
      "grad_norm": 0.4641205668449402,
      "learning_rate": 4.415949819573597e-05,
      "loss": 2.3594,
      "step": 101780
    },
    {
      "epoch": 2.094013819208546,
      "grad_norm": 0.43747636675834656,
      "learning_rate": 4.414107557819525e-05,
      "loss": 2.3427,
      "step": 101790
    },
    {
      "epoch": 2.0942195387254134,
      "grad_norm": 0.46970871090888977,
      "learning_rate": 4.4122655715870865e-05,
      "loss": 2.3977,
      "step": 101800
    },
    {
      "epoch": 2.094425258242281,
      "grad_norm": 0.4459516406059265,
      "learning_rate": 4.410423860967138e-05,
      "loss": 2.397,
      "step": 101810
    },
    {
      "epoch": 2.0946309777591487,
      "grad_norm": 0.45467618107795715,
      "learning_rate": 4.4085824260505216e-05,
      "loss": 2.3721,
      "step": 101820
    },
    {
      "epoch": 2.0948366972760164,
      "grad_norm": 0.4425286054611206,
      "learning_rate": 4.4067412669280675e-05,
      "loss": 2.3297,
      "step": 101830
    },
    {
      "epoch": 2.095042416792884,
      "grad_norm": 0.47545304894447327,
      "learning_rate": 4.40490038369058e-05,
      "loss": 2.401,
      "step": 101840
    },
    {
      "epoch": 2.0952481363097517,
      "grad_norm": 0.43996933102607727,
      "learning_rate": 4.403059776428871e-05,
      "loss": 2.4001,
      "step": 101850
    },
    {
      "epoch": 2.09545385582662,
      "grad_norm": 0.5201284289360046,
      "learning_rate": 4.4012194452337266e-05,
      "loss": 2.3445,
      "step": 101860
    },
    {
      "epoch": 2.0956595753434875,
      "grad_norm": 0.4474775195121765,
      "learning_rate": 4.39937939019591e-05,
      "loss": 2.3597,
      "step": 101870
    },
    {
      "epoch": 2.095865294860355,
      "grad_norm": 0.45897176861763,
      "learning_rate": 4.3975396114061926e-05,
      "loss": 2.3577,
      "step": 101880
    },
    {
      "epoch": 2.0960710143772228,
      "grad_norm": 0.4231002926826477,
      "learning_rate": 4.39570010895532e-05,
      "loss": 2.3687,
      "step": 101890
    },
    {
      "epoch": 2.0962767338940904,
      "grad_norm": 0.4385525584220886,
      "learning_rate": 4.393860882934013e-05,
      "loss": 2.3536,
      "step": 101900
    },
    {
      "epoch": 2.096482453410958,
      "grad_norm": 0.44899412989616394,
      "learning_rate": 4.392021933433007e-05,
      "loss": 2.3808,
      "step": 101910
    },
    {
      "epoch": 2.0966881729278257,
      "grad_norm": 0.48380255699157715,
      "learning_rate": 4.390183260542996e-05,
      "loss": 2.3879,
      "step": 101920
    },
    {
      "epoch": 2.096893892444694,
      "grad_norm": 0.4974973797798157,
      "learning_rate": 4.38834486435467e-05,
      "loss": 2.295,
      "step": 101930
    },
    {
      "epoch": 2.0970996119615615,
      "grad_norm": 0.46970611810684204,
      "learning_rate": 4.3865067449587203e-05,
      "loss": 2.3906,
      "step": 101940
    },
    {
      "epoch": 2.097305331478429,
      "grad_norm": 0.44469454884529114,
      "learning_rate": 4.3846689024457986e-05,
      "loss": 2.369,
      "step": 101950
    },
    {
      "epoch": 2.0975110509952968,
      "grad_norm": 0.4768025875091553,
      "learning_rate": 4.382831336906561e-05,
      "loss": 2.3458,
      "step": 101960
    },
    {
      "epoch": 2.0977167705121644,
      "grad_norm": 0.4869760274887085,
      "learning_rate": 4.380994048431642e-05,
      "loss": 2.3687,
      "step": 101970
    },
    {
      "epoch": 2.097922490029032,
      "grad_norm": 0.4948335289955139,
      "learning_rate": 4.3791570371116685e-05,
      "loss": 2.4393,
      "step": 101980
    },
    {
      "epoch": 2.0981282095458997,
      "grad_norm": 0.4651884436607361,
      "learning_rate": 4.377320303037248e-05,
      "loss": 2.3682,
      "step": 101990
    },
    {
      "epoch": 2.0983339290627674,
      "grad_norm": 0.4692099690437317,
      "learning_rate": 4.375483846298977e-05,
      "loss": 2.3821,
      "step": 102000
    },
    {
      "epoch": 2.0985396485796355,
      "grad_norm": 0.464990496635437,
      "learning_rate": 4.373647666987437e-05,
      "loss": 2.3717,
      "step": 102010
    },
    {
      "epoch": 2.098745368096503,
      "grad_norm": 0.4766272306442261,
      "learning_rate": 4.371811765193199e-05,
      "loss": 2.3907,
      "step": 102020
    },
    {
      "epoch": 2.098951087613371,
      "grad_norm": 0.47263002395629883,
      "learning_rate": 4.369976141006816e-05,
      "loss": 2.3718,
      "step": 102030
    },
    {
      "epoch": 2.0991568071302384,
      "grad_norm": 0.4671740233898163,
      "learning_rate": 4.36814079451883e-05,
      "loss": 2.3805,
      "step": 102040
    },
    {
      "epoch": 2.099362526647106,
      "grad_norm": 0.4612681567668915,
      "learning_rate": 4.366305725819768e-05,
      "loss": 2.3761,
      "step": 102050
    },
    {
      "epoch": 2.0995682461639738,
      "grad_norm": 0.43326130509376526,
      "learning_rate": 4.364470935000146e-05,
      "loss": 2.3515,
      "step": 102060
    },
    {
      "epoch": 2.0997739656808414,
      "grad_norm": 0.4499259889125824,
      "learning_rate": 4.3626364221504626e-05,
      "loss": 2.3599,
      "step": 102070
    },
    {
      "epoch": 2.0999796851977095,
      "grad_norm": 0.47218140959739685,
      "learning_rate": 4.360802187361204e-05,
      "loss": 2.3581,
      "step": 102080
    },
    {
      "epoch": 2.100185404714577,
      "grad_norm": 0.43675878643989563,
      "learning_rate": 4.358968230722843e-05,
      "loss": 2.4049,
      "step": 102090
    },
    {
      "epoch": 2.100391124231445,
      "grad_norm": 0.44582465291023254,
      "learning_rate": 4.35713455232584e-05,
      "loss": 2.3861,
      "step": 102100
    },
    {
      "epoch": 2.1005968437483125,
      "grad_norm": 0.4574514329433441,
      "learning_rate": 4.355301152260639e-05,
      "loss": 2.3985,
      "step": 102110
    },
    {
      "epoch": 2.10080256326518,
      "grad_norm": 0.49477455019950867,
      "learning_rate": 4.353468030617677e-05,
      "loss": 2.3923,
      "step": 102120
    },
    {
      "epoch": 2.1010082827820478,
      "grad_norm": 0.4879714250564575,
      "learning_rate": 4.351635187487359e-05,
      "loss": 2.3461,
      "step": 102130
    },
    {
      "epoch": 2.1012140022989154,
      "grad_norm": 0.44434911012649536,
      "learning_rate": 4.3498026229601007e-05,
      "loss": 2.3419,
      "step": 102140
    },
    {
      "epoch": 2.1014197218157835,
      "grad_norm": 0.4501716196537018,
      "learning_rate": 4.3479703371262935e-05,
      "loss": 2.3969,
      "step": 102150
    },
    {
      "epoch": 2.101625441332651,
      "grad_norm": 0.40988460183143616,
      "learning_rate": 4.3461383300763026e-05,
      "loss": 2.355,
      "step": 102160
    },
    {
      "epoch": 2.101831160849519,
      "grad_norm": 0.5295015573501587,
      "learning_rate": 4.3443066019005005e-05,
      "loss": 2.3766,
      "step": 102170
    },
    {
      "epoch": 2.1020368803663865,
      "grad_norm": 0.4262847304344177,
      "learning_rate": 4.342475152689238e-05,
      "loss": 2.3329,
      "step": 102180
    },
    {
      "epoch": 2.102242599883254,
      "grad_norm": 0.4524371325969696,
      "learning_rate": 4.3406439825328404e-05,
      "loss": 2.3312,
      "step": 102190
    },
    {
      "epoch": 2.102448319400122,
      "grad_norm": 0.4164566993713379,
      "learning_rate": 4.338813091521641e-05,
      "loss": 2.3707,
      "step": 102200
    },
    {
      "epoch": 2.1026540389169894,
      "grad_norm": 0.445279061794281,
      "learning_rate": 4.3369824797459394e-05,
      "loss": 2.3691,
      "step": 102210
    },
    {
      "epoch": 2.1028597584338575,
      "grad_norm": 0.435776025056839,
      "learning_rate": 4.335152147296028e-05,
      "loss": 2.3835,
      "step": 102220
    },
    {
      "epoch": 2.103065477950725,
      "grad_norm": 0.5480088591575623,
      "learning_rate": 4.3333220942621986e-05,
      "loss": 2.3655,
      "step": 102230
    },
    {
      "epoch": 2.103271197467593,
      "grad_norm": 0.45243993401527405,
      "learning_rate": 4.331492320734708e-05,
      "loss": 2.4023,
      "step": 102240
    },
    {
      "epoch": 2.1034769169844605,
      "grad_norm": 0.46969127655029297,
      "learning_rate": 4.329662826803808e-05,
      "loss": 2.3512,
      "step": 102250
    },
    {
      "epoch": 2.103682636501328,
      "grad_norm": 0.41605380177497864,
      "learning_rate": 4.327833612559746e-05,
      "loss": 2.3438,
      "step": 102260
    },
    {
      "epoch": 2.103888356018196,
      "grad_norm": 0.4714595079421997,
      "learning_rate": 4.326004678092741e-05,
      "loss": 2.3714,
      "step": 102270
    },
    {
      "epoch": 2.1040940755350634,
      "grad_norm": 0.4520770013332367,
      "learning_rate": 4.324176023493005e-05,
      "loss": 2.3617,
      "step": 102280
    },
    {
      "epoch": 2.104299795051931,
      "grad_norm": 0.5363025665283203,
      "learning_rate": 4.3223476488507365e-05,
      "loss": 2.3594,
      "step": 102290
    },
    {
      "epoch": 2.104505514568799,
      "grad_norm": 0.46295684576034546,
      "learning_rate": 4.3205195542561185e-05,
      "loss": 2.3622,
      "step": 102300
    },
    {
      "epoch": 2.104711234085667,
      "grad_norm": 0.45348021388053894,
      "learning_rate": 4.318691739799322e-05,
      "loss": 2.3893,
      "step": 102310
    },
    {
      "epoch": 2.1049169536025345,
      "grad_norm": 0.4622153043746948,
      "learning_rate": 4.316864205570502e-05,
      "loss": 2.3645,
      "step": 102320
    },
    {
      "epoch": 2.105122673119402,
      "grad_norm": 0.4262855052947998,
      "learning_rate": 4.315036951659801e-05,
      "loss": 2.321,
      "step": 102330
    },
    {
      "epoch": 2.10532839263627,
      "grad_norm": 0.4526243209838867,
      "learning_rate": 4.3132099781573486e-05,
      "loss": 2.3656,
      "step": 102340
    },
    {
      "epoch": 2.1055341121531375,
      "grad_norm": 0.4667002260684967,
      "learning_rate": 4.3113832851532584e-05,
      "loss": 2.4123,
      "step": 102350
    },
    {
      "epoch": 2.105739831670005,
      "grad_norm": 0.44084280729293823,
      "learning_rate": 4.309556872737632e-05,
      "loss": 2.377,
      "step": 102360
    },
    {
      "epoch": 2.105945551186873,
      "grad_norm": 0.46616804599761963,
      "learning_rate": 4.3077307410005554e-05,
      "loss": 2.345,
      "step": 102370
    },
    {
      "epoch": 2.106151270703741,
      "grad_norm": 0.4534100890159607,
      "learning_rate": 4.305904890032102e-05,
      "loss": 2.4417,
      "step": 102380
    },
    {
      "epoch": 2.1063569902206085,
      "grad_norm": 0.44360169768333435,
      "learning_rate": 4.304079319922332e-05,
      "loss": 2.3758,
      "step": 102390
    },
    {
      "epoch": 2.106562709737476,
      "grad_norm": 0.44864869117736816,
      "learning_rate": 4.302254030761289e-05,
      "loss": 2.3494,
      "step": 102400
    },
    {
      "epoch": 2.106768429254344,
      "grad_norm": 0.46427178382873535,
      "learning_rate": 4.3004290226390064e-05,
      "loss": 2.4061,
      "step": 102410
    },
    {
      "epoch": 2.1069741487712115,
      "grad_norm": 0.5073709487915039,
      "learning_rate": 4.298604295645501e-05,
      "loss": 2.3886,
      "step": 102420
    },
    {
      "epoch": 2.107179868288079,
      "grad_norm": 0.4536309242248535,
      "learning_rate": 4.2967798498707765e-05,
      "loss": 2.3544,
      "step": 102430
    },
    {
      "epoch": 2.1073855878049472,
      "grad_norm": 0.4483039081096649,
      "learning_rate": 4.2949556854048277e-05,
      "loss": 2.3759,
      "step": 102440
    },
    {
      "epoch": 2.107591307321815,
      "grad_norm": 0.46136054396629333,
      "learning_rate": 4.293131802337619e-05,
      "loss": 2.4373,
      "step": 102450
    },
    {
      "epoch": 2.1077970268386825,
      "grad_norm": 0.5104080438613892,
      "learning_rate": 4.291308200759122e-05,
      "loss": 2.366,
      "step": 102460
    },
    {
      "epoch": 2.10800274635555,
      "grad_norm": 0.41727888584136963,
      "learning_rate": 4.289484880759288e-05,
      "loss": 2.3216,
      "step": 102470
    },
    {
      "epoch": 2.108208465872418,
      "grad_norm": 0.4379061162471771,
      "learning_rate": 4.287661842428038e-05,
      "loss": 2.4117,
      "step": 102480
    },
    {
      "epoch": 2.1084141853892855,
      "grad_norm": 0.4625164568424225,
      "learning_rate": 4.2858390858553045e-05,
      "loss": 2.3547,
      "step": 102490
    },
    {
      "epoch": 2.108619904906153,
      "grad_norm": 0.4591653645038605,
      "learning_rate": 4.2840166111309945e-05,
      "loss": 2.4082,
      "step": 102500
    },
    {
      "epoch": 2.1088256244230212,
      "grad_norm": 0.4259033501148224,
      "learning_rate": 4.282194418344989e-05,
      "loss": 2.3695,
      "step": 102510
    },
    {
      "epoch": 2.109031343939889,
      "grad_norm": 0.5219757556915283,
      "learning_rate": 4.2803725075871824e-05,
      "loss": 2.3323,
      "step": 102520
    },
    {
      "epoch": 2.1092370634567565,
      "grad_norm": 0.43609753251075745,
      "learning_rate": 4.278550878947428e-05,
      "loss": 2.4359,
      "step": 102530
    },
    {
      "epoch": 2.109442782973624,
      "grad_norm": 0.41737493872642517,
      "learning_rate": 4.276729532515577e-05,
      "loss": 2.3547,
      "step": 102540
    },
    {
      "epoch": 2.109648502490492,
      "grad_norm": 0.43992170691490173,
      "learning_rate": 4.2749084683814764e-05,
      "loss": 2.3037,
      "step": 102550
    },
    {
      "epoch": 2.1098542220073595,
      "grad_norm": 0.4505115747451782,
      "learning_rate": 4.27308768663494e-05,
      "loss": 2.3766,
      "step": 102560
    },
    {
      "epoch": 2.110059941524227,
      "grad_norm": 0.4971991777420044,
      "learning_rate": 4.271267187365777e-05,
      "loss": 2.3266,
      "step": 102570
    },
    {
      "epoch": 2.1102656610410953,
      "grad_norm": 0.43123146891593933,
      "learning_rate": 4.269446970663793e-05,
      "loss": 2.3937,
      "step": 102580
    },
    {
      "epoch": 2.110471380557963,
      "grad_norm": 0.45646050572395325,
      "learning_rate": 4.267627036618758e-05,
      "loss": 2.3605,
      "step": 102590
    },
    {
      "epoch": 2.1106771000748306,
      "grad_norm": 0.4670813977718353,
      "learning_rate": 4.265807385320444e-05,
      "loss": 2.3818,
      "step": 102600
    },
    {
      "epoch": 2.110882819591698,
      "grad_norm": 0.4259374737739563,
      "learning_rate": 4.2639880168586035e-05,
      "loss": 2.3375,
      "step": 102610
    },
    {
      "epoch": 2.111088539108566,
      "grad_norm": 0.4521864950656891,
      "learning_rate": 4.2621689313229764e-05,
      "loss": 2.3166,
      "step": 102620
    },
    {
      "epoch": 2.1112942586254335,
      "grad_norm": 0.5031061172485352,
      "learning_rate": 4.26035012880329e-05,
      "loss": 2.4495,
      "step": 102630
    },
    {
      "epoch": 2.111499978142301,
      "grad_norm": 0.4445473253726959,
      "learning_rate": 4.258531609389252e-05,
      "loss": 2.3849,
      "step": 102640
    },
    {
      "epoch": 2.1117056976591693,
      "grad_norm": 0.4836522042751312,
      "learning_rate": 4.256713373170564e-05,
      "loss": 2.3594,
      "step": 102650
    },
    {
      "epoch": 2.111911417176037,
      "grad_norm": 0.4902491867542267,
      "learning_rate": 4.254895420236908e-05,
      "loss": 2.339,
      "step": 102660
    },
    {
      "epoch": 2.1121171366929046,
      "grad_norm": 0.4838384687900543,
      "learning_rate": 4.253077750677955e-05,
      "loss": 2.3404,
      "step": 102670
    },
    {
      "epoch": 2.1123228562097722,
      "grad_norm": 0.4001544117927551,
      "learning_rate": 4.2512603645833584e-05,
      "loss": 2.3991,
      "step": 102680
    },
    {
      "epoch": 2.11252857572664,
      "grad_norm": 0.4876827597618103,
      "learning_rate": 4.2494432620427604e-05,
      "loss": 2.3954,
      "step": 102690
    },
    {
      "epoch": 2.1127342952435075,
      "grad_norm": 0.4575216472148895,
      "learning_rate": 4.2476264431457916e-05,
      "loss": 2.3556,
      "step": 102700
    },
    {
      "epoch": 2.112940014760375,
      "grad_norm": 0.42520034313201904,
      "learning_rate": 4.2458099079820614e-05,
      "loss": 2.3515,
      "step": 102710
    },
    {
      "epoch": 2.113145734277243,
      "grad_norm": 0.48157715797424316,
      "learning_rate": 4.243993656641173e-05,
      "loss": 2.3289,
      "step": 102720
    },
    {
      "epoch": 2.113351453794111,
      "grad_norm": 0.448544442653656,
      "learning_rate": 4.2421776892127106e-05,
      "loss": 2.4018,
      "step": 102730
    },
    {
      "epoch": 2.1135571733109786,
      "grad_norm": 0.49365881085395813,
      "learning_rate": 4.240362005786247e-05,
      "loss": 2.3513,
      "step": 102740
    },
    {
      "epoch": 2.1137628928278462,
      "grad_norm": 0.48631829023361206,
      "learning_rate": 4.238546606451339e-05,
      "loss": 2.3514,
      "step": 102750
    },
    {
      "epoch": 2.113968612344714,
      "grad_norm": 0.40556085109710693,
      "learning_rate": 4.236731491297535e-05,
      "loss": 2.3669,
      "step": 102760
    },
    {
      "epoch": 2.1141743318615815,
      "grad_norm": 0.45659568905830383,
      "learning_rate": 4.234916660414352e-05,
      "loss": 2.3422,
      "step": 102770
    },
    {
      "epoch": 2.114380051378449,
      "grad_norm": 0.43866363167762756,
      "learning_rate": 4.2331021138913196e-05,
      "loss": 2.3875,
      "step": 102780
    },
    {
      "epoch": 2.114585770895317,
      "grad_norm": 0.4580429494380951,
      "learning_rate": 4.2312878518179364e-05,
      "loss": 2.3758,
      "step": 102790
    },
    {
      "epoch": 2.114791490412185,
      "grad_norm": 0.438403844833374,
      "learning_rate": 4.2294738742836806e-05,
      "loss": 2.3239,
      "step": 102800
    },
    {
      "epoch": 2.1149972099290526,
      "grad_norm": 0.45448189973831177,
      "learning_rate": 4.227660181378037e-05,
      "loss": 2.3502,
      "step": 102810
    },
    {
      "epoch": 2.1152029294459203,
      "grad_norm": 0.43196961283683777,
      "learning_rate": 4.225846773190465e-05,
      "loss": 2.3505,
      "step": 102820
    },
    {
      "epoch": 2.115408648962788,
      "grad_norm": 0.4164738059043884,
      "learning_rate": 4.224033649810398e-05,
      "loss": 2.3404,
      "step": 102830
    },
    {
      "epoch": 2.1156143684796556,
      "grad_norm": 0.4958520531654358,
      "learning_rate": 4.222220811327284e-05,
      "loss": 2.4004,
      "step": 102840
    },
    {
      "epoch": 2.115820087996523,
      "grad_norm": 0.45724084973335266,
      "learning_rate": 4.220408257830528e-05,
      "loss": 2.3401,
      "step": 102850
    },
    {
      "epoch": 2.116025807513391,
      "grad_norm": 0.5281282663345337,
      "learning_rate": 4.218595989409535e-05,
      "loss": 2.4068,
      "step": 102860
    },
    {
      "epoch": 2.116231527030259,
      "grad_norm": 0.4581701457500458,
      "learning_rate": 4.2167840061537034e-05,
      "loss": 2.3705,
      "step": 102870
    },
    {
      "epoch": 2.1164372465471266,
      "grad_norm": 0.4461882412433624,
      "learning_rate": 4.2149723081523984e-05,
      "loss": 2.3566,
      "step": 102880
    },
    {
      "epoch": 2.1166429660639943,
      "grad_norm": 0.5126150250434875,
      "learning_rate": 4.213160895494982e-05,
      "loss": 2.3985,
      "step": 102890
    },
    {
      "epoch": 2.116848685580862,
      "grad_norm": 0.4303469657897949,
      "learning_rate": 4.21134976827081e-05,
      "loss": 2.3473,
      "step": 102900
    },
    {
      "epoch": 2.1170544050977296,
      "grad_norm": 0.4420473277568817,
      "learning_rate": 4.2095389265692064e-05,
      "loss": 2.3931,
      "step": 102910
    },
    {
      "epoch": 2.1172601246145972,
      "grad_norm": 0.4696033000946045,
      "learning_rate": 4.2077283704794944e-05,
      "loss": 2.3842,
      "step": 102920
    },
    {
      "epoch": 2.117465844131465,
      "grad_norm": 0.4627602696418762,
      "learning_rate": 4.205918100090976e-05,
      "loss": 2.3402,
      "step": 102930
    },
    {
      "epoch": 2.117671563648333,
      "grad_norm": 0.5060898661613464,
      "learning_rate": 4.204108115492945e-05,
      "loss": 2.3424,
      "step": 102940
    },
    {
      "epoch": 2.1178772831652006,
      "grad_norm": 0.48061084747314453,
      "learning_rate": 4.202298416774677e-05,
      "loss": 2.4127,
      "step": 102950
    },
    {
      "epoch": 2.1180830026820683,
      "grad_norm": 0.435442715883255,
      "learning_rate": 4.200489004025434e-05,
      "loss": 2.3028,
      "step": 102960
    },
    {
      "epoch": 2.118288722198936,
      "grad_norm": 0.4991803467273712,
      "learning_rate": 4.198679877334465e-05,
      "loss": 2.382,
      "step": 102970
    },
    {
      "epoch": 2.1184944417158036,
      "grad_norm": 0.42345988750457764,
      "learning_rate": 4.1968710367910046e-05,
      "loss": 2.3681,
      "step": 102980
    },
    {
      "epoch": 2.1187001612326712,
      "grad_norm": 0.4361538887023926,
      "learning_rate": 4.195062482484272e-05,
      "loss": 2.3859,
      "step": 102990
    },
    {
      "epoch": 2.118905880749539,
      "grad_norm": 0.4329405426979065,
      "learning_rate": 4.193254214503475e-05,
      "loss": 2.4098,
      "step": 103000
    },
    {
      "epoch": 2.1191116002664065,
      "grad_norm": 0.455394446849823,
      "learning_rate": 4.1914462329378034e-05,
      "loss": 2.3418,
      "step": 103010
    },
    {
      "epoch": 2.1193173197832746,
      "grad_norm": 0.4264543354511261,
      "learning_rate": 4.189638537876437e-05,
      "loss": 2.3699,
      "step": 103020
    },
    {
      "epoch": 2.1195230393001423,
      "grad_norm": 0.44741228222846985,
      "learning_rate": 4.187831129408538e-05,
      "loss": 2.3666,
      "step": 103030
    },
    {
      "epoch": 2.11972875881701,
      "grad_norm": 0.4717406630516052,
      "learning_rate": 4.1860240076232585e-05,
      "loss": 2.4129,
      "step": 103040
    },
    {
      "epoch": 2.1199344783338776,
      "grad_norm": 0.4463454782962799,
      "learning_rate": 4.1842171726097315e-05,
      "loss": 2.3692,
      "step": 103050
    },
    {
      "epoch": 2.1201401978507453,
      "grad_norm": 0.4430302679538727,
      "learning_rate": 4.1824106244570783e-05,
      "loss": 2.3697,
      "step": 103060
    },
    {
      "epoch": 2.120345917367613,
      "grad_norm": 0.43416598439216614,
      "learning_rate": 4.1806043632544076e-05,
      "loss": 2.4108,
      "step": 103070
    },
    {
      "epoch": 2.1205516368844806,
      "grad_norm": 0.4843638837337494,
      "learning_rate": 4.178798389090816e-05,
      "loss": 2.3891,
      "step": 103080
    },
    {
      "epoch": 2.1207573564013487,
      "grad_norm": 0.4780138432979584,
      "learning_rate": 4.1769927020553704e-05,
      "loss": 2.3837,
      "step": 103090
    },
    {
      "epoch": 2.1209630759182163,
      "grad_norm": 0.4421308636665344,
      "learning_rate": 4.175187302237147e-05,
      "loss": 2.3135,
      "step": 103100
    },
    {
      "epoch": 2.121168795435084,
      "grad_norm": 0.4719792902469635,
      "learning_rate": 4.173382189725196e-05,
      "loss": 2.374,
      "step": 103110
    },
    {
      "epoch": 2.1213745149519516,
      "grad_norm": 0.43154996633529663,
      "learning_rate": 4.171577364608543e-05,
      "loss": 2.3822,
      "step": 103120
    },
    {
      "epoch": 2.1215802344688193,
      "grad_norm": 0.4392687976360321,
      "learning_rate": 4.169772826976225e-05,
      "loss": 2.3582,
      "step": 103130
    },
    {
      "epoch": 2.121785953985687,
      "grad_norm": 0.4610092043876648,
      "learning_rate": 4.1679685769172384e-05,
      "loss": 2.3664,
      "step": 103140
    },
    {
      "epoch": 2.1219916735025546,
      "grad_norm": 0.4530739188194275,
      "learning_rate": 4.166164614520578e-05,
      "loss": 2.3519,
      "step": 103150
    },
    {
      "epoch": 2.1221973930194227,
      "grad_norm": 0.44258150458335876,
      "learning_rate": 4.1643609398752334e-05,
      "loss": 2.4251,
      "step": 103160
    },
    {
      "epoch": 2.1224031125362903,
      "grad_norm": 0.42742720246315,
      "learning_rate": 4.1625575530701597e-05,
      "loss": 2.4263,
      "step": 103170
    },
    {
      "epoch": 2.122608832053158,
      "grad_norm": 0.5104546546936035,
      "learning_rate": 4.160754454194309e-05,
      "loss": 2.3763,
      "step": 103180
    },
    {
      "epoch": 2.1228145515700256,
      "grad_norm": 0.47905588150024414,
      "learning_rate": 4.158951643336627e-05,
      "loss": 2.3497,
      "step": 103190
    },
    {
      "epoch": 2.1230202710868933,
      "grad_norm": 0.4469675123691559,
      "learning_rate": 4.157149120586027e-05,
      "loss": 2.3853,
      "step": 103200
    },
    {
      "epoch": 2.123225990603761,
      "grad_norm": 0.4411224126815796,
      "learning_rate": 4.1553468860314214e-05,
      "loss": 2.3513,
      "step": 103210
    },
    {
      "epoch": 2.1234317101206286,
      "grad_norm": 0.45123347640037537,
      "learning_rate": 4.153544939761704e-05,
      "loss": 2.3722,
      "step": 103220
    },
    {
      "epoch": 2.1236374296374967,
      "grad_norm": 0.48088815808296204,
      "learning_rate": 4.1517432818657565e-05,
      "loss": 2.3685,
      "step": 103230
    },
    {
      "epoch": 2.1238431491543643,
      "grad_norm": 0.40478795766830444,
      "learning_rate": 4.1499419124324436e-05,
      "loss": 2.3113,
      "step": 103240
    },
    {
      "epoch": 2.124048868671232,
      "grad_norm": 0.4809674024581909,
      "learning_rate": 4.148140831550617e-05,
      "loss": 2.343,
      "step": 103250
    },
    {
      "epoch": 2.1242545881880996,
      "grad_norm": 0.5026120543479919,
      "learning_rate": 4.146340039309113e-05,
      "loss": 2.3737,
      "step": 103260
    },
    {
      "epoch": 2.1244603077049673,
      "grad_norm": 0.44749078154563904,
      "learning_rate": 4.1445395357967576e-05,
      "loss": 2.3795,
      "step": 103270
    },
    {
      "epoch": 2.124666027221835,
      "grad_norm": 0.47146308422088623,
      "learning_rate": 4.142739321102358e-05,
      "loss": 2.4141,
      "step": 103280
    },
    {
      "epoch": 2.1248717467387026,
      "grad_norm": 0.5423564314842224,
      "learning_rate": 4.140939395314709e-05,
      "loss": 2.4142,
      "step": 103290
    },
    {
      "epoch": 2.1250774662555707,
      "grad_norm": 0.432584285736084,
      "learning_rate": 4.139139758522592e-05,
      "loss": 2.3763,
      "step": 103300
    },
    {
      "epoch": 2.1252831857724384,
      "grad_norm": 0.45767006278038025,
      "learning_rate": 4.137340410814774e-05,
      "loss": 2.3663,
      "step": 103310
    },
    {
      "epoch": 2.125488905289306,
      "grad_norm": 0.46026888489723206,
      "learning_rate": 4.135541352280005e-05,
      "loss": 2.4245,
      "step": 103320
    },
    {
      "epoch": 2.1256946248061737,
      "grad_norm": 0.4345746338367462,
      "learning_rate": 4.133742583007023e-05,
      "loss": 2.3851,
      "step": 103330
    },
    {
      "epoch": 2.1259003443230413,
      "grad_norm": 0.44449159502983093,
      "learning_rate": 4.1319441030845526e-05,
      "loss": 2.3954,
      "step": 103340
    },
    {
      "epoch": 2.126106063839909,
      "grad_norm": 0.4679183065891266,
      "learning_rate": 4.130145912601302e-05,
      "loss": 2.3578,
      "step": 103350
    },
    {
      "epoch": 2.1263117833567766,
      "grad_norm": 0.5011098980903625,
      "learning_rate": 4.128348011645967e-05,
      "loss": 2.3745,
      "step": 103360
    },
    {
      "epoch": 2.1265175028736447,
      "grad_norm": 0.4898189902305603,
      "learning_rate": 4.126550400307232e-05,
      "loss": 2.3957,
      "step": 103370
    },
    {
      "epoch": 2.1267232223905124,
      "grad_norm": 0.4176974296569824,
      "learning_rate": 4.124753078673752e-05,
      "loss": 2.3442,
      "step": 103380
    },
    {
      "epoch": 2.12692894190738,
      "grad_norm": 0.43434062600135803,
      "learning_rate": 4.12295604683419e-05,
      "loss": 2.4011,
      "step": 103390
    },
    {
      "epoch": 2.1271346614242477,
      "grad_norm": 0.4359341561794281,
      "learning_rate": 4.1211593048771836e-05,
      "loss": 2.4007,
      "step": 103400
    },
    {
      "epoch": 2.1273403809411153,
      "grad_norm": 0.5008542537689209,
      "learning_rate": 4.1193628528913455e-05,
      "loss": 2.3549,
      "step": 103410
    },
    {
      "epoch": 2.127546100457983,
      "grad_norm": 0.4265229403972626,
      "learning_rate": 4.1175666909652954e-05,
      "loss": 2.3398,
      "step": 103420
    },
    {
      "epoch": 2.1277518199748506,
      "grad_norm": 0.45311400294303894,
      "learning_rate": 4.1157708191876296e-05,
      "loss": 2.375,
      "step": 103430
    },
    {
      "epoch": 2.1279575394917183,
      "grad_norm": 0.4557253122329712,
      "learning_rate": 4.1139752376469164e-05,
      "loss": 2.418,
      "step": 103440
    },
    {
      "epoch": 2.1281632590085864,
      "grad_norm": 0.41911840438842773,
      "learning_rate": 4.112179946431737e-05,
      "loss": 2.4076,
      "step": 103450
    },
    {
      "epoch": 2.128368978525454,
      "grad_norm": 0.45945602655410767,
      "learning_rate": 4.1103849456306324e-05,
      "loss": 2.3665,
      "step": 103460
    },
    {
      "epoch": 2.1285746980423217,
      "grad_norm": 0.44539642333984375,
      "learning_rate": 4.10859023533214e-05,
      "loss": 2.4108,
      "step": 103470
    },
    {
      "epoch": 2.1287804175591893,
      "grad_norm": 0.43194782733917236,
      "learning_rate": 4.106795815624795e-05,
      "loss": 2.3667,
      "step": 103480
    },
    {
      "epoch": 2.128986137076057,
      "grad_norm": 0.44866830110549927,
      "learning_rate": 4.105001686597094e-05,
      "loss": 2.3916,
      "step": 103490
    },
    {
      "epoch": 2.1291918565929246,
      "grad_norm": 0.5066178441047668,
      "learning_rate": 4.1032078483375326e-05,
      "loss": 2.3619,
      "step": 103500
    },
    {
      "epoch": 2.1293975761097923,
      "grad_norm": 0.4452197253704071,
      "learning_rate": 4.1014143009346014e-05,
      "loss": 2.3537,
      "step": 103510
    },
    {
      "epoch": 2.1296032956266604,
      "grad_norm": 0.4474986791610718,
      "learning_rate": 4.099621044476756e-05,
      "loss": 2.4068,
      "step": 103520
    },
    {
      "epoch": 2.129809015143528,
      "grad_norm": 0.4817008674144745,
      "learning_rate": 4.0978280790524505e-05,
      "loss": 2.3537,
      "step": 103530
    },
    {
      "epoch": 2.1300147346603957,
      "grad_norm": 0.472474604845047,
      "learning_rate": 4.096035404750124e-05,
      "loss": 2.3347,
      "step": 103540
    },
    {
      "epoch": 2.1302204541772634,
      "grad_norm": 0.4778900444507599,
      "learning_rate": 4.094243021658197e-05,
      "loss": 2.3524,
      "step": 103550
    },
    {
      "epoch": 2.130426173694131,
      "grad_norm": 0.4595556855201721,
      "learning_rate": 4.09245092986508e-05,
      "loss": 2.4258,
      "step": 103560
    },
    {
      "epoch": 2.1306318932109987,
      "grad_norm": 0.41605043411254883,
      "learning_rate": 4.090659129459167e-05,
      "loss": 2.4229,
      "step": 103570
    },
    {
      "epoch": 2.1308376127278663,
      "grad_norm": 0.4630662798881531,
      "learning_rate": 4.088867620528836e-05,
      "loss": 2.3967,
      "step": 103580
    },
    {
      "epoch": 2.1310433322447344,
      "grad_norm": 0.4422346353530884,
      "learning_rate": 4.0870764031624534e-05,
      "loss": 2.3952,
      "step": 103590
    },
    {
      "epoch": 2.131249051761602,
      "grad_norm": 0.4422447085380554,
      "learning_rate": 4.0852854774483705e-05,
      "loss": 2.4079,
      "step": 103600
    },
    {
      "epoch": 2.1314547712784697,
      "grad_norm": 0.4456024467945099,
      "learning_rate": 4.0834948434749244e-05,
      "loss": 2.3523,
      "step": 103610
    },
    {
      "epoch": 2.1316604907953374,
      "grad_norm": 0.4338642656803131,
      "learning_rate": 4.0817045013304355e-05,
      "loss": 2.3734,
      "step": 103620
    },
    {
      "epoch": 2.131866210312205,
      "grad_norm": 0.4244360327720642,
      "learning_rate": 4.079914451103214e-05,
      "loss": 2.329,
      "step": 103630
    },
    {
      "epoch": 2.1320719298290727,
      "grad_norm": 0.515972375869751,
      "learning_rate": 4.078124692881551e-05,
      "loss": 2.3528,
      "step": 103640
    },
    {
      "epoch": 2.1322776493459403,
      "grad_norm": 0.4557124376296997,
      "learning_rate": 4.0763352267537266e-05,
      "loss": 2.3895,
      "step": 103650
    },
    {
      "epoch": 2.1324833688628084,
      "grad_norm": 0.42152392864227295,
      "learning_rate": 4.074546052808006e-05,
      "loss": 2.3461,
      "step": 103660
    },
    {
      "epoch": 2.132689088379676,
      "grad_norm": 0.4293709397315979,
      "learning_rate": 4.072757171132637e-05,
      "loss": 2.3654,
      "step": 103670
    },
    {
      "epoch": 2.1328948078965437,
      "grad_norm": 0.45986032485961914,
      "learning_rate": 4.07096858181586e-05,
      "loss": 2.283,
      "step": 103680
    },
    {
      "epoch": 2.1331005274134114,
      "grad_norm": 0.443472295999527,
      "learning_rate": 4.0691802849458946e-05,
      "loss": 2.4217,
      "step": 103690
    },
    {
      "epoch": 2.133306246930279,
      "grad_norm": 0.46760693192481995,
      "learning_rate": 4.067392280610941e-05,
      "loss": 2.3965,
      "step": 103700
    },
    {
      "epoch": 2.1335119664471467,
      "grad_norm": 0.42602378129959106,
      "learning_rate": 4.0656045688991996e-05,
      "loss": 2.3689,
      "step": 103710
    },
    {
      "epoch": 2.1337176859640143,
      "grad_norm": 0.4360557496547699,
      "learning_rate": 4.0638171498988506e-05,
      "loss": 2.3313,
      "step": 103720
    },
    {
      "epoch": 2.133923405480882,
      "grad_norm": 0.452412873506546,
      "learning_rate": 4.062030023698047e-05,
      "loss": 2.3519,
      "step": 103730
    },
    {
      "epoch": 2.13412912499775,
      "grad_norm": 0.42602020502090454,
      "learning_rate": 4.060243190384947e-05,
      "loss": 2.3999,
      "step": 103740
    },
    {
      "epoch": 2.1343348445146177,
      "grad_norm": 0.437383234500885,
      "learning_rate": 4.058456650047686e-05,
      "loss": 2.3734,
      "step": 103750
    },
    {
      "epoch": 2.1345405640314854,
      "grad_norm": 0.4612447917461395,
      "learning_rate": 4.056670402774374e-05,
      "loss": 2.4212,
      "step": 103760
    },
    {
      "epoch": 2.134746283548353,
      "grad_norm": 0.44062262773513794,
      "learning_rate": 4.05488444865313e-05,
      "loss": 2.3743,
      "step": 103770
    },
    {
      "epoch": 2.1349520030652207,
      "grad_norm": 0.45573732256889343,
      "learning_rate": 4.0530987877720375e-05,
      "loss": 2.3707,
      "step": 103780
    },
    {
      "epoch": 2.1351577225820884,
      "grad_norm": 0.4572801887989044,
      "learning_rate": 4.051313420219171e-05,
      "loss": 2.3933,
      "step": 103790
    },
    {
      "epoch": 2.1353634420989565,
      "grad_norm": 0.4435082674026489,
      "learning_rate": 4.049528346082603e-05,
      "loss": 2.3936,
      "step": 103800
    },
    {
      "epoch": 2.135569161615824,
      "grad_norm": 0.5269014835357666,
      "learning_rate": 4.047743565450372e-05,
      "loss": 2.404,
      "step": 103810
    },
    {
      "epoch": 2.1357748811326918,
      "grad_norm": 0.4298485815525055,
      "learning_rate": 4.0459590784105115e-05,
      "loss": 2.3803,
      "step": 103820
    },
    {
      "epoch": 2.1359806006495594,
      "grad_norm": 0.4903249740600586,
      "learning_rate": 4.044174885051052e-05,
      "loss": 2.3651,
      "step": 103830
    },
    {
      "epoch": 2.136186320166427,
      "grad_norm": 0.43730437755584717,
      "learning_rate": 4.042390985459986e-05,
      "loss": 2.3247,
      "step": 103840
    },
    {
      "epoch": 2.1363920396832947,
      "grad_norm": 0.5283542275428772,
      "learning_rate": 4.0406073797253084e-05,
      "loss": 2.3911,
      "step": 103850
    },
    {
      "epoch": 2.1365977592001624,
      "grad_norm": 0.4802139103412628,
      "learning_rate": 4.038824067934993e-05,
      "loss": 2.3829,
      "step": 103860
    },
    {
      "epoch": 2.13680347871703,
      "grad_norm": 0.6191655993461609,
      "learning_rate": 4.037041050177003e-05,
      "loss": 2.4209,
      "step": 103870
    },
    {
      "epoch": 2.137009198233898,
      "grad_norm": 0.6367941498756409,
      "learning_rate": 4.035258326539282e-05,
      "loss": 2.4297,
      "step": 103880
    },
    {
      "epoch": 2.1372149177507658,
      "grad_norm": 0.45688003301620483,
      "learning_rate": 4.033475897109765e-05,
      "loss": 2.4357,
      "step": 103890
    },
    {
      "epoch": 2.1374206372676334,
      "grad_norm": 0.42742273211479187,
      "learning_rate": 4.031693761976368e-05,
      "loss": 2.3526,
      "step": 103900
    },
    {
      "epoch": 2.137626356784501,
      "grad_norm": 0.4772231876850128,
      "learning_rate": 4.029911921226994e-05,
      "loss": 2.3212,
      "step": 103910
    },
    {
      "epoch": 2.1378320763013687,
      "grad_norm": 0.473839670419693,
      "learning_rate": 4.0281303749495326e-05,
      "loss": 2.3258,
      "step": 103920
    },
    {
      "epoch": 2.1380377958182364,
      "grad_norm": 0.46208661794662476,
      "learning_rate": 4.026349123231857e-05,
      "loss": 2.3359,
      "step": 103930
    },
    {
      "epoch": 2.138243515335104,
      "grad_norm": 0.45218145847320557,
      "learning_rate": 4.024568166161826e-05,
      "loss": 2.3971,
      "step": 103940
    },
    {
      "epoch": 2.138449234851972,
      "grad_norm": 0.38426125049591064,
      "learning_rate": 4.022787503827286e-05,
      "loss": 2.2983,
      "step": 103950
    },
    {
      "epoch": 2.13865495436884,
      "grad_norm": 0.4416927099227905,
      "learning_rate": 4.0210071363160664e-05,
      "loss": 2.4099,
      "step": 103960
    },
    {
      "epoch": 2.1388606738857074,
      "grad_norm": 0.4457970857620239,
      "learning_rate": 4.019227063715982e-05,
      "loss": 2.3528,
      "step": 103970
    },
    {
      "epoch": 2.139066393402575,
      "grad_norm": 0.5060065984725952,
      "learning_rate": 4.017447286114836e-05,
      "loss": 2.3374,
      "step": 103980
    },
    {
      "epoch": 2.1392721129194427,
      "grad_norm": 0.46326902508735657,
      "learning_rate": 4.0156678036004146e-05,
      "loss": 2.4123,
      "step": 103990
    },
    {
      "epoch": 2.1394778324363104,
      "grad_norm": 0.448377400636673,
      "learning_rate": 4.013888616260489e-05,
      "loss": 2.38,
      "step": 104000
    },
    {
      "epoch": 2.139683551953178,
      "grad_norm": 0.42734798789024353,
      "learning_rate": 4.012109724182822e-05,
      "loss": 2.3776,
      "step": 104010
    },
    {
      "epoch": 2.139889271470046,
      "grad_norm": 0.5216462016105652,
      "learning_rate": 4.010331127455145e-05,
      "loss": 2.3902,
      "step": 104020
    },
    {
      "epoch": 2.140094990986914,
      "grad_norm": 0.4869300127029419,
      "learning_rate": 4.008552826165197e-05,
      "loss": 2.4278,
      "step": 104030
    },
    {
      "epoch": 2.1403007105037815,
      "grad_norm": 0.4721525013446808,
      "learning_rate": 4.006774820400693e-05,
      "loss": 2.341,
      "step": 104040
    },
    {
      "epoch": 2.140506430020649,
      "grad_norm": 0.5064951777458191,
      "learning_rate": 4.00499711024932e-05,
      "loss": 2.4211,
      "step": 104050
    },
    {
      "epoch": 2.1407121495375168,
      "grad_norm": 0.4518496096134186,
      "learning_rate": 4.0032196957987746e-05,
      "loss": 2.3496,
      "step": 104060
    },
    {
      "epoch": 2.1409178690543844,
      "grad_norm": 0.4494209587574005,
      "learning_rate": 4.001442577136726e-05,
      "loss": 2.3344,
      "step": 104070
    },
    {
      "epoch": 2.141123588571252,
      "grad_norm": 0.4428167939186096,
      "learning_rate": 3.9996657543508196e-05,
      "loss": 2.3813,
      "step": 104080
    },
    {
      "epoch": 2.14132930808812,
      "grad_norm": 0.4397243559360504,
      "learning_rate": 3.997889227528711e-05,
      "loss": 2.3706,
      "step": 104090
    },
    {
      "epoch": 2.141535027604988,
      "grad_norm": 0.4557649493217468,
      "learning_rate": 3.996112996758016e-05,
      "loss": 2.439,
      "step": 104100
    },
    {
      "epoch": 2.1417407471218555,
      "grad_norm": 0.4658229649066925,
      "learning_rate": 3.994337062126346e-05,
      "loss": 2.3549,
      "step": 104110
    },
    {
      "epoch": 2.141946466638723,
      "grad_norm": 0.5295984745025635,
      "learning_rate": 3.9925614237213096e-05,
      "loss": 2.425,
      "step": 104120
    },
    {
      "epoch": 2.1421521861555908,
      "grad_norm": 0.4400967061519623,
      "learning_rate": 3.990786081630476e-05,
      "loss": 2.3482,
      "step": 104130
    },
    {
      "epoch": 2.1423579056724584,
      "grad_norm": 0.4425058960914612,
      "learning_rate": 3.9890110359414166e-05,
      "loss": 2.3876,
      "step": 104140
    },
    {
      "epoch": 2.142563625189326,
      "grad_norm": 0.4548555314540863,
      "learning_rate": 3.987236286741695e-05,
      "loss": 2.4117,
      "step": 104150
    },
    {
      "epoch": 2.1427693447061937,
      "grad_norm": 0.4987005889415741,
      "learning_rate": 3.9854618341188366e-05,
      "loss": 2.418,
      "step": 104160
    },
    {
      "epoch": 2.142975064223062,
      "grad_norm": 0.500413179397583,
      "learning_rate": 3.98368767816037e-05,
      "loss": 2.4475,
      "step": 104170
    },
    {
      "epoch": 2.1431807837399295,
      "grad_norm": 0.43369805812835693,
      "learning_rate": 3.9819138189538066e-05,
      "loss": 2.4361,
      "step": 104180
    },
    {
      "epoch": 2.143386503256797,
      "grad_norm": 0.43209150433540344,
      "learning_rate": 3.98014025658664e-05,
      "loss": 2.3769,
      "step": 104190
    },
    {
      "epoch": 2.143592222773665,
      "grad_norm": 0.44761812686920166,
      "learning_rate": 3.9783669911463496e-05,
      "loss": 2.4009,
      "step": 104200
    },
    {
      "epoch": 2.1437979422905324,
      "grad_norm": 0.44876089692115784,
      "learning_rate": 3.976594022720401e-05,
      "loss": 2.3776,
      "step": 104210
    },
    {
      "epoch": 2.1440036618074,
      "grad_norm": 0.45680055022239685,
      "learning_rate": 3.974821351396247e-05,
      "loss": 2.3472,
      "step": 104220
    },
    {
      "epoch": 2.144209381324268,
      "grad_norm": 0.4490956962108612,
      "learning_rate": 3.973048977261322e-05,
      "loss": 2.3872,
      "step": 104230
    },
    {
      "epoch": 2.144415100841136,
      "grad_norm": 0.45290008187294006,
      "learning_rate": 3.971276900403047e-05,
      "loss": 2.3293,
      "step": 104240
    },
    {
      "epoch": 2.1446208203580035,
      "grad_norm": 0.5407864451408386,
      "learning_rate": 3.9695051209088306e-05,
      "loss": 2.372,
      "step": 104250
    },
    {
      "epoch": 2.144826539874871,
      "grad_norm": 0.4796225428581238,
      "learning_rate": 3.9677336388660647e-05,
      "loss": 2.3931,
      "step": 104260
    },
    {
      "epoch": 2.145032259391739,
      "grad_norm": 0.4316386878490448,
      "learning_rate": 3.965962454362127e-05,
      "loss": 2.3415,
      "step": 104270
    },
    {
      "epoch": 2.1452379789086065,
      "grad_norm": 0.45612552762031555,
      "learning_rate": 3.9641915674843785e-05,
      "loss": 2.392,
      "step": 104280
    },
    {
      "epoch": 2.145443698425474,
      "grad_norm": 0.487408310174942,
      "learning_rate": 3.9624209783201693e-05,
      "loss": 2.4138,
      "step": 104290
    },
    {
      "epoch": 2.1456494179423418,
      "grad_norm": 0.42949360609054565,
      "learning_rate": 3.960650686956836e-05,
      "loss": 2.3343,
      "step": 104300
    },
    {
      "epoch": 2.14585513745921,
      "grad_norm": 0.4678867757320404,
      "learning_rate": 3.9588806934816866e-05,
      "loss": 2.388,
      "step": 104310
    },
    {
      "epoch": 2.1460608569760775,
      "grad_norm": 0.4905627965927124,
      "learning_rate": 3.957110997982037e-05,
      "loss": 2.3926,
      "step": 104320
    },
    {
      "epoch": 2.146266576492945,
      "grad_norm": 0.65894615650177,
      "learning_rate": 3.955341600545174e-05,
      "loss": 2.3355,
      "step": 104330
    },
    {
      "epoch": 2.146472296009813,
      "grad_norm": 0.48405638337135315,
      "learning_rate": 3.953572501258363e-05,
      "loss": 2.4048,
      "step": 104340
    },
    {
      "epoch": 2.1466780155266805,
      "grad_norm": 0.5036649107933044,
      "learning_rate": 3.951803700208877e-05,
      "loss": 2.397,
      "step": 104350
    },
    {
      "epoch": 2.146883735043548,
      "grad_norm": 0.5483876466751099,
      "learning_rate": 3.950035197483958e-05,
      "loss": 2.3786,
      "step": 104360
    },
    {
      "epoch": 2.1470894545604158,
      "grad_norm": 0.4585094749927521,
      "learning_rate": 3.9482669931708286e-05,
      "loss": 2.3094,
      "step": 104370
    },
    {
      "epoch": 2.147295174077284,
      "grad_norm": 0.46958765387535095,
      "learning_rate": 3.946499087356715e-05,
      "loss": 2.363,
      "step": 104380
    },
    {
      "epoch": 2.1475008935941515,
      "grad_norm": 0.4377448558807373,
      "learning_rate": 3.944731480128813e-05,
      "loss": 2.3618,
      "step": 104390
    },
    {
      "epoch": 2.147706613111019,
      "grad_norm": 0.4216776192188263,
      "learning_rate": 3.942964171574305e-05,
      "loss": 2.3516,
      "step": 104400
    },
    {
      "epoch": 2.147912332627887,
      "grad_norm": 0.40867292881011963,
      "learning_rate": 3.941197161780374e-05,
      "loss": 2.3417,
      "step": 104410
    },
    {
      "epoch": 2.1481180521447545,
      "grad_norm": 0.46391168236732483,
      "learning_rate": 3.9394304508341674e-05,
      "loss": 2.4079,
      "step": 104420
    },
    {
      "epoch": 2.148323771661622,
      "grad_norm": 0.5517557263374329,
      "learning_rate": 3.937664038822827e-05,
      "loss": 2.3642,
      "step": 104430
    },
    {
      "epoch": 2.14852949117849,
      "grad_norm": 0.44272011518478394,
      "learning_rate": 3.9358979258334915e-05,
      "loss": 2.3559,
      "step": 104440
    },
    {
      "epoch": 2.148735210695358,
      "grad_norm": 0.49277934432029724,
      "learning_rate": 3.934132111953261e-05,
      "loss": 2.3295,
      "step": 104450
    },
    {
      "epoch": 2.1489409302122255,
      "grad_norm": 0.45676153898239136,
      "learning_rate": 3.93236659726924e-05,
      "loss": 2.3348,
      "step": 104460
    },
    {
      "epoch": 2.149146649729093,
      "grad_norm": 0.44344857335090637,
      "learning_rate": 3.9306013818685094e-05,
      "loss": 2.3931,
      "step": 104470
    },
    {
      "epoch": 2.149352369245961,
      "grad_norm": 0.42873725295066833,
      "learning_rate": 3.928836465838138e-05,
      "loss": 2.3757,
      "step": 104480
    },
    {
      "epoch": 2.1495580887628285,
      "grad_norm": 0.477674275636673,
      "learning_rate": 3.9270718492651803e-05,
      "loss": 2.3182,
      "step": 104490
    },
    {
      "epoch": 2.149763808279696,
      "grad_norm": 0.45499366521835327,
      "learning_rate": 3.925307532236675e-05,
      "loss": 2.3061,
      "step": 104500
    },
    {
      "epoch": 2.149969527796564,
      "grad_norm": 0.4614032208919525,
      "learning_rate": 3.923543514839646e-05,
      "loss": 2.4419,
      "step": 104510
    },
    {
      "epoch": 2.150175247313432,
      "grad_norm": 0.4554220139980316,
      "learning_rate": 3.9217797971611024e-05,
      "loss": 2.3982,
      "step": 104520
    },
    {
      "epoch": 2.1503809668302996,
      "grad_norm": 0.44136789441108704,
      "learning_rate": 3.920016379288039e-05,
      "loss": 2.4021,
      "step": 104530
    },
    {
      "epoch": 2.150586686347167,
      "grad_norm": 0.42121484875679016,
      "learning_rate": 3.9182532613074364e-05,
      "loss": 2.4407,
      "step": 104540
    },
    {
      "epoch": 2.150792405864035,
      "grad_norm": 0.4508419334888458,
      "learning_rate": 3.9164904433062575e-05,
      "loss": 2.4308,
      "step": 104550
    },
    {
      "epoch": 2.1509981253809025,
      "grad_norm": 0.4824492037296295,
      "learning_rate": 3.914727925371455e-05,
      "loss": 2.3714,
      "step": 104560
    },
    {
      "epoch": 2.15120384489777,
      "grad_norm": 0.4833582043647766,
      "learning_rate": 3.912965707589963e-05,
      "loss": 2.3326,
      "step": 104570
    },
    {
      "epoch": 2.151409564414638,
      "grad_norm": 0.462308794260025,
      "learning_rate": 3.911203790048702e-05,
      "loss": 2.3912,
      "step": 104580
    },
    {
      "epoch": 2.1516152839315055,
      "grad_norm": 0.4516623616218567,
      "learning_rate": 3.909442172834579e-05,
      "loss": 2.366,
      "step": 104590
    },
    {
      "epoch": 2.1518210034483736,
      "grad_norm": 0.5076042413711548,
      "learning_rate": 3.907680856034482e-05,
      "loss": 2.3917,
      "step": 104600
    },
    {
      "epoch": 2.1520267229652412,
      "grad_norm": 0.47729530930519104,
      "learning_rate": 3.9059198397352915e-05,
      "loss": 2.3272,
      "step": 104610
    },
    {
      "epoch": 2.152232442482109,
      "grad_norm": 0.45030200481414795,
      "learning_rate": 3.9041591240238694e-05,
      "loss": 2.404,
      "step": 104620
    },
    {
      "epoch": 2.1524381619989765,
      "grad_norm": 0.42854103446006775,
      "learning_rate": 3.9023987089870516e-05,
      "loss": 2.4308,
      "step": 104630
    },
    {
      "epoch": 2.152643881515844,
      "grad_norm": 0.4738062620162964,
      "learning_rate": 3.900638594711682e-05,
      "loss": 2.3724,
      "step": 104640
    },
    {
      "epoch": 2.152849601032712,
      "grad_norm": 0.457516074180603,
      "learning_rate": 3.898878781284575e-05,
      "loss": 2.4325,
      "step": 104650
    },
    {
      "epoch": 2.1530553205495795,
      "grad_norm": 0.40908321738243103,
      "learning_rate": 3.897119268792525e-05,
      "loss": 2.4243,
      "step": 104660
    },
    {
      "epoch": 2.1532610400664476,
      "grad_norm": 0.4676538109779358,
      "learning_rate": 3.895360057322328e-05,
      "loss": 2.3584,
      "step": 104670
    },
    {
      "epoch": 2.1534667595833152,
      "grad_norm": 0.44273748993873596,
      "learning_rate": 3.893601146960756e-05,
      "loss": 2.3585,
      "step": 104680
    },
    {
      "epoch": 2.153672479100183,
      "grad_norm": 0.44243666529655457,
      "learning_rate": 3.8918425377945564e-05,
      "loss": 2.4067,
      "step": 104690
    },
    {
      "epoch": 2.1538781986170505,
      "grad_norm": 0.43250107765197754,
      "learning_rate": 3.8900842299104864e-05,
      "loss": 2.3777,
      "step": 104700
    },
    {
      "epoch": 2.154083918133918,
      "grad_norm": 0.46022582054138184,
      "learning_rate": 3.888326223395263e-05,
      "loss": 2.4099,
      "step": 104710
    },
    {
      "epoch": 2.154289637650786,
      "grad_norm": 0.4795888066291809,
      "learning_rate": 3.886568518335599e-05,
      "loss": 2.3036,
      "step": 104720
    },
    {
      "epoch": 2.1544953571676535,
      "grad_norm": 0.4338851571083069,
      "learning_rate": 3.884811114818203e-05,
      "loss": 2.375,
      "step": 104730
    },
    {
      "epoch": 2.1547010766845216,
      "grad_norm": 0.4277336895465851,
      "learning_rate": 3.883054012929747e-05,
      "loss": 2.3731,
      "step": 104740
    },
    {
      "epoch": 2.1549067962013893,
      "grad_norm": 0.4323590397834778,
      "learning_rate": 3.881297212756899e-05,
      "loss": 2.3398,
      "step": 104750
    },
    {
      "epoch": 2.155112515718257,
      "grad_norm": 0.525911808013916,
      "learning_rate": 3.879540714386325e-05,
      "loss": 2.3303,
      "step": 104760
    },
    {
      "epoch": 2.1553182352351246,
      "grad_norm": 0.45975378155708313,
      "learning_rate": 3.87778451790465e-05,
      "loss": 2.3297,
      "step": 104770
    },
    {
      "epoch": 2.155523954751992,
      "grad_norm": 0.47029703855514526,
      "learning_rate": 3.876028623398504e-05,
      "loss": 2.3681,
      "step": 104780
    },
    {
      "epoch": 2.15572967426886,
      "grad_norm": 0.4459494948387146,
      "learning_rate": 3.874273030954494e-05,
      "loss": 2.3548,
      "step": 104790
    },
    {
      "epoch": 2.1559353937857275,
      "grad_norm": 0.4947368800640106,
      "learning_rate": 3.872517740659214e-05,
      "loss": 2.3695,
      "step": 104800
    },
    {
      "epoch": 2.1561411133025956,
      "grad_norm": 0.4346890151500702,
      "learning_rate": 3.8707627525992426e-05,
      "loss": 2.3197,
      "step": 104810
    },
    {
      "epoch": 2.1563468328194633,
      "grad_norm": 0.48582473397254944,
      "learning_rate": 3.869008066861145e-05,
      "loss": 2.3693,
      "step": 104820
    },
    {
      "epoch": 2.156552552336331,
      "grad_norm": 0.4421933591365814,
      "learning_rate": 3.8672536835314685e-05,
      "loss": 2.3989,
      "step": 104830
    },
    {
      "epoch": 2.1567582718531986,
      "grad_norm": 0.49745774269104004,
      "learning_rate": 3.8654996026967494e-05,
      "loss": 2.3303,
      "step": 104840
    },
    {
      "epoch": 2.1569639913700662,
      "grad_norm": 0.44575849175453186,
      "learning_rate": 3.8637458244435056e-05,
      "loss": 2.3813,
      "step": 104850
    },
    {
      "epoch": 2.157169710886934,
      "grad_norm": 0.4880051910877228,
      "learning_rate": 3.861992348858242e-05,
      "loss": 2.3445,
      "step": 104860
    },
    {
      "epoch": 2.1573754304038015,
      "grad_norm": 0.48946118354797363,
      "learning_rate": 3.860239176027447e-05,
      "loss": 2.3969,
      "step": 104870
    },
    {
      "epoch": 2.157581149920669,
      "grad_norm": 0.43404507637023926,
      "learning_rate": 3.858486306037596e-05,
      "loss": 2.3517,
      "step": 104880
    },
    {
      "epoch": 2.1577868694375373,
      "grad_norm": 0.4321660101413727,
      "learning_rate": 3.856733738975149e-05,
      "loss": 2.3891,
      "step": 104890
    },
    {
      "epoch": 2.157992588954405,
      "grad_norm": 0.43372419476509094,
      "learning_rate": 3.854981474926549e-05,
      "loss": 2.3196,
      "step": 104900
    },
    {
      "epoch": 2.1581983084712726,
      "grad_norm": 0.44460490345954895,
      "learning_rate": 3.853229513978227e-05,
      "loss": 2.423,
      "step": 104910
    },
    {
      "epoch": 2.1584040279881402,
      "grad_norm": 0.47175610065460205,
      "learning_rate": 3.851477856216597e-05,
      "loss": 2.36,
      "step": 104920
    },
    {
      "epoch": 2.158609747505008,
      "grad_norm": 0.47721096873283386,
      "learning_rate": 3.849726501728059e-05,
      "loss": 2.3998,
      "step": 104930
    },
    {
      "epoch": 2.1588154670218755,
      "grad_norm": 0.45164209604263306,
      "learning_rate": 3.847975450599001e-05,
      "loss": 2.3493,
      "step": 104940
    },
    {
      "epoch": 2.1590211865387436,
      "grad_norm": 0.42610347270965576,
      "learning_rate": 3.846224702915783e-05,
      "loss": 2.377,
      "step": 104950
    },
    {
      "epoch": 2.1592269060556113,
      "grad_norm": 0.460178017616272,
      "learning_rate": 3.8444742587647684e-05,
      "loss": 2.4021,
      "step": 104960
    },
    {
      "epoch": 2.159432625572479,
      "grad_norm": 0.4091867208480835,
      "learning_rate": 3.8427241182323004e-05,
      "loss": 2.4088,
      "step": 104970
    },
    {
      "epoch": 2.1596383450893466,
      "grad_norm": 0.41866180300712585,
      "learning_rate": 3.84097428140469e-05,
      "loss": 2.3567,
      "step": 104980
    },
    {
      "epoch": 2.1598440646062143,
      "grad_norm": 0.4416781961917877,
      "learning_rate": 3.839224748368261e-05,
      "loss": 2.3507,
      "step": 104990
    },
    {
      "epoch": 2.160049784123082,
      "grad_norm": 0.4794515073299408,
      "learning_rate": 3.8374755192093046e-05,
      "loss": 2.3727,
      "step": 105000
    },
    {
      "epoch": 2.1602555036399496,
      "grad_norm": 0.44223812222480774,
      "learning_rate": 3.835726594014093e-05,
      "loss": 2.3454,
      "step": 105010
    },
    {
      "epoch": 2.160461223156817,
      "grad_norm": 0.4618981182575226,
      "learning_rate": 3.833977972868904e-05,
      "loss": 2.3256,
      "step": 105020
    },
    {
      "epoch": 2.1606669426736853,
      "grad_norm": 0.48792755603790283,
      "learning_rate": 3.832229655859977e-05,
      "loss": 2.3401,
      "step": 105030
    },
    {
      "epoch": 2.160872662190553,
      "grad_norm": 0.4137689769268036,
      "learning_rate": 3.830481643073548e-05,
      "loss": 2.437,
      "step": 105040
    },
    {
      "epoch": 2.1610783817074206,
      "grad_norm": 0.46366557478904724,
      "learning_rate": 3.828733934595846e-05,
      "loss": 2.3711,
      "step": 105050
    },
    {
      "epoch": 2.1612841012242883,
      "grad_norm": 0.486991822719574,
      "learning_rate": 3.826986530513067e-05,
      "loss": 2.3764,
      "step": 105060
    },
    {
      "epoch": 2.161489820741156,
      "grad_norm": 0.5132266283035278,
      "learning_rate": 3.825239430911399e-05,
      "loss": 2.3517,
      "step": 105070
    },
    {
      "epoch": 2.1616955402580236,
      "grad_norm": 0.48954129219055176,
      "learning_rate": 3.82349263587703e-05,
      "loss": 2.3789,
      "step": 105080
    },
    {
      "epoch": 2.1619012597748912,
      "grad_norm": 0.4684509336948395,
      "learning_rate": 3.8217461454961065e-05,
      "loss": 2.339,
      "step": 105090
    },
    {
      "epoch": 2.1621069792917593,
      "grad_norm": 0.4560341238975525,
      "learning_rate": 3.8199999598547786e-05,
      "loss": 2.427,
      "step": 105100
    },
    {
      "epoch": 2.162312698808627,
      "grad_norm": 0.4411427080631256,
      "learning_rate": 3.818254079039175e-05,
      "loss": 2.3631,
      "step": 105110
    },
    {
      "epoch": 2.1625184183254946,
      "grad_norm": 0.48166224360466003,
      "learning_rate": 3.816508503135412e-05,
      "loss": 2.3513,
      "step": 105120
    },
    {
      "epoch": 2.1627241378423623,
      "grad_norm": 0.5149032473564148,
      "learning_rate": 3.814763232229589e-05,
      "loss": 2.3433,
      "step": 105130
    },
    {
      "epoch": 2.16292985735923,
      "grad_norm": 0.527990460395813,
      "learning_rate": 3.81301826640779e-05,
      "loss": 2.3819,
      "step": 105140
    },
    {
      "epoch": 2.1631355768760976,
      "grad_norm": 0.4950202703475952,
      "learning_rate": 3.811273605756084e-05,
      "loss": 2.3781,
      "step": 105150
    },
    {
      "epoch": 2.1633412963929652,
      "grad_norm": 0.46221157908439636,
      "learning_rate": 3.8095292503605274e-05,
      "loss": 2.3427,
      "step": 105160
    },
    {
      "epoch": 2.1635470159098333,
      "grad_norm": 0.5704693794250488,
      "learning_rate": 3.807785200307158e-05,
      "loss": 2.4151,
      "step": 105170
    },
    {
      "epoch": 2.163752735426701,
      "grad_norm": 0.47378724813461304,
      "learning_rate": 3.806041455682002e-05,
      "loss": 2.3695,
      "step": 105180
    },
    {
      "epoch": 2.1639584549435686,
      "grad_norm": 0.4295545518398285,
      "learning_rate": 3.804298016571067e-05,
      "loss": 2.3879,
      "step": 105190
    },
    {
      "epoch": 2.1641641744604363,
      "grad_norm": 0.43025943636894226,
      "learning_rate": 3.80255488306035e-05,
      "loss": 2.3266,
      "step": 105200
    },
    {
      "epoch": 2.164369893977304,
      "grad_norm": 0.49445492029190063,
      "learning_rate": 3.800812055235827e-05,
      "loss": 2.4444,
      "step": 105210
    },
    {
      "epoch": 2.1645756134941716,
      "grad_norm": 0.42496854066848755,
      "learning_rate": 3.7990695331834646e-05,
      "loss": 2.3767,
      "step": 105220
    },
    {
      "epoch": 2.1647813330110393,
      "grad_norm": 0.503955066204071,
      "learning_rate": 3.797327316989211e-05,
      "loss": 2.3362,
      "step": 105230
    },
    {
      "epoch": 2.1649870525279074,
      "grad_norm": 0.44656607508659363,
      "learning_rate": 3.795585406739e-05,
      "loss": 2.3762,
      "step": 105240
    },
    {
      "epoch": 2.165192772044775,
      "grad_norm": 0.5377201437950134,
      "learning_rate": 3.7938438025187514e-05,
      "loss": 2.4212,
      "step": 105250
    },
    {
      "epoch": 2.1653984915616427,
      "grad_norm": 0.4424552023410797,
      "learning_rate": 3.792102504414372e-05,
      "loss": 2.3207,
      "step": 105260
    },
    {
      "epoch": 2.1656042110785103,
      "grad_norm": 0.4746040105819702,
      "learning_rate": 3.79036151251174e-05,
      "loss": 2.3124,
      "step": 105270
    },
    {
      "epoch": 2.165809930595378,
      "grad_norm": 0.501274824142456,
      "learning_rate": 3.7886208268967396e-05,
      "loss": 2.3319,
      "step": 105280
    },
    {
      "epoch": 2.1660156501122456,
      "grad_norm": 0.539417564868927,
      "learning_rate": 3.7868804476552286e-05,
      "loss": 2.3826,
      "step": 105290
    },
    {
      "epoch": 2.1662213696291133,
      "grad_norm": 0.4597855806350708,
      "learning_rate": 3.7851403748730406e-05,
      "loss": 2.4003,
      "step": 105300
    },
    {
      "epoch": 2.166427089145981,
      "grad_norm": 0.4509132206439972,
      "learning_rate": 3.7834006086360166e-05,
      "loss": 2.3545,
      "step": 105310
    },
    {
      "epoch": 2.166632808662849,
      "grad_norm": 0.48978763818740845,
      "learning_rate": 3.781661149029966e-05,
      "loss": 2.3918,
      "step": 105320
    },
    {
      "epoch": 2.1668385281797167,
      "grad_norm": 0.4217803180217743,
      "learning_rate": 3.779921996140677e-05,
      "loss": 2.3997,
      "step": 105330
    },
    {
      "epoch": 2.1670442476965843,
      "grad_norm": 0.4515930712223053,
      "learning_rate": 3.77818315005395e-05,
      "loss": 2.4459,
      "step": 105340
    },
    {
      "epoch": 2.167249967213452,
      "grad_norm": 0.47980642318725586,
      "learning_rate": 3.776444610855538e-05,
      "loss": 2.34,
      "step": 105350
    },
    {
      "epoch": 2.1674556867303196,
      "grad_norm": 0.4180012047290802,
      "learning_rate": 3.774706378631196e-05,
      "loss": 2.3357,
      "step": 105360
    },
    {
      "epoch": 2.1676614062471873,
      "grad_norm": 0.42214691638946533,
      "learning_rate": 3.772968453466672e-05,
      "loss": 2.3001,
      "step": 105370
    },
    {
      "epoch": 2.167867125764055,
      "grad_norm": 0.4761604368686676,
      "learning_rate": 3.771230835447678e-05,
      "loss": 2.3942,
      "step": 105380
    },
    {
      "epoch": 2.168072845280923,
      "grad_norm": 0.4422810971736908,
      "learning_rate": 3.7694935246599195e-05,
      "loss": 2.3671,
      "step": 105390
    },
    {
      "epoch": 2.1682785647977907,
      "grad_norm": 0.563919186592102,
      "learning_rate": 3.767756521189101e-05,
      "loss": 2.3262,
      "step": 105400
    },
    {
      "epoch": 2.1684842843146583,
      "grad_norm": 0.43852266669273376,
      "learning_rate": 3.766019825120887e-05,
      "loss": 2.3844,
      "step": 105410
    },
    {
      "epoch": 2.168690003831526,
      "grad_norm": 0.42965301871299744,
      "learning_rate": 3.764283436540945e-05,
      "loss": 2.3844,
      "step": 105420
    },
    {
      "epoch": 2.1688957233483936,
      "grad_norm": 0.48655763268470764,
      "learning_rate": 3.76254735553492e-05,
      "loss": 2.3296,
      "step": 105430
    },
    {
      "epoch": 2.1691014428652613,
      "grad_norm": 0.4614947736263275,
      "learning_rate": 3.7608115821884435e-05,
      "loss": 2.385,
      "step": 105440
    },
    {
      "epoch": 2.169307162382129,
      "grad_norm": 0.5030674338340759,
      "learning_rate": 3.759076116587134e-05,
      "loss": 2.4023,
      "step": 105450
    },
    {
      "epoch": 2.169512881898997,
      "grad_norm": 0.46584030985832214,
      "learning_rate": 3.757340958816589e-05,
      "loss": 2.3688,
      "step": 105460
    },
    {
      "epoch": 2.1697186014158647,
      "grad_norm": 0.4512680470943451,
      "learning_rate": 3.7556061089623964e-05,
      "loss": 2.3601,
      "step": 105470
    },
    {
      "epoch": 2.1699243209327324,
      "grad_norm": 0.49495869874954224,
      "learning_rate": 3.7538715671101265e-05,
      "loss": 2.3807,
      "step": 105480
    },
    {
      "epoch": 2.1701300404496,
      "grad_norm": 0.46461108326911926,
      "learning_rate": 3.752137333345336e-05,
      "loss": 2.333,
      "step": 105490
    },
    {
      "epoch": 2.1703357599664677,
      "grad_norm": 0.4594067931175232,
      "learning_rate": 3.750403407753562e-05,
      "loss": 2.3306,
      "step": 105500
    },
    {
      "epoch": 2.1705414794833353,
      "grad_norm": 0.4900141656398773,
      "learning_rate": 3.748669790420334e-05,
      "loss": 2.3925,
      "step": 105510
    },
    {
      "epoch": 2.170747199000203,
      "grad_norm": 0.43137750029563904,
      "learning_rate": 3.746936481431158e-05,
      "loss": 2.3288,
      "step": 105520
    },
    {
      "epoch": 2.170952918517071,
      "grad_norm": 0.4256093502044678,
      "learning_rate": 3.745203480871531e-05,
      "loss": 2.3414,
      "step": 105530
    },
    {
      "epoch": 2.1711586380339387,
      "grad_norm": 0.45119616389274597,
      "learning_rate": 3.743470788826932e-05,
      "loss": 2.3748,
      "step": 105540
    },
    {
      "epoch": 2.1713643575508064,
      "grad_norm": 0.4439872205257416,
      "learning_rate": 3.7417384053828274e-05,
      "loss": 2.3502,
      "step": 105550
    },
    {
      "epoch": 2.171570077067674,
      "grad_norm": 0.46454188227653503,
      "learning_rate": 3.740006330624657e-05,
      "loss": 2.344,
      "step": 105560
    },
    {
      "epoch": 2.1717757965845417,
      "grad_norm": 0.45745161175727844,
      "learning_rate": 3.7382745646378655e-05,
      "loss": 2.3383,
      "step": 105570
    },
    {
      "epoch": 2.1719815161014093,
      "grad_norm": 0.4854779541492462,
      "learning_rate": 3.736543107507871e-05,
      "loss": 2.3713,
      "step": 105580
    },
    {
      "epoch": 2.172187235618277,
      "grad_norm": 0.4369990825653076,
      "learning_rate": 3.734811959320066e-05,
      "loss": 2.3787,
      "step": 105590
    },
    {
      "epoch": 2.1723929551351446,
      "grad_norm": 0.4449722170829773,
      "learning_rate": 3.733081120159849e-05,
      "loss": 2.3639,
      "step": 105600
    },
    {
      "epoch": 2.1725986746520127,
      "grad_norm": 0.5154091715812683,
      "learning_rate": 3.7313505901125936e-05,
      "loss": 2.3966,
      "step": 105610
    },
    {
      "epoch": 2.1728043941688804,
      "grad_norm": 0.494828999042511,
      "learning_rate": 3.7296203692636465e-05,
      "loss": 2.3949,
      "step": 105620
    },
    {
      "epoch": 2.173010113685748,
      "grad_norm": 0.4639929234981537,
      "learning_rate": 3.727890457698364e-05,
      "loss": 2.3191,
      "step": 105630
    },
    {
      "epoch": 2.1732158332026157,
      "grad_norm": 0.46539339423179626,
      "learning_rate": 3.7261608555020634e-05,
      "loss": 2.3854,
      "step": 105640
    },
    {
      "epoch": 2.1734215527194833,
      "grad_norm": 0.4409121572971344,
      "learning_rate": 3.7244315627600544e-05,
      "loss": 2.3749,
      "step": 105650
    },
    {
      "epoch": 2.173627272236351,
      "grad_norm": 0.4613018333911896,
      "learning_rate": 3.722702579557648e-05,
      "loss": 2.3486,
      "step": 105660
    },
    {
      "epoch": 2.173832991753219,
      "grad_norm": 0.55744469165802,
      "learning_rate": 3.7209739059801106e-05,
      "loss": 2.324,
      "step": 105670
    },
    {
      "epoch": 2.1740387112700867,
      "grad_norm": 0.4577888250350952,
      "learning_rate": 3.719245542112711e-05,
      "loss": 2.3835,
      "step": 105680
    },
    {
      "epoch": 2.1742444307869544,
      "grad_norm": 0.5357152819633484,
      "learning_rate": 3.717517488040711e-05,
      "loss": 2.3072,
      "step": 105690
    },
    {
      "epoch": 2.174450150303822,
      "grad_norm": 0.43637552857398987,
      "learning_rate": 3.715789743849333e-05,
      "loss": 2.2781,
      "step": 105700
    },
    {
      "epoch": 2.1746558698206897,
      "grad_norm": 0.43786337971687317,
      "learning_rate": 3.7140623096238024e-05,
      "loss": 2.4183,
      "step": 105710
    },
    {
      "epoch": 2.1748615893375574,
      "grad_norm": 0.48772862553596497,
      "learning_rate": 3.712335185449324e-05,
      "loss": 2.3737,
      "step": 105720
    },
    {
      "epoch": 2.175067308854425,
      "grad_norm": 0.4089905917644501,
      "learning_rate": 3.710608371411087e-05,
      "loss": 2.3316,
      "step": 105730
    },
    {
      "epoch": 2.1752730283712927,
      "grad_norm": 0.5338922142982483,
      "learning_rate": 3.708881867594266e-05,
      "loss": 2.3906,
      "step": 105740
    },
    {
      "epoch": 2.1754787478881608,
      "grad_norm": 0.4588496685028076,
      "learning_rate": 3.707155674084021e-05,
      "loss": 2.4256,
      "step": 105750
    },
    {
      "epoch": 2.1756844674050284,
      "grad_norm": 0.46752485632896423,
      "learning_rate": 3.7054297909654934e-05,
      "loss": 2.372,
      "step": 105760
    },
    {
      "epoch": 2.175890186921896,
      "grad_norm": 0.4397452175617218,
      "learning_rate": 3.703704218323812e-05,
      "loss": 2.3503,
      "step": 105770
    },
    {
      "epoch": 2.1760959064387637,
      "grad_norm": 0.44415518641471863,
      "learning_rate": 3.701978956244093e-05,
      "loss": 2.3755,
      "step": 105780
    },
    {
      "epoch": 2.1763016259556314,
      "grad_norm": 0.4871627986431122,
      "learning_rate": 3.700254004811431e-05,
      "loss": 2.375,
      "step": 105790
    },
    {
      "epoch": 2.176507345472499,
      "grad_norm": 0.41311538219451904,
      "learning_rate": 3.69852936411091e-05,
      "loss": 2.3814,
      "step": 105800
    },
    {
      "epoch": 2.1767130649893667,
      "grad_norm": 0.4965617060661316,
      "learning_rate": 3.6968050342275964e-05,
      "loss": 2.3678,
      "step": 105810
    },
    {
      "epoch": 2.1769187845062348,
      "grad_norm": 0.4133276045322418,
      "learning_rate": 3.695081015246541e-05,
      "loss": 2.363,
      "step": 105820
    },
    {
      "epoch": 2.1771245040231024,
      "grad_norm": 0.48180845379829407,
      "learning_rate": 3.693357307252783e-05,
      "loss": 2.3648,
      "step": 105830
    },
    {
      "epoch": 2.17733022353997,
      "grad_norm": 0.43727171421051025,
      "learning_rate": 3.691633910331341e-05,
      "loss": 2.3542,
      "step": 105840
    },
    {
      "epoch": 2.1775359430568377,
      "grad_norm": 0.6117385625839233,
      "learning_rate": 3.689910824567222e-05,
      "loss": 2.3761,
      "step": 105850
    },
    {
      "epoch": 2.1777416625737054,
      "grad_norm": 0.44325244426727295,
      "learning_rate": 3.688188050045416e-05,
      "loss": 2.3654,
      "step": 105860
    },
    {
      "epoch": 2.177947382090573,
      "grad_norm": 0.42648211121559143,
      "learning_rate": 3.6864655868509024e-05,
      "loss": 2.4574,
      "step": 105870
    },
    {
      "epoch": 2.1781531016074407,
      "grad_norm": 0.42626306414604187,
      "learning_rate": 3.684743435068629e-05,
      "loss": 2.3196,
      "step": 105880
    },
    {
      "epoch": 2.178358821124309,
      "grad_norm": 0.47205039858818054,
      "learning_rate": 3.6830215947835526e-05,
      "loss": 2.3561,
      "step": 105890
    },
    {
      "epoch": 2.1785645406411764,
      "grad_norm": 0.43941742181777954,
      "learning_rate": 3.6813000660806e-05,
      "loss": 2.333,
      "step": 105900
    },
    {
      "epoch": 2.178770260158044,
      "grad_norm": 0.42749765515327454,
      "learning_rate": 3.6795788490446745e-05,
      "loss": 2.2714,
      "step": 105910
    },
    {
      "epoch": 2.1789759796749117,
      "grad_norm": 0.4597763121128082,
      "learning_rate": 3.677857943760688e-05,
      "loss": 2.3842,
      "step": 105920
    },
    {
      "epoch": 2.1791816991917794,
      "grad_norm": 0.44917672872543335,
      "learning_rate": 3.676137350313519e-05,
      "loss": 2.3676,
      "step": 105930
    },
    {
      "epoch": 2.179387418708647,
      "grad_norm": 0.418924480676651,
      "learning_rate": 3.674417068788027e-05,
      "loss": 2.4039,
      "step": 105940
    },
    {
      "epoch": 2.1795931382255147,
      "grad_norm": 0.45686793327331543,
      "learning_rate": 3.672697099269078e-05,
      "loss": 2.3361,
      "step": 105950
    },
    {
      "epoch": 2.179798857742383,
      "grad_norm": 0.44922125339508057,
      "learning_rate": 3.670977441841498e-05,
      "loss": 2.3843,
      "step": 105960
    },
    {
      "epoch": 2.1800045772592505,
      "grad_norm": 0.4464486539363861,
      "learning_rate": 3.669258096590109e-05,
      "loss": 2.3848,
      "step": 105970
    },
    {
      "epoch": 2.180210296776118,
      "grad_norm": 0.4668668508529663,
      "learning_rate": 3.6675390635997254e-05,
      "loss": 2.4348,
      "step": 105980
    },
    {
      "epoch": 2.1804160162929858,
      "grad_norm": 0.4310121238231659,
      "learning_rate": 3.6658203429551294e-05,
      "loss": 2.3372,
      "step": 105990
    },
    {
      "epoch": 2.1806217358098534,
      "grad_norm": 0.4700854420661926,
      "learning_rate": 3.6641019347410945e-05,
      "loss": 2.35,
      "step": 106000
    },
    {
      "epoch": 2.180827455326721,
      "grad_norm": 0.46204254031181335,
      "learning_rate": 3.6623838390423924e-05,
      "loss": 2.4019,
      "step": 106010
    },
    {
      "epoch": 2.1810331748435887,
      "grad_norm": 0.4567584693431854,
      "learning_rate": 3.6606660559437554e-05,
      "loss": 2.4206,
      "step": 106020
    },
    {
      "epoch": 2.1812388943604564,
      "grad_norm": 0.47569540143013,
      "learning_rate": 3.6589485855299174e-05,
      "loss": 2.3635,
      "step": 106030
    },
    {
      "epoch": 2.1814446138773245,
      "grad_norm": 0.4684208631515503,
      "learning_rate": 3.657231427885591e-05,
      "loss": 2.3986,
      "step": 106040
    },
    {
      "epoch": 2.181650333394192,
      "grad_norm": 0.46680915355682373,
      "learning_rate": 3.655514583095474e-05,
      "loss": 2.4395,
      "step": 106050
    },
    {
      "epoch": 2.1818560529110598,
      "grad_norm": 0.44857338070869446,
      "learning_rate": 3.653798051244249e-05,
      "loss": 2.4038,
      "step": 106060
    },
    {
      "epoch": 2.1820617724279274,
      "grad_norm": 0.4906007945537567,
      "learning_rate": 3.652081832416584e-05,
      "loss": 2.3204,
      "step": 106070
    },
    {
      "epoch": 2.182267491944795,
      "grad_norm": 0.48621866106987,
      "learning_rate": 3.65036592669713e-05,
      "loss": 2.4259,
      "step": 106080
    },
    {
      "epoch": 2.1824732114616627,
      "grad_norm": 0.4161972999572754,
      "learning_rate": 3.648650334170524e-05,
      "loss": 2.3085,
      "step": 106090
    },
    {
      "epoch": 2.182678930978531,
      "grad_norm": 0.45904627442359924,
      "learning_rate": 3.6469350549213856e-05,
      "loss": 2.3549,
      "step": 106100
    },
    {
      "epoch": 2.1828846504953985,
      "grad_norm": 0.42619770765304565,
      "learning_rate": 3.6452200890343215e-05,
      "loss": 2.3678,
      "step": 106110
    },
    {
      "epoch": 2.183090370012266,
      "grad_norm": 0.4408056139945984,
      "learning_rate": 3.6435054365939206e-05,
      "loss": 2.3417,
      "step": 106120
    },
    {
      "epoch": 2.183296089529134,
      "grad_norm": 0.42495593428611755,
      "learning_rate": 3.641791097684758e-05,
      "loss": 2.4147,
      "step": 106130
    },
    {
      "epoch": 2.1835018090460014,
      "grad_norm": 0.4813357889652252,
      "learning_rate": 3.640077072391391e-05,
      "loss": 2.3532,
      "step": 106140
    },
    {
      "epoch": 2.183707528562869,
      "grad_norm": 0.4508582651615143,
      "learning_rate": 3.638363360798367e-05,
      "loss": 2.4203,
      "step": 106150
    },
    {
      "epoch": 2.1839132480797367,
      "grad_norm": 0.4357587695121765,
      "learning_rate": 3.6366499629902105e-05,
      "loss": 2.4021,
      "step": 106160
    },
    {
      "epoch": 2.1841189675966044,
      "grad_norm": 0.4565923810005188,
      "learning_rate": 3.634936879051435e-05,
      "loss": 2.4129,
      "step": 106170
    },
    {
      "epoch": 2.1843246871134725,
      "grad_norm": 0.4335745871067047,
      "learning_rate": 3.6332241090665387e-05,
      "loss": 2.3756,
      "step": 106180
    },
    {
      "epoch": 2.18453040663034,
      "grad_norm": 0.47424396872520447,
      "learning_rate": 3.631511653120004e-05,
      "loss": 2.3358,
      "step": 106190
    },
    {
      "epoch": 2.184736126147208,
      "grad_norm": 0.44707149267196655,
      "learning_rate": 3.62979951129629e-05,
      "loss": 2.3575,
      "step": 106200
    },
    {
      "epoch": 2.1849418456640755,
      "grad_norm": 0.5010864734649658,
      "learning_rate": 3.628087683679856e-05,
      "loss": 2.4534,
      "step": 106210
    },
    {
      "epoch": 2.185147565180943,
      "grad_norm": 0.4834759533405304,
      "learning_rate": 3.626376170355137e-05,
      "loss": 2.3573,
      "step": 106220
    },
    {
      "epoch": 2.1853532846978108,
      "grad_norm": 0.45733848214149475,
      "learning_rate": 3.624664971406543e-05,
      "loss": 2.383,
      "step": 106230
    },
    {
      "epoch": 2.1855590042146784,
      "grad_norm": 0.4355776011943817,
      "learning_rate": 3.622954086918488e-05,
      "loss": 2.3628,
      "step": 106240
    },
    {
      "epoch": 2.1857647237315465,
      "grad_norm": 0.4465191662311554,
      "learning_rate": 3.621243516975359e-05,
      "loss": 2.4367,
      "step": 106250
    },
    {
      "epoch": 2.185970443248414,
      "grad_norm": 0.447145938873291,
      "learning_rate": 3.619533261661522e-05,
      "loss": 2.3926,
      "step": 106260
    },
    {
      "epoch": 2.186176162765282,
      "grad_norm": 0.4761188328266144,
      "learning_rate": 3.6178233210613464e-05,
      "loss": 2.331,
      "step": 106270
    },
    {
      "epoch": 2.1863818822821495,
      "grad_norm": 0.45811623334884644,
      "learning_rate": 3.616113695259164e-05,
      "loss": 2.4136,
      "step": 106280
    },
    {
      "epoch": 2.186587601799017,
      "grad_norm": 0.4803504943847656,
      "learning_rate": 3.614404384339302e-05,
      "loss": 2.3674,
      "step": 106290
    },
    {
      "epoch": 2.1867933213158848,
      "grad_norm": 0.4396531283855438,
      "learning_rate": 3.61269538838608e-05,
      "loss": 2.3904,
      "step": 106300
    },
    {
      "epoch": 2.1869990408327524,
      "grad_norm": 0.43819254636764526,
      "learning_rate": 3.6109867074837856e-05,
      "loss": 2.3236,
      "step": 106310
    },
    {
      "epoch": 2.1872047603496205,
      "grad_norm": 0.4627227187156677,
      "learning_rate": 3.6092783417166974e-05,
      "loss": 2.3846,
      "step": 106320
    },
    {
      "epoch": 2.187410479866488,
      "grad_norm": 0.47295233607292175,
      "learning_rate": 3.60757029116909e-05,
      "loss": 2.3723,
      "step": 106330
    },
    {
      "epoch": 2.187616199383356,
      "grad_norm": 0.46389514207839966,
      "learning_rate": 3.605862555925202e-05,
      "loss": 2.4063,
      "step": 106340
    },
    {
      "epoch": 2.1878219189002235,
      "grad_norm": 0.43167924880981445,
      "learning_rate": 3.604155136069271e-05,
      "loss": 2.4194,
      "step": 106350
    },
    {
      "epoch": 2.188027638417091,
      "grad_norm": 0.48346972465515137,
      "learning_rate": 3.6024480316855136e-05,
      "loss": 2.4306,
      "step": 106360
    },
    {
      "epoch": 2.188233357933959,
      "grad_norm": 0.5727507472038269,
      "learning_rate": 3.6007412428581314e-05,
      "loss": 2.3745,
      "step": 106370
    },
    {
      "epoch": 2.1884390774508264,
      "grad_norm": 0.47876524925231934,
      "learning_rate": 3.5990347696713136e-05,
      "loss": 2.4272,
      "step": 106380
    },
    {
      "epoch": 2.1886447969676945,
      "grad_norm": 0.45782145857810974,
      "learning_rate": 3.5973286122092276e-05,
      "loss": 2.3191,
      "step": 106390
    },
    {
      "epoch": 2.188850516484562,
      "grad_norm": 0.4578005373477936,
      "learning_rate": 3.595622770556032e-05,
      "loss": 2.3528,
      "step": 106400
    },
    {
      "epoch": 2.18905623600143,
      "grad_norm": 0.5004605650901794,
      "learning_rate": 3.593917244795866e-05,
      "loss": 2.3821,
      "step": 106410
    },
    {
      "epoch": 2.1892619555182975,
      "grad_norm": 0.4213285446166992,
      "learning_rate": 3.592212035012852e-05,
      "loss": 2.3196,
      "step": 106420
    },
    {
      "epoch": 2.189467675035165,
      "grad_norm": 0.48402872681617737,
      "learning_rate": 3.590507141291101e-05,
      "loss": 2.4011,
      "step": 106430
    },
    {
      "epoch": 2.189673394552033,
      "grad_norm": 0.49028968811035156,
      "learning_rate": 3.588802563714704e-05,
      "loss": 2.3881,
      "step": 106440
    },
    {
      "epoch": 2.1898791140689005,
      "grad_norm": 0.48888102173805237,
      "learning_rate": 3.5870983023677406e-05,
      "loss": 2.3239,
      "step": 106450
    },
    {
      "epoch": 2.190084833585768,
      "grad_norm": 0.4585544168949127,
      "learning_rate": 3.585394357334272e-05,
      "loss": 2.362,
      "step": 106460
    },
    {
      "epoch": 2.190290553102636,
      "grad_norm": 0.44717809557914734,
      "learning_rate": 3.5836907286983444e-05,
      "loss": 2.3602,
      "step": 106470
    },
    {
      "epoch": 2.190496272619504,
      "grad_norm": 0.4413810968399048,
      "learning_rate": 3.581987416543988e-05,
      "loss": 2.3816,
      "step": 106480
    },
    {
      "epoch": 2.1907019921363715,
      "grad_norm": 0.4955475926399231,
      "learning_rate": 3.580284420955218e-05,
      "loss": 2.3399,
      "step": 106490
    },
    {
      "epoch": 2.190907711653239,
      "grad_norm": 0.5327619910240173,
      "learning_rate": 3.578581742016035e-05,
      "loss": 2.3368,
      "step": 106500
    },
    {
      "epoch": 2.191113431170107,
      "grad_norm": 0.47373655438423157,
      "learning_rate": 3.576879379810425e-05,
      "loss": 2.3557,
      "step": 106510
    },
    {
      "epoch": 2.1913191506869745,
      "grad_norm": 0.4721599221229553,
      "learning_rate": 3.5751773344223475e-05,
      "loss": 2.3347,
      "step": 106520
    },
    {
      "epoch": 2.191524870203842,
      "grad_norm": 0.47101008892059326,
      "learning_rate": 3.573475605935763e-05,
      "loss": 2.3807,
      "step": 106530
    },
    {
      "epoch": 2.19173058972071,
      "grad_norm": 0.4314134418964386,
      "learning_rate": 3.5717741944346114e-05,
      "loss": 2.3598,
      "step": 106540
    },
    {
      "epoch": 2.191936309237578,
      "grad_norm": 0.43879762291908264,
      "learning_rate": 3.5700731000028006e-05,
      "loss": 2.3704,
      "step": 106550
    },
    {
      "epoch": 2.1921420287544455,
      "grad_norm": 0.499773770570755,
      "learning_rate": 3.56837232272425e-05,
      "loss": 2.4228,
      "step": 106560
    },
    {
      "epoch": 2.192347748271313,
      "grad_norm": 0.4680856168270111,
      "learning_rate": 3.5666718626828475e-05,
      "loss": 2.3624,
      "step": 106570
    },
    {
      "epoch": 2.192553467788181,
      "grad_norm": 0.4810272455215454,
      "learning_rate": 3.5649717199624576e-05,
      "loss": 2.3529,
      "step": 106580
    },
    {
      "epoch": 2.1927591873050485,
      "grad_norm": 0.45719408988952637,
      "learning_rate": 3.5632718946469524e-05,
      "loss": 2.3276,
      "step": 106590
    },
    {
      "epoch": 2.192964906821916,
      "grad_norm": 0.4488208293914795,
      "learning_rate": 3.561572386820166e-05,
      "loss": 2.3278,
      "step": 106600
    },
    {
      "epoch": 2.1931706263387842,
      "grad_norm": 0.4491494297981262,
      "learning_rate": 3.559873196565925e-05,
      "loss": 2.2944,
      "step": 106610
    },
    {
      "epoch": 2.193376345855652,
      "grad_norm": 0.45281997323036194,
      "learning_rate": 3.558174323968051e-05,
      "loss": 2.3926,
      "step": 106620
    },
    {
      "epoch": 2.1935820653725195,
      "grad_norm": 0.4568294584751129,
      "learning_rate": 3.5564757691103314e-05,
      "loss": 2.3564,
      "step": 106630
    },
    {
      "epoch": 2.193787784889387,
      "grad_norm": 0.43688851594924927,
      "learning_rate": 3.554777532076545e-05,
      "loss": 2.3626,
      "step": 106640
    },
    {
      "epoch": 2.193993504406255,
      "grad_norm": 0.44179418683052063,
      "learning_rate": 3.5530796129504674e-05,
      "loss": 2.3989,
      "step": 106650
    },
    {
      "epoch": 2.1941992239231225,
      "grad_norm": 0.43974757194519043,
      "learning_rate": 3.551382011815838e-05,
      "loss": 2.3995,
      "step": 106660
    },
    {
      "epoch": 2.19440494343999,
      "grad_norm": 0.43830975890159607,
      "learning_rate": 3.549684728756393e-05,
      "loss": 2.3716,
      "step": 106670
    },
    {
      "epoch": 2.1946106629568582,
      "grad_norm": 0.47073453664779663,
      "learning_rate": 3.5479877638558504e-05,
      "loss": 2.3281,
      "step": 106680
    },
    {
      "epoch": 2.194816382473726,
      "grad_norm": 0.5253395438194275,
      "learning_rate": 3.5462911171979116e-05,
      "loss": 2.3515,
      "step": 106690
    },
    {
      "epoch": 2.1950221019905936,
      "grad_norm": 0.4315855801105499,
      "learning_rate": 3.5445947888662646e-05,
      "loss": 2.3394,
      "step": 106700
    },
    {
      "epoch": 2.195227821507461,
      "grad_norm": 0.4821223318576813,
      "learning_rate": 3.542898778944578e-05,
      "loss": 2.3612,
      "step": 106710
    },
    {
      "epoch": 2.195433541024329,
      "grad_norm": 0.42873644828796387,
      "learning_rate": 3.541203087516508e-05,
      "loss": 2.3752,
      "step": 106720
    },
    {
      "epoch": 2.1956392605411965,
      "grad_norm": 0.48303207755088806,
      "learning_rate": 3.539507714665693e-05,
      "loss": 2.3363,
      "step": 106730
    },
    {
      "epoch": 2.195844980058064,
      "grad_norm": 0.47214165329933167,
      "learning_rate": 3.537812660475758e-05,
      "loss": 2.4185,
      "step": 106740
    },
    {
      "epoch": 2.196050699574932,
      "grad_norm": 0.46124735474586487,
      "learning_rate": 3.5361179250303086e-05,
      "loss": 2.3442,
      "step": 106750
    },
    {
      "epoch": 2.1962564190918,
      "grad_norm": 0.47797104716300964,
      "learning_rate": 3.5344235084129395e-05,
      "loss": 2.3729,
      "step": 106760
    },
    {
      "epoch": 2.1964621386086676,
      "grad_norm": 0.515234112739563,
      "learning_rate": 3.532729410707224e-05,
      "loss": 2.3962,
      "step": 106770
    },
    {
      "epoch": 2.1966678581255352,
      "grad_norm": 0.4651948809623718,
      "learning_rate": 3.531035631996725e-05,
      "loss": 2.3656,
      "step": 106780
    },
    {
      "epoch": 2.196873577642403,
      "grad_norm": 0.45878905057907104,
      "learning_rate": 3.5293421723649867e-05,
      "loss": 2.3407,
      "step": 106790
    },
    {
      "epoch": 2.1970792971592705,
      "grad_norm": 0.44424909353256226,
      "learning_rate": 3.527649031895541e-05,
      "loss": 2.381,
      "step": 106800
    },
    {
      "epoch": 2.197285016676138,
      "grad_norm": 0.48967450857162476,
      "learning_rate": 3.525956210671892e-05,
      "loss": 2.3872,
      "step": 106810
    },
    {
      "epoch": 2.1974907361930063,
      "grad_norm": 0.4782370924949646,
      "learning_rate": 3.524263708777548e-05,
      "loss": 2.3629,
      "step": 106820
    },
    {
      "epoch": 2.197696455709874,
      "grad_norm": 0.4972091019153595,
      "learning_rate": 3.522571526295989e-05,
      "loss": 2.3692,
      "step": 106830
    },
    {
      "epoch": 2.1979021752267416,
      "grad_norm": 0.4833254814147949,
      "learning_rate": 3.5208796633106736e-05,
      "loss": 2.3938,
      "step": 106840
    },
    {
      "epoch": 2.1981078947436092,
      "grad_norm": 0.44635164737701416,
      "learning_rate": 3.5191881199050593e-05,
      "loss": 2.3694,
      "step": 106850
    },
    {
      "epoch": 2.198313614260477,
      "grad_norm": 0.44897279143333435,
      "learning_rate": 3.517496896162582e-05,
      "loss": 2.3993,
      "step": 106860
    },
    {
      "epoch": 2.1985193337773445,
      "grad_norm": 0.44276735186576843,
      "learning_rate": 3.515805992166652e-05,
      "loss": 2.3972,
      "step": 106870
    },
    {
      "epoch": 2.198725053294212,
      "grad_norm": 0.44495195150375366,
      "learning_rate": 3.5141154080006845e-05,
      "loss": 2.3992,
      "step": 106880
    },
    {
      "epoch": 2.19893077281108,
      "grad_norm": 0.4549778401851654,
      "learning_rate": 3.512425143748056e-05,
      "loss": 2.3228,
      "step": 106890
    },
    {
      "epoch": 2.199136492327948,
      "grad_norm": 0.4948471486568451,
      "learning_rate": 3.510735199492138e-05,
      "loss": 2.3769,
      "step": 106900
    },
    {
      "epoch": 2.1993422118448156,
      "grad_norm": 0.44199609756469727,
      "learning_rate": 3.509045575316299e-05,
      "loss": 2.3787,
      "step": 106910
    },
    {
      "epoch": 2.1995479313616833,
      "grad_norm": 0.46834108233451843,
      "learning_rate": 3.5073562713038656e-05,
      "loss": 2.4115,
      "step": 106920
    },
    {
      "epoch": 2.199753650878551,
      "grad_norm": 0.4899682402610779,
      "learning_rate": 3.505667287538164e-05,
      "loss": 2.42,
      "step": 106930
    },
    {
      "epoch": 2.1999593703954186,
      "grad_norm": 0.5314840078353882,
      "learning_rate": 3.503978624102511e-05,
      "loss": 2.4,
      "step": 106940
    },
    {
      "epoch": 2.200165089912286,
      "grad_norm": 0.46209123730659485,
      "learning_rate": 3.5022902810801904e-05,
      "loss": 2.3658,
      "step": 106950
    },
    {
      "epoch": 2.200370809429154,
      "grad_norm": 0.44773536920547485,
      "learning_rate": 3.500602258554482e-05,
      "loss": 2.3837,
      "step": 106960
    },
    {
      "epoch": 2.200576528946022,
      "grad_norm": 0.453723281621933,
      "learning_rate": 3.498914556608647e-05,
      "loss": 2.4078,
      "step": 106970
    },
    {
      "epoch": 2.2007822484628896,
      "grad_norm": 0.5253661274909973,
      "learning_rate": 3.49722717532593e-05,
      "loss": 2.3777,
      "step": 106980
    },
    {
      "epoch": 2.2009879679797573,
      "grad_norm": 0.4847729504108429,
      "learning_rate": 3.495540114789561e-05,
      "loss": 2.3727,
      "step": 106990
    },
    {
      "epoch": 2.201193687496625,
      "grad_norm": 0.4281175434589386,
      "learning_rate": 3.493853375082753e-05,
      "loss": 2.3522,
      "step": 107000
    },
    {
      "epoch": 2.2013994070134926,
      "grad_norm": 0.45916756987571716,
      "learning_rate": 3.492166956288703e-05,
      "loss": 2.413,
      "step": 107010
    },
    {
      "epoch": 2.2016051265303602,
      "grad_norm": 0.5072397589683533,
      "learning_rate": 3.4904808584905944e-05,
      "loss": 2.3965,
      "step": 107020
    },
    {
      "epoch": 2.201810846047228,
      "grad_norm": 0.47807231545448303,
      "learning_rate": 3.488795081771592e-05,
      "loss": 2.3043,
      "step": 107030
    },
    {
      "epoch": 2.202016565564096,
      "grad_norm": 0.4298616647720337,
      "learning_rate": 3.487109626214847e-05,
      "loss": 2.3023,
      "step": 107040
    },
    {
      "epoch": 2.2022222850809636,
      "grad_norm": 0.4497167766094208,
      "learning_rate": 3.485424491903493e-05,
      "loss": 2.3852,
      "step": 107050
    },
    {
      "epoch": 2.2024280045978313,
      "grad_norm": 0.4767823815345764,
      "learning_rate": 3.483739678920648e-05,
      "loss": 2.3622,
      "step": 107060
    },
    {
      "epoch": 2.202633724114699,
      "grad_norm": 0.43488073348999023,
      "learning_rate": 3.482055187349416e-05,
      "loss": 2.3806,
      "step": 107070
    },
    {
      "epoch": 2.2028394436315666,
      "grad_norm": 0.4476858675479889,
      "learning_rate": 3.480371017272882e-05,
      "loss": 2.3838,
      "step": 107080
    },
    {
      "epoch": 2.2030451631484342,
      "grad_norm": 0.42155221104621887,
      "learning_rate": 3.478687168774118e-05,
      "loss": 2.4047,
      "step": 107090
    },
    {
      "epoch": 2.203250882665302,
      "grad_norm": 0.45826587080955505,
      "learning_rate": 3.4770036419361803e-05,
      "loss": 2.3754,
      "step": 107100
    },
    {
      "epoch": 2.20345660218217,
      "grad_norm": 0.47047150135040283,
      "learning_rate": 3.475320436842107e-05,
      "loss": 2.3631,
      "step": 107110
    },
    {
      "epoch": 2.2036623216990376,
      "grad_norm": 0.4498136043548584,
      "learning_rate": 3.473637553574925e-05,
      "loss": 2.369,
      "step": 107120
    },
    {
      "epoch": 2.2038680412159053,
      "grad_norm": 0.485399454832077,
      "learning_rate": 3.4719549922176295e-05,
      "loss": 2.3346,
      "step": 107130
    },
    {
      "epoch": 2.204073760732773,
      "grad_norm": 0.4622446298599243,
      "learning_rate": 3.4702727528532245e-05,
      "loss": 2.3977,
      "step": 107140
    },
    {
      "epoch": 2.2042794802496406,
      "grad_norm": 0.4312572479248047,
      "learning_rate": 3.4685908355646855e-05,
      "loss": 2.3693,
      "step": 107150
    },
    {
      "epoch": 2.2044851997665083,
      "grad_norm": 0.4400237798690796,
      "learning_rate": 3.4669092404349624e-05,
      "loss": 2.3313,
      "step": 107160
    },
    {
      "epoch": 2.204690919283376,
      "grad_norm": 0.42554664611816406,
      "learning_rate": 3.465227967547008e-05,
      "loss": 2.3211,
      "step": 107170
    },
    {
      "epoch": 2.2048966388002436,
      "grad_norm": 0.4624696671962738,
      "learning_rate": 3.4635470169837514e-05,
      "loss": 2.3703,
      "step": 107180
    },
    {
      "epoch": 2.2051023583171117,
      "grad_norm": 0.4322573244571686,
      "learning_rate": 3.4618663888280944e-05,
      "loss": 2.2991,
      "step": 107190
    },
    {
      "epoch": 2.2053080778339793,
      "grad_norm": 0.5046806931495667,
      "learning_rate": 3.460186083162947e-05,
      "loss": 2.35,
      "step": 107200
    },
    {
      "epoch": 2.205513797350847,
      "grad_norm": 0.47331148386001587,
      "learning_rate": 3.4585061000711794e-05,
      "loss": 2.395,
      "step": 107210
    },
    {
      "epoch": 2.2057195168677146,
      "grad_norm": 0.5745893120765686,
      "learning_rate": 3.456826439635654e-05,
      "loss": 2.3844,
      "step": 107220
    },
    {
      "epoch": 2.2059252363845823,
      "grad_norm": 0.4660565257072449,
      "learning_rate": 3.455147101939232e-05,
      "loss": 2.3764,
      "step": 107230
    },
    {
      "epoch": 2.20613095590145,
      "grad_norm": 0.4470054805278778,
      "learning_rate": 3.453468087064735e-05,
      "loss": 2.3436,
      "step": 107240
    },
    {
      "epoch": 2.2063366754183176,
      "grad_norm": 0.45870158076286316,
      "learning_rate": 3.45178939509498e-05,
      "loss": 2.3707,
      "step": 107250
    },
    {
      "epoch": 2.2065423949351857,
      "grad_norm": 0.4281734228134155,
      "learning_rate": 3.4501110261127764e-05,
      "loss": 2.3915,
      "step": 107260
    },
    {
      "epoch": 2.2067481144520533,
      "grad_norm": 0.43683764338493347,
      "learning_rate": 3.448432980200901e-05,
      "loss": 2.35,
      "step": 107270
    },
    {
      "epoch": 2.206953833968921,
      "grad_norm": 0.4193509817123413,
      "learning_rate": 3.446755257442124e-05,
      "loss": 2.3775,
      "step": 107280
    },
    {
      "epoch": 2.2071595534857886,
      "grad_norm": 0.444696307182312,
      "learning_rate": 3.4450778579192e-05,
      "loss": 2.4106,
      "step": 107290
    },
    {
      "epoch": 2.2073652730026563,
      "grad_norm": 0.42853808403015137,
      "learning_rate": 3.443400781714864e-05,
      "loss": 2.4057,
      "step": 107300
    },
    {
      "epoch": 2.207570992519524,
      "grad_norm": 0.49108225107192993,
      "learning_rate": 3.441724028911839e-05,
      "loss": 2.3303,
      "step": 107310
    },
    {
      "epoch": 2.2077767120363916,
      "grad_norm": 0.44962480664253235,
      "learning_rate": 3.44004759959283e-05,
      "loss": 2.3204,
      "step": 107320
    },
    {
      "epoch": 2.2079824315532597,
      "grad_norm": 0.4139705300331116,
      "learning_rate": 3.438371493840524e-05,
      "loss": 2.4202,
      "step": 107330
    },
    {
      "epoch": 2.2081881510701273,
      "grad_norm": 0.47990158200263977,
      "learning_rate": 3.436695711737598e-05,
      "loss": 2.4024,
      "step": 107340
    },
    {
      "epoch": 2.208393870586995,
      "grad_norm": 0.44742336869239807,
      "learning_rate": 3.4350202533667044e-05,
      "loss": 2.3919,
      "step": 107350
    },
    {
      "epoch": 2.2085995901038626,
      "grad_norm": 0.47831282019615173,
      "learning_rate": 3.433345118810488e-05,
      "loss": 2.3378,
      "step": 107360
    },
    {
      "epoch": 2.2088053096207303,
      "grad_norm": 0.41908109188079834,
      "learning_rate": 3.4316703081515725e-05,
      "loss": 2.3681,
      "step": 107370
    },
    {
      "epoch": 2.209011029137598,
      "grad_norm": 0.4788927137851715,
      "learning_rate": 3.429995821472568e-05,
      "loss": 2.4103,
      "step": 107380
    },
    {
      "epoch": 2.2092167486544656,
      "grad_norm": 0.47563156485557556,
      "learning_rate": 3.4283216588560675e-05,
      "loss": 2.3395,
      "step": 107390
    },
    {
      "epoch": 2.2094224681713337,
      "grad_norm": 0.4954140782356262,
      "learning_rate": 3.4266478203846476e-05,
      "loss": 2.3371,
      "step": 107400
    },
    {
      "epoch": 2.2096281876882014,
      "grad_norm": 0.46748581528663635,
      "learning_rate": 3.42497430614087e-05,
      "loss": 2.3933,
      "step": 107410
    },
    {
      "epoch": 2.209833907205069,
      "grad_norm": 0.51874840259552,
      "learning_rate": 3.4233011162072815e-05,
      "loss": 2.408,
      "step": 107420
    },
    {
      "epoch": 2.2100396267219367,
      "grad_norm": 0.46561142802238464,
      "learning_rate": 3.421628250666409e-05,
      "loss": 2.413,
      "step": 107430
    },
    {
      "epoch": 2.2102453462388043,
      "grad_norm": 0.42866578698158264,
      "learning_rate": 3.419955709600771e-05,
      "loss": 2.3274,
      "step": 107440
    },
    {
      "epoch": 2.210451065755672,
      "grad_norm": 0.4781995415687561,
      "learning_rate": 3.418283493092854e-05,
      "loss": 2.3619,
      "step": 107450
    },
    {
      "epoch": 2.2106567852725396,
      "grad_norm": 0.4394232928752899,
      "learning_rate": 3.416611601225151e-05,
      "loss": 2.3521,
      "step": 107460
    },
    {
      "epoch": 2.2108625047894073,
      "grad_norm": 0.49332958459854126,
      "learning_rate": 3.414940034080124e-05,
      "loss": 2.3892,
      "step": 107470
    },
    {
      "epoch": 2.2110682243062754,
      "grad_norm": 0.48347845673561096,
      "learning_rate": 3.413268791740214e-05,
      "loss": 2.3639,
      "step": 107480
    },
    {
      "epoch": 2.211273943823143,
      "grad_norm": 0.44344931840896606,
      "learning_rate": 3.411597874287865e-05,
      "loss": 2.3814,
      "step": 107490
    },
    {
      "epoch": 2.2114796633400107,
      "grad_norm": 0.4674513339996338,
      "learning_rate": 3.409927281805495e-05,
      "loss": 2.2935,
      "step": 107500
    },
    {
      "epoch": 2.2116853828568783,
      "grad_norm": 0.435630738735199,
      "learning_rate": 3.4082570143754924e-05,
      "loss": 2.3245,
      "step": 107510
    },
    {
      "epoch": 2.211891102373746,
      "grad_norm": 0.4877210557460785,
      "learning_rate": 3.4065870720802575e-05,
      "loss": 2.3767,
      "step": 107520
    },
    {
      "epoch": 2.2120968218906136,
      "grad_norm": 0.44845548272132874,
      "learning_rate": 3.40491745500215e-05,
      "loss": 2.4026,
      "step": 107530
    },
    {
      "epoch": 2.2123025414074817,
      "grad_norm": 0.46211519837379456,
      "learning_rate": 3.4032481632235215e-05,
      "loss": 2.3765,
      "step": 107540
    },
    {
      "epoch": 2.2125082609243494,
      "grad_norm": 0.466898649930954,
      "learning_rate": 3.40157919682672e-05,
      "loss": 2.3626,
      "step": 107550
    },
    {
      "epoch": 2.212713980441217,
      "grad_norm": 0.4517350196838379,
      "learning_rate": 3.399910555894056e-05,
      "loss": 2.3785,
      "step": 107560
    },
    {
      "epoch": 2.2129196999580847,
      "grad_norm": 0.4296935498714447,
      "learning_rate": 3.398242240507835e-05,
      "loss": 2.379,
      "step": 107570
    },
    {
      "epoch": 2.2131254194749523,
      "grad_norm": 0.4710365831851959,
      "learning_rate": 3.396574250750356e-05,
      "loss": 2.3305,
      "step": 107580
    },
    {
      "epoch": 2.21333113899182,
      "grad_norm": 0.5035476088523865,
      "learning_rate": 3.394906586703881e-05,
      "loss": 2.3866,
      "step": 107590
    },
    {
      "epoch": 2.2135368585086876,
      "grad_norm": 0.46239182353019714,
      "learning_rate": 3.3932392484506714e-05,
      "loss": 2.3715,
      "step": 107600
    },
    {
      "epoch": 2.2137425780255553,
      "grad_norm": 0.43660613894462585,
      "learning_rate": 3.391572236072966e-05,
      "loss": 2.3751,
      "step": 107610
    },
    {
      "epoch": 2.2139482975424234,
      "grad_norm": 0.49277886748313904,
      "learning_rate": 3.389905549652992e-05,
      "loss": 2.3827,
      "step": 107620
    },
    {
      "epoch": 2.214154017059291,
      "grad_norm": 0.439831018447876,
      "learning_rate": 3.388239189272955e-05,
      "loss": 2.3527,
      "step": 107630
    },
    {
      "epoch": 2.2143597365761587,
      "grad_norm": 0.45006063580513,
      "learning_rate": 3.38657315501505e-05,
      "loss": 2.3534,
      "step": 107640
    },
    {
      "epoch": 2.2145654560930264,
      "grad_norm": 0.5046437382698059,
      "learning_rate": 3.384907446961452e-05,
      "loss": 2.3353,
      "step": 107650
    },
    {
      "epoch": 2.214771175609894,
      "grad_norm": 0.5060601234436035,
      "learning_rate": 3.383242065194322e-05,
      "loss": 2.2994,
      "step": 107660
    },
    {
      "epoch": 2.2149768951267617,
      "grad_norm": 0.4490974545478821,
      "learning_rate": 3.381577009795803e-05,
      "loss": 2.3635,
      "step": 107670
    },
    {
      "epoch": 2.2151826146436293,
      "grad_norm": 0.43343469500541687,
      "learning_rate": 3.3799122808480256e-05,
      "loss": 2.4327,
      "step": 107680
    },
    {
      "epoch": 2.2153883341604974,
      "grad_norm": 0.4547322392463684,
      "learning_rate": 3.3782478784331e-05,
      "loss": 2.3475,
      "step": 107690
    },
    {
      "epoch": 2.215594053677365,
      "grad_norm": 0.4591408669948578,
      "learning_rate": 3.376583802633121e-05,
      "loss": 2.3486,
      "step": 107700
    },
    {
      "epoch": 2.2157997731942327,
      "grad_norm": 0.44399309158325195,
      "learning_rate": 3.374920053530171e-05,
      "loss": 2.3698,
      "step": 107710
    },
    {
      "epoch": 2.2160054927111004,
      "grad_norm": 0.4449358880519867,
      "learning_rate": 3.3732566312063116e-05,
      "loss": 2.3949,
      "step": 107720
    },
    {
      "epoch": 2.216211212227968,
      "grad_norm": 0.4547037184238434,
      "learning_rate": 3.371593535743591e-05,
      "loss": 2.3722,
      "step": 107730
    },
    {
      "epoch": 2.2164169317448357,
      "grad_norm": 0.4476183354854584,
      "learning_rate": 3.369930767224042e-05,
      "loss": 2.3623,
      "step": 107740
    },
    {
      "epoch": 2.2166226512617033,
      "grad_norm": 0.48562511801719666,
      "learning_rate": 3.3682683257296777e-05,
      "loss": 2.3276,
      "step": 107750
    },
    {
      "epoch": 2.2168283707785714,
      "grad_norm": 0.44813647866249084,
      "learning_rate": 3.3666062113425025e-05,
      "loss": 2.3572,
      "step": 107760
    },
    {
      "epoch": 2.217034090295439,
      "grad_norm": 0.48103341460227966,
      "learning_rate": 3.3649444241444875e-05,
      "loss": 2.3326,
      "step": 107770
    },
    {
      "epoch": 2.2172398098123067,
      "grad_norm": 0.4768168032169342,
      "learning_rate": 3.363282964217611e-05,
      "loss": 2.305,
      "step": 107780
    },
    {
      "epoch": 2.2174455293291744,
      "grad_norm": 0.48343002796173096,
      "learning_rate": 3.361621831643824e-05,
      "loss": 2.3263,
      "step": 107790
    },
    {
      "epoch": 2.217651248846042,
      "grad_norm": 0.5268535017967224,
      "learning_rate": 3.35996102650505e-05,
      "loss": 2.3568,
      "step": 107800
    },
    {
      "epoch": 2.2178569683629097,
      "grad_norm": 0.8394525051116943,
      "learning_rate": 3.358300548883219e-05,
      "loss": 2.3693,
      "step": 107810
    },
    {
      "epoch": 2.2180626878797773,
      "grad_norm": 0.5037525296211243,
      "learning_rate": 3.35664039886023e-05,
      "loss": 2.4174,
      "step": 107820
    },
    {
      "epoch": 2.2182684073966454,
      "grad_norm": 0.4598591923713684,
      "learning_rate": 3.354980576517963e-05,
      "loss": 2.3918,
      "step": 107830
    },
    {
      "epoch": 2.218474126913513,
      "grad_norm": 0.4735850691795349,
      "learning_rate": 3.353321081938301e-05,
      "loss": 2.3853,
      "step": 107840
    },
    {
      "epoch": 2.2186798464303807,
      "grad_norm": 0.45559751987457275,
      "learning_rate": 3.351661915203085e-05,
      "loss": 2.3804,
      "step": 107850
    },
    {
      "epoch": 2.2188855659472484,
      "grad_norm": 0.5009523034095764,
      "learning_rate": 3.3500030763941546e-05,
      "loss": 2.3562,
      "step": 107860
    },
    {
      "epoch": 2.219091285464116,
      "grad_norm": 0.4593653976917267,
      "learning_rate": 3.348344565593341e-05,
      "loss": 2.3602,
      "step": 107870
    },
    {
      "epoch": 2.2192970049809837,
      "grad_norm": 0.4388653635978699,
      "learning_rate": 3.3466863828824404e-05,
      "loss": 2.4292,
      "step": 107880
    },
    {
      "epoch": 2.2195027244978514,
      "grad_norm": 0.46627411246299744,
      "learning_rate": 3.345028528343242e-05,
      "loss": 2.3882,
      "step": 107890
    },
    {
      "epoch": 2.219708444014719,
      "grad_norm": 0.5138727426528931,
      "learning_rate": 3.3433710020575235e-05,
      "loss": 2.3058,
      "step": 107900
    },
    {
      "epoch": 2.219914163531587,
      "grad_norm": 0.4882782995700836,
      "learning_rate": 3.3417138041070384e-05,
      "loss": 2.3334,
      "step": 107910
    },
    {
      "epoch": 2.2201198830484548,
      "grad_norm": 0.43423202633857727,
      "learning_rate": 3.3400569345735276e-05,
      "loss": 2.32,
      "step": 107920
    },
    {
      "epoch": 2.2203256025653224,
      "grad_norm": 0.49733835458755493,
      "learning_rate": 3.338400393538716e-05,
      "loss": 2.3951,
      "step": 107930
    },
    {
      "epoch": 2.22053132208219,
      "grad_norm": 0.4879337549209595,
      "learning_rate": 3.3367441810843126e-05,
      "loss": 2.3401,
      "step": 107940
    },
    {
      "epoch": 2.2207370415990577,
      "grad_norm": 0.48638877272605896,
      "learning_rate": 3.335088297292007e-05,
      "loss": 2.3875,
      "step": 107950
    },
    {
      "epoch": 2.2209427611159254,
      "grad_norm": 0.4756770133972168,
      "learning_rate": 3.333432742243476e-05,
      "loss": 2.399,
      "step": 107960
    },
    {
      "epoch": 2.2211484806327935,
      "grad_norm": 0.45582395792007446,
      "learning_rate": 3.331777516020381e-05,
      "loss": 2.3764,
      "step": 107970
    },
    {
      "epoch": 2.221354200149661,
      "grad_norm": 0.44283542037010193,
      "learning_rate": 3.3301226187043635e-05,
      "loss": 2.3518,
      "step": 107980
    },
    {
      "epoch": 2.2215599196665288,
      "grad_norm": 0.49144411087036133,
      "learning_rate": 3.3284680503770496e-05,
      "loss": 2.3457,
      "step": 107990
    },
    {
      "epoch": 2.2217656391833964,
      "grad_norm": 0.4466623067855835,
      "learning_rate": 3.326813811120052e-05,
      "loss": 2.3625,
      "step": 108000
    },
    {
      "epoch": 2.221971358700264,
      "grad_norm": 0.4757065176963806,
      "learning_rate": 3.325159901014964e-05,
      "loss": 2.3501,
      "step": 108010
    },
    {
      "epoch": 2.2221770782171317,
      "grad_norm": 0.4269319176673889,
      "learning_rate": 3.3235063201433646e-05,
      "loss": 2.4696,
      "step": 108020
    },
    {
      "epoch": 2.2223827977339994,
      "grad_norm": 0.4477194547653198,
      "learning_rate": 3.321853068586817e-05,
      "loss": 2.379,
      "step": 108030
    },
    {
      "epoch": 2.222588517250867,
      "grad_norm": 0.4461756646633148,
      "learning_rate": 3.320200146426865e-05,
      "loss": 2.3799,
      "step": 108040
    },
    {
      "epoch": 2.222794236767735,
      "grad_norm": 0.44316017627716064,
      "learning_rate": 3.318547553745043e-05,
      "loss": 2.3796,
      "step": 108050
    },
    {
      "epoch": 2.222999956284603,
      "grad_norm": 0.42495986819267273,
      "learning_rate": 3.316895290622853e-05,
      "loss": 2.4148,
      "step": 108060
    },
    {
      "epoch": 2.2232056758014704,
      "grad_norm": 0.4852753281593323,
      "learning_rate": 3.315243357141805e-05,
      "loss": 2.3705,
      "step": 108070
    },
    {
      "epoch": 2.223411395318338,
      "grad_norm": 0.44046473503112793,
      "learning_rate": 3.313591753383376e-05,
      "loss": 2.4203,
      "step": 108080
    },
    {
      "epoch": 2.2236171148352057,
      "grad_norm": 0.45704102516174316,
      "learning_rate": 3.311940479429024e-05,
      "loss": 2.3974,
      "step": 108090
    },
    {
      "epoch": 2.2238228343520734,
      "grad_norm": 0.422233521938324,
      "learning_rate": 3.3102895353602046e-05,
      "loss": 2.3447,
      "step": 108100
    },
    {
      "epoch": 2.224028553868941,
      "grad_norm": 0.46824127435684204,
      "learning_rate": 3.3086389212583523e-05,
      "loss": 2.3855,
      "step": 108110
    },
    {
      "epoch": 2.224234273385809,
      "grad_norm": 0.49486711621284485,
      "learning_rate": 3.306988637204872e-05,
      "loss": 2.3851,
      "step": 108120
    },
    {
      "epoch": 2.224439992902677,
      "grad_norm": 0.47461119294166565,
      "learning_rate": 3.305338683281176e-05,
      "loss": 2.3785,
      "step": 108130
    },
    {
      "epoch": 2.2246457124195445,
      "grad_norm": 0.446073055267334,
      "learning_rate": 3.303689059568638e-05,
      "loss": 2.3539,
      "step": 108140
    },
    {
      "epoch": 2.224851431936412,
      "grad_norm": 0.48213425278663635,
      "learning_rate": 3.302039766148624e-05,
      "loss": 2.408,
      "step": 108150
    },
    {
      "epoch": 2.2250571514532798,
      "grad_norm": 0.47622957825660706,
      "learning_rate": 3.3003908031024966e-05,
      "loss": 2.341,
      "step": 108160
    },
    {
      "epoch": 2.2252628709701474,
      "grad_norm": 0.41971132159233093,
      "learning_rate": 3.298742170511578e-05,
      "loss": 2.3667,
      "step": 108170
    },
    {
      "epoch": 2.225468590487015,
      "grad_norm": 0.46228742599487305,
      "learning_rate": 3.297093868457188e-05,
      "loss": 2.3286,
      "step": 108180
    },
    {
      "epoch": 2.225674310003883,
      "grad_norm": 0.4638909697532654,
      "learning_rate": 3.295445897020637e-05,
      "loss": 2.4204,
      "step": 108190
    },
    {
      "epoch": 2.225880029520751,
      "grad_norm": 0.42224743962287903,
      "learning_rate": 3.2937982562832014e-05,
      "loss": 2.4105,
      "step": 108200
    },
    {
      "epoch": 2.2260857490376185,
      "grad_norm": 0.45146676898002625,
      "learning_rate": 3.292150946326155e-05,
      "loss": 2.3745,
      "step": 108210
    },
    {
      "epoch": 2.226291468554486,
      "grad_norm": 0.4576053023338318,
      "learning_rate": 3.290503967230747e-05,
      "loss": 2.3759,
      "step": 108220
    },
    {
      "epoch": 2.2264971880713538,
      "grad_norm": 0.4687333106994629,
      "learning_rate": 3.2888573190782166e-05,
      "loss": 2.3924,
      "step": 108230
    },
    {
      "epoch": 2.2267029075882214,
      "grad_norm": 0.5240930914878845,
      "learning_rate": 3.287211001949784e-05,
      "loss": 2.3758,
      "step": 108240
    },
    {
      "epoch": 2.226908627105089,
      "grad_norm": 0.4851839542388916,
      "learning_rate": 3.28556501592665e-05,
      "loss": 2.3867,
      "step": 108250
    },
    {
      "epoch": 2.227114346621957,
      "grad_norm": 0.42492374777793884,
      "learning_rate": 3.2839193610900073e-05,
      "loss": 2.4306,
      "step": 108260
    },
    {
      "epoch": 2.227320066138825,
      "grad_norm": 0.4418625831604004,
      "learning_rate": 3.282274037521023e-05,
      "loss": 2.3756,
      "step": 108270
    },
    {
      "epoch": 2.2275257856556925,
      "grad_norm": 0.4846455454826355,
      "learning_rate": 3.280629045300854e-05,
      "loss": 2.3984,
      "step": 108280
    },
    {
      "epoch": 2.22773150517256,
      "grad_norm": 0.43471601605415344,
      "learning_rate": 3.2789843845106374e-05,
      "loss": 2.3378,
      "step": 108290
    },
    {
      "epoch": 2.227937224689428,
      "grad_norm": 0.4578084647655487,
      "learning_rate": 3.277340055231495e-05,
      "loss": 2.3813,
      "step": 108300
    },
    {
      "epoch": 2.2281429442062954,
      "grad_norm": 0.4462962746620178,
      "learning_rate": 3.275696057544535e-05,
      "loss": 2.3621,
      "step": 108310
    },
    {
      "epoch": 2.228348663723163,
      "grad_norm": 0.46156045794487,
      "learning_rate": 3.2740523915308443e-05,
      "loss": 2.3806,
      "step": 108320
    },
    {
      "epoch": 2.2285543832400307,
      "grad_norm": 0.5417677164077759,
      "learning_rate": 3.2724090572714985e-05,
      "loss": 2.3985,
      "step": 108330
    },
    {
      "epoch": 2.228760102756899,
      "grad_norm": 0.44550901651382446,
      "learning_rate": 3.270766054847551e-05,
      "loss": 2.3837,
      "step": 108340
    },
    {
      "epoch": 2.2289658222737665,
      "grad_norm": 0.4990435838699341,
      "learning_rate": 3.2691233843400457e-05,
      "loss": 2.2864,
      "step": 108350
    },
    {
      "epoch": 2.229171541790634,
      "grad_norm": 0.43539080023765564,
      "learning_rate": 3.267481045830004e-05,
      "loss": 2.3129,
      "step": 108360
    },
    {
      "epoch": 2.229377261307502,
      "grad_norm": 0.44676321744918823,
      "learning_rate": 3.2658390393984384e-05,
      "loss": 2.3605,
      "step": 108370
    },
    {
      "epoch": 2.2295829808243695,
      "grad_norm": 0.4479711651802063,
      "learning_rate": 3.26419736512633e-05,
      "loss": 2.3935,
      "step": 108380
    },
    {
      "epoch": 2.229788700341237,
      "grad_norm": 0.4270603060722351,
      "learning_rate": 3.262556023094663e-05,
      "loss": 2.3853,
      "step": 108390
    },
    {
      "epoch": 2.2299944198581048,
      "grad_norm": 0.4478003680706024,
      "learning_rate": 3.2609150133843955e-05,
      "loss": 2.3716,
      "step": 108400
    },
    {
      "epoch": 2.230200139374973,
      "grad_norm": 0.4738258123397827,
      "learning_rate": 3.25927433607646e-05,
      "loss": 2.4072,
      "step": 108410
    },
    {
      "epoch": 2.2304058588918405,
      "grad_norm": 0.4435449540615082,
      "learning_rate": 3.257633991251793e-05,
      "loss": 2.4108,
      "step": 108420
    },
    {
      "epoch": 2.230611578408708,
      "grad_norm": 0.435884028673172,
      "learning_rate": 3.255993978991303e-05,
      "loss": 2.3125,
      "step": 108430
    },
    {
      "epoch": 2.230817297925576,
      "grad_norm": 0.4377283751964569,
      "learning_rate": 3.2543542993758724e-05,
      "loss": 2.3661,
      "step": 108440
    },
    {
      "epoch": 2.2310230174424435,
      "grad_norm": 0.4788942337036133,
      "learning_rate": 3.252714952486391e-05,
      "loss": 2.3596,
      "step": 108450
    },
    {
      "epoch": 2.231228736959311,
      "grad_norm": 0.45338353514671326,
      "learning_rate": 3.25107593840371e-05,
      "loss": 2.43,
      "step": 108460
    },
    {
      "epoch": 2.2314344564761788,
      "grad_norm": 0.47477903962135315,
      "learning_rate": 3.2494372572086725e-05,
      "loss": 2.3161,
      "step": 108470
    },
    {
      "epoch": 2.231640175993047,
      "grad_norm": 0.4485011696815491,
      "learning_rate": 3.247798908982116e-05,
      "loss": 2.3821,
      "step": 108480
    },
    {
      "epoch": 2.2318458955099145,
      "grad_norm": 0.5084052085876465,
      "learning_rate": 3.24616089380484e-05,
      "loss": 2.3739,
      "step": 108490
    },
    {
      "epoch": 2.232051615026782,
      "grad_norm": 0.4878700375556946,
      "learning_rate": 3.2445232117576406e-05,
      "loss": 2.3435,
      "step": 108500
    },
    {
      "epoch": 2.23225733454365,
      "grad_norm": 0.4737696647644043,
      "learning_rate": 3.242885862921305e-05,
      "loss": 2.4007,
      "step": 108510
    },
    {
      "epoch": 2.2324630540605175,
      "grad_norm": 0.43834030628204346,
      "learning_rate": 3.241248847376583e-05,
      "loss": 2.4397,
      "step": 108520
    },
    {
      "epoch": 2.232668773577385,
      "grad_norm": 0.45357364416122437,
      "learning_rate": 3.239612165204227e-05,
      "loss": 2.2952,
      "step": 108530
    },
    {
      "epoch": 2.232874493094253,
      "grad_norm": 0.4303983747959137,
      "learning_rate": 3.2379758164849616e-05,
      "loss": 2.3541,
      "step": 108540
    },
    {
      "epoch": 2.233080212611121,
      "grad_norm": 0.4515525698661804,
      "learning_rate": 3.236339801299501e-05,
      "loss": 2.3775,
      "step": 108550
    },
    {
      "epoch": 2.2332859321279885,
      "grad_norm": 0.46186381578445435,
      "learning_rate": 3.23470411972854e-05,
      "loss": 2.3262,
      "step": 108560
    },
    {
      "epoch": 2.233491651644856,
      "grad_norm": 0.47432756423950195,
      "learning_rate": 3.233068771852758e-05,
      "loss": 2.369,
      "step": 108570
    },
    {
      "epoch": 2.233697371161724,
      "grad_norm": 0.4544151723384857,
      "learning_rate": 3.2314337577528196e-05,
      "loss": 2.3986,
      "step": 108580
    },
    {
      "epoch": 2.2339030906785915,
      "grad_norm": 0.4403282403945923,
      "learning_rate": 3.229799077509369e-05,
      "loss": 2.3993,
      "step": 108590
    },
    {
      "epoch": 2.234108810195459,
      "grad_norm": 0.473711222410202,
      "learning_rate": 3.228164731203036e-05,
      "loss": 2.3643,
      "step": 108600
    },
    {
      "epoch": 2.234314529712327,
      "grad_norm": 0.4586169719696045,
      "learning_rate": 3.226530718914436e-05,
      "loss": 2.3878,
      "step": 108610
    },
    {
      "epoch": 2.2345202492291945,
      "grad_norm": 0.45087605714797974,
      "learning_rate": 3.2248970407241644e-05,
      "loss": 2.3953,
      "step": 108620
    },
    {
      "epoch": 2.2347259687460626,
      "grad_norm": 0.42713257670402527,
      "learning_rate": 3.223263696712801e-05,
      "loss": 2.288,
      "step": 108630
    },
    {
      "epoch": 2.23493168826293,
      "grad_norm": 0.48592862486839294,
      "learning_rate": 3.2216306869609115e-05,
      "loss": 2.4256,
      "step": 108640
    },
    {
      "epoch": 2.235137407779798,
      "grad_norm": 0.4529052674770355,
      "learning_rate": 3.219998011549042e-05,
      "loss": 2.3122,
      "step": 108650
    },
    {
      "epoch": 2.2353431272966655,
      "grad_norm": 0.45485982298851013,
      "learning_rate": 3.218365670557723e-05,
      "loss": 2.4155,
      "step": 108660
    },
    {
      "epoch": 2.235548846813533,
      "grad_norm": 0.4442860186100006,
      "learning_rate": 3.216733664067472e-05,
      "loss": 2.3396,
      "step": 108670
    },
    {
      "epoch": 2.235754566330401,
      "grad_norm": 0.44643789529800415,
      "learning_rate": 3.215101992158784e-05,
      "loss": 2.3687,
      "step": 108680
    },
    {
      "epoch": 2.235960285847269,
      "grad_norm": 0.4734515845775604,
      "learning_rate": 3.213470654912145e-05,
      "loss": 2.417,
      "step": 108690
    },
    {
      "epoch": 2.2361660053641366,
      "grad_norm": 0.45683717727661133,
      "learning_rate": 3.21183965240801e-05,
      "loss": 2.3497,
      "step": 108700
    },
    {
      "epoch": 2.236371724881004,
      "grad_norm": 0.45310524106025696,
      "learning_rate": 3.210208984726837e-05,
      "loss": 2.3713,
      "step": 108710
    },
    {
      "epoch": 2.236577444397872,
      "grad_norm": 0.5005283951759338,
      "learning_rate": 3.208578651949058e-05,
      "loss": 2.4095,
      "step": 108720
    },
    {
      "epoch": 2.2367831639147395,
      "grad_norm": 0.5180076360702515,
      "learning_rate": 3.206948654155079e-05,
      "loss": 2.4036,
      "step": 108730
    },
    {
      "epoch": 2.236988883431607,
      "grad_norm": 0.47151508927345276,
      "learning_rate": 3.205318991425308e-05,
      "loss": 2.3994,
      "step": 108740
    },
    {
      "epoch": 2.237194602948475,
      "grad_norm": 0.49731460213661194,
      "learning_rate": 3.203689663840128e-05,
      "loss": 2.3751,
      "step": 108750
    },
    {
      "epoch": 2.2374003224653425,
      "grad_norm": 0.4788147211074829,
      "learning_rate": 3.202060671479896e-05,
      "loss": 2.2957,
      "step": 108760
    },
    {
      "epoch": 2.2376060419822106,
      "grad_norm": 0.42277318239212036,
      "learning_rate": 3.200432014424972e-05,
      "loss": 2.4092,
      "step": 108770
    },
    {
      "epoch": 2.2378117614990782,
      "grad_norm": 0.4629572331905365,
      "learning_rate": 3.198803692755681e-05,
      "loss": 2.3683,
      "step": 108780
    },
    {
      "epoch": 2.238017481015946,
      "grad_norm": 0.4765750765800476,
      "learning_rate": 3.197175706552338e-05,
      "loss": 2.345,
      "step": 108790
    },
    {
      "epoch": 2.2382232005328135,
      "grad_norm": 0.4655338227748871,
      "learning_rate": 3.195548055895253e-05,
      "loss": 2.3631,
      "step": 108800
    },
    {
      "epoch": 2.238428920049681,
      "grad_norm": 0.40686988830566406,
      "learning_rate": 3.1939207408647e-05,
      "loss": 2.3881,
      "step": 108810
    },
    {
      "epoch": 2.238634639566549,
      "grad_norm": 0.4951578378677368,
      "learning_rate": 3.192293761540943e-05,
      "loss": 2.3791,
      "step": 108820
    },
    {
      "epoch": 2.2388403590834165,
      "grad_norm": 0.4460641145706177,
      "learning_rate": 3.1906671180042455e-05,
      "loss": 2.3314,
      "step": 108830
    },
    {
      "epoch": 2.2390460786002846,
      "grad_norm": 0.45716559886932373,
      "learning_rate": 3.189040810334829e-05,
      "loss": 2.3607,
      "step": 108840
    },
    {
      "epoch": 2.2392517981171522,
      "grad_norm": 0.47644153237342834,
      "learning_rate": 3.187414838612914e-05,
      "loss": 2.4007,
      "step": 108850
    },
    {
      "epoch": 2.23945751763402,
      "grad_norm": 0.4523487389087677,
      "learning_rate": 3.185789202918701e-05,
      "loss": 2.3588,
      "step": 108860
    },
    {
      "epoch": 2.2396632371508876,
      "grad_norm": 0.5223392248153687,
      "learning_rate": 3.184163903332374e-05,
      "loss": 2.4454,
      "step": 108870
    },
    {
      "epoch": 2.239868956667755,
      "grad_norm": 0.4681459069252014,
      "learning_rate": 3.1825389399340986e-05,
      "loss": 2.4012,
      "step": 108880
    },
    {
      "epoch": 2.240074676184623,
      "grad_norm": 0.48909470438957214,
      "learning_rate": 3.1809143128040276e-05,
      "loss": 2.3464,
      "step": 108890
    },
    {
      "epoch": 2.2402803957014905,
      "grad_norm": 0.4712466299533844,
      "learning_rate": 3.1792900220222934e-05,
      "loss": 2.3378,
      "step": 108900
    },
    {
      "epoch": 2.2404861152183586,
      "grad_norm": 0.5424280166625977,
      "learning_rate": 3.177666067669015e-05,
      "loss": 2.3209,
      "step": 108910
    },
    {
      "epoch": 2.2406918347352263,
      "grad_norm": 0.7127067446708679,
      "learning_rate": 3.1760424498242914e-05,
      "loss": 2.3719,
      "step": 108920
    },
    {
      "epoch": 2.240897554252094,
      "grad_norm": 0.5230419635772705,
      "learning_rate": 3.174419168568208e-05,
      "loss": 2.3695,
      "step": 108930
    },
    {
      "epoch": 2.2411032737689616,
      "grad_norm": 0.5185674428939819,
      "learning_rate": 3.17279622398083e-05,
      "loss": 2.399,
      "step": 108940
    },
    {
      "epoch": 2.241308993285829,
      "grad_norm": 0.4361133575439453,
      "learning_rate": 3.1711736161422133e-05,
      "loss": 2.373,
      "step": 108950
    },
    {
      "epoch": 2.241514712802697,
      "grad_norm": 0.45771291851997375,
      "learning_rate": 3.169551345132389e-05,
      "loss": 2.3516,
      "step": 108960
    },
    {
      "epoch": 2.2417204323195645,
      "grad_norm": 0.44121235609054565,
      "learning_rate": 3.167929411031374e-05,
      "loss": 2.381,
      "step": 108970
    },
    {
      "epoch": 2.2419261518364326,
      "grad_norm": 0.5093668699264526,
      "learning_rate": 3.1663078139191716e-05,
      "loss": 2.3629,
      "step": 108980
    },
    {
      "epoch": 2.2421318713533003,
      "grad_norm": 0.4400545656681061,
      "learning_rate": 3.1646865538757665e-05,
      "loss": 2.3164,
      "step": 108990
    },
    {
      "epoch": 2.242337590870168,
      "grad_norm": 0.4401863217353821,
      "learning_rate": 3.163065630981125e-05,
      "loss": 2.329,
      "step": 109000
    },
    {
      "epoch": 2.2425433103870356,
      "grad_norm": 0.4957824945449829,
      "learning_rate": 3.161445045315201e-05,
      "loss": 2.4261,
      "step": 109010
    },
    {
      "epoch": 2.2427490299039032,
      "grad_norm": 0.5103000402450562,
      "learning_rate": 3.159824796957922e-05,
      "loss": 2.3752,
      "step": 109020
    },
    {
      "epoch": 2.242954749420771,
      "grad_norm": 0.4584418535232544,
      "learning_rate": 3.1582048859892156e-05,
      "loss": 2.3809,
      "step": 109030
    },
    {
      "epoch": 2.2431604689376385,
      "grad_norm": 0.43718746304512024,
      "learning_rate": 3.15658531248898e-05,
      "loss": 2.4221,
      "step": 109040
    },
    {
      "epoch": 2.243366188454506,
      "grad_norm": 0.46806588768959045,
      "learning_rate": 3.154966076537094e-05,
      "loss": 2.3668,
      "step": 109050
    },
    {
      "epoch": 2.2435719079713743,
      "grad_norm": 0.4810839593410492,
      "learning_rate": 3.153347178213436e-05,
      "loss": 2.3385,
      "step": 109060
    },
    {
      "epoch": 2.243777627488242,
      "grad_norm": 0.4942813813686371,
      "learning_rate": 3.151728617597849e-05,
      "loss": 2.3868,
      "step": 109070
    },
    {
      "epoch": 2.2439833470051096,
      "grad_norm": 0.5135388374328613,
      "learning_rate": 3.150110394770167e-05,
      "loss": 2.3763,
      "step": 109080
    },
    {
      "epoch": 2.2441890665219772,
      "grad_norm": 0.47859087586402893,
      "learning_rate": 3.148492509810219e-05,
      "loss": 2.402,
      "step": 109090
    },
    {
      "epoch": 2.244394786038845,
      "grad_norm": 0.47150278091430664,
      "learning_rate": 3.1468749627977955e-05,
      "loss": 2.3856,
      "step": 109100
    },
    {
      "epoch": 2.2446005055557126,
      "grad_norm": 0.5242059230804443,
      "learning_rate": 3.1452577538126816e-05,
      "loss": 2.3989,
      "step": 109110
    },
    {
      "epoch": 2.2448062250725807,
      "grad_norm": 0.4630630612373352,
      "learning_rate": 3.1436408829346565e-05,
      "loss": 2.3769,
      "step": 109120
    },
    {
      "epoch": 2.2450119445894483,
      "grad_norm": 0.5129086971282959,
      "learning_rate": 3.14202435024346e-05,
      "loss": 2.3801,
      "step": 109130
    },
    {
      "epoch": 2.245217664106316,
      "grad_norm": 0.4851841926574707,
      "learning_rate": 3.14040815581883e-05,
      "loss": 2.3474,
      "step": 109140
    },
    {
      "epoch": 2.2454233836231836,
      "grad_norm": 0.7166746854782104,
      "learning_rate": 3.138792299740486e-05,
      "loss": 2.3312,
      "step": 109150
    },
    {
      "epoch": 2.2456291031400513,
      "grad_norm": 0.4248064458370209,
      "learning_rate": 3.137176782088129e-05,
      "loss": 2.4128,
      "step": 109160
    },
    {
      "epoch": 2.245834822656919,
      "grad_norm": 0.45969051122665405,
      "learning_rate": 3.135561602941444e-05,
      "loss": 2.3431,
      "step": 109170
    },
    {
      "epoch": 2.2460405421737866,
      "grad_norm": 0.433274507522583,
      "learning_rate": 3.1339467623800975e-05,
      "loss": 2.4225,
      "step": 109180
    },
    {
      "epoch": 2.2462462616906542,
      "grad_norm": 0.4828137457370758,
      "learning_rate": 3.1323322604837414e-05,
      "loss": 2.3979,
      "step": 109190
    },
    {
      "epoch": 2.2464519812075223,
      "grad_norm": 0.4467783570289612,
      "learning_rate": 3.1307180973320114e-05,
      "loss": 2.3283,
      "step": 109200
    },
    {
      "epoch": 2.24665770072439,
      "grad_norm": 0.45340970158576965,
      "learning_rate": 3.129104273004524e-05,
      "loss": 2.3056,
      "step": 109210
    },
    {
      "epoch": 2.2468634202412576,
      "grad_norm": 0.481207937002182,
      "learning_rate": 3.127490787580881e-05,
      "loss": 2.3747,
      "step": 109220
    },
    {
      "epoch": 2.2470691397581253,
      "grad_norm": 0.4459216892719269,
      "learning_rate": 3.1258776411406675e-05,
      "loss": 2.3716,
      "step": 109230
    },
    {
      "epoch": 2.247274859274993,
      "grad_norm": 0.46413835883140564,
      "learning_rate": 3.12426483376345e-05,
      "loss": 2.361,
      "step": 109240
    },
    {
      "epoch": 2.2474805787918606,
      "grad_norm": 0.5514898896217346,
      "learning_rate": 3.12265236552878e-05,
      "loss": 2.3323,
      "step": 109250
    },
    {
      "epoch": 2.2476862983087282,
      "grad_norm": 0.46550995111465454,
      "learning_rate": 3.1210402365161915e-05,
      "loss": 2.3593,
      "step": 109260
    },
    {
      "epoch": 2.2478920178255963,
      "grad_norm": 0.5229527354240417,
      "learning_rate": 3.1194284468052026e-05,
      "loss": 2.3508,
      "step": 109270
    },
    {
      "epoch": 2.248097737342464,
      "grad_norm": 0.43950992822647095,
      "learning_rate": 3.117816996475313e-05,
      "loss": 2.4256,
      "step": 109280
    },
    {
      "epoch": 2.2483034568593316,
      "grad_norm": 0.5020959973335266,
      "learning_rate": 3.116205885606009e-05,
      "loss": 2.312,
      "step": 109290
    },
    {
      "epoch": 2.2485091763761993,
      "grad_norm": 0.46775248646736145,
      "learning_rate": 3.1145951142767595e-05,
      "loss": 2.3095,
      "step": 109300
    },
    {
      "epoch": 2.248714895893067,
      "grad_norm": 0.4219442903995514,
      "learning_rate": 3.112984682567005e-05,
      "loss": 2.3645,
      "step": 109310
    },
    {
      "epoch": 2.2489206154099346,
      "grad_norm": 0.45150190591812134,
      "learning_rate": 3.11137459055619e-05,
      "loss": 2.407,
      "step": 109320
    },
    {
      "epoch": 2.2491263349268023,
      "grad_norm": 0.4471588432788849,
      "learning_rate": 3.10976483832373e-05,
      "loss": 2.375,
      "step": 109330
    },
    {
      "epoch": 2.24933205444367,
      "grad_norm": 0.4726453125476837,
      "learning_rate": 3.1081554259490175e-05,
      "loss": 2.3424,
      "step": 109340
    },
    {
      "epoch": 2.249537773960538,
      "grad_norm": 0.4574148654937744,
      "learning_rate": 3.106546353511446e-05,
      "loss": 2.388,
      "step": 109350
    },
    {
      "epoch": 2.2497434934774057,
      "grad_norm": 0.44639232754707336,
      "learning_rate": 3.10493762109038e-05,
      "loss": 2.3876,
      "step": 109360
    },
    {
      "epoch": 2.2499492129942733,
      "grad_norm": 0.48764827847480774,
      "learning_rate": 3.1033292287651614e-05,
      "loss": 2.3147,
      "step": 109370
    },
    {
      "epoch": 2.250154932511141,
      "grad_norm": 0.5010854005813599,
      "learning_rate": 3.1017211766151365e-05,
      "loss": 2.3553,
      "step": 109380
    },
    {
      "epoch": 2.2503606520280086,
      "grad_norm": 0.4989083707332611,
      "learning_rate": 3.1001134647196106e-05,
      "loss": 2.4089,
      "step": 109390
    },
    {
      "epoch": 2.2505663715448763,
      "grad_norm": 0.4490000903606415,
      "learning_rate": 3.0985060931578856e-05,
      "loss": 2.3818,
      "step": 109400
    },
    {
      "epoch": 2.2507720910617444,
      "grad_norm": 0.46258604526519775,
      "learning_rate": 3.096899062009252e-05,
      "loss": 2.4014,
      "step": 109410
    },
    {
      "epoch": 2.250977810578612,
      "grad_norm": 0.5176346302032471,
      "learning_rate": 3.095292371352968e-05,
      "loss": 2.4008,
      "step": 109420
    },
    {
      "epoch": 2.2511835300954797,
      "grad_norm": 0.5279044508934021,
      "learning_rate": 3.093686021268282e-05,
      "loss": 2.3459,
      "step": 109430
    },
    {
      "epoch": 2.2513892496123473,
      "grad_norm": 0.4941222369670868,
      "learning_rate": 3.092080011834435e-05,
      "loss": 2.3761,
      "step": 109440
    },
    {
      "epoch": 2.251594969129215,
      "grad_norm": 0.4681216776371002,
      "learning_rate": 3.090474343130635e-05,
      "loss": 2.4077,
      "step": 109450
    },
    {
      "epoch": 2.2518006886460826,
      "grad_norm": 0.4557305574417114,
      "learning_rate": 3.0888690152360824e-05,
      "loss": 2.3396,
      "step": 109460
    },
    {
      "epoch": 2.2520064081629503,
      "grad_norm": 0.48398295044898987,
      "learning_rate": 3.0872640282299604e-05,
      "loss": 2.3607,
      "step": 109470
    },
    {
      "epoch": 2.252212127679818,
      "grad_norm": 0.5038684606552124,
      "learning_rate": 3.085659382191435e-05,
      "loss": 2.3613,
      "step": 109480
    },
    {
      "epoch": 2.252417847196686,
      "grad_norm": 0.48077529668807983,
      "learning_rate": 3.084055077199652e-05,
      "loss": 2.3301,
      "step": 109490
    },
    {
      "epoch": 2.2526235667135537,
      "grad_norm": 0.46762579679489136,
      "learning_rate": 3.082451113333745e-05,
      "loss": 2.3676,
      "step": 109500
    },
    {
      "epoch": 2.2528292862304213,
      "grad_norm": 0.4510817527770996,
      "learning_rate": 3.08084749067283e-05,
      "loss": 2.3403,
      "step": 109510
    },
    {
      "epoch": 2.253035005747289,
      "grad_norm": 0.4695590138435364,
      "learning_rate": 3.079244209296002e-05,
      "loss": 2.3519,
      "step": 109520
    },
    {
      "epoch": 2.2532407252641566,
      "grad_norm": 0.5176187753677368,
      "learning_rate": 3.077641269282344e-05,
      "loss": 2.3045,
      "step": 109530
    },
    {
      "epoch": 2.2534464447810243,
      "grad_norm": 0.43182483315467834,
      "learning_rate": 3.0760386707109236e-05,
      "loss": 2.3533,
      "step": 109540
    },
    {
      "epoch": 2.2536521642978924,
      "grad_norm": 0.49308761954307556,
      "learning_rate": 3.074436413660777e-05,
      "loss": 2.3473,
      "step": 109550
    },
    {
      "epoch": 2.25385788381476,
      "grad_norm": 0.4634612798690796,
      "learning_rate": 3.0728344982109456e-05,
      "loss": 2.3392,
      "step": 109560
    },
    {
      "epoch": 2.2540636033316277,
      "grad_norm": 0.46414101123809814,
      "learning_rate": 3.0712329244404405e-05,
      "loss": 2.4138,
      "step": 109570
    },
    {
      "epoch": 2.2542693228484953,
      "grad_norm": 0.4624062180519104,
      "learning_rate": 3.0696316924282586e-05,
      "loss": 2.3856,
      "step": 109580
    },
    {
      "epoch": 2.254475042365363,
      "grad_norm": 0.45494961738586426,
      "learning_rate": 3.0680308022533774e-05,
      "loss": 2.352,
      "step": 109590
    },
    {
      "epoch": 2.2546807618822307,
      "grad_norm": 0.4559052884578705,
      "learning_rate": 3.066430253994764e-05,
      "loss": 2.3783,
      "step": 109600
    },
    {
      "epoch": 2.2548864813990983,
      "grad_norm": 0.4717736542224884,
      "learning_rate": 3.064830047731362e-05,
      "loss": 2.417,
      "step": 109610
    },
    {
      "epoch": 2.255092200915966,
      "grad_norm": 0.43458041548728943,
      "learning_rate": 3.063230183542104e-05,
      "loss": 2.4251,
      "step": 109620
    },
    {
      "epoch": 2.2552979204328336,
      "grad_norm": 0.5033695101737976,
      "learning_rate": 3.061630661505895e-05,
      "loss": 2.3601,
      "step": 109630
    },
    {
      "epoch": 2.2555036399497017,
      "grad_norm": 0.49301859736442566,
      "learning_rate": 3.060031481701639e-05,
      "loss": 2.4338,
      "step": 109640
    },
    {
      "epoch": 2.2557093594665694,
      "grad_norm": 0.42321962118148804,
      "learning_rate": 3.0584326442082144e-05,
      "loss": 2.4195,
      "step": 109650
    },
    {
      "epoch": 2.255915078983437,
      "grad_norm": 0.4338395297527313,
      "learning_rate": 3.056834149104474e-05,
      "loss": 2.4403,
      "step": 109660
    },
    {
      "epoch": 2.2561207985003047,
      "grad_norm": 0.4569911062717438,
      "learning_rate": 3.0552359964692726e-05,
      "loss": 2.3855,
      "step": 109670
    },
    {
      "epoch": 2.2563265180171723,
      "grad_norm": 0.48361536860466003,
      "learning_rate": 3.053638186381438e-05,
      "loss": 2.3403,
      "step": 109680
    },
    {
      "epoch": 2.25653223753404,
      "grad_norm": 0.42417263984680176,
      "learning_rate": 3.052040718919773e-05,
      "loss": 2.3999,
      "step": 109690
    },
    {
      "epoch": 2.256737957050908,
      "grad_norm": 0.49714598059654236,
      "learning_rate": 3.0504435941630828e-05,
      "loss": 2.3565,
      "step": 109700
    },
    {
      "epoch": 2.2569436765677757,
      "grad_norm": 0.44993650913238525,
      "learning_rate": 3.048846812190137e-05,
      "loss": 2.3856,
      "step": 109710
    },
    {
      "epoch": 2.2571493960846434,
      "grad_norm": 0.4454319179058075,
      "learning_rate": 3.0472503730796952e-05,
      "loss": 2.3069,
      "step": 109720
    },
    {
      "epoch": 2.257355115601511,
      "grad_norm": 0.4493904113769531,
      "learning_rate": 3.0456542769105124e-05,
      "loss": 2.3282,
      "step": 109730
    },
    {
      "epoch": 2.2575608351183787,
      "grad_norm": 0.42372986674308777,
      "learning_rate": 3.044058523761303e-05,
      "loss": 2.3888,
      "step": 109740
    },
    {
      "epoch": 2.2577665546352463,
      "grad_norm": 0.4559849202632904,
      "learning_rate": 3.0424631137107775e-05,
      "loss": 2.4198,
      "step": 109750
    },
    {
      "epoch": 2.257972274152114,
      "grad_norm": 0.5371794104576111,
      "learning_rate": 3.0408680468376393e-05,
      "loss": 2.3912,
      "step": 109760
    },
    {
      "epoch": 2.2581779936689816,
      "grad_norm": 0.4060550928115845,
      "learning_rate": 3.039273323220555e-05,
      "loss": 2.3713,
      "step": 109770
    },
    {
      "epoch": 2.2583837131858497,
      "grad_norm": 0.43935078382492065,
      "learning_rate": 3.037678942938186e-05,
      "loss": 2.3854,
      "step": 109780
    },
    {
      "epoch": 2.2585894327027174,
      "grad_norm": 0.43355122208595276,
      "learning_rate": 3.0360849060691754e-05,
      "loss": 2.3455,
      "step": 109790
    },
    {
      "epoch": 2.258795152219585,
      "grad_norm": 0.4465610980987549,
      "learning_rate": 3.0344912126921464e-05,
      "loss": 2.3455,
      "step": 109800
    },
    {
      "epoch": 2.2590008717364527,
      "grad_norm": 0.43745872378349304,
      "learning_rate": 3.0328978628857084e-05,
      "loss": 2.3828,
      "step": 109810
    },
    {
      "epoch": 2.2592065912533204,
      "grad_norm": 0.45483243465423584,
      "learning_rate": 3.0313048567284542e-05,
      "loss": 2.3819,
      "step": 109820
    },
    {
      "epoch": 2.259412310770188,
      "grad_norm": 0.4645862877368927,
      "learning_rate": 3.0297121942989548e-05,
      "loss": 2.4065,
      "step": 109830
    },
    {
      "epoch": 2.259618030287056,
      "grad_norm": 0.43321114778518677,
      "learning_rate": 3.02811987567577e-05,
      "loss": 2.3233,
      "step": 109840
    },
    {
      "epoch": 2.2598237498039238,
      "grad_norm": 0.4654172956943512,
      "learning_rate": 3.0265279009374404e-05,
      "loss": 2.4103,
      "step": 109850
    },
    {
      "epoch": 2.2600294693207914,
      "grad_norm": 0.47968167066574097,
      "learning_rate": 3.0249362701624907e-05,
      "loss": 2.3695,
      "step": 109860
    },
    {
      "epoch": 2.260235188837659,
      "grad_norm": 0.45990240573883057,
      "learning_rate": 3.0233449834294193e-05,
      "loss": 2.3872,
      "step": 109870
    },
    {
      "epoch": 2.2604409083545267,
      "grad_norm": 0.4709460437297821,
      "learning_rate": 3.0217540408167243e-05,
      "loss": 2.337,
      "step": 109880
    },
    {
      "epoch": 2.2606466278713944,
      "grad_norm": 0.442004919052124,
      "learning_rate": 3.020163442402879e-05,
      "loss": 2.3476,
      "step": 109890
    },
    {
      "epoch": 2.260852347388262,
      "grad_norm": 0.4492343068122864,
      "learning_rate": 3.0185731882663293e-05,
      "loss": 2.3364,
      "step": 109900
    },
    {
      "epoch": 2.2610580669051297,
      "grad_norm": 0.45378345251083374,
      "learning_rate": 3.0169832784855235e-05,
      "loss": 2.4002,
      "step": 109910
    },
    {
      "epoch": 2.2612637864219978,
      "grad_norm": 0.48791685700416565,
      "learning_rate": 3.015393713138882e-05,
      "loss": 2.3973,
      "step": 109920
    },
    {
      "epoch": 2.2614695059388654,
      "grad_norm": 0.442184716463089,
      "learning_rate": 3.0138044923048014e-05,
      "loss": 2.3828,
      "step": 109930
    },
    {
      "epoch": 2.261675225455733,
      "grad_norm": 0.4418303668498993,
      "learning_rate": 3.012215616061682e-05,
      "loss": 2.3066,
      "step": 109940
    },
    {
      "epoch": 2.2618809449726007,
      "grad_norm": 0.4965416193008423,
      "learning_rate": 3.0106270844878805e-05,
      "loss": 2.4211,
      "step": 109950
    },
    {
      "epoch": 2.2620866644894684,
      "grad_norm": 0.49417805671691895,
      "learning_rate": 3.00903889766176e-05,
      "loss": 2.418,
      "step": 109960
    },
    {
      "epoch": 2.262292384006336,
      "grad_norm": 0.4723306894302368,
      "learning_rate": 3.0074510556616587e-05,
      "loss": 2.4109,
      "step": 109970
    },
    {
      "epoch": 2.2624981035232037,
      "grad_norm": 0.47873032093048096,
      "learning_rate": 3.0058635585658844e-05,
      "loss": 2.3202,
      "step": 109980
    },
    {
      "epoch": 2.262703823040072,
      "grad_norm": 0.4729183316230774,
      "learning_rate": 3.0042764064527507e-05,
      "loss": 2.3601,
      "step": 109990
    },
    {
      "epoch": 2.2629095425569394,
      "grad_norm": 0.460491418838501,
      "learning_rate": 3.0026895994005434e-05,
      "loss": 2.4185,
      "step": 110000
    },
    {
      "epoch": 2.263115262073807,
      "grad_norm": 0.4487256109714508,
      "learning_rate": 3.001103137487521e-05,
      "loss": 2.3858,
      "step": 110010
    },
    {
      "epoch": 2.2633209815906747,
      "grad_norm": 0.4762907326221466,
      "learning_rate": 2.9995170207919475e-05,
      "loss": 2.3672,
      "step": 110020
    },
    {
      "epoch": 2.2635267011075424,
      "grad_norm": 0.4700675308704376,
      "learning_rate": 2.9979312493920486e-05,
      "loss": 2.3502,
      "step": 110030
    },
    {
      "epoch": 2.26373242062441,
      "grad_norm": 0.5134893655776978,
      "learning_rate": 2.9963458233660412e-05,
      "loss": 2.4205,
      "step": 110040
    },
    {
      "epoch": 2.2639381401412777,
      "grad_norm": 0.42838039994239807,
      "learning_rate": 2.9947607427921353e-05,
      "loss": 2.2784,
      "step": 110050
    },
    {
      "epoch": 2.2641438596581454,
      "grad_norm": 0.47046002745628357,
      "learning_rate": 2.993176007748505e-05,
      "loss": 2.3938,
      "step": 110060
    },
    {
      "epoch": 2.2643495791750134,
      "grad_norm": 0.4196683168411255,
      "learning_rate": 2.9915916183133165e-05,
      "loss": 2.3502,
      "step": 110070
    },
    {
      "epoch": 2.264555298691881,
      "grad_norm": 0.4581882357597351,
      "learning_rate": 2.990007574564728e-05,
      "loss": 2.3829,
      "step": 110080
    },
    {
      "epoch": 2.2647610182087488,
      "grad_norm": 0.4441026449203491,
      "learning_rate": 2.9884238765808625e-05,
      "loss": 2.4005,
      "step": 110090
    },
    {
      "epoch": 2.2649667377256164,
      "grad_norm": 0.4353831112384796,
      "learning_rate": 2.98684052443984e-05,
      "loss": 2.3632,
      "step": 110100
    },
    {
      "epoch": 2.265172457242484,
      "grad_norm": 0.4544033110141754,
      "learning_rate": 2.985257518219756e-05,
      "loss": 2.3524,
      "step": 110110
    },
    {
      "epoch": 2.2653781767593517,
      "grad_norm": 0.4716453552246094,
      "learning_rate": 2.983674857998694e-05,
      "loss": 2.4001,
      "step": 110120
    },
    {
      "epoch": 2.26558389627622,
      "grad_norm": 0.4639774262905121,
      "learning_rate": 2.9820925438547154e-05,
      "loss": 2.3225,
      "step": 110130
    },
    {
      "epoch": 2.2657896157930875,
      "grad_norm": 0.45669499039649963,
      "learning_rate": 2.9805105758658704e-05,
      "loss": 2.4239,
      "step": 110140
    },
    {
      "epoch": 2.265995335309955,
      "grad_norm": 0.5086666941642761,
      "learning_rate": 2.978928954110185e-05,
      "loss": 2.4096,
      "step": 110150
    },
    {
      "epoch": 2.2662010548268228,
      "grad_norm": 0.44644662737846375,
      "learning_rate": 2.977347678665676e-05,
      "loss": 2.3587,
      "step": 110160
    },
    {
      "epoch": 2.2664067743436904,
      "grad_norm": 0.5048582553863525,
      "learning_rate": 2.975766749610336e-05,
      "loss": 2.4146,
      "step": 110170
    },
    {
      "epoch": 2.266612493860558,
      "grad_norm": 0.4990686774253845,
      "learning_rate": 2.974186167022148e-05,
      "loss": 2.4005,
      "step": 110180
    },
    {
      "epoch": 2.2668182133774257,
      "grad_norm": 0.4078260064125061,
      "learning_rate": 2.9726059309790643e-05,
      "loss": 2.3373,
      "step": 110190
    },
    {
      "epoch": 2.2670239328942934,
      "grad_norm": 0.39900028705596924,
      "learning_rate": 2.9710260415590386e-05,
      "loss": 2.3225,
      "step": 110200
    },
    {
      "epoch": 2.2672296524111615,
      "grad_norm": 0.4572151303291321,
      "learning_rate": 2.969446498839997e-05,
      "loss": 2.4106,
      "step": 110210
    },
    {
      "epoch": 2.267435371928029,
      "grad_norm": 0.4388563334941864,
      "learning_rate": 2.9678673028998427e-05,
      "loss": 2.4119,
      "step": 110220
    },
    {
      "epoch": 2.267641091444897,
      "grad_norm": 0.4622148871421814,
      "learning_rate": 2.9662884538164802e-05,
      "loss": 2.358,
      "step": 110230
    },
    {
      "epoch": 2.2678468109617644,
      "grad_norm": 0.4521782696247101,
      "learning_rate": 2.9647099516677755e-05,
      "loss": 2.3807,
      "step": 110240
    },
    {
      "epoch": 2.268052530478632,
      "grad_norm": 0.4500097632408142,
      "learning_rate": 2.9631317965315876e-05,
      "loss": 2.3832,
      "step": 110250
    },
    {
      "epoch": 2.2682582499954997,
      "grad_norm": 0.5071225166320801,
      "learning_rate": 2.961553988485769e-05,
      "loss": 2.3276,
      "step": 110260
    },
    {
      "epoch": 2.268463969512368,
      "grad_norm": 0.4844030737876892,
      "learning_rate": 2.9599765276081328e-05,
      "loss": 2.3631,
      "step": 110270
    },
    {
      "epoch": 2.2686696890292355,
      "grad_norm": 0.4286971092224121,
      "learning_rate": 2.9583994139764892e-05,
      "loss": 2.3494,
      "step": 110280
    },
    {
      "epoch": 2.268875408546103,
      "grad_norm": 0.44741567969322205,
      "learning_rate": 2.9568226476686355e-05,
      "loss": 2.3166,
      "step": 110290
    },
    {
      "epoch": 2.269081128062971,
      "grad_norm": 0.4804594814777374,
      "learning_rate": 2.955246228762337e-05,
      "loss": 2.3602,
      "step": 110300
    },
    {
      "epoch": 2.2692868475798385,
      "grad_norm": 0.443508118391037,
      "learning_rate": 2.953670157335353e-05,
      "loss": 2.3623,
      "step": 110310
    },
    {
      "epoch": 2.269492567096706,
      "grad_norm": 0.46612006425857544,
      "learning_rate": 2.952094433465423e-05,
      "loss": 2.3704,
      "step": 110320
    },
    {
      "epoch": 2.2696982866135738,
      "grad_norm": 0.4582917392253876,
      "learning_rate": 2.9505190572302645e-05,
      "loss": 2.4439,
      "step": 110330
    },
    {
      "epoch": 2.2699040061304414,
      "grad_norm": 0.45078691840171814,
      "learning_rate": 2.948944028707592e-05,
      "loss": 2.3117,
      "step": 110340
    },
    {
      "epoch": 2.2701097256473095,
      "grad_norm": 0.4736856520175934,
      "learning_rate": 2.947369347975083e-05,
      "loss": 2.3209,
      "step": 110350
    },
    {
      "epoch": 2.270315445164177,
      "grad_norm": 0.4885040819644928,
      "learning_rate": 2.9457950151104096e-05,
      "loss": 2.3905,
      "step": 110360
    },
    {
      "epoch": 2.270521164681045,
      "grad_norm": 0.43854933977127075,
      "learning_rate": 2.9442210301912332e-05,
      "loss": 2.3903,
      "step": 110370
    },
    {
      "epoch": 2.2707268841979125,
      "grad_norm": 0.5145407915115356,
      "learning_rate": 2.9426473932951815e-05,
      "loss": 2.3512,
      "step": 110380
    },
    {
      "epoch": 2.27093260371478,
      "grad_norm": 0.5012458562850952,
      "learning_rate": 2.9410741044998746e-05,
      "loss": 2.3801,
      "step": 110390
    },
    {
      "epoch": 2.2711383232316478,
      "grad_norm": 0.46179041266441345,
      "learning_rate": 2.939501163882917e-05,
      "loss": 2.3297,
      "step": 110400
    },
    {
      "epoch": 2.2713440427485154,
      "grad_norm": 0.4644233286380768,
      "learning_rate": 2.9379285715218907e-05,
      "loss": 2.3652,
      "step": 110410
    },
    {
      "epoch": 2.2715497622653835,
      "grad_norm": 0.46449756622314453,
      "learning_rate": 2.9363563274943652e-05,
      "loss": 2.3556,
      "step": 110420
    },
    {
      "epoch": 2.271755481782251,
      "grad_norm": 0.4227044880390167,
      "learning_rate": 2.9347844318778896e-05,
      "loss": 2.2939,
      "step": 110430
    },
    {
      "epoch": 2.271961201299119,
      "grad_norm": 0.4490154981613159,
      "learning_rate": 2.9332128847499974e-05,
      "loss": 2.3366,
      "step": 110440
    },
    {
      "epoch": 2.2721669208159865,
      "grad_norm": 0.4846031367778778,
      "learning_rate": 2.9316416861882034e-05,
      "loss": 2.3897,
      "step": 110450
    },
    {
      "epoch": 2.272372640332854,
      "grad_norm": 0.4307901859283447,
      "learning_rate": 2.930070836270008e-05,
      "loss": 2.3643,
      "step": 110460
    },
    {
      "epoch": 2.272578359849722,
      "grad_norm": 0.4528747498989105,
      "learning_rate": 2.9285003350728947e-05,
      "loss": 2.3877,
      "step": 110470
    },
    {
      "epoch": 2.2727840793665894,
      "grad_norm": 0.4409099817276001,
      "learning_rate": 2.9269301826743178e-05,
      "loss": 2.4294,
      "step": 110480
    },
    {
      "epoch": 2.272989798883457,
      "grad_norm": 0.4959632456302643,
      "learning_rate": 2.9253603791517346e-05,
      "loss": 2.3461,
      "step": 110490
    },
    {
      "epoch": 2.273195518400325,
      "grad_norm": 0.42707136273384094,
      "learning_rate": 2.9237909245825758e-05,
      "loss": 2.3375,
      "step": 110500
    },
    {
      "epoch": 2.273401237917193,
      "grad_norm": 0.4828132390975952,
      "learning_rate": 2.922221819044243e-05,
      "loss": 2.3656,
      "step": 110510
    },
    {
      "epoch": 2.2736069574340605,
      "grad_norm": 0.5011142492294312,
      "learning_rate": 2.9206530626141426e-05,
      "loss": 2.3431,
      "step": 110520
    },
    {
      "epoch": 2.273812676950928,
      "grad_norm": 0.46955642104148865,
      "learning_rate": 2.9190846553696504e-05,
      "loss": 2.3696,
      "step": 110530
    },
    {
      "epoch": 2.274018396467796,
      "grad_norm": 0.45928680896759033,
      "learning_rate": 2.9175165973881212e-05,
      "loss": 2.3554,
      "step": 110540
    },
    {
      "epoch": 2.2742241159846635,
      "grad_norm": 0.48403680324554443,
      "learning_rate": 2.915948888746909e-05,
      "loss": 2.379,
      "step": 110550
    },
    {
      "epoch": 2.2744298355015315,
      "grad_norm": 0.4402822256088257,
      "learning_rate": 2.914381529523331e-05,
      "loss": 2.3763,
      "step": 110560
    },
    {
      "epoch": 2.274635555018399,
      "grad_norm": 0.4959045648574829,
      "learning_rate": 2.9128145197946978e-05,
      "loss": 2.3398,
      "step": 110570
    },
    {
      "epoch": 2.274841274535267,
      "grad_norm": 0.44764837622642517,
      "learning_rate": 2.9112478596383097e-05,
      "loss": 2.3726,
      "step": 110580
    },
    {
      "epoch": 2.2750469940521345,
      "grad_norm": 0.4655272364616394,
      "learning_rate": 2.9096815491314322e-05,
      "loss": 2.4506,
      "step": 110590
    },
    {
      "epoch": 2.275252713569002,
      "grad_norm": 0.4479101300239563,
      "learning_rate": 2.9081155883513232e-05,
      "loss": 2.355,
      "step": 110600
    },
    {
      "epoch": 2.27545843308587,
      "grad_norm": 0.452913373708725,
      "learning_rate": 2.9065499773752324e-05,
      "loss": 2.3373,
      "step": 110610
    },
    {
      "epoch": 2.2756641526027375,
      "grad_norm": 0.43718934059143066,
      "learning_rate": 2.9049847162803733e-05,
      "loss": 2.3393,
      "step": 110620
    },
    {
      "epoch": 2.275869872119605,
      "grad_norm": 0.4845816195011139,
      "learning_rate": 2.9034198051439554e-05,
      "loss": 2.334,
      "step": 110630
    },
    {
      "epoch": 2.276075591636473,
      "grad_norm": 0.4563305675983429,
      "learning_rate": 2.9018552440431666e-05,
      "loss": 2.345,
      "step": 110640
    },
    {
      "epoch": 2.276281311153341,
      "grad_norm": 0.4308713972568512,
      "learning_rate": 2.900291033055178e-05,
      "loss": 2.3511,
      "step": 110650
    },
    {
      "epoch": 2.2764870306702085,
      "grad_norm": 0.5155539512634277,
      "learning_rate": 2.898727172257145e-05,
      "loss": 2.3828,
      "step": 110660
    },
    {
      "epoch": 2.276692750187076,
      "grad_norm": 0.49113163352012634,
      "learning_rate": 2.897163661726202e-05,
      "loss": 2.2762,
      "step": 110670
    },
    {
      "epoch": 2.276898469703944,
      "grad_norm": 0.4944115877151489,
      "learning_rate": 2.8956005015394704e-05,
      "loss": 2.436,
      "step": 110680
    },
    {
      "epoch": 2.2771041892208115,
      "grad_norm": 0.4711446762084961,
      "learning_rate": 2.894037691774052e-05,
      "loss": 2.3697,
      "step": 110690
    },
    {
      "epoch": 2.2773099087376796,
      "grad_norm": 0.4328397214412689,
      "learning_rate": 2.8924752325070313e-05,
      "loss": 2.314,
      "step": 110700
    },
    {
      "epoch": 2.2775156282545472,
      "grad_norm": 0.4963859021663666,
      "learning_rate": 2.8909131238154764e-05,
      "loss": 2.3768,
      "step": 110710
    },
    {
      "epoch": 2.277721347771415,
      "grad_norm": 0.4681400656700134,
      "learning_rate": 2.889351365776436e-05,
      "loss": 2.3907,
      "step": 110720
    },
    {
      "epoch": 2.2779270672882825,
      "grad_norm": 0.4347229599952698,
      "learning_rate": 2.8877899584669453e-05,
      "loss": 2.4111,
      "step": 110730
    },
    {
      "epoch": 2.27813278680515,
      "grad_norm": 0.4101843237876892,
      "learning_rate": 2.8862289019640187e-05,
      "loss": 2.3868,
      "step": 110740
    },
    {
      "epoch": 2.278338506322018,
      "grad_norm": 0.4322785437107086,
      "learning_rate": 2.8846681963446554e-05,
      "loss": 2.3364,
      "step": 110750
    },
    {
      "epoch": 2.2785442258388855,
      "grad_norm": 0.47490113973617554,
      "learning_rate": 2.883107841685835e-05,
      "loss": 2.3784,
      "step": 110760
    },
    {
      "epoch": 2.278749945355753,
      "grad_norm": 0.44643068313598633,
      "learning_rate": 2.8815478380645235e-05,
      "loss": 2.3807,
      "step": 110770
    },
    {
      "epoch": 2.278955664872621,
      "grad_norm": 0.4827776253223419,
      "learning_rate": 2.879988185557666e-05,
      "loss": 2.3437,
      "step": 110780
    },
    {
      "epoch": 2.279161384389489,
      "grad_norm": 0.46198368072509766,
      "learning_rate": 2.8784288842421946e-05,
      "loss": 2.3257,
      "step": 110790
    },
    {
      "epoch": 2.2793671039063566,
      "grad_norm": 0.46157872676849365,
      "learning_rate": 2.8768699341950123e-05,
      "loss": 2.3661,
      "step": 110800
    },
    {
      "epoch": 2.279572823423224,
      "grad_norm": 0.43863192200660706,
      "learning_rate": 2.8753113354930227e-05,
      "loss": 2.3878,
      "step": 110810
    },
    {
      "epoch": 2.279778542940092,
      "grad_norm": 0.4579114019870758,
      "learning_rate": 2.8737530882131037e-05,
      "loss": 2.3986,
      "step": 110820
    },
    {
      "epoch": 2.2799842624569595,
      "grad_norm": 0.44849440455436707,
      "learning_rate": 2.8721951924321044e-05,
      "loss": 2.3091,
      "step": 110830
    },
    {
      "epoch": 2.280189981973827,
      "grad_norm": 0.44194504618644714,
      "learning_rate": 2.870637648226878e-05,
      "loss": 2.3762,
      "step": 110840
    },
    {
      "epoch": 2.2803957014906953,
      "grad_norm": 0.5598713755607605,
      "learning_rate": 2.869080455674249e-05,
      "loss": 2.3819,
      "step": 110850
    },
    {
      "epoch": 2.280601421007563,
      "grad_norm": 0.4678436815738678,
      "learning_rate": 2.8675236148510166e-05,
      "loss": 2.3324,
      "step": 110860
    },
    {
      "epoch": 2.2808071405244306,
      "grad_norm": 0.4510978162288666,
      "learning_rate": 2.8659671258339828e-05,
      "loss": 2.3725,
      "step": 110870
    },
    {
      "epoch": 2.281012860041298,
      "grad_norm": 0.5627318024635315,
      "learning_rate": 2.8644109886999116e-05,
      "loss": 2.3475,
      "step": 110880
    },
    {
      "epoch": 2.281218579558166,
      "grad_norm": 0.8725825548171997,
      "learning_rate": 2.8628552035255596e-05,
      "loss": 2.348,
      "step": 110890
    },
    {
      "epoch": 2.2814242990750335,
      "grad_norm": 0.522555947303772,
      "learning_rate": 2.8612997703876733e-05,
      "loss": 2.4049,
      "step": 110900
    },
    {
      "epoch": 2.281630018591901,
      "grad_norm": 0.44471219182014465,
      "learning_rate": 2.8597446893629643e-05,
      "loss": 2.4085,
      "step": 110910
    },
    {
      "epoch": 2.281835738108769,
      "grad_norm": 0.4894287586212158,
      "learning_rate": 2.858189960528138e-05,
      "loss": 2.3433,
      "step": 110920
    },
    {
      "epoch": 2.282041457625637,
      "grad_norm": 0.4960642457008362,
      "learning_rate": 2.856635583959889e-05,
      "loss": 2.3441,
      "step": 110930
    },
    {
      "epoch": 2.2822471771425046,
      "grad_norm": 0.46456873416900635,
      "learning_rate": 2.8550815597348758e-05,
      "loss": 2.3877,
      "step": 110940
    },
    {
      "epoch": 2.2824528966593722,
      "grad_norm": 0.5722111463546753,
      "learning_rate": 2.8535278879297544e-05,
      "loss": 2.4165,
      "step": 110950
    },
    {
      "epoch": 2.28265861617624,
      "grad_norm": 0.455234169960022,
      "learning_rate": 2.8519745686211596e-05,
      "loss": 2.4008,
      "step": 110960
    },
    {
      "epoch": 2.2828643356931075,
      "grad_norm": 0.46309226751327515,
      "learning_rate": 2.8504216018857065e-05,
      "loss": 2.3726,
      "step": 110970
    },
    {
      "epoch": 2.283070055209975,
      "grad_norm": 0.475708544254303,
      "learning_rate": 2.8488689877999954e-05,
      "loss": 2.363,
      "step": 110980
    },
    {
      "epoch": 2.2832757747268433,
      "grad_norm": 0.45693135261535645,
      "learning_rate": 2.8473167264406088e-05,
      "loss": 2.4124,
      "step": 110990
    },
    {
      "epoch": 2.283481494243711,
      "grad_norm": 0.4672715663909912,
      "learning_rate": 2.84576481788411e-05,
      "loss": 2.3499,
      "step": 111000
    },
    {
      "epoch": 2.2836872137605786,
      "grad_norm": 0.4393484592437744,
      "learning_rate": 2.844213262207047e-05,
      "loss": 2.3471,
      "step": 111010
    },
    {
      "epoch": 2.2838929332774462,
      "grad_norm": 0.5016974806785583,
      "learning_rate": 2.8426620594859487e-05,
      "loss": 2.3503,
      "step": 111020
    },
    {
      "epoch": 2.284098652794314,
      "grad_norm": 0.47375378012657166,
      "learning_rate": 2.841111209797329e-05,
      "loss": 2.3632,
      "step": 111030
    },
    {
      "epoch": 2.2843043723111816,
      "grad_norm": 0.47280552983283997,
      "learning_rate": 2.8395607132176817e-05,
      "loss": 2.3226,
      "step": 111040
    },
    {
      "epoch": 2.284510091828049,
      "grad_norm": 0.8500194549560547,
      "learning_rate": 2.838010569823485e-05,
      "loss": 2.3331,
      "step": 111050
    },
    {
      "epoch": 2.284715811344917,
      "grad_norm": 0.4165990650653839,
      "learning_rate": 2.836460779691199e-05,
      "loss": 2.3412,
      "step": 111060
    },
    {
      "epoch": 2.284921530861785,
      "grad_norm": 0.4734727442264557,
      "learning_rate": 2.834911342897265e-05,
      "loss": 2.3785,
      "step": 111070
    },
    {
      "epoch": 2.2851272503786526,
      "grad_norm": 0.4334493577480316,
      "learning_rate": 2.833362259518111e-05,
      "loss": 2.386,
      "step": 111080
    },
    {
      "epoch": 2.2853329698955203,
      "grad_norm": 0.46317237615585327,
      "learning_rate": 2.8318135296301417e-05,
      "loss": 2.3858,
      "step": 111090
    },
    {
      "epoch": 2.285538689412388,
      "grad_norm": 0.4817892014980316,
      "learning_rate": 2.83026515330975e-05,
      "loss": 2.3779,
      "step": 111100
    },
    {
      "epoch": 2.2857444089292556,
      "grad_norm": 0.4603945016860962,
      "learning_rate": 2.8287171306333104e-05,
      "loss": 2.3254,
      "step": 111110
    },
    {
      "epoch": 2.285950128446123,
      "grad_norm": 0.4534227252006531,
      "learning_rate": 2.8271694616771705e-05,
      "loss": 2.382,
      "step": 111120
    },
    {
      "epoch": 2.286155847962991,
      "grad_norm": 0.44363731145858765,
      "learning_rate": 2.825622146517676e-05,
      "loss": 2.3449,
      "step": 111130
    },
    {
      "epoch": 2.286361567479859,
      "grad_norm": 0.7330839037895203,
      "learning_rate": 2.8240751852311485e-05,
      "loss": 2.3545,
      "step": 111140
    },
    {
      "epoch": 2.2865672869967266,
      "grad_norm": 0.48424845933914185,
      "learning_rate": 2.8225285778938825e-05,
      "loss": 2.3758,
      "step": 111150
    },
    {
      "epoch": 2.2867730065135943,
      "grad_norm": 0.46376949548721313,
      "learning_rate": 2.820982324582172e-05,
      "loss": 2.4102,
      "step": 111160
    },
    {
      "epoch": 2.286978726030462,
      "grad_norm": 0.4630819857120514,
      "learning_rate": 2.8194364253722862e-05,
      "loss": 2.3544,
      "step": 111170
    },
    {
      "epoch": 2.2871844455473296,
      "grad_norm": 0.4725273847579956,
      "learning_rate": 2.817890880340466e-05,
      "loss": 2.3767,
      "step": 111180
    },
    {
      "epoch": 2.2873901650641972,
      "grad_norm": 0.4468585252761841,
      "learning_rate": 2.8163456895629557e-05,
      "loss": 2.3645,
      "step": 111190
    },
    {
      "epoch": 2.287595884581065,
      "grad_norm": 0.48198485374450684,
      "learning_rate": 2.814800853115964e-05,
      "loss": 2.4305,
      "step": 111200
    },
    {
      "epoch": 2.2878016040979325,
      "grad_norm": 0.4706861972808838,
      "learning_rate": 2.8132563710756887e-05,
      "loss": 2.3733,
      "step": 111210
    },
    {
      "epoch": 2.2880073236148006,
      "grad_norm": 0.514840304851532,
      "learning_rate": 2.8117122435183196e-05,
      "loss": 2.3672,
      "step": 111220
    },
    {
      "epoch": 2.2882130431316683,
      "grad_norm": 0.5271784663200378,
      "learning_rate": 2.8101684705200104e-05,
      "loss": 2.3433,
      "step": 111230
    },
    {
      "epoch": 2.288418762648536,
      "grad_norm": 0.5485773086547852,
      "learning_rate": 2.808625052156908e-05,
      "loss": 2.38,
      "step": 111240
    },
    {
      "epoch": 2.2886244821654036,
      "grad_norm": 0.4492727518081665,
      "learning_rate": 2.8070819885051504e-05,
      "loss": 2.3932,
      "step": 111250
    },
    {
      "epoch": 2.2888302016822712,
      "grad_norm": 0.4453960955142975,
      "learning_rate": 2.8055392796408374e-05,
      "loss": 2.3748,
      "step": 111260
    },
    {
      "epoch": 2.289035921199139,
      "grad_norm": 0.45379745960235596,
      "learning_rate": 2.8039969256400666e-05,
      "loss": 2.3886,
      "step": 111270
    },
    {
      "epoch": 2.289241640716007,
      "grad_norm": 0.5187098383903503,
      "learning_rate": 2.8024549265789147e-05,
      "loss": 2.3398,
      "step": 111280
    },
    {
      "epoch": 2.2894473602328747,
      "grad_norm": 0.5025597214698792,
      "learning_rate": 2.8009132825334396e-05,
      "loss": 2.3609,
      "step": 111290
    },
    {
      "epoch": 2.2896530797497423,
      "grad_norm": 0.4281863272190094,
      "learning_rate": 2.7993719935796814e-05,
      "loss": 2.3247,
      "step": 111300
    },
    {
      "epoch": 2.28985879926661,
      "grad_norm": 0.4327210783958435,
      "learning_rate": 2.797831059793663e-05,
      "loss": 2.3648,
      "step": 111310
    },
    {
      "epoch": 2.2900645187834776,
      "grad_norm": 0.4374861419200897,
      "learning_rate": 2.7962904812513922e-05,
      "loss": 2.3616,
      "step": 111320
    },
    {
      "epoch": 2.2902702383003453,
      "grad_norm": 0.4166972041130066,
      "learning_rate": 2.7947502580288576e-05,
      "loss": 2.4064,
      "step": 111330
    },
    {
      "epoch": 2.290475957817213,
      "grad_norm": 0.4705306887626648,
      "learning_rate": 2.7932103902020278e-05,
      "loss": 2.3863,
      "step": 111340
    },
    {
      "epoch": 2.2906816773340806,
      "grad_norm": 0.44849893450737,
      "learning_rate": 2.791670877846857e-05,
      "loss": 2.3903,
      "step": 111350
    },
    {
      "epoch": 2.2908873968509487,
      "grad_norm": 0.47387123107910156,
      "learning_rate": 2.790131721039281e-05,
      "loss": 2.3826,
      "step": 111360
    },
    {
      "epoch": 2.2910931163678163,
      "grad_norm": 0.41922760009765625,
      "learning_rate": 2.7885929198552186e-05,
      "loss": 2.3412,
      "step": 111370
    },
    {
      "epoch": 2.291298835884684,
      "grad_norm": 0.4704445004463196,
      "learning_rate": 2.7870544743705696e-05,
      "loss": 2.3688,
      "step": 111380
    },
    {
      "epoch": 2.2915045554015516,
      "grad_norm": 0.47305405139923096,
      "learning_rate": 2.7855163846612188e-05,
      "loss": 2.3984,
      "step": 111390
    },
    {
      "epoch": 2.2917102749184193,
      "grad_norm": 0.47624993324279785,
      "learning_rate": 2.783978650803032e-05,
      "loss": 2.3562,
      "step": 111400
    },
    {
      "epoch": 2.291915994435287,
      "grad_norm": 0.4590045213699341,
      "learning_rate": 2.7824412728718508e-05,
      "loss": 2.3509,
      "step": 111410
    },
    {
      "epoch": 2.292121713952155,
      "grad_norm": 0.4517720937728882,
      "learning_rate": 2.7809042509435124e-05,
      "loss": 2.3949,
      "step": 111420
    },
    {
      "epoch": 2.2923274334690227,
      "grad_norm": 0.4543667137622833,
      "learning_rate": 2.7793675850938318e-05,
      "loss": 2.3835,
      "step": 111430
    },
    {
      "epoch": 2.2925331529858903,
      "grad_norm": 0.4416656494140625,
      "learning_rate": 2.7778312753985945e-05,
      "loss": 2.3788,
      "step": 111440
    },
    {
      "epoch": 2.292738872502758,
      "grad_norm": 0.5155435800552368,
      "learning_rate": 2.7762953219335875e-05,
      "loss": 2.3258,
      "step": 111450
    },
    {
      "epoch": 2.2929445920196256,
      "grad_norm": 0.4441216289997101,
      "learning_rate": 2.7747597247745706e-05,
      "loss": 2.4122,
      "step": 111460
    },
    {
      "epoch": 2.2931503115364933,
      "grad_norm": 0.4024996757507324,
      "learning_rate": 2.7732244839972764e-05,
      "loss": 2.3444,
      "step": 111470
    },
    {
      "epoch": 2.293356031053361,
      "grad_norm": 0.4663035273551941,
      "learning_rate": 2.771689599677445e-05,
      "loss": 2.3975,
      "step": 111480
    },
    {
      "epoch": 2.2935617505702286,
      "grad_norm": 0.4636293649673462,
      "learning_rate": 2.770155071890772e-05,
      "loss": 2.3581,
      "step": 111490
    },
    {
      "epoch": 2.2937674700870967,
      "grad_norm": 0.441829651594162,
      "learning_rate": 2.7686209007129494e-05,
      "loss": 2.3898,
      "step": 111500
    },
    {
      "epoch": 2.2939731896039643,
      "grad_norm": 0.4947473108768463,
      "learning_rate": 2.7670870862196562e-05,
      "loss": 2.3662,
      "step": 111510
    },
    {
      "epoch": 2.294178909120832,
      "grad_norm": 0.5084109306335449,
      "learning_rate": 2.76555362848654e-05,
      "loss": 2.4505,
      "step": 111520
    },
    {
      "epoch": 2.2943846286376997,
      "grad_norm": 0.47257551550865173,
      "learning_rate": 2.7640205275892374e-05,
      "loss": 2.3422,
      "step": 111530
    },
    {
      "epoch": 2.2945903481545673,
      "grad_norm": 0.47789400815963745,
      "learning_rate": 2.7624877836033768e-05,
      "loss": 2.352,
      "step": 111540
    },
    {
      "epoch": 2.294796067671435,
      "grad_norm": 0.4516882300376892,
      "learning_rate": 2.760955396604551e-05,
      "loss": 2.3492,
      "step": 111550
    },
    {
      "epoch": 2.2950017871883026,
      "grad_norm": 0.4563126564025879,
      "learning_rate": 2.759423366668348e-05,
      "loss": 2.3473,
      "step": 111560
    },
    {
      "epoch": 2.2952075067051707,
      "grad_norm": 0.45703786611557007,
      "learning_rate": 2.7578916938703337e-05,
      "loss": 2.407,
      "step": 111570
    },
    {
      "epoch": 2.2954132262220384,
      "grad_norm": 0.41024816036224365,
      "learning_rate": 2.756360378286057e-05,
      "loss": 2.3589,
      "step": 111580
    },
    {
      "epoch": 2.295618945738906,
      "grad_norm": 0.45465660095214844,
      "learning_rate": 2.7548294199910494e-05,
      "loss": 2.3826,
      "step": 111590
    },
    {
      "epoch": 2.2958246652557737,
      "grad_norm": 0.4968312978744507,
      "learning_rate": 2.7532988190608254e-05,
      "loss": 2.3322,
      "step": 111600
    },
    {
      "epoch": 2.2960303847726413,
      "grad_norm": 0.43403294682502747,
      "learning_rate": 2.751768575570881e-05,
      "loss": 2.4012,
      "step": 111610
    },
    {
      "epoch": 2.296236104289509,
      "grad_norm": 0.44314149022102356,
      "learning_rate": 2.7502386895966946e-05,
      "loss": 2.3804,
      "step": 111620
    },
    {
      "epoch": 2.2964418238063766,
      "grad_norm": 0.47524294257164,
      "learning_rate": 2.7487091612137272e-05,
      "loss": 2.3885,
      "step": 111630
    },
    {
      "epoch": 2.2966475433232443,
      "grad_norm": 0.42815032601356506,
      "learning_rate": 2.7471799904974216e-05,
      "loss": 2.3553,
      "step": 111640
    },
    {
      "epoch": 2.2968532628401124,
      "grad_norm": 0.4389416575431824,
      "learning_rate": 2.7456511775232052e-05,
      "loss": 2.3599,
      "step": 111650
    },
    {
      "epoch": 2.29705898235698,
      "grad_norm": 0.44605493545532227,
      "learning_rate": 2.7441227223664835e-05,
      "loss": 2.3338,
      "step": 111660
    },
    {
      "epoch": 2.2972647018738477,
      "grad_norm": 0.5609955787658691,
      "learning_rate": 2.742594625102649e-05,
      "loss": 2.3979,
      "step": 111670
    },
    {
      "epoch": 2.2974704213907153,
      "grad_norm": 0.4495196044445038,
      "learning_rate": 2.7410668858070733e-05,
      "loss": 2.384,
      "step": 111680
    },
    {
      "epoch": 2.297676140907583,
      "grad_norm": 0.4662005603313446,
      "learning_rate": 2.739539504555111e-05,
      "loss": 2.3604,
      "step": 111690
    },
    {
      "epoch": 2.2978818604244506,
      "grad_norm": 0.465679794549942,
      "learning_rate": 2.738012481422101e-05,
      "loss": 2.3349,
      "step": 111700
    },
    {
      "epoch": 2.2980875799413187,
      "grad_norm": 0.44826540350914,
      "learning_rate": 2.736485816483362e-05,
      "loss": 2.4325,
      "step": 111710
    },
    {
      "epoch": 2.2982932994581864,
      "grad_norm": 0.41153696179389954,
      "learning_rate": 2.734959509814199e-05,
      "loss": 2.3767,
      "step": 111720
    },
    {
      "epoch": 2.298499018975054,
      "grad_norm": 0.5071777701377869,
      "learning_rate": 2.7334335614898875e-05,
      "loss": 2.2926,
      "step": 111730
    },
    {
      "epoch": 2.2987047384919217,
      "grad_norm": 0.4457131028175354,
      "learning_rate": 2.731907971585703e-05,
      "loss": 2.331,
      "step": 111740
    },
    {
      "epoch": 2.2989104580087893,
      "grad_norm": 0.4215124547481537,
      "learning_rate": 2.7303827401768955e-05,
      "loss": 2.3604,
      "step": 111750
    },
    {
      "epoch": 2.299116177525657,
      "grad_norm": 0.4512816071510315,
      "learning_rate": 2.7288578673386878e-05,
      "loss": 2.4478,
      "step": 111760
    },
    {
      "epoch": 2.2993218970425247,
      "grad_norm": 0.4505968987941742,
      "learning_rate": 2.7273333531463e-05,
      "loss": 2.3968,
      "step": 111770
    },
    {
      "epoch": 2.2995276165593923,
      "grad_norm": 0.46423834562301636,
      "learning_rate": 2.7258091976749302e-05,
      "loss": 2.3091,
      "step": 111780
    },
    {
      "epoch": 2.2997333360762604,
      "grad_norm": 0.4584921598434448,
      "learning_rate": 2.7242854009997476e-05,
      "loss": 2.3178,
      "step": 111790
    },
    {
      "epoch": 2.299939055593128,
      "grad_norm": 0.4811294376850128,
      "learning_rate": 2.7227619631959232e-05,
      "loss": 2.3355,
      "step": 111800
    },
    {
      "epoch": 2.3001447751099957,
      "grad_norm": 0.4167312681674957,
      "learning_rate": 2.7212388843385926e-05,
      "loss": 2.3915,
      "step": 111810
    },
    {
      "epoch": 2.3003504946268634,
      "grad_norm": 0.5011445879936218,
      "learning_rate": 2.71971616450288e-05,
      "loss": 2.3883,
      "step": 111820
    },
    {
      "epoch": 2.300556214143731,
      "grad_norm": 0.4613228738307953,
      "learning_rate": 2.7181938037639033e-05,
      "loss": 2.3877,
      "step": 111830
    },
    {
      "epoch": 2.3007619336605987,
      "grad_norm": 0.4965682029724121,
      "learning_rate": 2.7166718021967408e-05,
      "loss": 2.3294,
      "step": 111840
    },
    {
      "epoch": 2.3009676531774663,
      "grad_norm": 0.4740675687789917,
      "learning_rate": 2.715150159876466e-05,
      "loss": 2.3195,
      "step": 111850
    },
    {
      "epoch": 2.3011733726943344,
      "grad_norm": 0.4685235619544983,
      "learning_rate": 2.7136288768781425e-05,
      "loss": 2.3787,
      "step": 111860
    },
    {
      "epoch": 2.301379092211202,
      "grad_norm": 0.4439912438392639,
      "learning_rate": 2.712107953276798e-05,
      "loss": 2.3898,
      "step": 111870
    },
    {
      "epoch": 2.3015848117280697,
      "grad_norm": 0.4701804220676422,
      "learning_rate": 2.710587389147453e-05,
      "loss": 2.2975,
      "step": 111880
    },
    {
      "epoch": 2.3017905312449374,
      "grad_norm": 0.47053563594818115,
      "learning_rate": 2.7090671845651107e-05,
      "loss": 2.3903,
      "step": 111890
    },
    {
      "epoch": 2.301996250761805,
      "grad_norm": 0.4333008825778961,
      "learning_rate": 2.7075473396047525e-05,
      "loss": 2.328,
      "step": 111900
    },
    {
      "epoch": 2.3022019702786727,
      "grad_norm": 0.5213091969490051,
      "learning_rate": 2.7060278543413454e-05,
      "loss": 2.3902,
      "step": 111910
    },
    {
      "epoch": 2.3024076897955403,
      "grad_norm": 0.4515223503112793,
      "learning_rate": 2.7045087288498372e-05,
      "loss": 2.3648,
      "step": 111920
    },
    {
      "epoch": 2.302613409312408,
      "grad_norm": 0.45736274123191833,
      "learning_rate": 2.702989963205157e-05,
      "loss": 2.3883,
      "step": 111930
    },
    {
      "epoch": 2.302819128829276,
      "grad_norm": 0.43786901235580444,
      "learning_rate": 2.701471557482218e-05,
      "loss": 2.3676,
      "step": 111940
    },
    {
      "epoch": 2.3030248483461437,
      "grad_norm": 0.4989616274833679,
      "learning_rate": 2.699953511755915e-05,
      "loss": 2.3773,
      "step": 111950
    },
    {
      "epoch": 2.3032305678630114,
      "grad_norm": 0.4519272446632385,
      "learning_rate": 2.6984358261011255e-05,
      "loss": 2.4165,
      "step": 111960
    },
    {
      "epoch": 2.303436287379879,
      "grad_norm": 0.4704861044883728,
      "learning_rate": 2.6969185005927067e-05,
      "loss": 2.3476,
      "step": 111970
    },
    {
      "epoch": 2.3036420068967467,
      "grad_norm": 0.4457259476184845,
      "learning_rate": 2.695401535305502e-05,
      "loss": 2.4016,
      "step": 111980
    },
    {
      "epoch": 2.3038477264136143,
      "grad_norm": 0.469741553068161,
      "learning_rate": 2.6938849303143342e-05,
      "loss": 2.3255,
      "step": 111990
    },
    {
      "epoch": 2.3040534459304824,
      "grad_norm": 0.4546036422252655,
      "learning_rate": 2.6923686856940088e-05,
      "loss": 2.3572,
      "step": 112000
    },
    {
      "epoch": 2.30425916544735,
      "grad_norm": 0.42549705505371094,
      "learning_rate": 2.6908528015193147e-05,
      "loss": 2.3446,
      "step": 112010
    },
    {
      "epoch": 2.3044648849642178,
      "grad_norm": 0.4802498519420624,
      "learning_rate": 2.6893372778650218e-05,
      "loss": 2.3763,
      "step": 112020
    },
    {
      "epoch": 2.3046706044810854,
      "grad_norm": 0.43920278549194336,
      "learning_rate": 2.6878221148058825e-05,
      "loss": 2.3854,
      "step": 112030
    },
    {
      "epoch": 2.304876323997953,
      "grad_norm": 0.4514409005641937,
      "learning_rate": 2.686307312416635e-05,
      "loss": 2.3584,
      "step": 112040
    },
    {
      "epoch": 2.3050820435148207,
      "grad_norm": 0.48679694533348083,
      "learning_rate": 2.684792870771987e-05,
      "loss": 2.2998,
      "step": 112050
    },
    {
      "epoch": 2.3052877630316884,
      "grad_norm": 0.48813965916633606,
      "learning_rate": 2.683278789946646e-05,
      "loss": 2.3179,
      "step": 112060
    },
    {
      "epoch": 2.305493482548556,
      "grad_norm": 0.4393535852432251,
      "learning_rate": 2.681765070015294e-05,
      "loss": 2.3718,
      "step": 112070
    },
    {
      "epoch": 2.305699202065424,
      "grad_norm": 0.5048711895942688,
      "learning_rate": 2.680251711052586e-05,
      "loss": 2.4664,
      "step": 112080
    },
    {
      "epoch": 2.3059049215822918,
      "grad_norm": 0.45813530683517456,
      "learning_rate": 2.6787387131331765e-05,
      "loss": 2.383,
      "step": 112090
    },
    {
      "epoch": 2.3061106410991594,
      "grad_norm": 0.4418959617614746,
      "learning_rate": 2.6772260763316925e-05,
      "loss": 2.3777,
      "step": 112100
    },
    {
      "epoch": 2.306316360616027,
      "grad_norm": 0.400709867477417,
      "learning_rate": 2.675713800722737e-05,
      "loss": 2.3681,
      "step": 112110
    },
    {
      "epoch": 2.3065220801328947,
      "grad_norm": 0.42697906494140625,
      "learning_rate": 2.6742018863809114e-05,
      "loss": 2.3282,
      "step": 112120
    },
    {
      "epoch": 2.3067277996497624,
      "grad_norm": 0.4429018795490265,
      "learning_rate": 2.672690333380784e-05,
      "loss": 2.3657,
      "step": 112130
    },
    {
      "epoch": 2.3069335191666305,
      "grad_norm": 0.4552491009235382,
      "learning_rate": 2.6711791417969102e-05,
      "loss": 2.3811,
      "step": 112140
    },
    {
      "epoch": 2.307139238683498,
      "grad_norm": 0.4612879455089569,
      "learning_rate": 2.669668311703838e-05,
      "loss": 2.3618,
      "step": 112150
    },
    {
      "epoch": 2.307344958200366,
      "grad_norm": 0.49056631326675415,
      "learning_rate": 2.668157843176079e-05,
      "loss": 2.3395,
      "step": 112160
    },
    {
      "epoch": 2.3075506777172334,
      "grad_norm": 0.4577334225177765,
      "learning_rate": 2.666647736288137e-05,
      "loss": 2.3957,
      "step": 112170
    },
    {
      "epoch": 2.307756397234101,
      "grad_norm": 0.4430738687515259,
      "learning_rate": 2.665137991114507e-05,
      "loss": 2.3692,
      "step": 112180
    },
    {
      "epoch": 2.3079621167509687,
      "grad_norm": 0.43965741991996765,
      "learning_rate": 2.663628607729646e-05,
      "loss": 2.3839,
      "step": 112190
    },
    {
      "epoch": 2.3081678362678364,
      "grad_norm": 0.4325494170188904,
      "learning_rate": 2.6621195862080072e-05,
      "loss": 2.366,
      "step": 112200
    },
    {
      "epoch": 2.308373555784704,
      "grad_norm": 0.5254819393157959,
      "learning_rate": 2.660610926624023e-05,
      "loss": 2.4295,
      "step": 112210
    },
    {
      "epoch": 2.308579275301572,
      "grad_norm": 0.4554785490036011,
      "learning_rate": 2.659102629052107e-05,
      "loss": 2.3439,
      "step": 112220
    },
    {
      "epoch": 2.30878499481844,
      "grad_norm": 0.46929532289505005,
      "learning_rate": 2.6575946935666553e-05,
      "loss": 2.404,
      "step": 112230
    },
    {
      "epoch": 2.3089907143353074,
      "grad_norm": 0.4525572955608368,
      "learning_rate": 2.6560871202420457e-05,
      "loss": 2.3563,
      "step": 112240
    },
    {
      "epoch": 2.309196433852175,
      "grad_norm": 0.48861679434776306,
      "learning_rate": 2.6545799091526392e-05,
      "loss": 2.3603,
      "step": 112250
    },
    {
      "epoch": 2.3094021533690428,
      "grad_norm": 0.4425332248210907,
      "learning_rate": 2.653073060372777e-05,
      "loss": 2.3901,
      "step": 112260
    },
    {
      "epoch": 2.3096078728859104,
      "grad_norm": 0.4389071762561798,
      "learning_rate": 2.6515665739767858e-05,
      "loss": 2.3304,
      "step": 112270
    },
    {
      "epoch": 2.309813592402778,
      "grad_norm": 0.4174850285053253,
      "learning_rate": 2.6500604500389714e-05,
      "loss": 2.2871,
      "step": 112280
    },
    {
      "epoch": 2.310019311919646,
      "grad_norm": 0.47507867217063904,
      "learning_rate": 2.6485546886336233e-05,
      "loss": 2.3552,
      "step": 112290
    },
    {
      "epoch": 2.310225031436514,
      "grad_norm": 0.48069700598716736,
      "learning_rate": 2.6470492898350108e-05,
      "loss": 2.3956,
      "step": 112300
    },
    {
      "epoch": 2.3104307509533815,
      "grad_norm": 0.4390973448753357,
      "learning_rate": 2.645544253717388e-05,
      "loss": 2.3612,
      "step": 112310
    },
    {
      "epoch": 2.310636470470249,
      "grad_norm": 0.477188378572464,
      "learning_rate": 2.64403958035499e-05,
      "loss": 2.3858,
      "step": 112320
    },
    {
      "epoch": 2.3108421899871168,
      "grad_norm": 0.43940019607543945,
      "learning_rate": 2.6425352698220353e-05,
      "loss": 2.3407,
      "step": 112330
    },
    {
      "epoch": 2.3110479095039844,
      "grad_norm": 0.5049059391021729,
      "learning_rate": 2.641031322192722e-05,
      "loss": 2.3339,
      "step": 112340
    },
    {
      "epoch": 2.311253629020852,
      "grad_norm": 0.4852531850337982,
      "learning_rate": 2.639527737541231e-05,
      "loss": 2.4269,
      "step": 112350
    },
    {
      "epoch": 2.3114593485377197,
      "grad_norm": 0.5467103123664856,
      "learning_rate": 2.6380245159417305e-05,
      "loss": 2.3687,
      "step": 112360
    },
    {
      "epoch": 2.311665068054588,
      "grad_norm": 0.4373704493045807,
      "learning_rate": 2.636521657468357e-05,
      "loss": 2.3747,
      "step": 112370
    },
    {
      "epoch": 2.3118707875714555,
      "grad_norm": 0.4514465928077698,
      "learning_rate": 2.635019162195247e-05,
      "loss": 2.3698,
      "step": 112380
    },
    {
      "epoch": 2.312076507088323,
      "grad_norm": 0.4342992305755615,
      "learning_rate": 2.6335170301965107e-05,
      "loss": 2.3294,
      "step": 112390
    },
    {
      "epoch": 2.312282226605191,
      "grad_norm": 0.4148256778717041,
      "learning_rate": 2.6320152615462303e-05,
      "loss": 2.3308,
      "step": 112400
    },
    {
      "epoch": 2.3124879461220584,
      "grad_norm": 0.4566865861415863,
      "learning_rate": 2.6305138563184896e-05,
      "loss": 2.3183,
      "step": 112410
    },
    {
      "epoch": 2.312693665638926,
      "grad_norm": 0.4186951816082001,
      "learning_rate": 2.6290128145873437e-05,
      "loss": 2.3769,
      "step": 112420
    },
    {
      "epoch": 2.312899385155794,
      "grad_norm": 0.46644142270088196,
      "learning_rate": 2.627512136426824e-05,
      "loss": 2.3489,
      "step": 112430
    },
    {
      "epoch": 2.313105104672662,
      "grad_norm": 0.4816140830516815,
      "learning_rate": 2.6260118219109597e-05,
      "loss": 2.4245,
      "step": 112440
    },
    {
      "epoch": 2.3133108241895295,
      "grad_norm": 0.4228176176548004,
      "learning_rate": 2.6245118711137475e-05,
      "loss": 2.373,
      "step": 112450
    },
    {
      "epoch": 2.313516543706397,
      "grad_norm": 0.44185101985931396,
      "learning_rate": 2.6230122841091687e-05,
      "loss": 2.3875,
      "step": 112460
    },
    {
      "epoch": 2.313722263223265,
      "grad_norm": 0.4797965884208679,
      "learning_rate": 2.6215130609712013e-05,
      "loss": 2.3635,
      "step": 112470
    },
    {
      "epoch": 2.3139279827401324,
      "grad_norm": 0.758668065071106,
      "learning_rate": 2.6200142017737827e-05,
      "loss": 2.4229,
      "step": 112480
    },
    {
      "epoch": 2.314133702257,
      "grad_norm": 0.4644817113876343,
      "learning_rate": 2.618515706590845e-05,
      "loss": 2.4059,
      "step": 112490
    },
    {
      "epoch": 2.3143394217738678,
      "grad_norm": 0.5082584619522095,
      "learning_rate": 2.617017575496309e-05,
      "loss": 2.3285,
      "step": 112500
    },
    {
      "epoch": 2.314545141290736,
      "grad_norm": 0.4810310900211334,
      "learning_rate": 2.615519808564061e-05,
      "loss": 2.3817,
      "step": 112510
    },
    {
      "epoch": 2.3147508608076035,
      "grad_norm": 0.45490172505378723,
      "learning_rate": 2.6140224058679796e-05,
      "loss": 2.3423,
      "step": 112520
    },
    {
      "epoch": 2.314956580324471,
      "grad_norm": 0.4573148787021637,
      "learning_rate": 2.612525367481924e-05,
      "loss": 2.3681,
      "step": 112530
    },
    {
      "epoch": 2.315162299841339,
      "grad_norm": 0.4525250792503357,
      "learning_rate": 2.6110286934797356e-05,
      "loss": 2.3225,
      "step": 112540
    },
    {
      "epoch": 2.3153680193582065,
      "grad_norm": 0.5209216475486755,
      "learning_rate": 2.609532383935237e-05,
      "loss": 2.3787,
      "step": 112550
    },
    {
      "epoch": 2.315573738875074,
      "grad_norm": 0.48560830950737,
      "learning_rate": 2.6080364389222322e-05,
      "loss": 2.3717,
      "step": 112560
    },
    {
      "epoch": 2.315779458391942,
      "grad_norm": 0.4596981704235077,
      "learning_rate": 2.606540858514509e-05,
      "loss": 2.3443,
      "step": 112570
    },
    {
      "epoch": 2.31598517790881,
      "grad_norm": 0.4328109323978424,
      "learning_rate": 2.6050456427858348e-05,
      "loss": 2.3613,
      "step": 112580
    },
    {
      "epoch": 2.3161908974256775,
      "grad_norm": 0.45881354808807373,
      "learning_rate": 2.6035507918099622e-05,
      "loss": 2.414,
      "step": 112590
    },
    {
      "epoch": 2.316396616942545,
      "grad_norm": 0.4342547357082367,
      "learning_rate": 2.6020563056606228e-05,
      "loss": 2.3603,
      "step": 112600
    },
    {
      "epoch": 2.316602336459413,
      "grad_norm": 0.4867490828037262,
      "learning_rate": 2.6005621844115323e-05,
      "loss": 2.2892,
      "step": 112610
    },
    {
      "epoch": 2.3168080559762805,
      "grad_norm": 0.4780968725681305,
      "learning_rate": 2.5990684281363875e-05,
      "loss": 2.3732,
      "step": 112620
    },
    {
      "epoch": 2.317013775493148,
      "grad_norm": 0.44482699036598206,
      "learning_rate": 2.5975750369088657e-05,
      "loss": 2.4069,
      "step": 112630
    },
    {
      "epoch": 2.317219495010016,
      "grad_norm": 0.4518154263496399,
      "learning_rate": 2.5960820108026295e-05,
      "loss": 2.3722,
      "step": 112640
    },
    {
      "epoch": 2.3174252145268834,
      "grad_norm": 0.4519748091697693,
      "learning_rate": 2.5945893498913244e-05,
      "loss": 2.4044,
      "step": 112650
    },
    {
      "epoch": 2.3176309340437515,
      "grad_norm": 0.459139347076416,
      "learning_rate": 2.5930970542485654e-05,
      "loss": 2.3849,
      "step": 112660
    },
    {
      "epoch": 2.317836653560619,
      "grad_norm": 0.5316437482833862,
      "learning_rate": 2.59160512394797e-05,
      "loss": 2.3409,
      "step": 112670
    },
    {
      "epoch": 2.318042373077487,
      "grad_norm": 0.46002674102783203,
      "learning_rate": 2.5901135590631254e-05,
      "loss": 2.347,
      "step": 112680
    },
    {
      "epoch": 2.3182480925943545,
      "grad_norm": 0.5242493152618408,
      "learning_rate": 2.588622359667594e-05,
      "loss": 2.3139,
      "step": 112690
    },
    {
      "epoch": 2.318453812111222,
      "grad_norm": 0.42333984375,
      "learning_rate": 2.587131525834937e-05,
      "loss": 2.3757,
      "step": 112700
    },
    {
      "epoch": 2.31865953162809,
      "grad_norm": 0.4513963758945465,
      "learning_rate": 2.5856410576386892e-05,
      "loss": 2.3661,
      "step": 112710
    },
    {
      "epoch": 2.318865251144958,
      "grad_norm": 0.45472320914268494,
      "learning_rate": 2.58415095515236e-05,
      "loss": 2.3079,
      "step": 112720
    },
    {
      "epoch": 2.3190709706618255,
      "grad_norm": 0.47596997022628784,
      "learning_rate": 2.582661218449457e-05,
      "loss": 2.3722,
      "step": 112730
    },
    {
      "epoch": 2.319276690178693,
      "grad_norm": 0.4998280704021454,
      "learning_rate": 2.581171847603455e-05,
      "loss": 2.414,
      "step": 112740
    },
    {
      "epoch": 2.319482409695561,
      "grad_norm": 0.4437778890132904,
      "learning_rate": 2.579682842687814e-05,
      "loss": 2.3447,
      "step": 112750
    },
    {
      "epoch": 2.3196881292124285,
      "grad_norm": 0.4274294078350067,
      "learning_rate": 2.5781942037759888e-05,
      "loss": 2.3586,
      "step": 112760
    },
    {
      "epoch": 2.319893848729296,
      "grad_norm": 0.49025678634643555,
      "learning_rate": 2.5767059309413967e-05,
      "loss": 2.3684,
      "step": 112770
    },
    {
      "epoch": 2.320099568246164,
      "grad_norm": 0.40762701630592346,
      "learning_rate": 2.5752180242574453e-05,
      "loss": 2.3479,
      "step": 112780
    },
    {
      "epoch": 2.3203052877630315,
      "grad_norm": 0.5237839818000793,
      "learning_rate": 2.5737304837975352e-05,
      "loss": 2.3872,
      "step": 112790
    },
    {
      "epoch": 2.3205110072798996,
      "grad_norm": 0.4078086316585541,
      "learning_rate": 2.5722433096350286e-05,
      "loss": 2.3792,
      "step": 112800
    },
    {
      "epoch": 2.320716726796767,
      "grad_norm": 0.4399734139442444,
      "learning_rate": 2.5707565018432835e-05,
      "loss": 2.3211,
      "step": 112810
    },
    {
      "epoch": 2.320922446313635,
      "grad_norm": 0.47764909267425537,
      "learning_rate": 2.5692700604956354e-05,
      "loss": 2.3979,
      "step": 112820
    },
    {
      "epoch": 2.3211281658305025,
      "grad_norm": 0.45640331506729126,
      "learning_rate": 2.567783985665403e-05,
      "loss": 2.37,
      "step": 112830
    },
    {
      "epoch": 2.32133388534737,
      "grad_norm": 0.46401774883270264,
      "learning_rate": 2.566298277425886e-05,
      "loss": 2.3459,
      "step": 112840
    },
    {
      "epoch": 2.321539604864238,
      "grad_norm": 0.44607222080230713,
      "learning_rate": 2.5648129358503648e-05,
      "loss": 2.376,
      "step": 112850
    },
    {
      "epoch": 2.321745324381106,
      "grad_norm": 0.4299321174621582,
      "learning_rate": 2.5633279610121064e-05,
      "loss": 2.4008,
      "step": 112860
    },
    {
      "epoch": 2.3219510438979736,
      "grad_norm": 0.4615519940853119,
      "learning_rate": 2.5618433529843532e-05,
      "loss": 2.3542,
      "step": 112870
    },
    {
      "epoch": 2.3221567634148412,
      "grad_norm": 0.4991686940193176,
      "learning_rate": 2.5603591118403346e-05,
      "loss": 2.3832,
      "step": 112880
    },
    {
      "epoch": 2.322362482931709,
      "grad_norm": 0.447009414434433,
      "learning_rate": 2.55887523765326e-05,
      "loss": 2.352,
      "step": 112890
    },
    {
      "epoch": 2.3225682024485765,
      "grad_norm": 0.4732416272163391,
      "learning_rate": 2.5573917304963214e-05,
      "loss": 2.4049,
      "step": 112900
    },
    {
      "epoch": 2.322773921965444,
      "grad_norm": 0.47482001781463623,
      "learning_rate": 2.5559085904426903e-05,
      "loss": 2.3361,
      "step": 112910
    },
    {
      "epoch": 2.322979641482312,
      "grad_norm": 0.45103585720062256,
      "learning_rate": 2.5544258175655233e-05,
      "loss": 2.365,
      "step": 112920
    },
    {
      "epoch": 2.3231853609991795,
      "grad_norm": 0.4386520981788635,
      "learning_rate": 2.552943411937958e-05,
      "loss": 2.4197,
      "step": 112930
    },
    {
      "epoch": 2.3233910805160476,
      "grad_norm": 0.5593730807304382,
      "learning_rate": 2.5514613736331118e-05,
      "loss": 2.377,
      "step": 112940
    },
    {
      "epoch": 2.3235968000329152,
      "grad_norm": 0.4617118835449219,
      "learning_rate": 2.549979702724086e-05,
      "loss": 2.3906,
      "step": 112950
    },
    {
      "epoch": 2.323802519549783,
      "grad_norm": 0.4630429744720459,
      "learning_rate": 2.5484983992839638e-05,
      "loss": 2.3787,
      "step": 112960
    },
    {
      "epoch": 2.3240082390666505,
      "grad_norm": 0.4734352231025696,
      "learning_rate": 2.5470174633858134e-05,
      "loss": 2.309,
      "step": 112970
    },
    {
      "epoch": 2.324213958583518,
      "grad_norm": 0.43129393458366394,
      "learning_rate": 2.5455368951026715e-05,
      "loss": 2.3845,
      "step": 112980
    },
    {
      "epoch": 2.324419678100386,
      "grad_norm": 0.471562922000885,
      "learning_rate": 2.5440566945075763e-05,
      "loss": 2.3866,
      "step": 112990
    },
    {
      "epoch": 2.3246253976172535,
      "grad_norm": 0.4232329726219177,
      "learning_rate": 2.5425768616735357e-05,
      "loss": 2.3292,
      "step": 113000
    },
    {
      "epoch": 2.3248311171341216,
      "grad_norm": 0.46608874201774597,
      "learning_rate": 2.5410973966735353e-05,
      "loss": 2.3862,
      "step": 113010
    },
    {
      "epoch": 2.3250368366509893,
      "grad_norm": 0.5026090741157532,
      "learning_rate": 2.5396182995805574e-05,
      "loss": 2.3652,
      "step": 113020
    },
    {
      "epoch": 2.325242556167857,
      "grad_norm": 0.4142747223377228,
      "learning_rate": 2.538139570467557e-05,
      "loss": 2.3493,
      "step": 113030
    },
    {
      "epoch": 2.3254482756847246,
      "grad_norm": 0.48082220554351807,
      "learning_rate": 2.536661209407464e-05,
      "loss": 2.3402,
      "step": 113040
    },
    {
      "epoch": 2.325653995201592,
      "grad_norm": 0.4754911959171295,
      "learning_rate": 2.5351832164732093e-05,
      "loss": 2.3167,
      "step": 113050
    },
    {
      "epoch": 2.32585971471846,
      "grad_norm": 0.42029449343681335,
      "learning_rate": 2.533705591737685e-05,
      "loss": 2.3709,
      "step": 113060
    },
    {
      "epoch": 2.3260654342353275,
      "grad_norm": 0.48041701316833496,
      "learning_rate": 2.532228335273774e-05,
      "loss": 2.383,
      "step": 113070
    },
    {
      "epoch": 2.326271153752195,
      "grad_norm": 0.45057520270347595,
      "learning_rate": 2.5307514471543502e-05,
      "loss": 2.3549,
      "step": 113080
    },
    {
      "epoch": 2.3264768732690633,
      "grad_norm": 0.505113422870636,
      "learning_rate": 2.529274927452253e-05,
      "loss": 2.3714,
      "step": 113090
    },
    {
      "epoch": 2.326682592785931,
      "grad_norm": 0.454280287027359,
      "learning_rate": 2.52779877624031e-05,
      "loss": 2.346,
      "step": 113100
    },
    {
      "epoch": 2.3268883123027986,
      "grad_norm": 0.4377133548259735,
      "learning_rate": 2.52632299359134e-05,
      "loss": 2.3502,
      "step": 113110
    },
    {
      "epoch": 2.3270940318196662,
      "grad_norm": 0.45472702383995056,
      "learning_rate": 2.524847579578128e-05,
      "loss": 2.2982,
      "step": 113120
    },
    {
      "epoch": 2.327299751336534,
      "grad_norm": 0.49054840207099915,
      "learning_rate": 2.5233725342734505e-05,
      "loss": 2.3955,
      "step": 113130
    },
    {
      "epoch": 2.3275054708534015,
      "grad_norm": 0.4778646230697632,
      "learning_rate": 2.521897857750062e-05,
      "loss": 2.3966,
      "step": 113140
    },
    {
      "epoch": 2.3277111903702696,
      "grad_norm": 0.4450434744358063,
      "learning_rate": 2.5204235500807018e-05,
      "loss": 2.351,
      "step": 113150
    },
    {
      "epoch": 2.3279169098871373,
      "grad_norm": 0.4434875547885895,
      "learning_rate": 2.5189496113380895e-05,
      "loss": 2.4117,
      "step": 113160
    },
    {
      "epoch": 2.328122629404005,
      "grad_norm": 0.48071298003196716,
      "learning_rate": 2.517476041594925e-05,
      "loss": 2.4117,
      "step": 113170
    },
    {
      "epoch": 2.3283283489208726,
      "grad_norm": 0.4695415198802948,
      "learning_rate": 2.5160028409238933e-05,
      "loss": 2.3868,
      "step": 113180
    },
    {
      "epoch": 2.3285340684377402,
      "grad_norm": 0.45415595173835754,
      "learning_rate": 2.5145300093976586e-05,
      "loss": 2.3445,
      "step": 113190
    },
    {
      "epoch": 2.328739787954608,
      "grad_norm": 0.4626120924949646,
      "learning_rate": 2.5130575470888673e-05,
      "loss": 2.3197,
      "step": 113200
    },
    {
      "epoch": 2.3289455074714756,
      "grad_norm": 0.46247583627700806,
      "learning_rate": 2.511585454070149e-05,
      "loss": 2.3181,
      "step": 113210
    },
    {
      "epoch": 2.329151226988343,
      "grad_norm": 0.453289270401001,
      "learning_rate": 2.510113730414112e-05,
      "loss": 2.4576,
      "step": 113220
    },
    {
      "epoch": 2.3293569465052113,
      "grad_norm": 0.4696764647960663,
      "learning_rate": 2.508642376193351e-05,
      "loss": 2.3462,
      "step": 113230
    },
    {
      "epoch": 2.329562666022079,
      "grad_norm": 0.4397740066051483,
      "learning_rate": 2.5071713914804383e-05,
      "loss": 2.3563,
      "step": 113240
    },
    {
      "epoch": 2.3297683855389466,
      "grad_norm": 0.46190503239631653,
      "learning_rate": 2.505700776347929e-05,
      "loss": 2.3485,
      "step": 113250
    },
    {
      "epoch": 2.3299741050558143,
      "grad_norm": 0.45875874161720276,
      "learning_rate": 2.5042305308683623e-05,
      "loss": 2.3573,
      "step": 113260
    },
    {
      "epoch": 2.330179824572682,
      "grad_norm": 0.46651917695999146,
      "learning_rate": 2.5027606551142557e-05,
      "loss": 2.3198,
      "step": 113270
    },
    {
      "epoch": 2.3303855440895496,
      "grad_norm": 0.458661824464798,
      "learning_rate": 2.5012911491581113e-05,
      "loss": 2.3455,
      "step": 113280
    },
    {
      "epoch": 2.3305912636064177,
      "grad_norm": 0.4718257486820221,
      "learning_rate": 2.499822013072415e-05,
      "loss": 2.396,
      "step": 113290
    },
    {
      "epoch": 2.3307969831232853,
      "grad_norm": 0.4740731120109558,
      "learning_rate": 2.4983532469296212e-05,
      "loss": 2.4266,
      "step": 113300
    },
    {
      "epoch": 2.331002702640153,
      "grad_norm": 0.4851270318031311,
      "learning_rate": 2.4968848508021846e-05,
      "loss": 2.3867,
      "step": 113310
    },
    {
      "epoch": 2.3312084221570206,
      "grad_norm": 0.4229912757873535,
      "learning_rate": 2.4954168247625354e-05,
      "loss": 2.4054,
      "step": 113320
    },
    {
      "epoch": 2.3314141416738883,
      "grad_norm": 0.5505468845367432,
      "learning_rate": 2.493949168883073e-05,
      "loss": 2.3527,
      "step": 113330
    },
    {
      "epoch": 2.331619861190756,
      "grad_norm": 0.4532543420791626,
      "learning_rate": 2.4924818832361986e-05,
      "loss": 2.413,
      "step": 113340
    },
    {
      "epoch": 2.3318255807076236,
      "grad_norm": 0.48145800828933716,
      "learning_rate": 2.491014967894284e-05,
      "loss": 2.375,
      "step": 113350
    },
    {
      "epoch": 2.3320313002244912,
      "grad_norm": 0.4811289310455322,
      "learning_rate": 2.489548422929676e-05,
      "loss": 2.3664,
      "step": 113360
    },
    {
      "epoch": 2.3322370197413593,
      "grad_norm": 0.455713152885437,
      "learning_rate": 2.488082248414724e-05,
      "loss": 2.3614,
      "step": 113370
    },
    {
      "epoch": 2.332442739258227,
      "grad_norm": 0.5234171748161316,
      "learning_rate": 2.4866164444217353e-05,
      "loss": 2.3596,
      "step": 113380
    },
    {
      "epoch": 2.3326484587750946,
      "grad_norm": 0.46312063932418823,
      "learning_rate": 2.4851510110230125e-05,
      "loss": 2.3843,
      "step": 113390
    },
    {
      "epoch": 2.3328541782919623,
      "grad_norm": 0.4890000820159912,
      "learning_rate": 2.483685948290846e-05,
      "loss": 2.356,
      "step": 113400
    },
    {
      "epoch": 2.33305989780883,
      "grad_norm": 0.44718268513679504,
      "learning_rate": 2.482221256297488e-05,
      "loss": 2.3875,
      "step": 113410
    },
    {
      "epoch": 2.3332656173256976,
      "grad_norm": 0.43654099106788635,
      "learning_rate": 2.4807569351151862e-05,
      "loss": 2.3942,
      "step": 113420
    },
    {
      "epoch": 2.3334713368425652,
      "grad_norm": 0.7882264852523804,
      "learning_rate": 2.479292984816176e-05,
      "loss": 2.3479,
      "step": 113430
    },
    {
      "epoch": 2.3336770563594333,
      "grad_norm": 0.45231473445892334,
      "learning_rate": 2.4778294054726568e-05,
      "loss": 2.4197,
      "step": 113440
    },
    {
      "epoch": 2.333882775876301,
      "grad_norm": 0.4251667559146881,
      "learning_rate": 2.476366197156822e-05,
      "loss": 2.3764,
      "step": 113450
    },
    {
      "epoch": 2.3340884953931686,
      "grad_norm": 0.42788296937942505,
      "learning_rate": 2.4749033599408432e-05,
      "loss": 2.4077,
      "step": 113460
    },
    {
      "epoch": 2.3342942149100363,
      "grad_norm": 0.5018112063407898,
      "learning_rate": 2.4734408938968755e-05,
      "loss": 2.3683,
      "step": 113470
    },
    {
      "epoch": 2.334499934426904,
      "grad_norm": 0.4654906392097473,
      "learning_rate": 2.4719787990970534e-05,
      "loss": 2.3293,
      "step": 113480
    },
    {
      "epoch": 2.3347056539437716,
      "grad_norm": 0.47224390506744385,
      "learning_rate": 2.4705170756134955e-05,
      "loss": 2.3922,
      "step": 113490
    },
    {
      "epoch": 2.3349113734606393,
      "grad_norm": 0.4478452801704407,
      "learning_rate": 2.4690557235182976e-05,
      "loss": 2.3485,
      "step": 113500
    },
    {
      "epoch": 2.335117092977507,
      "grad_norm": 0.45996955037117004,
      "learning_rate": 2.4675947428835435e-05,
      "loss": 2.3847,
      "step": 113510
    },
    {
      "epoch": 2.335322812494375,
      "grad_norm": 0.42081552743911743,
      "learning_rate": 2.4661341337812937e-05,
      "loss": 2.3763,
      "step": 113520
    },
    {
      "epoch": 2.3355285320112427,
      "grad_norm": 0.46526700258255005,
      "learning_rate": 2.4646738962835924e-05,
      "loss": 2.3984,
      "step": 113530
    },
    {
      "epoch": 2.3357342515281103,
      "grad_norm": 0.47627851366996765,
      "learning_rate": 2.4632140304624653e-05,
      "loss": 2.3025,
      "step": 113540
    },
    {
      "epoch": 2.335939971044978,
      "grad_norm": 0.521388053894043,
      "learning_rate": 2.4617545363899187e-05,
      "loss": 2.3751,
      "step": 113550
    },
    {
      "epoch": 2.3361456905618456,
      "grad_norm": 0.42004233598709106,
      "learning_rate": 2.460295414137943e-05,
      "loss": 2.4005,
      "step": 113560
    },
    {
      "epoch": 2.3363514100787133,
      "grad_norm": 0.47170883417129517,
      "learning_rate": 2.4588366637785076e-05,
      "loss": 2.43,
      "step": 113570
    },
    {
      "epoch": 2.3365571295955814,
      "grad_norm": 0.4473525285720825,
      "learning_rate": 2.4573782853835658e-05,
      "loss": 2.3154,
      "step": 113580
    },
    {
      "epoch": 2.336762849112449,
      "grad_norm": 0.4491293728351593,
      "learning_rate": 2.4559202790250514e-05,
      "loss": 2.4113,
      "step": 113590
    },
    {
      "epoch": 2.3369685686293167,
      "grad_norm": 0.44697311520576477,
      "learning_rate": 2.4544626447748785e-05,
      "loss": 2.3695,
      "step": 113600
    },
    {
      "epoch": 2.3371742881461843,
      "grad_norm": 0.4369208812713623,
      "learning_rate": 2.453005382704948e-05,
      "loss": 2.365,
      "step": 113610
    },
    {
      "epoch": 2.337380007663052,
      "grad_norm": 0.4826463758945465,
      "learning_rate": 2.45154849288713e-05,
      "loss": 2.4029,
      "step": 113620
    },
    {
      "epoch": 2.3375857271799196,
      "grad_norm": 0.4496610164642334,
      "learning_rate": 2.4500919753932927e-05,
      "loss": 2.3845,
      "step": 113630
    },
    {
      "epoch": 2.3377914466967873,
      "grad_norm": 0.4845094680786133,
      "learning_rate": 2.4486358302952804e-05,
      "loss": 2.3701,
      "step": 113640
    },
    {
      "epoch": 2.337997166213655,
      "grad_norm": 0.476159006357193,
      "learning_rate": 2.447180057664906e-05,
      "loss": 2.3572,
      "step": 113650
    },
    {
      "epoch": 2.338202885730523,
      "grad_norm": 0.4729081690311432,
      "learning_rate": 2.4457246575739857e-05,
      "loss": 2.3424,
      "step": 113660
    },
    {
      "epoch": 2.3384086052473907,
      "grad_norm": 0.47397109866142273,
      "learning_rate": 2.4442696300943035e-05,
      "loss": 2.3474,
      "step": 113670
    },
    {
      "epoch": 2.3386143247642583,
      "grad_norm": 0.4217773675918579,
      "learning_rate": 2.4428149752976215e-05,
      "loss": 2.473,
      "step": 113680
    },
    {
      "epoch": 2.338820044281126,
      "grad_norm": 0.4742661714553833,
      "learning_rate": 2.441360693255701e-05,
      "loss": 2.4419,
      "step": 113690
    },
    {
      "epoch": 2.3390257637979937,
      "grad_norm": 0.4246578514575958,
      "learning_rate": 2.4399067840402646e-05,
      "loss": 2.388,
      "step": 113700
    },
    {
      "epoch": 2.3392314833148613,
      "grad_norm": 0.4467702805995941,
      "learning_rate": 2.438453247723026e-05,
      "loss": 2.3202,
      "step": 113710
    },
    {
      "epoch": 2.339437202831729,
      "grad_norm": 0.4485399127006531,
      "learning_rate": 2.43700008437569e-05,
      "loss": 2.3201,
      "step": 113720
    },
    {
      "epoch": 2.339642922348597,
      "grad_norm": 0.4453132748603821,
      "learning_rate": 2.435547294069922e-05,
      "loss": 2.3271,
      "step": 113730
    },
    {
      "epoch": 2.3398486418654647,
      "grad_norm": 0.43977072834968567,
      "learning_rate": 2.434094876877383e-05,
      "loss": 2.3237,
      "step": 113740
    },
    {
      "epoch": 2.3400543613823324,
      "grad_norm": 0.44315317273139954,
      "learning_rate": 2.432642832869719e-05,
      "loss": 2.3828,
      "step": 113750
    },
    {
      "epoch": 2.3402600808992,
      "grad_norm": 0.45098376274108887,
      "learning_rate": 2.4311911621185456e-05,
      "loss": 2.3393,
      "step": 113760
    },
    {
      "epoch": 2.3404658004160677,
      "grad_norm": 0.4342569410800934,
      "learning_rate": 2.429739864695466e-05,
      "loss": 2.3713,
      "step": 113770
    },
    {
      "epoch": 2.3406715199329353,
      "grad_norm": 0.4380702078342438,
      "learning_rate": 2.4282889406720665e-05,
      "loss": 2.3814,
      "step": 113780
    },
    {
      "epoch": 2.340877239449803,
      "grad_norm": 0.4800928235054016,
      "learning_rate": 2.4268383901199122e-05,
      "loss": 2.3504,
      "step": 113790
    },
    {
      "epoch": 2.3410829589666706,
      "grad_norm": 0.4254027009010315,
      "learning_rate": 2.425388213110552e-05,
      "loss": 2.3907,
      "step": 113800
    },
    {
      "epoch": 2.3412886784835387,
      "grad_norm": 0.48288920521736145,
      "learning_rate": 2.4239384097155136e-05,
      "loss": 2.4077,
      "step": 113810
    },
    {
      "epoch": 2.3414943980004064,
      "grad_norm": 0.48348039388656616,
      "learning_rate": 2.42248898000631e-05,
      "loss": 2.3122,
      "step": 113820
    },
    {
      "epoch": 2.341700117517274,
      "grad_norm": 0.4399733543395996,
      "learning_rate": 2.421039924054431e-05,
      "loss": 2.4288,
      "step": 113830
    },
    {
      "epoch": 2.3419058370341417,
      "grad_norm": 0.5600791573524475,
      "learning_rate": 2.4195912419313536e-05,
      "loss": 2.4294,
      "step": 113840
    },
    {
      "epoch": 2.3421115565510093,
      "grad_norm": 0.5000714063644409,
      "learning_rate": 2.4181429337085314e-05,
      "loss": 2.297,
      "step": 113850
    },
    {
      "epoch": 2.342317276067877,
      "grad_norm": 0.49996286630630493,
      "learning_rate": 2.416694999457403e-05,
      "loss": 2.4,
      "step": 113860
    },
    {
      "epoch": 2.342522995584745,
      "grad_norm": 0.5141134262084961,
      "learning_rate": 2.415247439249385e-05,
      "loss": 2.3144,
      "step": 113870
    },
    {
      "epoch": 2.3427287151016127,
      "grad_norm": 0.4503198564052582,
      "learning_rate": 2.413800253155879e-05,
      "loss": 2.2947,
      "step": 113880
    },
    {
      "epoch": 2.3429344346184804,
      "grad_norm": 0.4474110007286072,
      "learning_rate": 2.4123534412482674e-05,
      "loss": 2.4385,
      "step": 113890
    },
    {
      "epoch": 2.343140154135348,
      "grad_norm": 0.42722710967063904,
      "learning_rate": 2.410907003597914e-05,
      "loss": 2.3193,
      "step": 113900
    },
    {
      "epoch": 2.3433458736522157,
      "grad_norm": 0.428147554397583,
      "learning_rate": 2.409460940276159e-05,
      "loss": 2.3347,
      "step": 113910
    },
    {
      "epoch": 2.3435515931690833,
      "grad_norm": 0.4806476831436157,
      "learning_rate": 2.4080152513543342e-05,
      "loss": 2.3891,
      "step": 113920
    },
    {
      "epoch": 2.343757312685951,
      "grad_norm": 0.47981104254722595,
      "learning_rate": 2.4065699369037486e-05,
      "loss": 2.363,
      "step": 113930
    },
    {
      "epoch": 2.3439630322028187,
      "grad_norm": 0.48475101590156555,
      "learning_rate": 2.4051249969956823e-05,
      "loss": 2.3851,
      "step": 113940
    },
    {
      "epoch": 2.3441687517196867,
      "grad_norm": 0.5017681121826172,
      "learning_rate": 2.4036804317014174e-05,
      "loss": 2.3688,
      "step": 113950
    },
    {
      "epoch": 2.3443744712365544,
      "grad_norm": 0.44681859016418457,
      "learning_rate": 2.4022362410922027e-05,
      "loss": 2.3613,
      "step": 113960
    },
    {
      "epoch": 2.344580190753422,
      "grad_norm": 0.5397639870643616,
      "learning_rate": 2.4007924252392667e-05,
      "loss": 2.3655,
      "step": 113970
    },
    {
      "epoch": 2.3447859102702897,
      "grad_norm": 0.4806710183620453,
      "learning_rate": 2.399348984213836e-05,
      "loss": 2.3967,
      "step": 113980
    },
    {
      "epoch": 2.3449916297871574,
      "grad_norm": 0.43756479024887085,
      "learning_rate": 2.3979059180870977e-05,
      "loss": 2.3922,
      "step": 113990
    },
    {
      "epoch": 2.345197349304025,
      "grad_norm": 0.467933714389801,
      "learning_rate": 2.3964632269302313e-05,
      "loss": 2.3568,
      "step": 114000
    },
    {
      "epoch": 2.345403068820893,
      "grad_norm": 0.46253615617752075,
      "learning_rate": 2.3950209108144052e-05,
      "loss": 2.3656,
      "step": 114010
    },
    {
      "epoch": 2.3456087883377608,
      "grad_norm": 0.3986484408378601,
      "learning_rate": 2.3935789698107525e-05,
      "loss": 2.4256,
      "step": 114020
    },
    {
      "epoch": 2.3458145078546284,
      "grad_norm": 0.4430732727050781,
      "learning_rate": 2.3921374039903966e-05,
      "loss": 2.3468,
      "step": 114030
    },
    {
      "epoch": 2.346020227371496,
      "grad_norm": 0.464754194021225,
      "learning_rate": 2.3906962134244503e-05,
      "loss": 2.389,
      "step": 114040
    },
    {
      "epoch": 2.3462259468883637,
      "grad_norm": 0.4612109363079071,
      "learning_rate": 2.3892553981839905e-05,
      "loss": 2.3735,
      "step": 114050
    },
    {
      "epoch": 2.3464316664052314,
      "grad_norm": 0.5083715319633484,
      "learning_rate": 2.387814958340089e-05,
      "loss": 2.3279,
      "step": 114060
    },
    {
      "epoch": 2.346637385922099,
      "grad_norm": 0.4830329716205597,
      "learning_rate": 2.3863748939637943e-05,
      "loss": 2.3284,
      "step": 114070
    },
    {
      "epoch": 2.3468431054389667,
      "grad_norm": 0.45021212100982666,
      "learning_rate": 2.3849352051261364e-05,
      "loss": 2.3917,
      "step": 114080
    },
    {
      "epoch": 2.347048824955835,
      "grad_norm": 0.4362785220146179,
      "learning_rate": 2.3834958918981288e-05,
      "loss": 2.2911,
      "step": 114090
    },
    {
      "epoch": 2.3472545444727024,
      "grad_norm": 0.4913279116153717,
      "learning_rate": 2.382056954350762e-05,
      "loss": 2.3519,
      "step": 114100
    },
    {
      "epoch": 2.34746026398957,
      "grad_norm": 0.7580115795135498,
      "learning_rate": 2.380618392555014e-05,
      "loss": 2.3677,
      "step": 114110
    },
    {
      "epoch": 2.3476659835064377,
      "grad_norm": 0.4171440899372101,
      "learning_rate": 2.3791802065818404e-05,
      "loss": 2.4222,
      "step": 114120
    },
    {
      "epoch": 2.3478717030233054,
      "grad_norm": 0.4415181577205658,
      "learning_rate": 2.3777423965021784e-05,
      "loss": 2.3552,
      "step": 114130
    },
    {
      "epoch": 2.348077422540173,
      "grad_norm": 0.4361213743686676,
      "learning_rate": 2.376304962386947e-05,
      "loss": 2.3785,
      "step": 114140
    },
    {
      "epoch": 2.3482831420570407,
      "grad_norm": 0.4629322588443756,
      "learning_rate": 2.3748679043070478e-05,
      "loss": 2.39,
      "step": 114150
    },
    {
      "epoch": 2.348488861573909,
      "grad_norm": 0.4390176832675934,
      "learning_rate": 2.3734312223333633e-05,
      "loss": 2.3736,
      "step": 114160
    },
    {
      "epoch": 2.3486945810907764,
      "grad_norm": 0.4822191596031189,
      "learning_rate": 2.371994916536757e-05,
      "loss": 2.4015,
      "step": 114170
    },
    {
      "epoch": 2.348900300607644,
      "grad_norm": 0.456297367811203,
      "learning_rate": 2.370558986988074e-05,
      "loss": 2.335,
      "step": 114180
    },
    {
      "epoch": 2.3491060201245118,
      "grad_norm": 0.4504135847091675,
      "learning_rate": 2.3691234337581404e-05,
      "loss": 2.4093,
      "step": 114190
    },
    {
      "epoch": 2.3493117396413794,
      "grad_norm": 0.4558906853199005,
      "learning_rate": 2.3676882569177637e-05,
      "loss": 2.4022,
      "step": 114200
    },
    {
      "epoch": 2.349517459158247,
      "grad_norm": 0.44594812393188477,
      "learning_rate": 2.3662534565377347e-05,
      "loss": 2.3839,
      "step": 114210
    },
    {
      "epoch": 2.3497231786751147,
      "grad_norm": 0.5437304973602295,
      "learning_rate": 2.3648190326888264e-05,
      "loss": 2.3673,
      "step": 114220
    },
    {
      "epoch": 2.3499288981919824,
      "grad_norm": 0.4537048637866974,
      "learning_rate": 2.3633849854417832e-05,
      "loss": 2.3392,
      "step": 114230
    },
    {
      "epoch": 2.3501346177088505,
      "grad_norm": 0.4935181736946106,
      "learning_rate": 2.3619513148673478e-05,
      "loss": 2.3899,
      "step": 114240
    },
    {
      "epoch": 2.350340337225718,
      "grad_norm": 0.45445650815963745,
      "learning_rate": 2.360518021036233e-05,
      "loss": 2.3345,
      "step": 114250
    },
    {
      "epoch": 2.3505460567425858,
      "grad_norm": 0.5665886402130127,
      "learning_rate": 2.3590851040191287e-05,
      "loss": 2.3718,
      "step": 114260
    },
    {
      "epoch": 2.3507517762594534,
      "grad_norm": 0.429487943649292,
      "learning_rate": 2.357652563886722e-05,
      "loss": 2.301,
      "step": 114270
    },
    {
      "epoch": 2.350957495776321,
      "grad_norm": 0.47147977352142334,
      "learning_rate": 2.35622040070967e-05,
      "loss": 2.4427,
      "step": 114280
    },
    {
      "epoch": 2.3511632152931887,
      "grad_norm": 0.45456159114837646,
      "learning_rate": 2.3547886145586072e-05,
      "loss": 2.3476,
      "step": 114290
    },
    {
      "epoch": 2.351368934810057,
      "grad_norm": 0.4851846694946289,
      "learning_rate": 2.3533572055041662e-05,
      "loss": 2.3568,
      "step": 114300
    },
    {
      "epoch": 2.3515746543269245,
      "grad_norm": 0.4304838478565216,
      "learning_rate": 2.3519261736169416e-05,
      "loss": 2.3769,
      "step": 114310
    },
    {
      "epoch": 2.351780373843792,
      "grad_norm": 0.509147584438324,
      "learning_rate": 2.3504955189675203e-05,
      "loss": 2.3551,
      "step": 114320
    },
    {
      "epoch": 2.35198609336066,
      "grad_norm": 0.474593847990036,
      "learning_rate": 2.3490652416264747e-05,
      "loss": 2.382,
      "step": 114330
    },
    {
      "epoch": 2.3521918128775274,
      "grad_norm": 0.5104540586471558,
      "learning_rate": 2.347635341664346e-05,
      "loss": 2.3497,
      "step": 114340
    },
    {
      "epoch": 2.352397532394395,
      "grad_norm": 0.42753881216049194,
      "learning_rate": 2.346205819151662e-05,
      "loss": 2.346,
      "step": 114350
    },
    {
      "epoch": 2.3526032519112627,
      "grad_norm": 0.43098047375679016,
      "learning_rate": 2.3447766741589427e-05,
      "loss": 2.3725,
      "step": 114360
    },
    {
      "epoch": 2.3528089714281304,
      "grad_norm": 0.4758322834968567,
      "learning_rate": 2.343347906756672e-05,
      "loss": 2.3978,
      "step": 114370
    },
    {
      "epoch": 2.3530146909449985,
      "grad_norm": 0.4566279947757721,
      "learning_rate": 2.3419195170153253e-05,
      "loss": 2.3804,
      "step": 114380
    },
    {
      "epoch": 2.353220410461866,
      "grad_norm": 0.4645872116088867,
      "learning_rate": 2.3404915050053567e-05,
      "loss": 2.3217,
      "step": 114390
    },
    {
      "epoch": 2.353426129978734,
      "grad_norm": 0.4622351825237274,
      "learning_rate": 2.3390638707972025e-05,
      "loss": 2.4278,
      "step": 114400
    },
    {
      "epoch": 2.3536318494956014,
      "grad_norm": 0.49708977341651917,
      "learning_rate": 2.3376366144612806e-05,
      "loss": 2.443,
      "step": 114410
    },
    {
      "epoch": 2.353837569012469,
      "grad_norm": 0.48978155851364136,
      "learning_rate": 2.3362097360679903e-05,
      "loss": 2.4107,
      "step": 114420
    },
    {
      "epoch": 2.3540432885293368,
      "grad_norm": 0.45310837030410767,
      "learning_rate": 2.334783235687711e-05,
      "loss": 2.3646,
      "step": 114430
    },
    {
      "epoch": 2.354249008046205,
      "grad_norm": 0.4974512457847595,
      "learning_rate": 2.333357113390804e-05,
      "loss": 2.3634,
      "step": 114440
    },
    {
      "epoch": 2.3544547275630725,
      "grad_norm": 0.45730334520339966,
      "learning_rate": 2.331931369247612e-05,
      "loss": 2.4049,
      "step": 114450
    },
    {
      "epoch": 2.35466044707994,
      "grad_norm": 0.5142088532447815,
      "learning_rate": 2.33050600332846e-05,
      "loss": 2.4218,
      "step": 114460
    },
    {
      "epoch": 2.354866166596808,
      "grad_norm": 0.4549695551395416,
      "learning_rate": 2.3290810157036525e-05,
      "loss": 2.428,
      "step": 114470
    },
    {
      "epoch": 2.3550718861136755,
      "grad_norm": 0.47938472032546997,
      "learning_rate": 2.327656406443478e-05,
      "loss": 2.3725,
      "step": 114480
    },
    {
      "epoch": 2.355277605630543,
      "grad_norm": 0.4543834924697876,
      "learning_rate": 2.3262321756182026e-05,
      "loss": 2.391,
      "step": 114490
    },
    {
      "epoch": 2.3554833251474108,
      "grad_norm": 0.45926398038864136,
      "learning_rate": 2.3248083232980778e-05,
      "loss": 2.3401,
      "step": 114500
    },
    {
      "epoch": 2.3556890446642784,
      "grad_norm": 0.42972540855407715,
      "learning_rate": 2.3233848495533338e-05,
      "loss": 2.3937,
      "step": 114510
    },
    {
      "epoch": 2.355894764181146,
      "grad_norm": 0.4588410258293152,
      "learning_rate": 2.3219617544541815e-05,
      "loss": 2.3958,
      "step": 114520
    },
    {
      "epoch": 2.356100483698014,
      "grad_norm": 0.4069403409957886,
      "learning_rate": 2.320539038070817e-05,
      "loss": 2.3155,
      "step": 114530
    },
    {
      "epoch": 2.356306203214882,
      "grad_norm": 0.4553472697734833,
      "learning_rate": 2.319116700473416e-05,
      "loss": 2.3436,
      "step": 114540
    },
    {
      "epoch": 2.3565119227317495,
      "grad_norm": 0.5073931217193604,
      "learning_rate": 2.317694741732126e-05,
      "loss": 2.3385,
      "step": 114550
    },
    {
      "epoch": 2.356717642248617,
      "grad_norm": 0.46899616718292236,
      "learning_rate": 2.316273161917094e-05,
      "loss": 2.355,
      "step": 114560
    },
    {
      "epoch": 2.356923361765485,
      "grad_norm": 0.5006463527679443,
      "learning_rate": 2.31485196109844e-05,
      "loss": 2.378,
      "step": 114570
    },
    {
      "epoch": 2.3571290812823524,
      "grad_norm": 0.597360372543335,
      "learning_rate": 2.3134311393462515e-05,
      "loss": 2.388,
      "step": 114580
    },
    {
      "epoch": 2.3573348007992205,
      "grad_norm": 0.4867550730705261,
      "learning_rate": 2.312010696730623e-05,
      "loss": 2.4127,
      "step": 114590
    },
    {
      "epoch": 2.357540520316088,
      "grad_norm": 0.46342572569847107,
      "learning_rate": 2.3105906333216143e-05,
      "loss": 2.4067,
      "step": 114600
    },
    {
      "epoch": 2.357746239832956,
      "grad_norm": 0.47029659152030945,
      "learning_rate": 2.309170949189262e-05,
      "loss": 2.3596,
      "step": 114610
    },
    {
      "epoch": 2.3579519593498235,
      "grad_norm": 0.46732237935066223,
      "learning_rate": 2.3077516444036017e-05,
      "loss": 2.3698,
      "step": 114620
    },
    {
      "epoch": 2.358157678866691,
      "grad_norm": 0.4670465290546417,
      "learning_rate": 2.306332719034633e-05,
      "loss": 2.3869,
      "step": 114630
    },
    {
      "epoch": 2.358363398383559,
      "grad_norm": 0.4401659071445465,
      "learning_rate": 2.3049141731523428e-05,
      "loss": 2.3529,
      "step": 114640
    },
    {
      "epoch": 2.3585691179004264,
      "grad_norm": 0.46756142377853394,
      "learning_rate": 2.3034960068267087e-05,
      "loss": 2.3676,
      "step": 114650
    },
    {
      "epoch": 2.358774837417294,
      "grad_norm": 0.44648927450180054,
      "learning_rate": 2.3020782201276735e-05,
      "loss": 2.4241,
      "step": 114660
    },
    {
      "epoch": 2.358980556934162,
      "grad_norm": 0.4784720838069916,
      "learning_rate": 2.300660813125167e-05,
      "loss": 2.3869,
      "step": 114670
    },
    {
      "epoch": 2.35918627645103,
      "grad_norm": 0.474365770816803,
      "learning_rate": 2.2992437858891136e-05,
      "loss": 2.3155,
      "step": 114680
    },
    {
      "epoch": 2.3593919959678975,
      "grad_norm": 0.4442816376686096,
      "learning_rate": 2.2978271384893957e-05,
      "loss": 2.3996,
      "step": 114690
    },
    {
      "epoch": 2.359597715484765,
      "grad_norm": 0.5368083119392395,
      "learning_rate": 2.296410870995894e-05,
      "loss": 2.3528,
      "step": 114700
    },
    {
      "epoch": 2.359803435001633,
      "grad_norm": 0.47966235876083374,
      "learning_rate": 2.2949949834784647e-05,
      "loss": 2.3399,
      "step": 114710
    },
    {
      "epoch": 2.3600091545185005,
      "grad_norm": 0.4704231321811676,
      "learning_rate": 2.2935794760069452e-05,
      "loss": 2.34,
      "step": 114720
    },
    {
      "epoch": 2.3602148740353686,
      "grad_norm": 0.4575102627277374,
      "learning_rate": 2.2921643486511556e-05,
      "loss": 2.3726,
      "step": 114730
    },
    {
      "epoch": 2.360420593552236,
      "grad_norm": 0.4466116726398468,
      "learning_rate": 2.2907496014808948e-05,
      "loss": 2.3884,
      "step": 114740
    },
    {
      "epoch": 2.360626313069104,
      "grad_norm": 0.48051732778549194,
      "learning_rate": 2.289335234565947e-05,
      "loss": 2.3448,
      "step": 114750
    },
    {
      "epoch": 2.3608320325859715,
      "grad_norm": 0.44962286949157715,
      "learning_rate": 2.2879212479760738e-05,
      "loss": 2.3502,
      "step": 114760
    },
    {
      "epoch": 2.361037752102839,
      "grad_norm": 0.5038344264030457,
      "learning_rate": 2.2865076417810192e-05,
      "loss": 2.3958,
      "step": 114770
    },
    {
      "epoch": 2.361243471619707,
      "grad_norm": 0.45205312967300415,
      "learning_rate": 2.2850944160505105e-05,
      "loss": 2.3644,
      "step": 114780
    },
    {
      "epoch": 2.3614491911365745,
      "grad_norm": 0.479445219039917,
      "learning_rate": 2.2836815708542514e-05,
      "loss": 2.3909,
      "step": 114790
    },
    {
      "epoch": 2.361654910653442,
      "grad_norm": 0.5073657035827637,
      "learning_rate": 2.282269106261933e-05,
      "loss": 2.4474,
      "step": 114800
    },
    {
      "epoch": 2.3618606301703102,
      "grad_norm": 0.4300731122493744,
      "learning_rate": 2.2808570223432213e-05,
      "loss": 2.3976,
      "step": 114810
    },
    {
      "epoch": 2.362066349687178,
      "grad_norm": 0.42322129011154175,
      "learning_rate": 2.27944531916777e-05,
      "loss": 2.3898,
      "step": 114820
    },
    {
      "epoch": 2.3622720692040455,
      "grad_norm": 0.43470725417137146,
      "learning_rate": 2.2780339968052078e-05,
      "loss": 2.3488,
      "step": 114830
    },
    {
      "epoch": 2.362477788720913,
      "grad_norm": 0.4538939595222473,
      "learning_rate": 2.27662305532515e-05,
      "loss": 2.3963,
      "step": 114840
    },
    {
      "epoch": 2.362683508237781,
      "grad_norm": 0.4806022644042969,
      "learning_rate": 2.2752124947971886e-05,
      "loss": 2.353,
      "step": 114850
    },
    {
      "epoch": 2.3628892277546485,
      "grad_norm": 0.4302135705947876,
      "learning_rate": 2.2738023152909028e-05,
      "loss": 2.3338,
      "step": 114860
    },
    {
      "epoch": 2.363094947271516,
      "grad_norm": 0.43472278118133545,
      "learning_rate": 2.272392516875841e-05,
      "loss": 2.3735,
      "step": 114870
    },
    {
      "epoch": 2.3633006667883842,
      "grad_norm": 0.517594039440155,
      "learning_rate": 2.2709830996215486e-05,
      "loss": 2.3989,
      "step": 114880
    },
    {
      "epoch": 2.363506386305252,
      "grad_norm": 0.43213748931884766,
      "learning_rate": 2.2695740635975437e-05,
      "loss": 2.3328,
      "step": 114890
    },
    {
      "epoch": 2.3637121058221195,
      "grad_norm": 0.44823840260505676,
      "learning_rate": 2.2681654088733207e-05,
      "loss": 2.3267,
      "step": 114900
    },
    {
      "epoch": 2.363917825338987,
      "grad_norm": 0.4196564257144928,
      "learning_rate": 2.266757135518366e-05,
      "loss": 2.2901,
      "step": 114910
    },
    {
      "epoch": 2.364123544855855,
      "grad_norm": 0.4684927761554718,
      "learning_rate": 2.265349243602144e-05,
      "loss": 2.3505,
      "step": 114920
    },
    {
      "epoch": 2.3643292643727225,
      "grad_norm": 0.4570789933204651,
      "learning_rate": 2.263941733194088e-05,
      "loss": 2.3243,
      "step": 114930
    },
    {
      "epoch": 2.36453498388959,
      "grad_norm": 0.5442087650299072,
      "learning_rate": 2.2625346043636374e-05,
      "loss": 2.3529,
      "step": 114940
    },
    {
      "epoch": 2.364740703406458,
      "grad_norm": 0.4631034731864929,
      "learning_rate": 2.2611278571801874e-05,
      "loss": 2.348,
      "step": 114950
    },
    {
      "epoch": 2.364946422923326,
      "grad_norm": 0.5059112906455994,
      "learning_rate": 2.259721491713125e-05,
      "loss": 2.3453,
      "step": 114960
    },
    {
      "epoch": 2.3651521424401936,
      "grad_norm": 0.4573344588279724,
      "learning_rate": 2.2583155080318274e-05,
      "loss": 2.3694,
      "step": 114970
    },
    {
      "epoch": 2.365357861957061,
      "grad_norm": 0.4565828740596771,
      "learning_rate": 2.2569099062056366e-05,
      "loss": 2.3548,
      "step": 114980
    },
    {
      "epoch": 2.365563581473929,
      "grad_norm": 0.4530324637889862,
      "learning_rate": 2.2555046863038854e-05,
      "loss": 2.3709,
      "step": 114990
    },
    {
      "epoch": 2.3657693009907965,
      "grad_norm": 0.45041021704673767,
      "learning_rate": 2.2540998483958852e-05,
      "loss": 2.4082,
      "step": 115000
    },
    {
      "epoch": 2.365975020507664,
      "grad_norm": 0.4874078333377838,
      "learning_rate": 2.2526953925509297e-05,
      "loss": 2.4581,
      "step": 115010
    },
    {
      "epoch": 2.3661807400245323,
      "grad_norm": 0.4898906350135803,
      "learning_rate": 2.2512913188382924e-05,
      "loss": 2.3201,
      "step": 115020
    },
    {
      "epoch": 2.3663864595414,
      "grad_norm": 0.44865643978118896,
      "learning_rate": 2.2498876273272283e-05,
      "loss": 2.3144,
      "step": 115030
    },
    {
      "epoch": 2.3665921790582676,
      "grad_norm": 0.45965301990509033,
      "learning_rate": 2.2484843180869754e-05,
      "loss": 2.4051,
      "step": 115040
    },
    {
      "epoch": 2.3667978985751352,
      "grad_norm": 0.4589802622795105,
      "learning_rate": 2.2470813911867506e-05,
      "loss": 2.3719,
      "step": 115050
    },
    {
      "epoch": 2.367003618092003,
      "grad_norm": 0.43993091583251953,
      "learning_rate": 2.2456788466957512e-05,
      "loss": 2.4284,
      "step": 115060
    },
    {
      "epoch": 2.3672093376088705,
      "grad_norm": 0.4572935402393341,
      "learning_rate": 2.24427668468316e-05,
      "loss": 2.3837,
      "step": 115070
    },
    {
      "epoch": 2.367415057125738,
      "grad_norm": 0.45133382081985474,
      "learning_rate": 2.242874905218135e-05,
      "loss": 2.3858,
      "step": 115080
    },
    {
      "epoch": 2.367620776642606,
      "grad_norm": 0.47160249948501587,
      "learning_rate": 2.2414735083698202e-05,
      "loss": 2.298,
      "step": 115090
    },
    {
      "epoch": 2.367826496159474,
      "grad_norm": 0.5188169479370117,
      "learning_rate": 2.240072494207338e-05,
      "loss": 2.3845,
      "step": 115100
    },
    {
      "epoch": 2.3680322156763416,
      "grad_norm": 0.4189320206642151,
      "learning_rate": 2.2386718627997938e-05,
      "loss": 2.3565,
      "step": 115110
    },
    {
      "epoch": 2.3682379351932092,
      "grad_norm": 0.4727168679237366,
      "learning_rate": 2.237271614216272e-05,
      "loss": 2.3596,
      "step": 115120
    },
    {
      "epoch": 2.368443654710077,
      "grad_norm": 0.47215715050697327,
      "learning_rate": 2.2358717485258396e-05,
      "loss": 2.3988,
      "step": 115130
    },
    {
      "epoch": 2.3686493742269445,
      "grad_norm": 0.4538353383541107,
      "learning_rate": 2.2344722657975447e-05,
      "loss": 2.3637,
      "step": 115140
    },
    {
      "epoch": 2.368855093743812,
      "grad_norm": 0.4564771354198456,
      "learning_rate": 2.233073166100419e-05,
      "loss": 2.3896,
      "step": 115150
    },
    {
      "epoch": 2.3690608132606803,
      "grad_norm": 0.4365641474723816,
      "learning_rate": 2.2316744495034626e-05,
      "loss": 2.3792,
      "step": 115160
    },
    {
      "epoch": 2.369266532777548,
      "grad_norm": 0.47547367215156555,
      "learning_rate": 2.230276116075677e-05,
      "loss": 2.3787,
      "step": 115170
    },
    {
      "epoch": 2.3694722522944156,
      "grad_norm": 0.4603493809700012,
      "learning_rate": 2.2288781658860337e-05,
      "loss": 2.3676,
      "step": 115180
    },
    {
      "epoch": 2.3696779718112833,
      "grad_norm": 0.4127286672592163,
      "learning_rate": 2.2274805990034764e-05,
      "loss": 2.2962,
      "step": 115190
    },
    {
      "epoch": 2.369883691328151,
      "grad_norm": 0.4275887608528137,
      "learning_rate": 2.226083415496949e-05,
      "loss": 2.348,
      "step": 115200
    },
    {
      "epoch": 2.3700894108450186,
      "grad_norm": 0.43835294246673584,
      "learning_rate": 2.2246866154353675e-05,
      "loss": 2.3878,
      "step": 115210
    },
    {
      "epoch": 2.370295130361886,
      "grad_norm": 0.49116218090057373,
      "learning_rate": 2.2232901988876188e-05,
      "loss": 2.3126,
      "step": 115220
    },
    {
      "epoch": 2.370500849878754,
      "grad_norm": 0.4494785964488983,
      "learning_rate": 2.2218941659225923e-05,
      "loss": 2.3792,
      "step": 115230
    },
    {
      "epoch": 2.370706569395622,
      "grad_norm": 0.47243911027908325,
      "learning_rate": 2.2204985166091374e-05,
      "loss": 2.4092,
      "step": 115240
    },
    {
      "epoch": 2.3709122889124896,
      "grad_norm": 0.49764174222946167,
      "learning_rate": 2.2191032510160957e-05,
      "loss": 2.3921,
      "step": 115250
    },
    {
      "epoch": 2.3711180084293573,
      "grad_norm": 0.46947628259658813,
      "learning_rate": 2.2177083692122946e-05,
      "loss": 2.2921,
      "step": 115260
    },
    {
      "epoch": 2.371323727946225,
      "grad_norm": 0.4988020062446594,
      "learning_rate": 2.21631387126653e-05,
      "loss": 2.3986,
      "step": 115270
    },
    {
      "epoch": 2.3715294474630926,
      "grad_norm": 0.42170992493629456,
      "learning_rate": 2.2149197572475823e-05,
      "loss": 2.3934,
      "step": 115280
    },
    {
      "epoch": 2.3717351669799602,
      "grad_norm": 0.44054538011550903,
      "learning_rate": 2.2135260272242242e-05,
      "loss": 2.3957,
      "step": 115290
    },
    {
      "epoch": 2.371940886496828,
      "grad_norm": 0.4782883822917938,
      "learning_rate": 2.2121326812651943e-05,
      "loss": 2.3112,
      "step": 115300
    },
    {
      "epoch": 2.372146606013696,
      "grad_norm": 0.44343534111976624,
      "learning_rate": 2.2107397194392198e-05,
      "loss": 2.3868,
      "step": 115310
    },
    {
      "epoch": 2.3723523255305636,
      "grad_norm": 0.47154688835144043,
      "learning_rate": 2.209347141815009e-05,
      "loss": 2.4135,
      "step": 115320
    },
    {
      "epoch": 2.3725580450474313,
      "grad_norm": 0.4382615387439728,
      "learning_rate": 2.2079549484612494e-05,
      "loss": 2.3609,
      "step": 115330
    },
    {
      "epoch": 2.372763764564299,
      "grad_norm": 0.4646027982234955,
      "learning_rate": 2.2065631394466112e-05,
      "loss": 2.3572,
      "step": 115340
    },
    {
      "epoch": 2.3729694840811666,
      "grad_norm": 0.45341041684150696,
      "learning_rate": 2.2051717148397432e-05,
      "loss": 2.3769,
      "step": 115350
    },
    {
      "epoch": 2.3731752035980342,
      "grad_norm": 0.4053000509738922,
      "learning_rate": 2.2037806747092794e-05,
      "loss": 2.3644,
      "step": 115360
    },
    {
      "epoch": 2.373380923114902,
      "grad_norm": 0.4759518802165985,
      "learning_rate": 2.202390019123829e-05,
      "loss": 2.3137,
      "step": 115370
    },
    {
      "epoch": 2.3735866426317695,
      "grad_norm": 0.40922632813453674,
      "learning_rate": 2.2009997481519873e-05,
      "loss": 2.3695,
      "step": 115380
    },
    {
      "epoch": 2.3737923621486376,
      "grad_norm": 0.5229470133781433,
      "learning_rate": 2.1996098618623295e-05,
      "loss": 2.3479,
      "step": 115390
    },
    {
      "epoch": 2.3739980816655053,
      "grad_norm": 0.4474952518939972,
      "learning_rate": 2.1982203603234096e-05,
      "loss": 2.4033,
      "step": 115400
    },
    {
      "epoch": 2.374203801182373,
      "grad_norm": 0.5170164704322815,
      "learning_rate": 2.196831243603764e-05,
      "loss": 2.3217,
      "step": 115410
    },
    {
      "epoch": 2.3744095206992406,
      "grad_norm": 0.4524814784526825,
      "learning_rate": 2.1954425117719124e-05,
      "loss": 2.3614,
      "step": 115420
    },
    {
      "epoch": 2.3746152402161083,
      "grad_norm": 0.45160767436027527,
      "learning_rate": 2.1940541648963508e-05,
      "loss": 2.3208,
      "step": 115430
    },
    {
      "epoch": 2.374820959732976,
      "grad_norm": 0.4837617576122284,
      "learning_rate": 2.1926662030455612e-05,
      "loss": 2.3603,
      "step": 115440
    },
    {
      "epoch": 2.375026679249844,
      "grad_norm": 0.5021911859512329,
      "learning_rate": 2.1912786262880016e-05,
      "loss": 2.3757,
      "step": 115450
    },
    {
      "epoch": 2.3752323987667117,
      "grad_norm": 0.5112220644950867,
      "learning_rate": 2.189891434692116e-05,
      "loss": 2.3119,
      "step": 115460
    },
    {
      "epoch": 2.3754381182835793,
      "grad_norm": 0.44389960169792175,
      "learning_rate": 2.1885046283263278e-05,
      "loss": 2.3769,
      "step": 115470
    },
    {
      "epoch": 2.375643837800447,
      "grad_norm": 0.4517652094364166,
      "learning_rate": 2.1871182072590336e-05,
      "loss": 2.3927,
      "step": 115480
    },
    {
      "epoch": 2.3758495573173146,
      "grad_norm": 0.4873409867286682,
      "learning_rate": 2.1857321715586264e-05,
      "loss": 2.3229,
      "step": 115490
    },
    {
      "epoch": 2.3760552768341823,
      "grad_norm": 0.48478448390960693,
      "learning_rate": 2.184346521293471e-05,
      "loss": 2.3543,
      "step": 115500
    },
    {
      "epoch": 2.37626099635105,
      "grad_norm": 0.4808616042137146,
      "learning_rate": 2.1829612565319057e-05,
      "loss": 2.3549,
      "step": 115510
    },
    {
      "epoch": 2.3764667158679176,
      "grad_norm": 0.45004984736442566,
      "learning_rate": 2.181576377342267e-05,
      "loss": 2.3787,
      "step": 115520
    },
    {
      "epoch": 2.3766724353847857,
      "grad_norm": 0.45355698466300964,
      "learning_rate": 2.1801918837928625e-05,
      "loss": 2.3924,
      "step": 115530
    },
    {
      "epoch": 2.3768781549016533,
      "grad_norm": 0.49874162673950195,
      "learning_rate": 2.1788077759519742e-05,
      "loss": 2.3588,
      "step": 115540
    },
    {
      "epoch": 2.377083874418521,
      "grad_norm": 0.4506874680519104,
      "learning_rate": 2.1774240538878832e-05,
      "loss": 2.3458,
      "step": 115550
    },
    {
      "epoch": 2.3772895939353886,
      "grad_norm": 0.443438857793808,
      "learning_rate": 2.1760407176688337e-05,
      "loss": 2.3571,
      "step": 115560
    },
    {
      "epoch": 2.3774953134522563,
      "grad_norm": 0.45304074883461,
      "learning_rate": 2.1746577673630564e-05,
      "loss": 2.3728,
      "step": 115570
    },
    {
      "epoch": 2.377701032969124,
      "grad_norm": 0.4496435523033142,
      "learning_rate": 2.173275203038775e-05,
      "loss": 2.3179,
      "step": 115580
    },
    {
      "epoch": 2.3779067524859916,
      "grad_norm": 0.5064374804496765,
      "learning_rate": 2.1718930247641746e-05,
      "loss": 2.3667,
      "step": 115590
    },
    {
      "epoch": 2.3781124720028597,
      "grad_norm": 0.4336903989315033,
      "learning_rate": 2.1705112326074294e-05,
      "loss": 2.3704,
      "step": 115600
    },
    {
      "epoch": 2.3783181915197273,
      "grad_norm": 0.5231748223304749,
      "learning_rate": 2.1691298266367055e-05,
      "loss": 2.3873,
      "step": 115610
    },
    {
      "epoch": 2.378523911036595,
      "grad_norm": 0.4990323483943939,
      "learning_rate": 2.1677488069201312e-05,
      "loss": 2.3817,
      "step": 115620
    },
    {
      "epoch": 2.3787296305534626,
      "grad_norm": 0.5120471119880676,
      "learning_rate": 2.1663681735258278e-05,
      "loss": 2.3571,
      "step": 115630
    },
    {
      "epoch": 2.3789353500703303,
      "grad_norm": 0.4279468059539795,
      "learning_rate": 2.164987926521894e-05,
      "loss": 2.335,
      "step": 115640
    },
    {
      "epoch": 2.379141069587198,
      "grad_norm": 0.4753990173339844,
      "learning_rate": 2.1636080659764103e-05,
      "loss": 2.3909,
      "step": 115650
    },
    {
      "epoch": 2.3793467891040656,
      "grad_norm": 0.46788290143013,
      "learning_rate": 2.162228591957438e-05,
      "loss": 2.3491,
      "step": 115660
    },
    {
      "epoch": 2.3795525086209333,
      "grad_norm": 0.47903886437416077,
      "learning_rate": 2.160849504533018e-05,
      "loss": 2.3294,
      "step": 115670
    },
    {
      "epoch": 2.3797582281378014,
      "grad_norm": 0.5037473440170288,
      "learning_rate": 2.159470803771174e-05,
      "loss": 2.4402,
      "step": 115680
    },
    {
      "epoch": 2.379963947654669,
      "grad_norm": 0.5377346873283386,
      "learning_rate": 2.158092489739909e-05,
      "loss": 2.3364,
      "step": 115690
    },
    {
      "epoch": 2.3801696671715367,
      "grad_norm": 0.4946332573890686,
      "learning_rate": 2.1567145625072082e-05,
      "loss": 2.3183,
      "step": 115700
    },
    {
      "epoch": 2.3803753866884043,
      "grad_norm": 0.4548279047012329,
      "learning_rate": 2.1553370221410374e-05,
      "loss": 2.3763,
      "step": 115710
    },
    {
      "epoch": 2.380581106205272,
      "grad_norm": 0.464485764503479,
      "learning_rate": 2.1539598687093432e-05,
      "loss": 2.3405,
      "step": 115720
    },
    {
      "epoch": 2.3807868257221396,
      "grad_norm": 0.42884087562561035,
      "learning_rate": 2.1525831022800526e-05,
      "loss": 2.3649,
      "step": 115730
    },
    {
      "epoch": 2.3809925452390077,
      "grad_norm": 0.4534442126750946,
      "learning_rate": 2.1512067229210754e-05,
      "loss": 2.3783,
      "step": 115740
    },
    {
      "epoch": 2.3811982647558754,
      "grad_norm": 0.5484820604324341,
      "learning_rate": 2.1498307307002985e-05,
      "loss": 2.3615,
      "step": 115750
    },
    {
      "epoch": 2.381403984272743,
      "grad_norm": 0.5206906795501709,
      "learning_rate": 2.1484551256855935e-05,
      "loss": 2.3152,
      "step": 115760
    },
    {
      "epoch": 2.3816097037896107,
      "grad_norm": 0.443361759185791,
      "learning_rate": 2.147079907944811e-05,
      "loss": 2.3335,
      "step": 115770
    },
    {
      "epoch": 2.3818154233064783,
      "grad_norm": 0.4664711058139801,
      "learning_rate": 2.1457050775457845e-05,
      "loss": 2.3273,
      "step": 115780
    },
    {
      "epoch": 2.382021142823346,
      "grad_norm": 0.43473002314567566,
      "learning_rate": 2.1443306345563274e-05,
      "loss": 2.3852,
      "step": 115790
    },
    {
      "epoch": 2.3822268623402136,
      "grad_norm": 0.44551751017570496,
      "learning_rate": 2.1429565790442273e-05,
      "loss": 2.322,
      "step": 115800
    },
    {
      "epoch": 2.3824325818570813,
      "grad_norm": 0.4785127341747284,
      "learning_rate": 2.1415829110772657e-05,
      "loss": 2.4048,
      "step": 115810
    },
    {
      "epoch": 2.3826383013739494,
      "grad_norm": 0.48996153473854065,
      "learning_rate": 2.1402096307231988e-05,
      "loss": 2.3601,
      "step": 115820
    },
    {
      "epoch": 2.382844020890817,
      "grad_norm": 0.4494706392288208,
      "learning_rate": 2.1388367380497544e-05,
      "loss": 2.3315,
      "step": 115830
    },
    {
      "epoch": 2.3830497404076847,
      "grad_norm": 0.4476298987865448,
      "learning_rate": 2.1374642331246574e-05,
      "loss": 2.3714,
      "step": 115840
    },
    {
      "epoch": 2.3832554599245523,
      "grad_norm": 0.47849708795547485,
      "learning_rate": 2.1360921160156076e-05,
      "loss": 2.2899,
      "step": 115850
    },
    {
      "epoch": 2.38346117944142,
      "grad_norm": 0.47786185145378113,
      "learning_rate": 2.1347203867902754e-05,
      "loss": 2.4097,
      "step": 115860
    },
    {
      "epoch": 2.3836668989582876,
      "grad_norm": 0.43869486451148987,
      "learning_rate": 2.133349045516332e-05,
      "loss": 2.3921,
      "step": 115870
    },
    {
      "epoch": 2.3838726184751557,
      "grad_norm": 0.4605509638786316,
      "learning_rate": 2.1319780922614074e-05,
      "loss": 2.4052,
      "step": 115880
    },
    {
      "epoch": 2.3840783379920234,
      "grad_norm": 0.4456225633621216,
      "learning_rate": 2.1306075270931268e-05,
      "loss": 2.3311,
      "step": 115890
    },
    {
      "epoch": 2.384284057508891,
      "grad_norm": 0.4269108772277832,
      "learning_rate": 2.1292373500790984e-05,
      "loss": 2.3855,
      "step": 115900
    },
    {
      "epoch": 2.3844897770257587,
      "grad_norm": 0.4413381516933441,
      "learning_rate": 2.1278675612868993e-05,
      "loss": 2.3757,
      "step": 115910
    },
    {
      "epoch": 2.3846954965426264,
      "grad_norm": 0.46367353200912476,
      "learning_rate": 2.1264981607840918e-05,
      "loss": 2.3913,
      "step": 115920
    },
    {
      "epoch": 2.384901216059494,
      "grad_norm": 0.5615624189376831,
      "learning_rate": 2.1251291486382296e-05,
      "loss": 2.331,
      "step": 115930
    },
    {
      "epoch": 2.3851069355763617,
      "grad_norm": 0.4487035572528839,
      "learning_rate": 2.1237605249168323e-05,
      "loss": 2.3684,
      "step": 115940
    },
    {
      "epoch": 2.3853126550932293,
      "grad_norm": 0.4282940924167633,
      "learning_rate": 2.1223922896874072e-05,
      "loss": 2.3957,
      "step": 115950
    },
    {
      "epoch": 2.3855183746100974,
      "grad_norm": 0.43484267592430115,
      "learning_rate": 2.121024443017442e-05,
      "loss": 2.3603,
      "step": 115960
    },
    {
      "epoch": 2.385724094126965,
      "grad_norm": 0.4819789230823517,
      "learning_rate": 2.1196569849744074e-05,
      "loss": 2.4064,
      "step": 115970
    },
    {
      "epoch": 2.3859298136438327,
      "grad_norm": 0.47275876998901367,
      "learning_rate": 2.1182899156257498e-05,
      "loss": 2.363,
      "step": 115980
    },
    {
      "epoch": 2.3861355331607004,
      "grad_norm": 0.44542333483695984,
      "learning_rate": 2.1169232350389e-05,
      "loss": 2.3487,
      "step": 115990
    },
    {
      "epoch": 2.386341252677568,
      "grad_norm": 0.44877707958221436,
      "learning_rate": 2.1155569432812704e-05,
      "loss": 2.3982,
      "step": 116000
    },
    {
      "epoch": 2.3865469721944357,
      "grad_norm": 0.462408185005188,
      "learning_rate": 2.1141910404202515e-05,
      "loss": 2.3502,
      "step": 116010
    },
    {
      "epoch": 2.3867526917113033,
      "grad_norm": 0.4652726352214813,
      "learning_rate": 2.1128255265232168e-05,
      "loss": 2.3731,
      "step": 116020
    },
    {
      "epoch": 2.3869584112281714,
      "grad_norm": 0.46796634793281555,
      "learning_rate": 2.1114604016575178e-05,
      "loss": 2.4006,
      "step": 116030
    },
    {
      "epoch": 2.387164130745039,
      "grad_norm": 0.4760853946208954,
      "learning_rate": 2.1100956658904903e-05,
      "loss": 2.3059,
      "step": 116040
    },
    {
      "epoch": 2.3873698502619067,
      "grad_norm": 0.48392629623413086,
      "learning_rate": 2.1087313192894477e-05,
      "loss": 2.3751,
      "step": 116050
    },
    {
      "epoch": 2.3875755697787744,
      "grad_norm": 0.44304171204566956,
      "learning_rate": 2.1073673619216884e-05,
      "loss": 2.3556,
      "step": 116060
    },
    {
      "epoch": 2.387781289295642,
      "grad_norm": 0.429973840713501,
      "learning_rate": 2.1060037938544862e-05,
      "loss": 2.2984,
      "step": 116070
    },
    {
      "epoch": 2.3879870088125097,
      "grad_norm": 0.4906666874885559,
      "learning_rate": 2.104640615155101e-05,
      "loss": 2.4114,
      "step": 116080
    },
    {
      "epoch": 2.3881927283293773,
      "grad_norm": 0.48344507813453674,
      "learning_rate": 2.103277825890769e-05,
      "loss": 2.3784,
      "step": 116090
    },
    {
      "epoch": 2.388398447846245,
      "grad_norm": 0.4938086271286011,
      "learning_rate": 2.10191542612871e-05,
      "loss": 2.385,
      "step": 116100
    },
    {
      "epoch": 2.388604167363113,
      "grad_norm": 0.49110516905784607,
      "learning_rate": 2.1005534159361264e-05,
      "loss": 2.381,
      "step": 116110
    },
    {
      "epoch": 2.3888098868799807,
      "grad_norm": 0.49873432517051697,
      "learning_rate": 2.09919179538019e-05,
      "loss": 2.3492,
      "step": 116120
    },
    {
      "epoch": 2.3890156063968484,
      "grad_norm": 0.4517284035682678,
      "learning_rate": 2.0978305645280716e-05,
      "loss": 2.4315,
      "step": 116130
    },
    {
      "epoch": 2.389221325913716,
      "grad_norm": 0.463308185338974,
      "learning_rate": 2.0964697234469123e-05,
      "loss": 2.3567,
      "step": 116140
    },
    {
      "epoch": 2.3894270454305837,
      "grad_norm": 0.4862222671508789,
      "learning_rate": 2.095109272203827e-05,
      "loss": 2.3758,
      "step": 116150
    },
    {
      "epoch": 2.3896327649474514,
      "grad_norm": 0.4677072763442993,
      "learning_rate": 2.0937492108659296e-05,
      "loss": 2.3714,
      "step": 116160
    },
    {
      "epoch": 2.3898384844643195,
      "grad_norm": 0.48059093952178955,
      "learning_rate": 2.092389539500298e-05,
      "loss": 2.3689,
      "step": 116170
    },
    {
      "epoch": 2.390044203981187,
      "grad_norm": 0.4526878297328949,
      "learning_rate": 2.091030258173995e-05,
      "loss": 2.3772,
      "step": 116180
    },
    {
      "epoch": 2.3902499234980548,
      "grad_norm": 0.45309558510780334,
      "learning_rate": 2.089671366954077e-05,
      "loss": 2.3671,
      "step": 116190
    },
    {
      "epoch": 2.3904556430149224,
      "grad_norm": 0.4578971564769745,
      "learning_rate": 2.0883128659075625e-05,
      "loss": 2.3164,
      "step": 116200
    },
    {
      "epoch": 2.39066136253179,
      "grad_norm": 0.5115106105804443,
      "learning_rate": 2.086954755101457e-05,
      "loss": 2.3765,
      "step": 116210
    },
    {
      "epoch": 2.3908670820486577,
      "grad_norm": 0.44765618443489075,
      "learning_rate": 2.0855970346027575e-05,
      "loss": 2.3916,
      "step": 116220
    },
    {
      "epoch": 2.3910728015655254,
      "grad_norm": 0.477252334356308,
      "learning_rate": 2.0842397044784256e-05,
      "loss": 2.4323,
      "step": 116230
    },
    {
      "epoch": 2.391278521082393,
      "grad_norm": 0.48716509342193604,
      "learning_rate": 2.082882764795414e-05,
      "loss": 2.3448,
      "step": 116240
    },
    {
      "epoch": 2.391484240599261,
      "grad_norm": 0.4452148377895355,
      "learning_rate": 2.0815262156206526e-05,
      "loss": 2.3296,
      "step": 116250
    },
    {
      "epoch": 2.3916899601161288,
      "grad_norm": 0.48759913444519043,
      "learning_rate": 2.0801700570210513e-05,
      "loss": 2.3954,
      "step": 116260
    },
    {
      "epoch": 2.3918956796329964,
      "grad_norm": 0.5121688842773438,
      "learning_rate": 2.0788142890635042e-05,
      "loss": 2.3935,
      "step": 116270
    },
    {
      "epoch": 2.392101399149864,
      "grad_norm": 0.4786699414253235,
      "learning_rate": 2.0774589118148824e-05,
      "loss": 2.3474,
      "step": 116280
    },
    {
      "epoch": 2.3923071186667317,
      "grad_norm": 0.4629749059677124,
      "learning_rate": 2.07610392534204e-05,
      "loss": 2.342,
      "step": 116290
    },
    {
      "epoch": 2.3925128381835994,
      "grad_norm": 0.4724314510822296,
      "learning_rate": 2.0747493297118116e-05,
      "loss": 2.2871,
      "step": 116300
    },
    {
      "epoch": 2.3927185577004675,
      "grad_norm": 0.567022979259491,
      "learning_rate": 2.073395124991011e-05,
      "loss": 2.3759,
      "step": 116310
    },
    {
      "epoch": 2.392924277217335,
      "grad_norm": 0.4509736895561218,
      "learning_rate": 2.0720413112464333e-05,
      "loss": 2.3652,
      "step": 116320
    },
    {
      "epoch": 2.393129996734203,
      "grad_norm": 0.4779638350009918,
      "learning_rate": 2.0706878885448554e-05,
      "loss": 2.3424,
      "step": 116330
    },
    {
      "epoch": 2.3933357162510704,
      "grad_norm": 0.44655147194862366,
      "learning_rate": 2.0693348569530356e-05,
      "loss": 2.3226,
      "step": 116340
    },
    {
      "epoch": 2.393541435767938,
      "grad_norm": 0.46689149737358093,
      "learning_rate": 2.0679822165377093e-05,
      "loss": 2.3492,
      "step": 116350
    },
    {
      "epoch": 2.3937471552848057,
      "grad_norm": 0.46764737367630005,
      "learning_rate": 2.0666299673655965e-05,
      "loss": 2.3267,
      "step": 116360
    },
    {
      "epoch": 2.3939528748016734,
      "grad_norm": 0.4575289487838745,
      "learning_rate": 2.0652781095033957e-05,
      "loss": 2.3621,
      "step": 116370
    },
    {
      "epoch": 2.394158594318541,
      "grad_norm": 0.4433532953262329,
      "learning_rate": 2.0639266430177874e-05,
      "loss": 2.4126,
      "step": 116380
    },
    {
      "epoch": 2.3943643138354087,
      "grad_norm": 0.5160008072853088,
      "learning_rate": 2.06257556797543e-05,
      "loss": 2.3787,
      "step": 116390
    },
    {
      "epoch": 2.394570033352277,
      "grad_norm": 0.4373287260532379,
      "learning_rate": 2.0612248844429693e-05,
      "loss": 2.3333,
      "step": 116400
    },
    {
      "epoch": 2.3947757528691445,
      "grad_norm": 0.44860610365867615,
      "learning_rate": 2.059874592487019e-05,
      "loss": 2.43,
      "step": 116410
    },
    {
      "epoch": 2.394981472386012,
      "grad_norm": 0.48016873002052307,
      "learning_rate": 2.0585246921741887e-05,
      "loss": 2.4135,
      "step": 116420
    },
    {
      "epoch": 2.3951871919028798,
      "grad_norm": 0.5113213062286377,
      "learning_rate": 2.0571751835710618e-05,
      "loss": 2.339,
      "step": 116430
    },
    {
      "epoch": 2.3953929114197474,
      "grad_norm": 0.4522201120853424,
      "learning_rate": 2.0558260667441952e-05,
      "loss": 2.3762,
      "step": 116440
    },
    {
      "epoch": 2.395598630936615,
      "grad_norm": 0.5080753564834595,
      "learning_rate": 2.0544773417601405e-05,
      "loss": 2.3976,
      "step": 116450
    },
    {
      "epoch": 2.395804350453483,
      "grad_norm": 0.4743369221687317,
      "learning_rate": 2.0531290086854237e-05,
      "loss": 2.3616,
      "step": 116460
    },
    {
      "epoch": 2.396010069970351,
      "grad_norm": 0.4489666223526001,
      "learning_rate": 2.0517810675865423e-05,
      "loss": 2.3766,
      "step": 116470
    },
    {
      "epoch": 2.3962157894872185,
      "grad_norm": 0.4462505877017975,
      "learning_rate": 2.0504335185299938e-05,
      "loss": 2.3565,
      "step": 116480
    },
    {
      "epoch": 2.396421509004086,
      "grad_norm": 0.47344666719436646,
      "learning_rate": 2.0490863615822364e-05,
      "loss": 2.3739,
      "step": 116490
    },
    {
      "epoch": 2.396627228520954,
      "grad_norm": 0.4878144860267639,
      "learning_rate": 2.047739596809719e-05,
      "loss": 2.3419,
      "step": 116500
    },
    {
      "epoch": 2.3968329480378214,
      "grad_norm": 0.5369247794151306,
      "learning_rate": 2.0463932242788787e-05,
      "loss": 2.4099,
      "step": 116510
    },
    {
      "epoch": 2.397038667554689,
      "grad_norm": 0.465709388256073,
      "learning_rate": 2.0450472440561152e-05,
      "loss": 2.4434,
      "step": 116520
    },
    {
      "epoch": 2.3972443870715567,
      "grad_norm": 0.4604262113571167,
      "learning_rate": 2.0437016562078194e-05,
      "loss": 2.3525,
      "step": 116530
    },
    {
      "epoch": 2.397450106588425,
      "grad_norm": 0.43740713596343994,
      "learning_rate": 2.04235646080037e-05,
      "loss": 2.3807,
      "step": 116540
    },
    {
      "epoch": 2.3976558261052925,
      "grad_norm": 0.4598834812641144,
      "learning_rate": 2.0410116579001094e-05,
      "loss": 2.4218,
      "step": 116550
    },
    {
      "epoch": 2.39786154562216,
      "grad_norm": 0.47351276874542236,
      "learning_rate": 2.0396672475733725e-05,
      "loss": 2.4297,
      "step": 116560
    },
    {
      "epoch": 2.398067265139028,
      "grad_norm": 0.46797066926956177,
      "learning_rate": 2.038323229886472e-05,
      "loss": 2.377,
      "step": 116570
    },
    {
      "epoch": 2.3982729846558954,
      "grad_norm": 0.43441253900527954,
      "learning_rate": 2.0369796049057e-05,
      "loss": 2.3748,
      "step": 116580
    },
    {
      "epoch": 2.398478704172763,
      "grad_norm": 0.44439712166786194,
      "learning_rate": 2.0356363726973314e-05,
      "loss": 2.3366,
      "step": 116590
    },
    {
      "epoch": 2.398684423689631,
      "grad_norm": 0.4433395266532898,
      "learning_rate": 2.0342935333276193e-05,
      "loss": 2.3422,
      "step": 116600
    },
    {
      "epoch": 2.398890143206499,
      "grad_norm": 0.4774652421474457,
      "learning_rate": 2.0329510868627998e-05,
      "loss": 2.3457,
      "step": 116610
    },
    {
      "epoch": 2.3990958627233665,
      "grad_norm": 0.45333364605903625,
      "learning_rate": 2.0316090333690886e-05,
      "loss": 2.3432,
      "step": 116620
    },
    {
      "epoch": 2.399301582240234,
      "grad_norm": 0.45609021186828613,
      "learning_rate": 2.03026737291268e-05,
      "loss": 2.3262,
      "step": 116630
    },
    {
      "epoch": 2.399507301757102,
      "grad_norm": 0.46108001470565796,
      "learning_rate": 2.028926105559753e-05,
      "loss": 2.3723,
      "step": 116640
    },
    {
      "epoch": 2.3997130212739695,
      "grad_norm": 0.46380090713500977,
      "learning_rate": 2.027585231376463e-05,
      "loss": 2.405,
      "step": 116650
    },
    {
      "epoch": 2.399918740790837,
      "grad_norm": 0.44982683658599854,
      "learning_rate": 2.0262447504289505e-05,
      "loss": 2.3429,
      "step": 116660
    },
    {
      "epoch": 2.4001244603077048,
      "grad_norm": 0.4495559632778168,
      "learning_rate": 2.0249046627833314e-05,
      "loss": 2.3556,
      "step": 116670
    },
    {
      "epoch": 2.400330179824573,
      "grad_norm": 0.4952470362186432,
      "learning_rate": 2.0235649685057067e-05,
      "loss": 2.3652,
      "step": 116680
    },
    {
      "epoch": 2.4005358993414405,
      "grad_norm": 0.47671326994895935,
      "learning_rate": 2.0222256676621564e-05,
      "loss": 2.4015,
      "step": 116690
    },
    {
      "epoch": 2.400741618858308,
      "grad_norm": 0.4891293942928314,
      "learning_rate": 2.0208867603187397e-05,
      "loss": 2.3724,
      "step": 116700
    },
    {
      "epoch": 2.400947338375176,
      "grad_norm": 0.4767203629016876,
      "learning_rate": 2.0195482465414983e-05,
      "loss": 2.3719,
      "step": 116710
    },
    {
      "epoch": 2.4011530578920435,
      "grad_norm": 0.4298558831214905,
      "learning_rate": 2.0182101263964557e-05,
      "loss": 2.3587,
      "step": 116720
    },
    {
      "epoch": 2.401358777408911,
      "grad_norm": 0.4931729733943939,
      "learning_rate": 2.0168723999496074e-05,
      "loss": 2.4033,
      "step": 116730
    },
    {
      "epoch": 2.401564496925779,
      "grad_norm": 0.44094976782798767,
      "learning_rate": 2.0155350672669426e-05,
      "loss": 2.4331,
      "step": 116740
    },
    {
      "epoch": 2.401770216442647,
      "grad_norm": 0.818716287612915,
      "learning_rate": 2.0141981284144262e-05,
      "loss": 2.3821,
      "step": 116750
    },
    {
      "epoch": 2.4019759359595145,
      "grad_norm": 0.44883015751838684,
      "learning_rate": 2.0128615834579933e-05,
      "loss": 2.3426,
      "step": 116760
    },
    {
      "epoch": 2.402181655476382,
      "grad_norm": 0.4244145154953003,
      "learning_rate": 2.011525432463577e-05,
      "loss": 2.3736,
      "step": 116770
    },
    {
      "epoch": 2.40238737499325,
      "grad_norm": 0.45725488662719727,
      "learning_rate": 2.0101896754970817e-05,
      "loss": 2.3198,
      "step": 116780
    },
    {
      "epoch": 2.4025930945101175,
      "grad_norm": 0.5879389643669128,
      "learning_rate": 2.008854312624384e-05,
      "loss": 2.3661,
      "step": 116790
    },
    {
      "epoch": 2.402798814026985,
      "grad_norm": 0.43540406227111816,
      "learning_rate": 2.0075193439113628e-05,
      "loss": 2.3361,
      "step": 116800
    },
    {
      "epoch": 2.403004533543853,
      "grad_norm": 0.39329323172569275,
      "learning_rate": 2.0061847694238554e-05,
      "loss": 2.3843,
      "step": 116810
    },
    {
      "epoch": 2.4032102530607204,
      "grad_norm": 0.4037106931209564,
      "learning_rate": 2.0048505892276904e-05,
      "loss": 2.3565,
      "step": 116820
    },
    {
      "epoch": 2.4034159725775885,
      "grad_norm": 0.4469988942146301,
      "learning_rate": 2.003516803388682e-05,
      "loss": 2.3781,
      "step": 116830
    },
    {
      "epoch": 2.403621692094456,
      "grad_norm": 0.4287412166595459,
      "learning_rate": 2.0021834119726123e-05,
      "loss": 2.3426,
      "step": 116840
    },
    {
      "epoch": 2.403827411611324,
      "grad_norm": 0.4570537805557251,
      "learning_rate": 2.000850415045248e-05,
      "loss": 2.362,
      "step": 116850
    },
    {
      "epoch": 2.4040331311281915,
      "grad_norm": 0.46401795744895935,
      "learning_rate": 1.999517812672349e-05,
      "loss": 2.3848,
      "step": 116860
    },
    {
      "epoch": 2.404238850645059,
      "grad_norm": 0.4716286063194275,
      "learning_rate": 1.9981856049196357e-05,
      "loss": 2.3875,
      "step": 116870
    },
    {
      "epoch": 2.404444570161927,
      "grad_norm": 0.47556447982788086,
      "learning_rate": 1.9968537918528218e-05,
      "loss": 2.3704,
      "step": 116880
    },
    {
      "epoch": 2.404650289678795,
      "grad_norm": 0.46428588032722473,
      "learning_rate": 1.9955223735375995e-05,
      "loss": 2.3516,
      "step": 116890
    },
    {
      "epoch": 2.4048560091956626,
      "grad_norm": 0.522011935710907,
      "learning_rate": 1.994191350039638e-05,
      "loss": 2.3687,
      "step": 116900
    },
    {
      "epoch": 2.40506172871253,
      "grad_norm": 0.4751463234424591,
      "learning_rate": 1.9928607214245922e-05,
      "loss": 2.3964,
      "step": 116910
    },
    {
      "epoch": 2.405267448229398,
      "grad_norm": 0.46326833963394165,
      "learning_rate": 1.991530487758092e-05,
      "loss": 2.3418,
      "step": 116920
    },
    {
      "epoch": 2.4054731677462655,
      "grad_norm": 0.464675635099411,
      "learning_rate": 1.9902006491057522e-05,
      "loss": 2.3863,
      "step": 116930
    },
    {
      "epoch": 2.405678887263133,
      "grad_norm": 0.47020044922828674,
      "learning_rate": 1.988871205533166e-05,
      "loss": 2.3817,
      "step": 116940
    },
    {
      "epoch": 2.405884606780001,
      "grad_norm": 0.48507365584373474,
      "learning_rate": 1.9875421571059083e-05,
      "loss": 2.3622,
      "step": 116950
    },
    {
      "epoch": 2.4060903262968685,
      "grad_norm": 0.4469335377216339,
      "learning_rate": 1.9862135038895336e-05,
      "loss": 2.3498,
      "step": 116960
    },
    {
      "epoch": 2.4062960458137366,
      "grad_norm": 0.46637070178985596,
      "learning_rate": 1.984885245949577e-05,
      "loss": 2.3963,
      "step": 116970
    },
    {
      "epoch": 2.4065017653306042,
      "grad_norm": 0.5311108827590942,
      "learning_rate": 1.983557383351553e-05,
      "loss": 2.3038,
      "step": 116980
    },
    {
      "epoch": 2.406707484847472,
      "grad_norm": 0.3940955102443695,
      "learning_rate": 1.9822299161609592e-05,
      "loss": 2.3236,
      "step": 116990
    },
    {
      "epoch": 2.4069132043643395,
      "grad_norm": 0.5835140943527222,
      "learning_rate": 1.9809028444432732e-05,
      "loss": 2.3805,
      "step": 117000
    },
    {
      "epoch": 2.407118923881207,
      "grad_norm": 0.4710802733898163,
      "learning_rate": 1.9795761682639503e-05,
      "loss": 2.2966,
      "step": 117010
    },
    {
      "epoch": 2.407324643398075,
      "grad_norm": 0.5205511450767517,
      "learning_rate": 1.9782498876884283e-05,
      "loss": 2.3485,
      "step": 117020
    },
    {
      "epoch": 2.407530362914943,
      "grad_norm": 0.47536197304725647,
      "learning_rate": 1.9769240027821268e-05,
      "loss": 2.3581,
      "step": 117030
    },
    {
      "epoch": 2.4077360824318106,
      "grad_norm": 0.466279000043869,
      "learning_rate": 1.975598513610446e-05,
      "loss": 2.4242,
      "step": 117040
    },
    {
      "epoch": 2.4079418019486782,
      "grad_norm": 0.48716574907302856,
      "learning_rate": 1.974273420238757e-05,
      "loss": 2.351,
      "step": 117050
    },
    {
      "epoch": 2.408147521465546,
      "grad_norm": 0.48984938859939575,
      "learning_rate": 1.9729487227324282e-05,
      "loss": 2.3148,
      "step": 117060
    },
    {
      "epoch": 2.4083532409824135,
      "grad_norm": 0.44229957461357117,
      "learning_rate": 1.9716244211568003e-05,
      "loss": 2.3709,
      "step": 117070
    },
    {
      "epoch": 2.408558960499281,
      "grad_norm": 0.49718812108039856,
      "learning_rate": 1.9703005155771847e-05,
      "loss": 2.4,
      "step": 117080
    },
    {
      "epoch": 2.408764680016149,
      "grad_norm": 0.43518078327178955,
      "learning_rate": 1.9689770060588897e-05,
      "loss": 2.3834,
      "step": 117090
    },
    {
      "epoch": 2.4089703995330165,
      "grad_norm": 0.44583866000175476,
      "learning_rate": 1.9676538926671982e-05,
      "loss": 2.3399,
      "step": 117100
    },
    {
      "epoch": 2.4091761190498846,
      "grad_norm": 0.5027990341186523,
      "learning_rate": 1.9663311754673642e-05,
      "loss": 2.3241,
      "step": 117110
    },
    {
      "epoch": 2.4093818385667523,
      "grad_norm": 0.5182008743286133,
      "learning_rate": 1.9650088545246405e-05,
      "loss": 2.361,
      "step": 117120
    },
    {
      "epoch": 2.40958755808362,
      "grad_norm": 0.4459203779697418,
      "learning_rate": 1.9636869299042414e-05,
      "loss": 2.3083,
      "step": 117130
    },
    {
      "epoch": 2.4097932776004876,
      "grad_norm": 0.4448859989643097,
      "learning_rate": 1.962365401671371e-05,
      "loss": 2.3609,
      "step": 117140
    },
    {
      "epoch": 2.409998997117355,
      "grad_norm": 0.44715166091918945,
      "learning_rate": 1.9610442698912214e-05,
      "loss": 2.3353,
      "step": 117150
    },
    {
      "epoch": 2.410204716634223,
      "grad_norm": 0.48862990736961365,
      "learning_rate": 1.9597235346289476e-05,
      "loss": 2.3746,
      "step": 117160
    },
    {
      "epoch": 2.4104104361510905,
      "grad_norm": 0.46971094608306885,
      "learning_rate": 1.958403195949695e-05,
      "loss": 2.3762,
      "step": 117170
    },
    {
      "epoch": 2.4106161556679586,
      "grad_norm": 0.4566718637943268,
      "learning_rate": 1.9570832539185968e-05,
      "loss": 2.4045,
      "step": 117180
    },
    {
      "epoch": 2.4108218751848263,
      "grad_norm": 0.4654318392276764,
      "learning_rate": 1.9557637086007497e-05,
      "loss": 2.3505,
      "step": 117190
    },
    {
      "epoch": 2.411027594701694,
      "grad_norm": 0.427225798368454,
      "learning_rate": 1.9544445600612437e-05,
      "loss": 2.3594,
      "step": 117200
    },
    {
      "epoch": 2.4112333142185616,
      "grad_norm": 0.4730932116508484,
      "learning_rate": 1.9531258083651438e-05,
      "loss": 2.4002,
      "step": 117210
    },
    {
      "epoch": 2.4114390337354292,
      "grad_norm": 0.49735817313194275,
      "learning_rate": 1.9518074535774988e-05,
      "loss": 2.3472,
      "step": 117220
    },
    {
      "epoch": 2.411644753252297,
      "grad_norm": 0.45250406861305237,
      "learning_rate": 1.9504894957633336e-05,
      "loss": 2.3144,
      "step": 117230
    },
    {
      "epoch": 2.4118504727691645,
      "grad_norm": 0.46006202697753906,
      "learning_rate": 1.949171934987658e-05,
      "loss": 2.3808,
      "step": 117240
    },
    {
      "epoch": 2.412056192286032,
      "grad_norm": 0.4656190574169159,
      "learning_rate": 1.9478547713154584e-05,
      "loss": 2.3277,
      "step": 117250
    },
    {
      "epoch": 2.4122619118029003,
      "grad_norm": 0.43482670187950134,
      "learning_rate": 1.9465380048117042e-05,
      "loss": 2.3806,
      "step": 117260
    },
    {
      "epoch": 2.412467631319768,
      "grad_norm": 0.45863670110702515,
      "learning_rate": 1.9452216355413433e-05,
      "loss": 2.3444,
      "step": 117270
    },
    {
      "epoch": 2.4126733508366356,
      "grad_norm": 0.43395546078681946,
      "learning_rate": 1.9439056635693065e-05,
      "loss": 2.3199,
      "step": 117280
    },
    {
      "epoch": 2.4128790703535032,
      "grad_norm": 0.45910802483558655,
      "learning_rate": 1.942590088960503e-05,
      "loss": 2.3333,
      "step": 117290
    },
    {
      "epoch": 2.413084789870371,
      "grad_norm": 0.5080814957618713,
      "learning_rate": 1.9412749117798236e-05,
      "loss": 2.3631,
      "step": 117300
    },
    {
      "epoch": 2.4132905093872385,
      "grad_norm": 0.5418339371681213,
      "learning_rate": 1.939960132092137e-05,
      "loss": 2.3388,
      "step": 117310
    },
    {
      "epoch": 2.4134962289041066,
      "grad_norm": 0.4803546965122223,
      "learning_rate": 1.9386457499622947e-05,
      "loss": 2.4394,
      "step": 117320
    },
    {
      "epoch": 2.4137019484209743,
      "grad_norm": 0.47581613063812256,
      "learning_rate": 1.9373317654551316e-05,
      "loss": 2.3916,
      "step": 117330
    },
    {
      "epoch": 2.413907667937842,
      "grad_norm": 0.4602029323577881,
      "learning_rate": 1.9360181786354515e-05,
      "loss": 2.3441,
      "step": 117340
    },
    {
      "epoch": 2.4141133874547096,
      "grad_norm": 0.44454947113990784,
      "learning_rate": 1.9347049895680537e-05,
      "loss": 2.3532,
      "step": 117350
    },
    {
      "epoch": 2.4143191069715773,
      "grad_norm": 0.4220083951950073,
      "learning_rate": 1.9333921983177107e-05,
      "loss": 2.3792,
      "step": 117360
    },
    {
      "epoch": 2.414524826488445,
      "grad_norm": 0.41838201880455017,
      "learning_rate": 1.9320798049491672e-05,
      "loss": 2.3608,
      "step": 117370
    },
    {
      "epoch": 2.4147305460053126,
      "grad_norm": 0.4242427349090576,
      "learning_rate": 1.930767809527164e-05,
      "loss": 2.3763,
      "step": 117380
    },
    {
      "epoch": 2.41493626552218,
      "grad_norm": 0.4151768982410431,
      "learning_rate": 1.9294562121164163e-05,
      "loss": 2.3453,
      "step": 117390
    },
    {
      "epoch": 2.4151419850390483,
      "grad_norm": 0.44378581643104553,
      "learning_rate": 1.928145012781609e-05,
      "loss": 2.368,
      "step": 117400
    },
    {
      "epoch": 2.415347704555916,
      "grad_norm": 0.45991939306259155,
      "learning_rate": 1.926834211587427e-05,
      "loss": 2.2996,
      "step": 117410
    },
    {
      "epoch": 2.4155534240727836,
      "grad_norm": 0.5028611421585083,
      "learning_rate": 1.9255238085985183e-05,
      "loss": 2.4483,
      "step": 117420
    },
    {
      "epoch": 2.4157591435896513,
      "grad_norm": 0.4539567828178406,
      "learning_rate": 1.9242138038795153e-05,
      "loss": 2.3827,
      "step": 117430
    },
    {
      "epoch": 2.415964863106519,
      "grad_norm": 0.4467514455318451,
      "learning_rate": 1.9229041974950447e-05,
      "loss": 2.3751,
      "step": 117440
    },
    {
      "epoch": 2.4161705826233866,
      "grad_norm": 0.44817522168159485,
      "learning_rate": 1.9215949895096908e-05,
      "loss": 2.3275,
      "step": 117450
    },
    {
      "epoch": 2.4163763021402547,
      "grad_norm": 0.5431987643241882,
      "learning_rate": 1.9202861799880323e-05,
      "loss": 2.3905,
      "step": 117460
    },
    {
      "epoch": 2.4165820216571223,
      "grad_norm": 0.45044389367103577,
      "learning_rate": 1.918977768994632e-05,
      "loss": 2.3335,
      "step": 117470
    },
    {
      "epoch": 2.41678774117399,
      "grad_norm": 0.4748692810535431,
      "learning_rate": 1.9176697565940192e-05,
      "loss": 2.3329,
      "step": 117480
    },
    {
      "epoch": 2.4169934606908576,
      "grad_norm": 0.6116832494735718,
      "learning_rate": 1.916362142850715e-05,
      "loss": 2.3341,
      "step": 117490
    },
    {
      "epoch": 2.4171991802077253,
      "grad_norm": 0.4463059604167938,
      "learning_rate": 1.9150549278292153e-05,
      "loss": 2.406,
      "step": 117500
    },
    {
      "epoch": 2.417404899724593,
      "grad_norm": 0.4708870053291321,
      "learning_rate": 1.913748111593998e-05,
      "loss": 2.3237,
      "step": 117510
    },
    {
      "epoch": 2.4176106192414606,
      "grad_norm": 0.5083490014076233,
      "learning_rate": 1.9124416942095224e-05,
      "loss": 2.3741,
      "step": 117520
    },
    {
      "epoch": 2.4178163387583282,
      "grad_norm": 0.44629037380218506,
      "learning_rate": 1.9111356757402256e-05,
      "loss": 2.33,
      "step": 117530
    },
    {
      "epoch": 2.418022058275196,
      "grad_norm": 0.49374234676361084,
      "learning_rate": 1.9098300562505266e-05,
      "loss": 2.4055,
      "step": 117540
    },
    {
      "epoch": 2.418227777792064,
      "grad_norm": 0.4437359869480133,
      "learning_rate": 1.9085248358048246e-05,
      "loss": 2.3597,
      "step": 117550
    },
    {
      "epoch": 2.4184334973089316,
      "grad_norm": 0.4743788242340088,
      "learning_rate": 1.9072200144675e-05,
      "loss": 2.3702,
      "step": 117560
    },
    {
      "epoch": 2.4186392168257993,
      "grad_norm": 0.4650304615497589,
      "learning_rate": 1.9059155923029115e-05,
      "loss": 2.3804,
      "step": 117570
    },
    {
      "epoch": 2.418844936342667,
      "grad_norm": 0.4590970277786255,
      "learning_rate": 1.9046115693753985e-05,
      "loss": 2.3886,
      "step": 117580
    },
    {
      "epoch": 2.4190506558595346,
      "grad_norm": 0.45046964287757874,
      "learning_rate": 1.903307945749283e-05,
      "loss": 2.3915,
      "step": 117590
    },
    {
      "epoch": 2.4192563753764023,
      "grad_norm": 0.46090084314346313,
      "learning_rate": 1.9020047214888646e-05,
      "loss": 2.3853,
      "step": 117600
    },
    {
      "epoch": 2.4194620948932704,
      "grad_norm": 0.4144419729709625,
      "learning_rate": 1.9007018966584257e-05,
      "loss": 2.3383,
      "step": 117610
    },
    {
      "epoch": 2.419667814410138,
      "grad_norm": 0.425731360912323,
      "learning_rate": 1.899399471322225e-05,
      "loss": 2.3205,
      "step": 117620
    },
    {
      "epoch": 2.4198735339270057,
      "grad_norm": 0.4586377441883087,
      "learning_rate": 1.8980974455445054e-05,
      "loss": 2.3674,
      "step": 117630
    },
    {
      "epoch": 2.4200792534438733,
      "grad_norm": 0.5265889763832092,
      "learning_rate": 1.89679581938949e-05,
      "loss": 2.3525,
      "step": 117640
    },
    {
      "epoch": 2.420284972960741,
      "grad_norm": 0.4554455578327179,
      "learning_rate": 1.895494592921381e-05,
      "loss": 2.333,
      "step": 117650
    },
    {
      "epoch": 2.4204906924776086,
      "grad_norm": 0.49130916595458984,
      "learning_rate": 1.894193766204354e-05,
      "loss": 2.3358,
      "step": 117660
    },
    {
      "epoch": 2.4206964119944763,
      "grad_norm": 0.459690123796463,
      "learning_rate": 1.8928933393025806e-05,
      "loss": 2.3461,
      "step": 117670
    },
    {
      "epoch": 2.420902131511344,
      "grad_norm": 0.4700104594230652,
      "learning_rate": 1.8915933122802034e-05,
      "loss": 2.374,
      "step": 117680
    },
    {
      "epoch": 2.421107851028212,
      "grad_norm": 0.4372125566005707,
      "learning_rate": 1.8902936852013365e-05,
      "loss": 2.3367,
      "step": 117690
    },
    {
      "epoch": 2.4213135705450797,
      "grad_norm": 0.4810478687286377,
      "learning_rate": 1.888994458130092e-05,
      "loss": 2.3544,
      "step": 117700
    },
    {
      "epoch": 2.4215192900619473,
      "grad_norm": 0.4621316194534302,
      "learning_rate": 1.887695631130554e-05,
      "loss": 2.3528,
      "step": 117710
    },
    {
      "epoch": 2.421725009578815,
      "grad_norm": 0.4316920340061188,
      "learning_rate": 1.8863972042667788e-05,
      "loss": 2.3937,
      "step": 117720
    },
    {
      "epoch": 2.4219307290956826,
      "grad_norm": 0.47376516461372375,
      "learning_rate": 1.8850991776028203e-05,
      "loss": 2.3603,
      "step": 117730
    },
    {
      "epoch": 2.4221364486125503,
      "grad_norm": 0.5358167290687561,
      "learning_rate": 1.8838015512026964e-05,
      "loss": 2.4004,
      "step": 117740
    },
    {
      "epoch": 2.4223421681294184,
      "grad_norm": 0.43687528371810913,
      "learning_rate": 1.882504325130412e-05,
      "loss": 2.318,
      "step": 117750
    },
    {
      "epoch": 2.422547887646286,
      "grad_norm": 0.4648306667804718,
      "learning_rate": 1.8812074994499586e-05,
      "loss": 2.3792,
      "step": 117760
    },
    {
      "epoch": 2.4227536071631537,
      "grad_norm": 0.4902937412261963,
      "learning_rate": 1.8799110742252944e-05,
      "loss": 2.3514,
      "step": 117770
    },
    {
      "epoch": 2.4229593266800213,
      "grad_norm": 0.4605465829372406,
      "learning_rate": 1.8786150495203647e-05,
      "loss": 2.3163,
      "step": 117780
    },
    {
      "epoch": 2.423165046196889,
      "grad_norm": 0.469481498003006,
      "learning_rate": 1.8773194253991044e-05,
      "loss": 2.3836,
      "step": 117790
    },
    {
      "epoch": 2.4233707657137566,
      "grad_norm": 0.4540218114852905,
      "learning_rate": 1.8760242019254115e-05,
      "loss": 2.434,
      "step": 117800
    },
    {
      "epoch": 2.4235764852306243,
      "grad_norm": 0.42447131872177124,
      "learning_rate": 1.8747293791631738e-05,
      "loss": 2.3873,
      "step": 117810
    },
    {
      "epoch": 2.423782204747492,
      "grad_norm": 0.4697533845901489,
      "learning_rate": 1.8734349571762576e-05,
      "loss": 2.3827,
      "step": 117820
    },
    {
      "epoch": 2.42398792426436,
      "grad_norm": 0.4992826282978058,
      "learning_rate": 1.8721409360285113e-05,
      "loss": 2.3995,
      "step": 117830
    },
    {
      "epoch": 2.4241936437812277,
      "grad_norm": 0.4787074625492096,
      "learning_rate": 1.8708473157837615e-05,
      "loss": 2.3424,
      "step": 117840
    },
    {
      "epoch": 2.4243993632980954,
      "grad_norm": 0.5052712559700012,
      "learning_rate": 1.8695540965058146e-05,
      "loss": 2.3821,
      "step": 117850
    },
    {
      "epoch": 2.424605082814963,
      "grad_norm": 0.45408228039741516,
      "learning_rate": 1.8682612782584574e-05,
      "loss": 2.3561,
      "step": 117860
    },
    {
      "epoch": 2.4248108023318307,
      "grad_norm": 0.42938071489334106,
      "learning_rate": 1.8669688611054602e-05,
      "loss": 2.3874,
      "step": 117870
    },
    {
      "epoch": 2.4250165218486983,
      "grad_norm": 0.4577879011631012,
      "learning_rate": 1.865676845110569e-05,
      "loss": 2.4013,
      "step": 117880
    },
    {
      "epoch": 2.425222241365566,
      "grad_norm": 0.5112865567207336,
      "learning_rate": 1.864385230337512e-05,
      "loss": 2.3768,
      "step": 117890
    },
    {
      "epoch": 2.425427960882434,
      "grad_norm": 0.5642050504684448,
      "learning_rate": 1.8630940168499977e-05,
      "loss": 2.3819,
      "step": 117900
    },
    {
      "epoch": 2.4256336803993017,
      "grad_norm": 0.4481269419193268,
      "learning_rate": 1.861803204711715e-05,
      "loss": 2.3692,
      "step": 117910
    },
    {
      "epoch": 2.4258393999161694,
      "grad_norm": 0.5242125391960144,
      "learning_rate": 1.860512793986332e-05,
      "loss": 2.3566,
      "step": 117920
    },
    {
      "epoch": 2.426045119433037,
      "grad_norm": 0.5139936804771423,
      "learning_rate": 1.8592227847374988e-05,
      "loss": 2.3851,
      "step": 117930
    },
    {
      "epoch": 2.4262508389499047,
      "grad_norm": 0.4599677324295044,
      "learning_rate": 1.8579331770288433e-05,
      "loss": 2.3503,
      "step": 117940
    },
    {
      "epoch": 2.4264565584667723,
      "grad_norm": 0.4562903940677643,
      "learning_rate": 1.8566439709239747e-05,
      "loss": 2.3453,
      "step": 117950
    },
    {
      "epoch": 2.42666227798364,
      "grad_norm": 0.4997638463973999,
      "learning_rate": 1.8553551664864845e-05,
      "loss": 2.3651,
      "step": 117960
    },
    {
      "epoch": 2.4268679975005076,
      "grad_norm": 0.47747617959976196,
      "learning_rate": 1.8540667637799424e-05,
      "loss": 2.3744,
      "step": 117970
    },
    {
      "epoch": 2.4270737170173757,
      "grad_norm": 0.48386135697364807,
      "learning_rate": 1.8527787628678916e-05,
      "loss": 2.3564,
      "step": 117980
    },
    {
      "epoch": 2.4272794365342434,
      "grad_norm": 0.45212191343307495,
      "learning_rate": 1.85149116381387e-05,
      "loss": 2.3419,
      "step": 117990
    },
    {
      "epoch": 2.427485156051111,
      "grad_norm": 0.4360426068305969,
      "learning_rate": 1.8502039666813874e-05,
      "loss": 2.3335,
      "step": 118000
    },
    {
      "epoch": 2.4276908755679787,
      "grad_norm": 0.4629100263118744,
      "learning_rate": 1.8489171715339283e-05,
      "loss": 2.365,
      "step": 118010
    },
    {
      "epoch": 2.4278965950848463,
      "grad_norm": 0.44895076751708984,
      "learning_rate": 1.847630778434969e-05,
      "loss": 2.3607,
      "step": 118020
    },
    {
      "epoch": 2.428102314601714,
      "grad_norm": 0.4983079731464386,
      "learning_rate": 1.8463447874479602e-05,
      "loss": 2.3577,
      "step": 118030
    },
    {
      "epoch": 2.428308034118582,
      "grad_norm": 0.4340800642967224,
      "learning_rate": 1.8450591986363265e-05,
      "loss": 2.3602,
      "step": 118040
    },
    {
      "epoch": 2.4285137536354497,
      "grad_norm": 0.5017897486686707,
      "learning_rate": 1.8437740120634884e-05,
      "loss": 2.3444,
      "step": 118050
    },
    {
      "epoch": 2.4287194731523174,
      "grad_norm": 0.44006726145744324,
      "learning_rate": 1.8424892277928297e-05,
      "loss": 2.3813,
      "step": 118060
    },
    {
      "epoch": 2.428925192669185,
      "grad_norm": 0.4689069390296936,
      "learning_rate": 1.8412048458877216e-05,
      "loss": 2.3704,
      "step": 118070
    },
    {
      "epoch": 2.4291309121860527,
      "grad_norm": 0.45447245240211487,
      "learning_rate": 1.839920866411523e-05,
      "loss": 2.3556,
      "step": 118080
    },
    {
      "epoch": 2.4293366317029204,
      "grad_norm": 0.4482298195362091,
      "learning_rate": 1.8386372894275594e-05,
      "loss": 2.4148,
      "step": 118090
    },
    {
      "epoch": 2.429542351219788,
      "grad_norm": 0.44096893072128296,
      "learning_rate": 1.8373541149991414e-05,
      "loss": 2.3566,
      "step": 118100
    },
    {
      "epoch": 2.4297480707366557,
      "grad_norm": 0.645027756690979,
      "learning_rate": 1.8360713431895683e-05,
      "loss": 2.3597,
      "step": 118110
    },
    {
      "epoch": 2.4299537902535238,
      "grad_norm": 0.48986300826072693,
      "learning_rate": 1.834788974062106e-05,
      "loss": 2.3644,
      "step": 118120
    },
    {
      "epoch": 2.4301595097703914,
      "grad_norm": 0.4809279441833496,
      "learning_rate": 1.8335070076800075e-05,
      "loss": 2.3511,
      "step": 118130
    },
    {
      "epoch": 2.430365229287259,
      "grad_norm": 0.46613144874572754,
      "learning_rate": 1.832225444106508e-05,
      "loss": 2.2856,
      "step": 118140
    },
    {
      "epoch": 2.4305709488041267,
      "grad_norm": 0.4830095171928406,
      "learning_rate": 1.8309442834048162e-05,
      "loss": 2.3501,
      "step": 118150
    },
    {
      "epoch": 2.4307766683209944,
      "grad_norm": 0.5774713754653931,
      "learning_rate": 1.829663525638129e-05,
      "loss": 2.4163,
      "step": 118160
    },
    {
      "epoch": 2.430982387837862,
      "grad_norm": 0.4332338273525238,
      "learning_rate": 1.8283831708696154e-05,
      "loss": 2.3621,
      "step": 118170
    },
    {
      "epoch": 2.43118810735473,
      "grad_norm": 0.5153682827949524,
      "learning_rate": 1.8271032191624304e-05,
      "loss": 2.4208,
      "step": 118180
    },
    {
      "epoch": 2.4313938268715978,
      "grad_norm": 0.4452734589576721,
      "learning_rate": 1.825823670579707e-05,
      "loss": 2.3594,
      "step": 118190
    },
    {
      "epoch": 2.4315995463884654,
      "grad_norm": 0.4522452652454376,
      "learning_rate": 1.8245445251845582e-05,
      "loss": 2.4278,
      "step": 118200
    },
    {
      "epoch": 2.431805265905333,
      "grad_norm": 0.43046367168426514,
      "learning_rate": 1.823265783040077e-05,
      "loss": 2.3238,
      "step": 118210
    },
    {
      "epoch": 2.4320109854222007,
      "grad_norm": 0.4288264513015747,
      "learning_rate": 1.8219874442093367e-05,
      "loss": 2.418,
      "step": 118220
    },
    {
      "epoch": 2.4322167049390684,
      "grad_norm": 0.47108593583106995,
      "learning_rate": 1.8207095087553906e-05,
      "loss": 2.3411,
      "step": 118230
    },
    {
      "epoch": 2.432422424455936,
      "grad_norm": 0.43557897210121155,
      "learning_rate": 1.8194319767412726e-05,
      "loss": 2.3166,
      "step": 118240
    },
    {
      "epoch": 2.4326281439728037,
      "grad_norm": 0.48975056409835815,
      "learning_rate": 1.818154848229997e-05,
      "loss": 2.4149,
      "step": 118250
    },
    {
      "epoch": 2.432833863489672,
      "grad_norm": 0.46516522765159607,
      "learning_rate": 1.8168781232845565e-05,
      "loss": 2.3712,
      "step": 118260
    },
    {
      "epoch": 2.4330395830065394,
      "grad_norm": 0.4253521263599396,
      "learning_rate": 1.8156018019679265e-05,
      "loss": 2.3303,
      "step": 118270
    },
    {
      "epoch": 2.433245302523407,
      "grad_norm": 0.48988786339759827,
      "learning_rate": 1.814325884343059e-05,
      "loss": 2.3981,
      "step": 118280
    },
    {
      "epoch": 2.4334510220402747,
      "grad_norm": 0.43801426887512207,
      "learning_rate": 1.813050370472892e-05,
      "loss": 2.3416,
      "step": 118290
    },
    {
      "epoch": 2.4336567415571424,
      "grad_norm": 0.4829547703266144,
      "learning_rate": 1.811775260420332e-05,
      "loss": 2.3077,
      "step": 118300
    },
    {
      "epoch": 2.43386246107401,
      "grad_norm": 0.47825220227241516,
      "learning_rate": 1.810500554248279e-05,
      "loss": 2.3616,
      "step": 118310
    },
    {
      "epoch": 2.4340681805908777,
      "grad_norm": 0.6024912595748901,
      "learning_rate": 1.80922625201961e-05,
      "loss": 2.3513,
      "step": 118320
    },
    {
      "epoch": 2.434273900107746,
      "grad_norm": 0.4442799985408783,
      "learning_rate": 1.807952353797171e-05,
      "loss": 2.3918,
      "step": 118330
    },
    {
      "epoch": 2.4344796196246135,
      "grad_norm": 0.4584372341632843,
      "learning_rate": 1.806678859643802e-05,
      "loss": 2.3754,
      "step": 118340
    },
    {
      "epoch": 2.434685339141481,
      "grad_norm": 0.41296902298927307,
      "learning_rate": 1.8054057696223203e-05,
      "loss": 2.3878,
      "step": 118350
    },
    {
      "epoch": 2.4348910586583488,
      "grad_norm": 0.47089487314224243,
      "learning_rate": 1.8041330837955117e-05,
      "loss": 2.4272,
      "step": 118360
    },
    {
      "epoch": 2.4350967781752164,
      "grad_norm": 0.5030527114868164,
      "learning_rate": 1.8028608022261605e-05,
      "loss": 2.4475,
      "step": 118370
    },
    {
      "epoch": 2.435302497692084,
      "grad_norm": 0.4660448729991913,
      "learning_rate": 1.8015889249770146e-05,
      "loss": 2.3825,
      "step": 118380
    },
    {
      "epoch": 2.4355082172089517,
      "grad_norm": 0.49701735377311707,
      "learning_rate": 1.800317452110809e-05,
      "loss": 2.3661,
      "step": 118390
    },
    {
      "epoch": 2.4357139367258194,
      "grad_norm": 0.4655417203903198,
      "learning_rate": 1.799046383690265e-05,
      "loss": 2.34,
      "step": 118400
    },
    {
      "epoch": 2.4359196562426875,
      "grad_norm": 0.48659762740135193,
      "learning_rate": 1.7977757197780707e-05,
      "loss": 2.3183,
      "step": 118410
    },
    {
      "epoch": 2.436125375759555,
      "grad_norm": 0.4842087924480438,
      "learning_rate": 1.7965054604369015e-05,
      "loss": 2.3749,
      "step": 118420
    },
    {
      "epoch": 2.4363310952764228,
      "grad_norm": 0.5065140128135681,
      "learning_rate": 1.7952356057294183e-05,
      "loss": 2.3593,
      "step": 118430
    },
    {
      "epoch": 2.4365368147932904,
      "grad_norm": 0.47250300645828247,
      "learning_rate": 1.7939661557182498e-05,
      "loss": 2.3329,
      "step": 118440
    },
    {
      "epoch": 2.436742534310158,
      "grad_norm": 0.5047730803489685,
      "learning_rate": 1.792697110466014e-05,
      "loss": 2.3169,
      "step": 118450
    },
    {
      "epoch": 2.4369482538270257,
      "grad_norm": 0.5289542078971863,
      "learning_rate": 1.7914284700353046e-05,
      "loss": 2.4305,
      "step": 118460
    },
    {
      "epoch": 2.437153973343894,
      "grad_norm": 0.4464206099510193,
      "learning_rate": 1.790160234488697e-05,
      "loss": 2.3271,
      "step": 118470
    },
    {
      "epoch": 2.4373596928607615,
      "grad_norm": 0.5025458931922913,
      "learning_rate": 1.788892403888748e-05,
      "loss": 2.4482,
      "step": 118480
    },
    {
      "epoch": 2.437565412377629,
      "grad_norm": 0.4702777862548828,
      "learning_rate": 1.7876249782979903e-05,
      "loss": 2.4432,
      "step": 118490
    },
    {
      "epoch": 2.437771131894497,
      "grad_norm": 0.4717347025871277,
      "learning_rate": 1.7863579577789412e-05,
      "loss": 2.35,
      "step": 118500
    },
    {
      "epoch": 2.4379768514113644,
      "grad_norm": 0.4513704180717468,
      "learning_rate": 1.785091342394095e-05,
      "loss": 2.3824,
      "step": 118510
    },
    {
      "epoch": 2.438182570928232,
      "grad_norm": 0.49650704860687256,
      "learning_rate": 1.7838251322059275e-05,
      "loss": 2.3651,
      "step": 118520
    },
    {
      "epoch": 2.4383882904450997,
      "grad_norm": 0.43468889594078064,
      "learning_rate": 1.7825593272768924e-05,
      "loss": 2.3475,
      "step": 118530
    },
    {
      "epoch": 2.4385940099619674,
      "grad_norm": 0.4663691520690918,
      "learning_rate": 1.7812939276694263e-05,
      "loss": 2.3493,
      "step": 118540
    },
    {
      "epoch": 2.4387997294788355,
      "grad_norm": 0.4261114299297333,
      "learning_rate": 1.7800289334459443e-05,
      "loss": 2.3897,
      "step": 118550
    },
    {
      "epoch": 2.439005448995703,
      "grad_norm": 0.4288750886917114,
      "learning_rate": 1.7787643446688408e-05,
      "loss": 2.3425,
      "step": 118560
    },
    {
      "epoch": 2.439211168512571,
      "grad_norm": 0.5164039134979248,
      "learning_rate": 1.7775001614004928e-05,
      "loss": 2.3566,
      "step": 118570
    },
    {
      "epoch": 2.4394168880294385,
      "grad_norm": 0.4805741608142853,
      "learning_rate": 1.7762363837032568e-05,
      "loss": 2.3696,
      "step": 118580
    },
    {
      "epoch": 2.439622607546306,
      "grad_norm": 0.45272642374038696,
      "learning_rate": 1.7749730116394615e-05,
      "loss": 2.4169,
      "step": 118590
    },
    {
      "epoch": 2.4398283270631738,
      "grad_norm": 0.4563297927379608,
      "learning_rate": 1.7737100452714272e-05,
      "loss": 2.3429,
      "step": 118600
    },
    {
      "epoch": 2.4400340465800414,
      "grad_norm": 0.4441746175289154,
      "learning_rate": 1.7724474846614525e-05,
      "loss": 2.3719,
      "step": 118610
    },
    {
      "epoch": 2.4402397660969095,
      "grad_norm": 0.4463050067424774,
      "learning_rate": 1.7711853298718028e-05,
      "loss": 2.3705,
      "step": 118620
    },
    {
      "epoch": 2.440445485613777,
      "grad_norm": 0.48561593890190125,
      "learning_rate": 1.769923580964743e-05,
      "loss": 2.3789,
      "step": 118630
    },
    {
      "epoch": 2.440651205130645,
      "grad_norm": 0.43180567026138306,
      "learning_rate": 1.7686622380025053e-05,
      "loss": 2.2915,
      "step": 118640
    },
    {
      "epoch": 2.4408569246475125,
      "grad_norm": 0.4487450420856476,
      "learning_rate": 1.7674013010473e-05,
      "loss": 2.3817,
      "step": 118650
    },
    {
      "epoch": 2.44106264416438,
      "grad_norm": 0.47165897488594055,
      "learning_rate": 1.7661407701613307e-05,
      "loss": 2.3876,
      "step": 118660
    },
    {
      "epoch": 2.4412683636812478,
      "grad_norm": 0.4842449426651001,
      "learning_rate": 1.7648806454067657e-05,
      "loss": 2.368,
      "step": 118670
    },
    {
      "epoch": 2.4414740831981154,
      "grad_norm": 0.5089910626411438,
      "learning_rate": 1.7636209268457603e-05,
      "loss": 2.3995,
      "step": 118680
    },
    {
      "epoch": 2.441679802714983,
      "grad_norm": 0.46744412183761597,
      "learning_rate": 1.7623616145404565e-05,
      "loss": 2.3518,
      "step": 118690
    },
    {
      "epoch": 2.441885522231851,
      "grad_norm": 0.4979032278060913,
      "learning_rate": 1.7611027085529618e-05,
      "loss": 2.388,
      "step": 118700
    },
    {
      "epoch": 2.442091241748719,
      "grad_norm": 0.8161389827728271,
      "learning_rate": 1.7598442089453715e-05,
      "loss": 2.4056,
      "step": 118710
    },
    {
      "epoch": 2.4422969612655865,
      "grad_norm": 0.49862533807754517,
      "learning_rate": 1.7585861157797678e-05,
      "loss": 2.3154,
      "step": 118720
    },
    {
      "epoch": 2.442502680782454,
      "grad_norm": 0.46992573142051697,
      "learning_rate": 1.7573284291181978e-05,
      "loss": 2.3569,
      "step": 118730
    },
    {
      "epoch": 2.442708400299322,
      "grad_norm": 0.4475109279155731,
      "learning_rate": 1.7560711490227e-05,
      "loss": 2.3695,
      "step": 118740
    },
    {
      "epoch": 2.4429141198161894,
      "grad_norm": 0.5198569297790527,
      "learning_rate": 1.7548142755552878e-05,
      "loss": 2.4089,
      "step": 118750
    },
    {
      "epoch": 2.4431198393330575,
      "grad_norm": 0.45372965931892395,
      "learning_rate": 1.753557808777957e-05,
      "loss": 2.3267,
      "step": 118760
    },
    {
      "epoch": 2.443325558849925,
      "grad_norm": 0.47709932923316956,
      "learning_rate": 1.7523017487526816e-05,
      "loss": 2.3647,
      "step": 118770
    },
    {
      "epoch": 2.443531278366793,
      "grad_norm": 0.4524867832660675,
      "learning_rate": 1.7510460955414165e-05,
      "loss": 2.3839,
      "step": 118780
    },
    {
      "epoch": 2.4437369978836605,
      "grad_norm": 0.47457602620124817,
      "learning_rate": 1.7497908492060954e-05,
      "loss": 2.3666,
      "step": 118790
    },
    {
      "epoch": 2.443942717400528,
      "grad_norm": 0.4520626962184906,
      "learning_rate": 1.7485360098086345e-05,
      "loss": 2.3685,
      "step": 118800
    },
    {
      "epoch": 2.444148436917396,
      "grad_norm": 0.4411678910255432,
      "learning_rate": 1.7472815774109263e-05,
      "loss": 2.4124,
      "step": 118810
    },
    {
      "epoch": 2.4443541564342635,
      "grad_norm": 0.4679115414619446,
      "learning_rate": 1.746027552074848e-05,
      "loss": 2.3616,
      "step": 118820
    },
    {
      "epoch": 2.444559875951131,
      "grad_norm": 0.4853520095348358,
      "learning_rate": 1.744773933862248e-05,
      "loss": 2.386,
      "step": 118830
    },
    {
      "epoch": 2.444765595467999,
      "grad_norm": 0.4895145297050476,
      "learning_rate": 1.743520722834967e-05,
      "loss": 2.4168,
      "step": 118840
    },
    {
      "epoch": 2.444971314984867,
      "grad_norm": 0.4750576615333557,
      "learning_rate": 1.7422679190548153e-05,
      "loss": 2.3788,
      "step": 118850
    },
    {
      "epoch": 2.4451770345017345,
      "grad_norm": 0.43432706594467163,
      "learning_rate": 1.741015522583588e-05,
      "loss": 2.3099,
      "step": 118860
    },
    {
      "epoch": 2.445382754018602,
      "grad_norm": 0.4538324773311615,
      "learning_rate": 1.7397635334830596e-05,
      "loss": 2.3696,
      "step": 118870
    },
    {
      "epoch": 2.44558847353547,
      "grad_norm": 0.47904908657073975,
      "learning_rate": 1.738511951814983e-05,
      "loss": 2.3455,
      "step": 118880
    },
    {
      "epoch": 2.4457941930523375,
      "grad_norm": 0.4550684094429016,
      "learning_rate": 1.737260777641091e-05,
      "loss": 2.3474,
      "step": 118890
    },
    {
      "epoch": 2.4459999125692056,
      "grad_norm": 0.48018231987953186,
      "learning_rate": 1.7360100110231025e-05,
      "loss": 2.3585,
      "step": 118900
    },
    {
      "epoch": 2.4462056320860732,
      "grad_norm": 0.5276297926902771,
      "learning_rate": 1.7347596520227005e-05,
      "loss": 2.3745,
      "step": 118910
    },
    {
      "epoch": 2.446411351602941,
      "grad_norm": 0.48746946454048157,
      "learning_rate": 1.7335097007015677e-05,
      "loss": 2.376,
      "step": 118920
    },
    {
      "epoch": 2.4466170711198085,
      "grad_norm": 0.4418182075023651,
      "learning_rate": 1.732260157121357e-05,
      "loss": 2.3826,
      "step": 118930
    },
    {
      "epoch": 2.446822790636676,
      "grad_norm": 0.5002593994140625,
      "learning_rate": 1.7310110213436937e-05,
      "loss": 2.3592,
      "step": 118940
    },
    {
      "epoch": 2.447028510153544,
      "grad_norm": 0.45709431171417236,
      "learning_rate": 1.7297622934301995e-05,
      "loss": 2.3427,
      "step": 118950
    },
    {
      "epoch": 2.4472342296704115,
      "grad_norm": 0.4451645016670227,
      "learning_rate": 1.728513973442465e-05,
      "loss": 2.3195,
      "step": 118960
    },
    {
      "epoch": 2.447439949187279,
      "grad_norm": 0.44890734553337097,
      "learning_rate": 1.727266061442058e-05,
      "loss": 2.4031,
      "step": 118970
    },
    {
      "epoch": 2.4476456687041472,
      "grad_norm": 0.4852641522884369,
      "learning_rate": 1.7260185574905395e-05,
      "loss": 2.3753,
      "step": 118980
    },
    {
      "epoch": 2.447851388221015,
      "grad_norm": 0.4710506200790405,
      "learning_rate": 1.724771461649436e-05,
      "loss": 2.3464,
      "step": 118990
    },
    {
      "epoch": 2.4480571077378825,
      "grad_norm": 0.4514932632446289,
      "learning_rate": 1.7235247739802597e-05,
      "loss": 2.382,
      "step": 119000
    },
    {
      "epoch": 2.44826282725475,
      "grad_norm": 0.4299815893173218,
      "learning_rate": 1.722278494544509e-05,
      "loss": 2.3834,
      "step": 119010
    },
    {
      "epoch": 2.448468546771618,
      "grad_norm": 0.45239245891571045,
      "learning_rate": 1.721032623403649e-05,
      "loss": 2.3392,
      "step": 119020
    },
    {
      "epoch": 2.4486742662884855,
      "grad_norm": 0.45710504055023193,
      "learning_rate": 1.719787160619133e-05,
      "loss": 2.3465,
      "step": 119030
    },
    {
      "epoch": 2.448879985805353,
      "grad_norm": 0.6962902545928955,
      "learning_rate": 1.7185421062524e-05,
      "loss": 2.3571,
      "step": 119040
    },
    {
      "epoch": 2.4490857053222213,
      "grad_norm": 0.48192083835601807,
      "learning_rate": 1.7172974603648528e-05,
      "loss": 2.3678,
      "step": 119050
    },
    {
      "epoch": 2.449291424839089,
      "grad_norm": 0.42484307289123535,
      "learning_rate": 1.7160532230178873e-05,
      "loss": 2.4012,
      "step": 119060
    },
    {
      "epoch": 2.4494971443559566,
      "grad_norm": 0.4660187065601349,
      "learning_rate": 1.7148093942728738e-05,
      "loss": 2.3842,
      "step": 119070
    },
    {
      "epoch": 2.449702863872824,
      "grad_norm": 0.4531363248825073,
      "learning_rate": 1.7135659741911646e-05,
      "loss": 2.3554,
      "step": 119080
    },
    {
      "epoch": 2.449908583389692,
      "grad_norm": 0.46794578433036804,
      "learning_rate": 1.7123229628340897e-05,
      "loss": 2.3222,
      "step": 119090
    },
    {
      "epoch": 2.4501143029065595,
      "grad_norm": 0.43153318762779236,
      "learning_rate": 1.711080360262961e-05,
      "loss": 2.3604,
      "step": 119100
    },
    {
      "epoch": 2.450320022423427,
      "grad_norm": 0.5167234539985657,
      "learning_rate": 1.7098381665390696e-05,
      "loss": 2.3508,
      "step": 119110
    },
    {
      "epoch": 2.450525741940295,
      "grad_norm": 0.4658120274543762,
      "learning_rate": 1.708596381723684e-05,
      "loss": 2.3259,
      "step": 119120
    },
    {
      "epoch": 2.450731461457163,
      "grad_norm": 0.45529815554618835,
      "learning_rate": 1.707355005878056e-05,
      "loss": 2.3549,
      "step": 119130
    },
    {
      "epoch": 2.4509371809740306,
      "grad_norm": 0.4224773943424225,
      "learning_rate": 1.7061140390634202e-05,
      "loss": 2.3421,
      "step": 119140
    },
    {
      "epoch": 2.4511429004908982,
      "grad_norm": 0.43629202246665955,
      "learning_rate": 1.7048734813409762e-05,
      "loss": 2.3697,
      "step": 119150
    },
    {
      "epoch": 2.451348620007766,
      "grad_norm": 0.4850514233112335,
      "learning_rate": 1.7036333327719234e-05,
      "loss": 2.3995,
      "step": 119160
    },
    {
      "epoch": 2.4515543395246335,
      "grad_norm": 0.46952924132347107,
      "learning_rate": 1.702393593417432e-05,
      "loss": 2.383,
      "step": 119170
    },
    {
      "epoch": 2.451760059041501,
      "grad_norm": 0.48288387060165405,
      "learning_rate": 1.7011542633386413e-05,
      "loss": 2.3621,
      "step": 119180
    },
    {
      "epoch": 2.4519657785583693,
      "grad_norm": 0.4697633981704712,
      "learning_rate": 1.6999153425966906e-05,
      "loss": 2.3637,
      "step": 119190
    },
    {
      "epoch": 2.452171498075237,
      "grad_norm": 0.448709636926651,
      "learning_rate": 1.698676831252689e-05,
      "loss": 2.415,
      "step": 119200
    },
    {
      "epoch": 2.4523772175921046,
      "grad_norm": 0.44525909423828125,
      "learning_rate": 1.6974387293677174e-05,
      "loss": 2.4093,
      "step": 119210
    },
    {
      "epoch": 2.4525829371089722,
      "grad_norm": 0.45035603642463684,
      "learning_rate": 1.6962010370028547e-05,
      "loss": 2.3873,
      "step": 119220
    },
    {
      "epoch": 2.45278865662584,
      "grad_norm": 0.6139146089553833,
      "learning_rate": 1.69496375421914e-05,
      "loss": 2.2938,
      "step": 119230
    },
    {
      "epoch": 2.4529943761427075,
      "grad_norm": 0.4043203294277191,
      "learning_rate": 1.6937268810776075e-05,
      "loss": 2.3688,
      "step": 119240
    },
    {
      "epoch": 2.453200095659575,
      "grad_norm": 0.45563212037086487,
      "learning_rate": 1.6924904176392675e-05,
      "loss": 2.3879,
      "step": 119250
    },
    {
      "epoch": 2.453405815176443,
      "grad_norm": 0.44774866104125977,
      "learning_rate": 1.6912543639651003e-05,
      "loss": 2.3394,
      "step": 119260
    },
    {
      "epoch": 2.453611534693311,
      "grad_norm": 0.4455356001853943,
      "learning_rate": 1.69001872011608e-05,
      "loss": 2.358,
      "step": 119270
    },
    {
      "epoch": 2.4538172542101786,
      "grad_norm": 0.43351462483406067,
      "learning_rate": 1.6887834861531537e-05,
      "loss": 2.364,
      "step": 119280
    },
    {
      "epoch": 2.4540229737270463,
      "grad_norm": 0.4540936052799225,
      "learning_rate": 1.687548662137244e-05,
      "loss": 2.3756,
      "step": 119290
    },
    {
      "epoch": 2.454228693243914,
      "grad_norm": 0.4704791307449341,
      "learning_rate": 1.6863142481292658e-05,
      "loss": 2.3943,
      "step": 119300
    },
    {
      "epoch": 2.4544344127607816,
      "grad_norm": 0.4659383296966553,
      "learning_rate": 1.6850802441900992e-05,
      "loss": 2.4325,
      "step": 119310
    },
    {
      "epoch": 2.454640132277649,
      "grad_norm": 0.487911581993103,
      "learning_rate": 1.6838466503806105e-05,
      "loss": 2.3952,
      "step": 119320
    },
    {
      "epoch": 2.4548458517945173,
      "grad_norm": 0.4272407293319702,
      "learning_rate": 1.6826134667616533e-05,
      "loss": 2.4055,
      "step": 119330
    },
    {
      "epoch": 2.455051571311385,
      "grad_norm": 0.4606637954711914,
      "learning_rate": 1.681380693394048e-05,
      "loss": 2.3823,
      "step": 119340
    },
    {
      "epoch": 2.4552572908282526,
      "grad_norm": 0.5218238830566406,
      "learning_rate": 1.680148330338599e-05,
      "loss": 2.2977,
      "step": 119350
    },
    {
      "epoch": 2.4554630103451203,
      "grad_norm": 0.8702858090400696,
      "learning_rate": 1.6789163776561013e-05,
      "loss": 2.4,
      "step": 119360
    },
    {
      "epoch": 2.455668729861988,
      "grad_norm": 0.47523829340934753,
      "learning_rate": 1.6776848354073104e-05,
      "loss": 2.361,
      "step": 119370
    },
    {
      "epoch": 2.4558744493788556,
      "grad_norm": 0.4869484007358551,
      "learning_rate": 1.676453703652976e-05,
      "loss": 2.3695,
      "step": 119380
    },
    {
      "epoch": 2.4560801688957232,
      "grad_norm": 0.4558899700641632,
      "learning_rate": 1.6752229824538223e-05,
      "loss": 2.3724,
      "step": 119390
    },
    {
      "epoch": 2.456285888412591,
      "grad_norm": 0.5519571900367737,
      "learning_rate": 1.6739926718705535e-05,
      "loss": 2.3713,
      "step": 119400
    },
    {
      "epoch": 2.4564916079294585,
      "grad_norm": 0.48131677508354187,
      "learning_rate": 1.672762771963856e-05,
      "loss": 2.3942,
      "step": 119410
    },
    {
      "epoch": 2.4566973274463266,
      "grad_norm": 0.4610005021095276,
      "learning_rate": 1.671533282794393e-05,
      "loss": 2.3617,
      "step": 119420
    },
    {
      "epoch": 2.4569030469631943,
      "grad_norm": 0.4310823678970337,
      "learning_rate": 1.670304204422808e-05,
      "loss": 2.3943,
      "step": 119430
    },
    {
      "epoch": 2.457108766480062,
      "grad_norm": 0.44726112484931946,
      "learning_rate": 1.6690755369097242e-05,
      "loss": 2.3964,
      "step": 119440
    },
    {
      "epoch": 2.4573144859969296,
      "grad_norm": 0.4648490250110626,
      "learning_rate": 1.6678472803157474e-05,
      "loss": 2.2994,
      "step": 119450
    },
    {
      "epoch": 2.4575202055137972,
      "grad_norm": 0.43408894538879395,
      "learning_rate": 1.666619434701462e-05,
      "loss": 2.317,
      "step": 119460
    },
    {
      "epoch": 2.457725925030665,
      "grad_norm": 0.4624204933643341,
      "learning_rate": 1.6653920001274237e-05,
      "loss": 2.3658,
      "step": 119470
    },
    {
      "epoch": 2.457931644547533,
      "grad_norm": 0.47569671273231506,
      "learning_rate": 1.6641649766541813e-05,
      "loss": 2.3889,
      "step": 119480
    },
    {
      "epoch": 2.4581373640644006,
      "grad_norm": 0.46176770329475403,
      "learning_rate": 1.6629383643422592e-05,
      "loss": 2.3414,
      "step": 119490
    },
    {
      "epoch": 2.4583430835812683,
      "grad_norm": 0.4764411449432373,
      "learning_rate": 1.6617121632521502e-05,
      "loss": 2.415,
      "step": 119500
    },
    {
      "epoch": 2.458548803098136,
      "grad_norm": 0.4410575032234192,
      "learning_rate": 1.6604863734443444e-05,
      "loss": 2.3697,
      "step": 119510
    },
    {
      "epoch": 2.4587545226150036,
      "grad_norm": 0.5046290159225464,
      "learning_rate": 1.659260994979305e-05,
      "loss": 2.3016,
      "step": 119520
    },
    {
      "epoch": 2.4589602421318713,
      "grad_norm": 0.4600771963596344,
      "learning_rate": 1.658036027917462e-05,
      "loss": 2.345,
      "step": 119530
    },
    {
      "epoch": 2.459165961648739,
      "grad_norm": 0.44425833225250244,
      "learning_rate": 1.6568114723192495e-05,
      "loss": 2.3949,
      "step": 119540
    },
    {
      "epoch": 2.4593716811656066,
      "grad_norm": 0.47143247723579407,
      "learning_rate": 1.6555873282450595e-05,
      "loss": 2.355,
      "step": 119550
    },
    {
      "epoch": 2.4595774006824747,
      "grad_norm": 0.5356699824333191,
      "learning_rate": 1.6543635957552726e-05,
      "loss": 2.3486,
      "step": 119560
    },
    {
      "epoch": 2.4597831201993423,
      "grad_norm": 0.47323083877563477,
      "learning_rate": 1.6531402749102563e-05,
      "loss": 2.3258,
      "step": 119570
    },
    {
      "epoch": 2.45998883971621,
      "grad_norm": 0.4455546438694,
      "learning_rate": 1.6519173657703403e-05,
      "loss": 2.4201,
      "step": 119580
    },
    {
      "epoch": 2.4601945592330776,
      "grad_norm": 0.5040024518966675,
      "learning_rate": 1.6506948683958523e-05,
      "loss": 2.3585,
      "step": 119590
    },
    {
      "epoch": 2.4604002787499453,
      "grad_norm": 0.465596467256546,
      "learning_rate": 1.649472782847089e-05,
      "loss": 2.4033,
      "step": 119600
    },
    {
      "epoch": 2.460605998266813,
      "grad_norm": 0.4665621221065521,
      "learning_rate": 1.6482511091843244e-05,
      "loss": 2.3824,
      "step": 119610
    },
    {
      "epoch": 2.460811717783681,
      "grad_norm": 0.4830234944820404,
      "learning_rate": 1.6470298474678268e-05,
      "loss": 2.3861,
      "step": 119620
    },
    {
      "epoch": 2.4610174373005487,
      "grad_norm": 0.535685122013092,
      "learning_rate": 1.6458089977578252e-05,
      "loss": 2.3935,
      "step": 119630
    },
    {
      "epoch": 2.4612231568174163,
      "grad_norm": 0.45602405071258545,
      "learning_rate": 1.6445885601145385e-05,
      "loss": 2.4274,
      "step": 119640
    },
    {
      "epoch": 2.461428876334284,
      "grad_norm": 0.4538393020629883,
      "learning_rate": 1.6433685345981707e-05,
      "loss": 2.3906,
      "step": 119650
    },
    {
      "epoch": 2.4616345958511516,
      "grad_norm": 0.49930304288864136,
      "learning_rate": 1.6421489212688935e-05,
      "loss": 2.389,
      "step": 119660
    },
    {
      "epoch": 2.4618403153680193,
      "grad_norm": 0.45268574357032776,
      "learning_rate": 1.6409297201868623e-05,
      "loss": 2.3414,
      "step": 119670
    },
    {
      "epoch": 2.462046034884887,
      "grad_norm": 0.4625531733036041,
      "learning_rate": 1.6397109314122216e-05,
      "loss": 2.3075,
      "step": 119680
    },
    {
      "epoch": 2.4622517544017546,
      "grad_norm": 0.46307098865509033,
      "learning_rate": 1.63849255500508e-05,
      "loss": 2.376,
      "step": 119690
    },
    {
      "epoch": 2.4624574739186227,
      "grad_norm": 0.4507822096347809,
      "learning_rate": 1.637274591025536e-05,
      "loss": 2.3867,
      "step": 119700
    },
    {
      "epoch": 2.4626631934354903,
      "grad_norm": 0.514531672000885,
      "learning_rate": 1.6360570395336648e-05,
      "loss": 2.3452,
      "step": 119710
    },
    {
      "epoch": 2.462868912952358,
      "grad_norm": 0.4700368046760559,
      "learning_rate": 1.6348399005895222e-05,
      "loss": 2.3377,
      "step": 119720
    },
    {
      "epoch": 2.4630746324692256,
      "grad_norm": 0.49220630526542664,
      "learning_rate": 1.6336231742531426e-05,
      "loss": 2.3366,
      "step": 119730
    },
    {
      "epoch": 2.4632803519860933,
      "grad_norm": 0.4518611431121826,
      "learning_rate": 1.6324068605845412e-05,
      "loss": 2.3915,
      "step": 119740
    },
    {
      "epoch": 2.463486071502961,
      "grad_norm": 0.4901718199253082,
      "learning_rate": 1.6311909596437125e-05,
      "loss": 2.3451,
      "step": 119750
    },
    {
      "epoch": 2.4636917910198286,
      "grad_norm": 0.4141239523887634,
      "learning_rate": 1.6299754714906247e-05,
      "loss": 2.327,
      "step": 119760
    },
    {
      "epoch": 2.4638975105366967,
      "grad_norm": 0.4664149880409241,
      "learning_rate": 1.628760396185238e-05,
      "loss": 2.3613,
      "step": 119770
    },
    {
      "epoch": 2.4641032300535644,
      "grad_norm": 0.47580060362815857,
      "learning_rate": 1.6275457337874866e-05,
      "loss": 2.3363,
      "step": 119780
    },
    {
      "epoch": 2.464308949570432,
      "grad_norm": 0.4539470374584198,
      "learning_rate": 1.6263314843572742e-05,
      "loss": 2.368,
      "step": 119790
    },
    {
      "epoch": 2.4645146690872997,
      "grad_norm": 0.4953679144382477,
      "learning_rate": 1.6251176479545015e-05,
      "loss": 2.3263,
      "step": 119800
    },
    {
      "epoch": 2.4647203886041673,
      "grad_norm": 0.47974398732185364,
      "learning_rate": 1.6239042246390402e-05,
      "loss": 2.3705,
      "step": 119810
    },
    {
      "epoch": 2.464926108121035,
      "grad_norm": 0.45490145683288574,
      "learning_rate": 1.622691214470733e-05,
      "loss": 2.3484,
      "step": 119820
    },
    {
      "epoch": 2.4651318276379026,
      "grad_norm": 0.4575671851634979,
      "learning_rate": 1.621478617509423e-05,
      "loss": 2.3668,
      "step": 119830
    },
    {
      "epoch": 2.4653375471547703,
      "grad_norm": 0.5809119343757629,
      "learning_rate": 1.620266433814913e-05,
      "loss": 2.3588,
      "step": 119840
    },
    {
      "epoch": 2.4655432666716384,
      "grad_norm": 0.4756545126438141,
      "learning_rate": 1.6190546634469937e-05,
      "loss": 2.3513,
      "step": 119850
    },
    {
      "epoch": 2.465748986188506,
      "grad_norm": 0.5119239687919617,
      "learning_rate": 1.617843306465442e-05,
      "loss": 2.3716,
      "step": 119860
    },
    {
      "epoch": 2.4659547057053737,
      "grad_norm": 0.4213971197605133,
      "learning_rate": 1.6166323629299995e-05,
      "loss": 2.375,
      "step": 119870
    },
    {
      "epoch": 2.4661604252222413,
      "grad_norm": 0.4444810152053833,
      "learning_rate": 1.6154218329003966e-05,
      "loss": 2.3622,
      "step": 119880
    },
    {
      "epoch": 2.466366144739109,
      "grad_norm": 0.4220640957355499,
      "learning_rate": 1.614211716436349e-05,
      "loss": 2.3413,
      "step": 119890
    },
    {
      "epoch": 2.4665718642559766,
      "grad_norm": 0.48366013169288635,
      "learning_rate": 1.613002013597539e-05,
      "loss": 2.3922,
      "step": 119900
    },
    {
      "epoch": 2.4667775837728447,
      "grad_norm": 0.474143385887146,
      "learning_rate": 1.611792724443635e-05,
      "loss": 2.3653,
      "step": 119910
    },
    {
      "epoch": 2.4669833032897124,
      "grad_norm": 0.45466354489326477,
      "learning_rate": 1.610583849034285e-05,
      "loss": 2.3648,
      "step": 119920
    },
    {
      "epoch": 2.46718902280658,
      "grad_norm": 0.4653295576572418,
      "learning_rate": 1.6093753874291186e-05,
      "loss": 2.3835,
      "step": 119930
    },
    {
      "epoch": 2.4673947423234477,
      "grad_norm": 0.4251374900341034,
      "learning_rate": 1.6081673396877394e-05,
      "loss": 2.3838,
      "step": 119940
    },
    {
      "epoch": 2.4676004618403153,
      "grad_norm": 0.44684070348739624,
      "learning_rate": 1.6069597058697362e-05,
      "loss": 2.3418,
      "step": 119950
    },
    {
      "epoch": 2.467806181357183,
      "grad_norm": 0.47516462206840515,
      "learning_rate": 1.6057524860346705e-05,
      "loss": 2.361,
      "step": 119960
    },
    {
      "epoch": 2.4680119008740506,
      "grad_norm": 0.43374863266944885,
      "learning_rate": 1.6045456802420976e-05,
      "loss": 2.3607,
      "step": 119970
    },
    {
      "epoch": 2.4682176203909183,
      "grad_norm": 0.4571564197540283,
      "learning_rate": 1.603339288551534e-05,
      "loss": 2.391,
      "step": 119980
    },
    {
      "epoch": 2.4684233399077864,
      "grad_norm": 0.4565590023994446,
      "learning_rate": 1.6021333110224867e-05,
      "loss": 2.3879,
      "step": 119990
    },
    {
      "epoch": 2.468629059424654,
      "grad_norm": 0.48602649569511414,
      "learning_rate": 1.6009277477144403e-05,
      "loss": 2.3925,
      "step": 120000
    },
    {
      "epoch": 2.4688347789415217,
      "grad_norm": 0.48806098103523254,
      "learning_rate": 1.599722598686858e-05,
      "loss": 2.4003,
      "step": 120010
    },
    {
      "epoch": 2.4690404984583894,
      "grad_norm": 0.42001357674598694,
      "learning_rate": 1.5985178639991848e-05,
      "loss": 2.3527,
      "step": 120020
    },
    {
      "epoch": 2.469246217975257,
      "grad_norm": 0.47757408022880554,
      "learning_rate": 1.5973135437108432e-05,
      "loss": 2.2835,
      "step": 120030
    },
    {
      "epoch": 2.4694519374921247,
      "grad_norm": 0.4370655119419098,
      "learning_rate": 1.5961096378812346e-05,
      "loss": 2.4005,
      "step": 120040
    },
    {
      "epoch": 2.4696576570089928,
      "grad_norm": 0.43730512261390686,
      "learning_rate": 1.5949061465697413e-05,
      "loss": 2.3509,
      "step": 120050
    },
    {
      "epoch": 2.4698633765258604,
      "grad_norm": 0.4921916723251343,
      "learning_rate": 1.5937030698357268e-05,
      "loss": 2.3377,
      "step": 120060
    },
    {
      "epoch": 2.470069096042728,
      "grad_norm": 0.43303465843200684,
      "learning_rate": 1.592500407738533e-05,
      "loss": 2.3423,
      "step": 120070
    },
    {
      "epoch": 2.4702748155595957,
      "grad_norm": 0.4774499237537384,
      "learning_rate": 1.591298160337473e-05,
      "loss": 2.3829,
      "step": 120080
    },
    {
      "epoch": 2.4704805350764634,
      "grad_norm": 0.45033547282218933,
      "learning_rate": 1.5900963276918558e-05,
      "loss": 2.3558,
      "step": 120090
    },
    {
      "epoch": 2.470686254593331,
      "grad_norm": 0.4781058430671692,
      "learning_rate": 1.5888949098609608e-05,
      "loss": 2.36,
      "step": 120100
    },
    {
      "epoch": 2.4708919741101987,
      "grad_norm": 0.43570980429649353,
      "learning_rate": 1.587693906904041e-05,
      "loss": 2.354,
      "step": 120110
    },
    {
      "epoch": 2.4710976936270663,
      "grad_norm": 0.5282850861549377,
      "learning_rate": 1.5864933188803398e-05,
      "loss": 2.3705,
      "step": 120120
    },
    {
      "epoch": 2.4713034131439344,
      "grad_norm": 0.44626182317733765,
      "learning_rate": 1.5852931458490783e-05,
      "loss": 2.3193,
      "step": 120130
    },
    {
      "epoch": 2.471509132660802,
      "grad_norm": 0.5258769392967224,
      "learning_rate": 1.5840933878694463e-05,
      "loss": 2.4286,
      "step": 120140
    },
    {
      "epoch": 2.4717148521776697,
      "grad_norm": 0.4463919997215271,
      "learning_rate": 1.5828940450006314e-05,
      "loss": 2.3764,
      "step": 120150
    },
    {
      "epoch": 2.4719205716945374,
      "grad_norm": 0.4530789852142334,
      "learning_rate": 1.5816951173017837e-05,
      "loss": 2.2884,
      "step": 120160
    },
    {
      "epoch": 2.472126291211405,
      "grad_norm": 0.46153831481933594,
      "learning_rate": 1.580496604832038e-05,
      "loss": 2.3872,
      "step": 120170
    },
    {
      "epoch": 2.4723320107282727,
      "grad_norm": 0.4448438584804535,
      "learning_rate": 1.5792985076505197e-05,
      "loss": 2.3964,
      "step": 120180
    },
    {
      "epoch": 2.4725377302451403,
      "grad_norm": 0.460616797208786,
      "learning_rate": 1.5781008258163156e-05,
      "loss": 2.3881,
      "step": 120190
    },
    {
      "epoch": 2.4727434497620084,
      "grad_norm": 0.4692266881465912,
      "learning_rate": 1.5769035593885018e-05,
      "loss": 2.3647,
      "step": 120200
    },
    {
      "epoch": 2.472949169278876,
      "grad_norm": 0.45464858412742615,
      "learning_rate": 1.57570670842614e-05,
      "loss": 2.351,
      "step": 120210
    },
    {
      "epoch": 2.4731548887957437,
      "grad_norm": 0.42210668325424194,
      "learning_rate": 1.5745102729882566e-05,
      "loss": 2.363,
      "step": 120220
    },
    {
      "epoch": 2.4733606083126114,
      "grad_norm": 0.4511195123195648,
      "learning_rate": 1.5733142531338684e-05,
      "loss": 2.3436,
      "step": 120230
    },
    {
      "epoch": 2.473566327829479,
      "grad_norm": 0.512816846370697,
      "learning_rate": 1.5721186489219687e-05,
      "loss": 2.3666,
      "step": 120240
    },
    {
      "epoch": 2.4737720473463467,
      "grad_norm": 0.46499642729759216,
      "learning_rate": 1.5709234604115287e-05,
      "loss": 2.3332,
      "step": 120250
    },
    {
      "epoch": 2.4739777668632144,
      "grad_norm": 0.47499001026153564,
      "learning_rate": 1.5697286876615015e-05,
      "loss": 2.3813,
      "step": 120260
    },
    {
      "epoch": 2.474183486380082,
      "grad_norm": 0.43999800086021423,
      "learning_rate": 1.5685343307308186e-05,
      "loss": 2.4094,
      "step": 120270
    },
    {
      "epoch": 2.47438920589695,
      "grad_norm": 0.5069276094436646,
      "learning_rate": 1.567340389678391e-05,
      "loss": 2.4101,
      "step": 120280
    },
    {
      "epoch": 2.4745949254138178,
      "grad_norm": 0.4778929650783539,
      "learning_rate": 1.5661468645631106e-05,
      "loss": 2.3035,
      "step": 120290
    },
    {
      "epoch": 2.4748006449306854,
      "grad_norm": 0.4400317072868347,
      "learning_rate": 1.5649537554438454e-05,
      "loss": 2.3427,
      "step": 120300
    },
    {
      "epoch": 2.475006364447553,
      "grad_norm": 0.4480052888393402,
      "learning_rate": 1.5637610623794464e-05,
      "loss": 2.3334,
      "step": 120310
    },
    {
      "epoch": 2.4752120839644207,
      "grad_norm": 0.44620150327682495,
      "learning_rate": 1.5625687854287418e-05,
      "loss": 2.332,
      "step": 120320
    },
    {
      "epoch": 2.4754178034812884,
      "grad_norm": 0.4633603096008301,
      "learning_rate": 1.5613769246505417e-05,
      "loss": 2.3843,
      "step": 120330
    },
    {
      "epoch": 2.4756235229981565,
      "grad_norm": 0.4755334258079529,
      "learning_rate": 1.560185480103632e-05,
      "loss": 2.3039,
      "step": 120340
    },
    {
      "epoch": 2.475829242515024,
      "grad_norm": 0.433100163936615,
      "learning_rate": 1.55899445184678e-05,
      "loss": 2.3565,
      "step": 120350
    },
    {
      "epoch": 2.4760349620318918,
      "grad_norm": 0.4928017258644104,
      "learning_rate": 1.557803839938735e-05,
      "loss": 2.4345,
      "step": 120360
    },
    {
      "epoch": 2.4762406815487594,
      "grad_norm": 0.49838656187057495,
      "learning_rate": 1.5566136444382227e-05,
      "loss": 2.3959,
      "step": 120370
    },
    {
      "epoch": 2.476446401065627,
      "grad_norm": 0.4730953872203827,
      "learning_rate": 1.5554238654039477e-05,
      "loss": 2.3955,
      "step": 120380
    },
    {
      "epoch": 2.4766521205824947,
      "grad_norm": 0.47209790349006653,
      "learning_rate": 1.554234502894598e-05,
      "loss": 2.4026,
      "step": 120390
    },
    {
      "epoch": 2.4768578400993624,
      "grad_norm": 0.7152694463729858,
      "learning_rate": 1.553045556968832e-05,
      "loss": 2.4299,
      "step": 120400
    },
    {
      "epoch": 2.47706355961623,
      "grad_norm": 0.4333048462867737,
      "learning_rate": 1.5518570276853006e-05,
      "loss": 2.3773,
      "step": 120410
    },
    {
      "epoch": 2.477269279133098,
      "grad_norm": 0.4550331234931946,
      "learning_rate": 1.5506689151026278e-05,
      "loss": 2.3531,
      "step": 120420
    },
    {
      "epoch": 2.477474998649966,
      "grad_norm": 0.4642952084541321,
      "learning_rate": 1.5494812192794083e-05,
      "loss": 2.3906,
      "step": 120430
    },
    {
      "epoch": 2.4776807181668334,
      "grad_norm": 0.4754270613193512,
      "learning_rate": 1.5482939402742347e-05,
      "loss": 2.3298,
      "step": 120440
    },
    {
      "epoch": 2.477886437683701,
      "grad_norm": 0.4768267571926117,
      "learning_rate": 1.5471070781456652e-05,
      "loss": 2.4056,
      "step": 120450
    },
    {
      "epoch": 2.4780921572005687,
      "grad_norm": 0.43542519211769104,
      "learning_rate": 1.545920632952237e-05,
      "loss": 2.38,
      "step": 120460
    },
    {
      "epoch": 2.4782978767174364,
      "grad_norm": 0.4342304468154907,
      "learning_rate": 1.5447346047524792e-05,
      "loss": 2.3538,
      "step": 120470
    },
    {
      "epoch": 2.478503596234304,
      "grad_norm": 0.4733204245567322,
      "learning_rate": 1.5435489936048864e-05,
      "loss": 2.3321,
      "step": 120480
    },
    {
      "epoch": 2.478709315751172,
      "grad_norm": 0.5214249491691589,
      "learning_rate": 1.5423637995679363e-05,
      "loss": 2.3904,
      "step": 120490
    },
    {
      "epoch": 2.47891503526804,
      "grad_norm": 0.507008969783783,
      "learning_rate": 1.5411790227000954e-05,
      "loss": 2.3776,
      "step": 120500
    },
    {
      "epoch": 2.4791207547849075,
      "grad_norm": 0.4503067135810852,
      "learning_rate": 1.539994663059797e-05,
      "loss": 2.3901,
      "step": 120510
    },
    {
      "epoch": 2.479326474301775,
      "grad_norm": 0.4529589116573334,
      "learning_rate": 1.538810720705458e-05,
      "loss": 2.3462,
      "step": 120520
    },
    {
      "epoch": 2.4795321938186428,
      "grad_norm": 0.5112876296043396,
      "learning_rate": 1.5376271956954823e-05,
      "loss": 2.4157,
      "step": 120530
    },
    {
      "epoch": 2.4797379133355104,
      "grad_norm": 0.4585523009300232,
      "learning_rate": 1.5364440880882404e-05,
      "loss": 2.3607,
      "step": 120540
    },
    {
      "epoch": 2.479943632852378,
      "grad_norm": 0.458274245262146,
      "learning_rate": 1.5352613979420905e-05,
      "loss": 2.3354,
      "step": 120550
    },
    {
      "epoch": 2.4801493523692457,
      "grad_norm": 0.4725438058376312,
      "learning_rate": 1.53407912531537e-05,
      "loss": 2.392,
      "step": 120560
    },
    {
      "epoch": 2.480355071886114,
      "grad_norm": 0.44340020418167114,
      "learning_rate": 1.532897270266391e-05,
      "loss": 2.4019,
      "step": 120570
    },
    {
      "epoch": 2.4805607914029815,
      "grad_norm": 0.4793086647987366,
      "learning_rate": 1.5317158328534497e-05,
      "loss": 2.4457,
      "step": 120580
    },
    {
      "epoch": 2.480766510919849,
      "grad_norm": 0.4300512373447418,
      "learning_rate": 1.530534813134821e-05,
      "loss": 2.3585,
      "step": 120590
    },
    {
      "epoch": 2.4809722304367168,
      "grad_norm": 0.46436458826065063,
      "learning_rate": 1.5293542111687552e-05,
      "loss": 2.3792,
      "step": 120600
    },
    {
      "epoch": 2.4811779499535844,
      "grad_norm": 0.42852455377578735,
      "learning_rate": 1.528174027013487e-05,
      "loss": 2.3299,
      "step": 120610
    },
    {
      "epoch": 2.481383669470452,
      "grad_norm": 0.4472208321094513,
      "learning_rate": 1.526994260727229e-05,
      "loss": 2.3892,
      "step": 120620
    },
    {
      "epoch": 2.48158938898732,
      "grad_norm": 0.45997223258018494,
      "learning_rate": 1.5258149123681709e-05,
      "loss": 2.3419,
      "step": 120630
    },
    {
      "epoch": 2.481795108504188,
      "grad_norm": 0.4350530803203583,
      "learning_rate": 1.5246359819944855e-05,
      "loss": 2.2644,
      "step": 120640
    },
    {
      "epoch": 2.4820008280210555,
      "grad_norm": 0.4515288174152374,
      "learning_rate": 1.5234574696643212e-05,
      "loss": 2.3909,
      "step": 120650
    },
    {
      "epoch": 2.482206547537923,
      "grad_norm": 0.49300992488861084,
      "learning_rate": 1.5222793754358078e-05,
      "loss": 2.3557,
      "step": 120660
    },
    {
      "epoch": 2.482412267054791,
      "grad_norm": 0.42313364148139954,
      "learning_rate": 1.5211016993670557e-05,
      "loss": 2.307,
      "step": 120670
    },
    {
      "epoch": 2.4826179865716584,
      "grad_norm": 0.4666394591331482,
      "learning_rate": 1.5199244415161517e-05,
      "loss": 2.3404,
      "step": 120680
    },
    {
      "epoch": 2.482823706088526,
      "grad_norm": 0.5439922213554382,
      "learning_rate": 1.518747601941165e-05,
      "loss": 2.3768,
      "step": 120690
    },
    {
      "epoch": 2.4830294256053937,
      "grad_norm": 0.4772302508354187,
      "learning_rate": 1.5175711807001414e-05,
      "loss": 2.3623,
      "step": 120700
    },
    {
      "epoch": 2.483235145122262,
      "grad_norm": 0.4539859890937805,
      "learning_rate": 1.5163951778511099e-05,
      "loss": 2.3748,
      "step": 120710
    },
    {
      "epoch": 2.4834408646391295,
      "grad_norm": 0.44746771454811096,
      "learning_rate": 1.5152195934520685e-05,
      "loss": 2.3343,
      "step": 120720
    },
    {
      "epoch": 2.483646584155997,
      "grad_norm": 0.5226244330406189,
      "learning_rate": 1.5140444275610122e-05,
      "loss": 2.2969,
      "step": 120730
    },
    {
      "epoch": 2.483852303672865,
      "grad_norm": 0.46081846952438354,
      "learning_rate": 1.5128696802359022e-05,
      "loss": 2.3507,
      "step": 120740
    },
    {
      "epoch": 2.4840580231897325,
      "grad_norm": 0.44585612416267395,
      "learning_rate": 1.5116953515346777e-05,
      "loss": 2.3703,
      "step": 120750
    },
    {
      "epoch": 2.4842637427066,
      "grad_norm": 0.457595556974411,
      "learning_rate": 1.510521441515268e-05,
      "loss": 2.4009,
      "step": 120760
    },
    {
      "epoch": 2.484469462223468,
      "grad_norm": 0.4985050559043884,
      "learning_rate": 1.5093479502355755e-05,
      "loss": 2.3563,
      "step": 120770
    },
    {
      "epoch": 2.484675181740336,
      "grad_norm": 0.5662168264389038,
      "learning_rate": 1.5081748777534766e-05,
      "loss": 2.3638,
      "step": 120780
    },
    {
      "epoch": 2.4848809012572035,
      "grad_norm": 0.4674089550971985,
      "learning_rate": 1.50700222412684e-05,
      "loss": 2.3631,
      "step": 120790
    },
    {
      "epoch": 2.485086620774071,
      "grad_norm": 0.46087944507598877,
      "learning_rate": 1.5058299894134998e-05,
      "loss": 2.3331,
      "step": 120800
    },
    {
      "epoch": 2.485292340290939,
      "grad_norm": 0.4389030337333679,
      "learning_rate": 1.5046581736712761e-05,
      "loss": 2.3798,
      "step": 120810
    },
    {
      "epoch": 2.4854980598078065,
      "grad_norm": 0.5120183825492859,
      "learning_rate": 1.5034867769579752e-05,
      "loss": 2.3547,
      "step": 120820
    },
    {
      "epoch": 2.485703779324674,
      "grad_norm": 0.4870474636554718,
      "learning_rate": 1.5023157993313697e-05,
      "loss": 2.3369,
      "step": 120830
    },
    {
      "epoch": 2.4859094988415418,
      "grad_norm": 0.4964906573295593,
      "learning_rate": 1.5011452408492166e-05,
      "loss": 2.3667,
      "step": 120840
    },
    {
      "epoch": 2.48611521835841,
      "grad_norm": 0.47784796357154846,
      "learning_rate": 1.4999751015692597e-05,
      "loss": 2.4244,
      "step": 120850
    },
    {
      "epoch": 2.4863209378752775,
      "grad_norm": 0.4692992568016052,
      "learning_rate": 1.4988053815492088e-05,
      "loss": 2.3426,
      "step": 120860
    },
    {
      "epoch": 2.486526657392145,
      "grad_norm": 0.4909796118736267,
      "learning_rate": 1.4976360808467627e-05,
      "loss": 2.3717,
      "step": 120870
    },
    {
      "epoch": 2.486732376909013,
      "grad_norm": 0.5399055480957031,
      "learning_rate": 1.4964671995195967e-05,
      "loss": 2.3547,
      "step": 120880
    },
    {
      "epoch": 2.4869380964258805,
      "grad_norm": 0.43532952666282654,
      "learning_rate": 1.4952987376253646e-05,
      "loss": 2.345,
      "step": 120890
    },
    {
      "epoch": 2.487143815942748,
      "grad_norm": 0.47999492287635803,
      "learning_rate": 1.4941306952217016e-05,
      "loss": 2.2996,
      "step": 120900
    },
    {
      "epoch": 2.487349535459616,
      "grad_norm": 0.4383837580680847,
      "learning_rate": 1.492963072366219e-05,
      "loss": 2.3982,
      "step": 120910
    },
    {
      "epoch": 2.487555254976484,
      "grad_norm": 0.44543352723121643,
      "learning_rate": 1.4917958691165112e-05,
      "loss": 2.3446,
      "step": 120920
    },
    {
      "epoch": 2.4877609744933515,
      "grad_norm": 0.44636020064353943,
      "learning_rate": 1.4906290855301486e-05,
      "loss": 2.3326,
      "step": 120930
    },
    {
      "epoch": 2.487966694010219,
      "grad_norm": 0.5332462787628174,
      "learning_rate": 1.4894627216646828e-05,
      "loss": 2.3606,
      "step": 120940
    },
    {
      "epoch": 2.488172413527087,
      "grad_norm": 0.5570500493049622,
      "learning_rate": 1.4882967775776435e-05,
      "loss": 2.3546,
      "step": 120950
    },
    {
      "epoch": 2.4883781330439545,
      "grad_norm": 0.48587435483932495,
      "learning_rate": 1.4871312533265413e-05,
      "loss": 2.3589,
      "step": 120960
    },
    {
      "epoch": 2.488583852560822,
      "grad_norm": 0.5104873776435852,
      "learning_rate": 1.4859661489688647e-05,
      "loss": 2.3191,
      "step": 120970
    },
    {
      "epoch": 2.48878957207769,
      "grad_norm": 0.43261492252349854,
      "learning_rate": 1.4848014645620822e-05,
      "loss": 2.3685,
      "step": 120980
    },
    {
      "epoch": 2.4889952915945575,
      "grad_norm": 0.4781744182109833,
      "learning_rate": 1.4836372001636411e-05,
      "loss": 2.3535,
      "step": 120990
    },
    {
      "epoch": 2.4892010111114256,
      "grad_norm": 0.4612908363342285,
      "learning_rate": 1.4824733558309711e-05,
      "loss": 2.3731,
      "step": 121000
    },
    {
      "epoch": 2.489406730628293,
      "grad_norm": 0.4786209166049957,
      "learning_rate": 1.4813099316214707e-05,
      "loss": 2.3337,
      "step": 121010
    },
    {
      "epoch": 2.489612450145161,
      "grad_norm": 0.480008602142334,
      "learning_rate": 1.4801469275925317e-05,
      "loss": 2.377,
      "step": 121020
    },
    {
      "epoch": 2.4898181696620285,
      "grad_norm": 0.4591602385044098,
      "learning_rate": 1.4789843438015194e-05,
      "loss": 2.3813,
      "step": 121030
    },
    {
      "epoch": 2.490023889178896,
      "grad_norm": 0.47601228952407837,
      "learning_rate": 1.4778221803057723e-05,
      "loss": 2.3725,
      "step": 121040
    },
    {
      "epoch": 2.490229608695764,
      "grad_norm": 0.45858779549598694,
      "learning_rate": 1.4766604371626181e-05,
      "loss": 2.351,
      "step": 121050
    },
    {
      "epoch": 2.490435328212632,
      "grad_norm": 0.452519029378891,
      "learning_rate": 1.4754991144293595e-05,
      "loss": 2.3562,
      "step": 121060
    },
    {
      "epoch": 2.4906410477294996,
      "grad_norm": 0.45447808504104614,
      "learning_rate": 1.4743382121632731e-05,
      "loss": 2.3914,
      "step": 121070
    },
    {
      "epoch": 2.490846767246367,
      "grad_norm": 0.45058783888816833,
      "learning_rate": 1.4731777304216276e-05,
      "loss": 2.2824,
      "step": 121080
    },
    {
      "epoch": 2.491052486763235,
      "grad_norm": 0.5048093199729919,
      "learning_rate": 1.4720176692616572e-05,
      "loss": 2.4237,
      "step": 121090
    },
    {
      "epoch": 2.4912582062801025,
      "grad_norm": 0.4589461386203766,
      "learning_rate": 1.4708580287405804e-05,
      "loss": 2.4178,
      "step": 121100
    },
    {
      "epoch": 2.49146392579697,
      "grad_norm": 0.47005727887153625,
      "learning_rate": 1.4696988089156027e-05,
      "loss": 2.3832,
      "step": 121110
    },
    {
      "epoch": 2.491669645313838,
      "grad_norm": 0.48768001794815063,
      "learning_rate": 1.4685400098438972e-05,
      "loss": 2.4113,
      "step": 121120
    },
    {
      "epoch": 2.4918753648307055,
      "grad_norm": 0.4645431637763977,
      "learning_rate": 1.4673816315826194e-05,
      "loss": 2.3211,
      "step": 121130
    },
    {
      "epoch": 2.4920810843475736,
      "grad_norm": 0.4695071578025818,
      "learning_rate": 1.4662236741889123e-05,
      "loss": 2.3952,
      "step": 121140
    },
    {
      "epoch": 2.4922868038644412,
      "grad_norm": 0.43583276867866516,
      "learning_rate": 1.465066137719887e-05,
      "loss": 2.4108,
      "step": 121150
    },
    {
      "epoch": 2.492492523381309,
      "grad_norm": 0.5175884366035461,
      "learning_rate": 1.463909022232638e-05,
      "loss": 2.3605,
      "step": 121160
    },
    {
      "epoch": 2.4926982428981765,
      "grad_norm": 0.4575228989124298,
      "learning_rate": 1.4627523277842414e-05,
      "loss": 2.3469,
      "step": 121170
    },
    {
      "epoch": 2.492903962415044,
      "grad_norm": 0.47008654475212097,
      "learning_rate": 1.4615960544317508e-05,
      "loss": 2.4206,
      "step": 121180
    },
    {
      "epoch": 2.493109681931912,
      "grad_norm": 0.48504000902175903,
      "learning_rate": 1.4604402022321972e-05,
      "loss": 2.3369,
      "step": 121190
    },
    {
      "epoch": 2.49331540144878,
      "grad_norm": 0.4155278205871582,
      "learning_rate": 1.459284771242594e-05,
      "loss": 2.3813,
      "step": 121200
    },
    {
      "epoch": 2.4935211209656476,
      "grad_norm": 0.444612979888916,
      "learning_rate": 1.4581297615199307e-05,
      "loss": 2.3718,
      "step": 121210
    },
    {
      "epoch": 2.4937268404825153,
      "grad_norm": 0.4598957896232605,
      "learning_rate": 1.456975173121179e-05,
      "loss": 2.4172,
      "step": 121220
    },
    {
      "epoch": 2.493932559999383,
      "grad_norm": 0.4894040822982788,
      "learning_rate": 1.4558210061032884e-05,
      "loss": 2.3641,
      "step": 121230
    },
    {
      "epoch": 2.4941382795162506,
      "grad_norm": 0.46054807305336,
      "learning_rate": 1.454667260523188e-05,
      "loss": 2.33,
      "step": 121240
    },
    {
      "epoch": 2.494343999033118,
      "grad_norm": 0.4661101996898651,
      "learning_rate": 1.4535139364377847e-05,
      "loss": 2.3649,
      "step": 121250
    },
    {
      "epoch": 2.494549718549986,
      "grad_norm": 0.46156755089759827,
      "learning_rate": 1.452361033903965e-05,
      "loss": 2.3843,
      "step": 121260
    },
    {
      "epoch": 2.4947554380668535,
      "grad_norm": 0.44537755846977234,
      "learning_rate": 1.451208552978598e-05,
      "loss": 2.3911,
      "step": 121270
    },
    {
      "epoch": 2.494961157583721,
      "grad_norm": 0.44901174306869507,
      "learning_rate": 1.4500564937185269e-05,
      "loss": 2.3988,
      "step": 121280
    },
    {
      "epoch": 2.4951668771005893,
      "grad_norm": 0.44552457332611084,
      "learning_rate": 1.4489048561805774e-05,
      "loss": 2.405,
      "step": 121290
    },
    {
      "epoch": 2.495372596617457,
      "grad_norm": 0.4476237893104553,
      "learning_rate": 1.4477536404215542e-05,
      "loss": 2.3385,
      "step": 121300
    },
    {
      "epoch": 2.4955783161343246,
      "grad_norm": 0.4158536493778229,
      "learning_rate": 1.4466028464982396e-05,
      "loss": 2.3251,
      "step": 121310
    },
    {
      "epoch": 2.4957840356511922,
      "grad_norm": 0.463588684797287,
      "learning_rate": 1.445452474467398e-05,
      "loss": 2.3647,
      "step": 121320
    },
    {
      "epoch": 2.49598975516806,
      "grad_norm": 0.445135235786438,
      "learning_rate": 1.4443025243857644e-05,
      "loss": 2.3916,
      "step": 121330
    },
    {
      "epoch": 2.4961954746849275,
      "grad_norm": 0.47662365436553955,
      "learning_rate": 1.4431529963100666e-05,
      "loss": 2.3904,
      "step": 121340
    },
    {
      "epoch": 2.4964011942017956,
      "grad_norm": 0.45362570881843567,
      "learning_rate": 1.4420038902970056e-05,
      "loss": 2.3791,
      "step": 121350
    },
    {
      "epoch": 2.4966069137186633,
      "grad_norm": 0.4417431354522705,
      "learning_rate": 1.440855206403251e-05,
      "loss": 2.3869,
      "step": 121360
    },
    {
      "epoch": 2.496812633235531,
      "grad_norm": 0.4619591534137726,
      "learning_rate": 1.4397069446854705e-05,
      "loss": 2.3462,
      "step": 121370
    },
    {
      "epoch": 2.4970183527523986,
      "grad_norm": 0.4540495276451111,
      "learning_rate": 1.438559105200301e-05,
      "loss": 2.3725,
      "step": 121380
    },
    {
      "epoch": 2.4972240722692662,
      "grad_norm": 0.4866037666797638,
      "learning_rate": 1.437411688004352e-05,
      "loss": 2.3871,
      "step": 121390
    },
    {
      "epoch": 2.497429791786134,
      "grad_norm": 0.5408475995063782,
      "learning_rate": 1.4362646931542278e-05,
      "loss": 2.3782,
      "step": 121400
    },
    {
      "epoch": 2.4976355113030015,
      "grad_norm": 0.4587545692920685,
      "learning_rate": 1.4351181207064978e-05,
      "loss": 2.3815,
      "step": 121410
    },
    {
      "epoch": 2.497841230819869,
      "grad_norm": 0.4134628474712372,
      "learning_rate": 1.4339719707177157e-05,
      "loss": 2.4128,
      "step": 121420
    },
    {
      "epoch": 2.4980469503367373,
      "grad_norm": 0.44719764590263367,
      "learning_rate": 1.4328262432444207e-05,
      "loss": 2.3956,
      "step": 121430
    },
    {
      "epoch": 2.498252669853605,
      "grad_norm": 0.49759939312934875,
      "learning_rate": 1.4316809383431206e-05,
      "loss": 2.3694,
      "step": 121440
    },
    {
      "epoch": 2.4984583893704726,
      "grad_norm": 0.45285817980766296,
      "learning_rate": 1.4305360560703052e-05,
      "loss": 2.4194,
      "step": 121450
    },
    {
      "epoch": 2.4986641088873403,
      "grad_norm": 0.45644310116767883,
      "learning_rate": 1.4293915964824522e-05,
      "loss": 2.3649,
      "step": 121460
    },
    {
      "epoch": 2.498869828404208,
      "grad_norm": 0.48487669229507446,
      "learning_rate": 1.4282475596360056e-05,
      "loss": 2.3817,
      "step": 121470
    },
    {
      "epoch": 2.4990755479210756,
      "grad_norm": 0.498044490814209,
      "learning_rate": 1.4271039455873968e-05,
      "loss": 2.3759,
      "step": 121480
    },
    {
      "epoch": 2.4992812674379437,
      "grad_norm": 0.437331885099411,
      "learning_rate": 1.425960754393033e-05,
      "loss": 2.3556,
      "step": 121490
    },
    {
      "epoch": 2.4994869869548113,
      "grad_norm": 0.4480432868003845,
      "learning_rate": 1.4248179861093036e-05,
      "loss": 2.3433,
      "step": 121500
    },
    {
      "epoch": 2.499692706471679,
      "grad_norm": 0.4898258149623871,
      "learning_rate": 1.4236756407925723e-05,
      "loss": 2.3441,
      "step": 121510
    },
    {
      "epoch": 2.4998984259885466,
      "grad_norm": 0.4535132050514221,
      "learning_rate": 1.422533718499187e-05,
      "loss": 2.336,
      "step": 121520
    },
    {
      "epoch": 2.5001041455054143,
      "grad_norm": 0.45170876383781433,
      "learning_rate": 1.4213922192854723e-05,
      "loss": 2.3495,
      "step": 121530
    },
    {
      "epoch": 2.500309865022282,
      "grad_norm": 0.4915030300617218,
      "learning_rate": 1.4202511432077304e-05,
      "loss": 2.3824,
      "step": 121540
    },
    {
      "epoch": 2.5005155845391496,
      "grad_norm": 0.4742446839809418,
      "learning_rate": 1.4191104903222463e-05,
      "loss": 2.3452,
      "step": 121550
    },
    {
      "epoch": 2.5007213040560172,
      "grad_norm": 0.42471370100975037,
      "learning_rate": 1.4179702606852818e-05,
      "loss": 2.3281,
      "step": 121560
    },
    {
      "epoch": 2.500927023572885,
      "grad_norm": 0.4658678472042084,
      "learning_rate": 1.4168304543530775e-05,
      "loss": 2.3409,
      "step": 121570
    },
    {
      "epoch": 2.501132743089753,
      "grad_norm": 0.439119428396225,
      "learning_rate": 1.4156910713818538e-05,
      "loss": 2.3199,
      "step": 121580
    },
    {
      "epoch": 2.5013384626066206,
      "grad_norm": 0.4451715052127838,
      "learning_rate": 1.4145521118278105e-05,
      "loss": 2.357,
      "step": 121590
    },
    {
      "epoch": 2.5015441821234883,
      "grad_norm": 0.4395448863506317,
      "learning_rate": 1.4134135757471278e-05,
      "loss": 2.3133,
      "step": 121600
    },
    {
      "epoch": 2.501749901640356,
      "grad_norm": 0.4423798620700836,
      "learning_rate": 1.4122754631959612e-05,
      "loss": 2.3433,
      "step": 121610
    },
    {
      "epoch": 2.5019556211572236,
      "grad_norm": 0.5307577848434448,
      "learning_rate": 1.4111377742304488e-05,
      "loss": 2.315,
      "step": 121620
    },
    {
      "epoch": 2.5021613406740917,
      "grad_norm": 0.4997197985649109,
      "learning_rate": 1.4100005089067059e-05,
      "loss": 2.4054,
      "step": 121630
    },
    {
      "epoch": 2.5023670601909593,
      "grad_norm": 0.6196839809417725,
      "learning_rate": 1.4088636672808308e-05,
      "loss": 2.3941,
      "step": 121640
    },
    {
      "epoch": 2.502572779707827,
      "grad_norm": 0.4496268630027771,
      "learning_rate": 1.40772724940889e-05,
      "loss": 2.3287,
      "step": 121650
    },
    {
      "epoch": 2.5027784992246946,
      "grad_norm": 0.4486823081970215,
      "learning_rate": 1.406591255346944e-05,
      "loss": 2.377,
      "step": 121660
    },
    {
      "epoch": 2.5029842187415623,
      "grad_norm": 0.47417446970939636,
      "learning_rate": 1.4054556851510247e-05,
      "loss": 2.37,
      "step": 121670
    },
    {
      "epoch": 2.50318993825843,
      "grad_norm": 0.49765709042549133,
      "learning_rate": 1.4043205388771385e-05,
      "loss": 2.31,
      "step": 121680
    },
    {
      "epoch": 2.5033956577752976,
      "grad_norm": 0.477705717086792,
      "learning_rate": 1.4031858165812805e-05,
      "loss": 2.3521,
      "step": 121690
    },
    {
      "epoch": 2.5036013772921653,
      "grad_norm": 0.4615841805934906,
      "learning_rate": 1.4020515183194216e-05,
      "loss": 2.3737,
      "step": 121700
    },
    {
      "epoch": 2.503807096809033,
      "grad_norm": 0.49931129813194275,
      "learning_rate": 1.4009176441475036e-05,
      "loss": 2.268,
      "step": 121710
    },
    {
      "epoch": 2.504012816325901,
      "grad_norm": 0.4744875133037567,
      "learning_rate": 1.399784194121464e-05,
      "loss": 2.3702,
      "step": 121720
    },
    {
      "epoch": 2.5042185358427687,
      "grad_norm": 0.4659070074558258,
      "learning_rate": 1.3986511682972014e-05,
      "loss": 2.333,
      "step": 121730
    },
    {
      "epoch": 2.5044242553596363,
      "grad_norm": 0.4334866404533386,
      "learning_rate": 1.3975185667306034e-05,
      "loss": 2.3726,
      "step": 121740
    },
    {
      "epoch": 2.504629974876504,
      "grad_norm": 0.4700016975402832,
      "learning_rate": 1.3963863894775408e-05,
      "loss": 2.3602,
      "step": 121750
    },
    {
      "epoch": 2.5048356943933716,
      "grad_norm": 0.4696458876132965,
      "learning_rate": 1.3952546365938513e-05,
      "loss": 2.352,
      "step": 121760
    },
    {
      "epoch": 2.5050414139102393,
      "grad_norm": 0.43822744488716125,
      "learning_rate": 1.3941233081353577e-05,
      "loss": 2.3377,
      "step": 121770
    },
    {
      "epoch": 2.5052471334271074,
      "grad_norm": 0.47928687930107117,
      "learning_rate": 1.3929924041578702e-05,
      "loss": 2.315,
      "step": 121780
    },
    {
      "epoch": 2.505452852943975,
      "grad_norm": 0.5125001668930054,
      "learning_rate": 1.3918619247171627e-05,
      "loss": 2.4085,
      "step": 121790
    },
    {
      "epoch": 2.5056585724608427,
      "grad_norm": 0.4818896949291229,
      "learning_rate": 1.3907318698689964e-05,
      "loss": 2.3731,
      "step": 121800
    },
    {
      "epoch": 2.5058642919777103,
      "grad_norm": 0.4485580623149872,
      "learning_rate": 1.3896022396691132e-05,
      "loss": 2.3819,
      "step": 121810
    },
    {
      "epoch": 2.506070011494578,
      "grad_norm": 0.5341921448707581,
      "learning_rate": 1.38847303417323e-05,
      "loss": 2.3453,
      "step": 121820
    },
    {
      "epoch": 2.5062757310114456,
      "grad_norm": 0.4483206570148468,
      "learning_rate": 1.387344253437045e-05,
      "loss": 2.3419,
      "step": 121830
    },
    {
      "epoch": 2.5064814505283133,
      "grad_norm": 0.4743603765964508,
      "learning_rate": 1.3862158975162343e-05,
      "loss": 2.3565,
      "step": 121840
    },
    {
      "epoch": 2.506687170045181,
      "grad_norm": 0.45163992047309875,
      "learning_rate": 1.385087966466455e-05,
      "loss": 2.366,
      "step": 121850
    },
    {
      "epoch": 2.5068928895620486,
      "grad_norm": 0.4406457543373108,
      "learning_rate": 1.3839604603433398e-05,
      "loss": 2.42,
      "step": 121860
    },
    {
      "epoch": 2.5070986090789167,
      "grad_norm": 0.45073002576828003,
      "learning_rate": 1.3828333792025028e-05,
      "loss": 2.3838,
      "step": 121870
    },
    {
      "epoch": 2.5073043285957843,
      "grad_norm": 0.4347136914730072,
      "learning_rate": 1.3817067230995383e-05,
      "loss": 2.3337,
      "step": 121880
    },
    {
      "epoch": 2.507510048112652,
      "grad_norm": 0.4373835325241089,
      "learning_rate": 1.3805804920900167e-05,
      "loss": 2.3234,
      "step": 121890
    },
    {
      "epoch": 2.5077157676295196,
      "grad_norm": 0.4690604507923126,
      "learning_rate": 1.3794546862294888e-05,
      "loss": 2.3827,
      "step": 121900
    },
    {
      "epoch": 2.5079214871463873,
      "grad_norm": 0.43854743242263794,
      "learning_rate": 1.3783293055734858e-05,
      "loss": 2.3199,
      "step": 121910
    },
    {
      "epoch": 2.5081272066632554,
      "grad_norm": 0.5175980925559998,
      "learning_rate": 1.3772043501775144e-05,
      "loss": 2.3293,
      "step": 121920
    },
    {
      "epoch": 2.508332926180123,
      "grad_norm": 0.41264277696609497,
      "learning_rate": 1.3760798200970649e-05,
      "loss": 2.4096,
      "step": 121930
    },
    {
      "epoch": 2.5085386456969907,
      "grad_norm": 0.4307766854763031,
      "learning_rate": 1.3749557153876026e-05,
      "loss": 2.3924,
      "step": 121940
    },
    {
      "epoch": 2.5087443652138584,
      "grad_norm": 0.4762203097343445,
      "learning_rate": 1.3738320361045742e-05,
      "loss": 2.346,
      "step": 121950
    },
    {
      "epoch": 2.508950084730726,
      "grad_norm": 0.4486727714538574,
      "learning_rate": 1.3727087823034068e-05,
      "loss": 2.3368,
      "step": 121960
    },
    {
      "epoch": 2.5091558042475937,
      "grad_norm": 0.4258396625518799,
      "learning_rate": 1.371585954039497e-05,
      "loss": 2.3072,
      "step": 121970
    },
    {
      "epoch": 2.5093615237644613,
      "grad_norm": 0.520625114440918,
      "learning_rate": 1.3704635513682362e-05,
      "loss": 2.3551,
      "step": 121980
    },
    {
      "epoch": 2.509567243281329,
      "grad_norm": 0.4496710002422333,
      "learning_rate": 1.3693415743449845e-05,
      "loss": 2.3053,
      "step": 121990
    },
    {
      "epoch": 2.5097729627981966,
      "grad_norm": 0.43671926856040955,
      "learning_rate": 1.3682200230250775e-05,
      "loss": 2.3369,
      "step": 122000
    },
    {
      "epoch": 2.5099786823150647,
      "grad_norm": 0.438241571187973,
      "learning_rate": 1.367098897463841e-05,
      "loss": 2.3167,
      "step": 122010
    },
    {
      "epoch": 2.5101844018319324,
      "grad_norm": 0.45215314626693726,
      "learning_rate": 1.3659781977165742e-05,
      "loss": 2.4122,
      "step": 122020
    },
    {
      "epoch": 2.5103901213488,
      "grad_norm": 0.4720391631126404,
      "learning_rate": 1.3648579238385494e-05,
      "loss": 2.3592,
      "step": 122030
    },
    {
      "epoch": 2.5105958408656677,
      "grad_norm": 0.45179569721221924,
      "learning_rate": 1.3637380758850315e-05,
      "loss": 2.3565,
      "step": 122040
    },
    {
      "epoch": 2.5108015603825353,
      "grad_norm": 0.4368971884250641,
      "learning_rate": 1.3626186539112507e-05,
      "loss": 2.3964,
      "step": 122050
    },
    {
      "epoch": 2.5110072798994034,
      "grad_norm": 0.45470696687698364,
      "learning_rate": 1.3614996579724204e-05,
      "loss": 2.4159,
      "step": 122060
    },
    {
      "epoch": 2.511212999416271,
      "grad_norm": 0.44195765256881714,
      "learning_rate": 1.3603810881237423e-05,
      "loss": 2.3802,
      "step": 122070
    },
    {
      "epoch": 2.5114187189331387,
      "grad_norm": 0.4586310386657715,
      "learning_rate": 1.359262944420383e-05,
      "loss": 2.3688,
      "step": 122080
    },
    {
      "epoch": 2.5116244384500064,
      "grad_norm": 0.47727030515670776,
      "learning_rate": 1.3581452269174966e-05,
      "loss": 2.3572,
      "step": 122090
    },
    {
      "epoch": 2.511830157966874,
      "grad_norm": 0.4670566916465759,
      "learning_rate": 1.357027935670213e-05,
      "loss": 2.345,
      "step": 122100
    },
    {
      "epoch": 2.5120358774837417,
      "grad_norm": 0.427195280790329,
      "learning_rate": 1.355911070733643e-05,
      "loss": 2.4063,
      "step": 122110
    },
    {
      "epoch": 2.5122415970006093,
      "grad_norm": 0.4907054603099823,
      "learning_rate": 1.3547946321628758e-05,
      "loss": 2.3373,
      "step": 122120
    },
    {
      "epoch": 2.512447316517477,
      "grad_norm": 0.449341356754303,
      "learning_rate": 1.3536786200129781e-05,
      "loss": 2.3299,
      "step": 122130
    },
    {
      "epoch": 2.5126530360343446,
      "grad_norm": 0.45883747935295105,
      "learning_rate": 1.3525630343389972e-05,
      "loss": 2.3357,
      "step": 122140
    },
    {
      "epoch": 2.5128587555512127,
      "grad_norm": 0.47522637248039246,
      "learning_rate": 1.3514478751959591e-05,
      "loss": 2.3779,
      "step": 122150
    },
    {
      "epoch": 2.5130644750680804,
      "grad_norm": 0.48550379276275635,
      "learning_rate": 1.3503331426388688e-05,
      "loss": 2.3805,
      "step": 122160
    },
    {
      "epoch": 2.513270194584948,
      "grad_norm": 0.4788079261779785,
      "learning_rate": 1.3492188367227087e-05,
      "loss": 2.3055,
      "step": 122170
    },
    {
      "epoch": 2.5134759141018157,
      "grad_norm": 0.4931047558784485,
      "learning_rate": 1.348104957502443e-05,
      "loss": 2.4068,
      "step": 122180
    },
    {
      "epoch": 2.5136816336186834,
      "grad_norm": 0.5215725898742676,
      "learning_rate": 1.3469915050330129e-05,
      "loss": 2.3274,
      "step": 122190
    },
    {
      "epoch": 2.513887353135551,
      "grad_norm": 0.4636968672275543,
      "learning_rate": 1.345878479369338e-05,
      "loss": 2.3848,
      "step": 122200
    },
    {
      "epoch": 2.514093072652419,
      "grad_norm": 0.4856201708316803,
      "learning_rate": 1.3447658805663177e-05,
      "loss": 2.3739,
      "step": 122210
    },
    {
      "epoch": 2.5142987921692868,
      "grad_norm": 0.4374290704727173,
      "learning_rate": 1.3436537086788326e-05,
      "loss": 2.382,
      "step": 122220
    },
    {
      "epoch": 2.5145045116861544,
      "grad_norm": 0.44536709785461426,
      "learning_rate": 1.3425419637617375e-05,
      "loss": 2.3707,
      "step": 122230
    },
    {
      "epoch": 2.514710231203022,
      "grad_norm": 0.4740983247756958,
      "learning_rate": 1.34143064586987e-05,
      "loss": 2.3416,
      "step": 122240
    },
    {
      "epoch": 2.5149159507198897,
      "grad_norm": 0.4544358551502228,
      "learning_rate": 1.3403197550580481e-05,
      "loss": 2.384,
      "step": 122250
    },
    {
      "epoch": 2.5151216702367574,
      "grad_norm": 0.5009296536445618,
      "learning_rate": 1.3392092913810584e-05,
      "loss": 2.4122,
      "step": 122260
    },
    {
      "epoch": 2.515327389753625,
      "grad_norm": 0.4832887351512909,
      "learning_rate": 1.3380992548936799e-05,
      "loss": 2.3795,
      "step": 122270
    },
    {
      "epoch": 2.5155331092704927,
      "grad_norm": 0.4374493360519409,
      "learning_rate": 1.336989645650667e-05,
      "loss": 2.3488,
      "step": 122280
    },
    {
      "epoch": 2.5157388287873603,
      "grad_norm": 0.46719953417778015,
      "learning_rate": 1.3358804637067424e-05,
      "loss": 2.3443,
      "step": 122290
    },
    {
      "epoch": 2.5159445483042284,
      "grad_norm": 0.47588756680488586,
      "learning_rate": 1.3347717091166223e-05,
      "loss": 2.3605,
      "step": 122300
    },
    {
      "epoch": 2.516150267821096,
      "grad_norm": 0.42373231053352356,
      "learning_rate": 1.3336633819349965e-05,
      "loss": 2.3804,
      "step": 122310
    },
    {
      "epoch": 2.5163559873379637,
      "grad_norm": 0.46179088950157166,
      "learning_rate": 1.3325554822165265e-05,
      "loss": 2.3702,
      "step": 122320
    },
    {
      "epoch": 2.5165617068548314,
      "grad_norm": 0.5354700684547424,
      "learning_rate": 1.3314480100158656e-05,
      "loss": 2.3403,
      "step": 122330
    },
    {
      "epoch": 2.516767426371699,
      "grad_norm": 0.44636672735214233,
      "learning_rate": 1.3303409653876354e-05,
      "loss": 2.4232,
      "step": 122340
    },
    {
      "epoch": 2.516973145888567,
      "grad_norm": 0.504207968711853,
      "learning_rate": 1.3292343483864389e-05,
      "loss": 2.379,
      "step": 122350
    },
    {
      "epoch": 2.517178865405435,
      "grad_norm": 0.47103676199913025,
      "learning_rate": 1.328128159066866e-05,
      "loss": 2.3538,
      "step": 122360
    },
    {
      "epoch": 2.5173845849223024,
      "grad_norm": 0.4636664390563965,
      "learning_rate": 1.3270223974834739e-05,
      "loss": 2.3867,
      "step": 122370
    },
    {
      "epoch": 2.51759030443917,
      "grad_norm": 0.4549371302127838,
      "learning_rate": 1.3259170636908013e-05,
      "loss": 2.3547,
      "step": 122380
    },
    {
      "epoch": 2.5177960239560377,
      "grad_norm": 0.43026480078697205,
      "learning_rate": 1.324812157743377e-05,
      "loss": 2.3982,
      "step": 122390
    },
    {
      "epoch": 2.5180017434729054,
      "grad_norm": 0.5087674260139465,
      "learning_rate": 1.3237076796956927e-05,
      "loss": 2.3734,
      "step": 122400
    },
    {
      "epoch": 2.518207462989773,
      "grad_norm": 0.5107643604278564,
      "learning_rate": 1.3226036296022283e-05,
      "loss": 2.3953,
      "step": 122410
    },
    {
      "epoch": 2.5184131825066407,
      "grad_norm": 0.47511914372444153,
      "learning_rate": 1.3215000075174411e-05,
      "loss": 2.4008,
      "step": 122420
    },
    {
      "epoch": 2.5186189020235084,
      "grad_norm": 0.4759206175804138,
      "learning_rate": 1.3203968134957656e-05,
      "loss": 2.3448,
      "step": 122430
    },
    {
      "epoch": 2.5188246215403765,
      "grad_norm": 0.49114689230918884,
      "learning_rate": 1.3192940475916171e-05,
      "loss": 2.3278,
      "step": 122440
    },
    {
      "epoch": 2.519030341057244,
      "grad_norm": 0.42920005321502686,
      "learning_rate": 1.31819170985939e-05,
      "loss": 2.3929,
      "step": 122450
    },
    {
      "epoch": 2.5192360605741118,
      "grad_norm": 0.4679914712905884,
      "learning_rate": 1.3170898003534549e-05,
      "loss": 2.4068,
      "step": 122460
    },
    {
      "epoch": 2.5194417800909794,
      "grad_norm": 0.44649574160575867,
      "learning_rate": 1.3159883191281642e-05,
      "loss": 2.4047,
      "step": 122470
    },
    {
      "epoch": 2.519647499607847,
      "grad_norm": 0.4805200397968292,
      "learning_rate": 1.3148872662378476e-05,
      "loss": 2.353,
      "step": 122480
    },
    {
      "epoch": 2.519853219124715,
      "grad_norm": 0.4540472626686096,
      "learning_rate": 1.3137866417368128e-05,
      "loss": 2.3412,
      "step": 122490
    },
    {
      "epoch": 2.520058938641583,
      "grad_norm": 0.4540160894393921,
      "learning_rate": 1.3126864456793497e-05,
      "loss": 2.387,
      "step": 122500
    },
    {
      "epoch": 2.5202646581584505,
      "grad_norm": 0.4326395094394684,
      "learning_rate": 1.311586678119724e-05,
      "loss": 2.3781,
      "step": 122510
    },
    {
      "epoch": 2.520470377675318,
      "grad_norm": 0.44982877373695374,
      "learning_rate": 1.3104873391121819e-05,
      "loss": 2.3505,
      "step": 122520
    },
    {
      "epoch": 2.5206760971921858,
      "grad_norm": 0.4119751751422882,
      "learning_rate": 1.3093884287109459e-05,
      "loss": 2.3896,
      "step": 122530
    },
    {
      "epoch": 2.5208818167090534,
      "grad_norm": 0.47808560729026794,
      "learning_rate": 1.3082899469702214e-05,
      "loss": 2.3648,
      "step": 122540
    },
    {
      "epoch": 2.521087536225921,
      "grad_norm": 0.501481294631958,
      "learning_rate": 1.3071918939441896e-05,
      "loss": 2.2844,
      "step": 122550
    },
    {
      "epoch": 2.5212932557427887,
      "grad_norm": 0.4772438108921051,
      "learning_rate": 1.3060942696870115e-05,
      "loss": 2.3989,
      "step": 122560
    },
    {
      "epoch": 2.5214989752596564,
      "grad_norm": 0.46211764216423035,
      "learning_rate": 1.3049970742528294e-05,
      "loss": 2.3372,
      "step": 122570
    },
    {
      "epoch": 2.5217046947765245,
      "grad_norm": 0.4694186747074127,
      "learning_rate": 1.3039003076957546e-05,
      "loss": 2.4038,
      "step": 122580
    },
    {
      "epoch": 2.521910414293392,
      "grad_norm": 0.4785143733024597,
      "learning_rate": 1.3028039700698923e-05,
      "loss": 2.3685,
      "step": 122590
    },
    {
      "epoch": 2.52211613381026,
      "grad_norm": 0.48862630128860474,
      "learning_rate": 1.3017080614293176e-05,
      "loss": 2.3901,
      "step": 122600
    },
    {
      "epoch": 2.5223218533271274,
      "grad_norm": 0.4465858042240143,
      "learning_rate": 1.3006125818280801e-05,
      "loss": 2.3676,
      "step": 122610
    },
    {
      "epoch": 2.522527572843995,
      "grad_norm": 0.4579145312309265,
      "learning_rate": 1.2995175313202213e-05,
      "loss": 2.391,
      "step": 122620
    },
    {
      "epoch": 2.5227332923608627,
      "grad_norm": 0.4786069095134735,
      "learning_rate": 1.2984229099597511e-05,
      "loss": 2.2898,
      "step": 122630
    },
    {
      "epoch": 2.522939011877731,
      "grad_norm": 0.4844931662082672,
      "learning_rate": 1.2973287178006577e-05,
      "loss": 2.4108,
      "step": 122640
    },
    {
      "epoch": 2.5231447313945985,
      "grad_norm": 0.4497312009334564,
      "learning_rate": 1.2962349548969189e-05,
      "loss": 2.3692,
      "step": 122650
    },
    {
      "epoch": 2.523350450911466,
      "grad_norm": 0.4905453622341156,
      "learning_rate": 1.2951416213024781e-05,
      "loss": 2.372,
      "step": 122660
    },
    {
      "epoch": 2.523556170428334,
      "grad_norm": 0.4758817255496979,
      "learning_rate": 1.2940487170712623e-05,
      "loss": 2.3637,
      "step": 122670
    },
    {
      "epoch": 2.5237618899452015,
      "grad_norm": 0.437421590089798,
      "learning_rate": 1.2929562422571861e-05,
      "loss": 2.3132,
      "step": 122680
    },
    {
      "epoch": 2.523967609462069,
      "grad_norm": 0.49148446321487427,
      "learning_rate": 1.2918641969141287e-05,
      "loss": 2.3712,
      "step": 122690
    },
    {
      "epoch": 2.5241733289789368,
      "grad_norm": 0.4638087749481201,
      "learning_rate": 1.2907725810959547e-05,
      "loss": 2.3078,
      "step": 122700
    },
    {
      "epoch": 2.5243790484958044,
      "grad_norm": 0.4558507800102234,
      "learning_rate": 1.2896813948565146e-05,
      "loss": 2.3947,
      "step": 122710
    },
    {
      "epoch": 2.524584768012672,
      "grad_norm": 0.47494083642959595,
      "learning_rate": 1.2885906382496226e-05,
      "loss": 2.4145,
      "step": 122720
    },
    {
      "epoch": 2.52479048752954,
      "grad_norm": 0.43252497911453247,
      "learning_rate": 1.287500311329084e-05,
      "loss": 2.4288,
      "step": 122730
    },
    {
      "epoch": 2.524996207046408,
      "grad_norm": 0.48086217045783997,
      "learning_rate": 1.2864104141486766e-05,
      "loss": 2.369,
      "step": 122740
    },
    {
      "epoch": 2.5252019265632755,
      "grad_norm": 0.4764488935470581,
      "learning_rate": 1.2853209467621607e-05,
      "loss": 2.3295,
      "step": 122750
    },
    {
      "epoch": 2.525407646080143,
      "grad_norm": 0.43918922543525696,
      "learning_rate": 1.2842319092232724e-05,
      "loss": 2.3377,
      "step": 122760
    },
    {
      "epoch": 2.5256133655970108,
      "grad_norm": 0.5278118848800659,
      "learning_rate": 1.2831433015857298e-05,
      "loss": 2.4175,
      "step": 122770
    },
    {
      "epoch": 2.525819085113879,
      "grad_norm": 0.46478456258773804,
      "learning_rate": 1.2820551239032264e-05,
      "loss": 2.391,
      "step": 122780
    },
    {
      "epoch": 2.5260248046307465,
      "grad_norm": 0.45444589853286743,
      "learning_rate": 1.2809673762294372e-05,
      "loss": 2.4326,
      "step": 122790
    },
    {
      "epoch": 2.526230524147614,
      "grad_norm": 0.4888964891433716,
      "learning_rate": 1.2798800586180137e-05,
      "loss": 2.3482,
      "step": 122800
    },
    {
      "epoch": 2.526436243664482,
      "grad_norm": 0.5808175802230835,
      "learning_rate": 1.2787931711225876e-05,
      "loss": 2.3438,
      "step": 122810
    },
    {
      "epoch": 2.5266419631813495,
      "grad_norm": 0.44452330470085144,
      "learning_rate": 1.2777067137967703e-05,
      "loss": 2.3562,
      "step": 122820
    },
    {
      "epoch": 2.526847682698217,
      "grad_norm": 0.4734523296356201,
      "learning_rate": 1.2766206866941499e-05,
      "loss": 2.3648,
      "step": 122830
    },
    {
      "epoch": 2.527053402215085,
      "grad_norm": 0.4559032917022705,
      "learning_rate": 1.275535089868294e-05,
      "loss": 2.3811,
      "step": 122840
    },
    {
      "epoch": 2.5272591217319524,
      "grad_norm": 0.46369317173957825,
      "learning_rate": 1.2744499233727492e-05,
      "loss": 2.3426,
      "step": 122850
    },
    {
      "epoch": 2.52746484124882,
      "grad_norm": 0.4186379909515381,
      "learning_rate": 1.2733651872610419e-05,
      "loss": 2.3983,
      "step": 122860
    },
    {
      "epoch": 2.527670560765688,
      "grad_norm": 0.4662076234817505,
      "learning_rate": 1.272280881586675e-05,
      "loss": 2.3631,
      "step": 122870
    },
    {
      "epoch": 2.527876280282556,
      "grad_norm": 0.4595494866371155,
      "learning_rate": 1.271197006403132e-05,
      "loss": 2.3486,
      "step": 122880
    },
    {
      "epoch": 2.5280819997994235,
      "grad_norm": 0.4961686134338379,
      "learning_rate": 1.2701135617638771e-05,
      "loss": 2.3327,
      "step": 122890
    },
    {
      "epoch": 2.528287719316291,
      "grad_norm": 0.47348862886428833,
      "learning_rate": 1.269030547722343e-05,
      "loss": 2.3341,
      "step": 122900
    },
    {
      "epoch": 2.528493438833159,
      "grad_norm": 0.4645666182041168,
      "learning_rate": 1.2679479643319559e-05,
      "loss": 2.3516,
      "step": 122910
    },
    {
      "epoch": 2.5286991583500265,
      "grad_norm": 0.45860686898231506,
      "learning_rate": 1.2668658116461141e-05,
      "loss": 2.3082,
      "step": 122920
    },
    {
      "epoch": 2.5289048778668946,
      "grad_norm": 0.4712425172328949,
      "learning_rate": 1.2657840897181872e-05,
      "loss": 2.3683,
      "step": 122930
    },
    {
      "epoch": 2.529110597383762,
      "grad_norm": 0.524557888507843,
      "learning_rate": 1.264702798601538e-05,
      "loss": 2.3311,
      "step": 122940
    },
    {
      "epoch": 2.52931631690063,
      "grad_norm": 0.4529890716075897,
      "learning_rate": 1.2636219383495007e-05,
      "loss": 2.3469,
      "step": 122950
    },
    {
      "epoch": 2.5295220364174975,
      "grad_norm": 0.4014985263347626,
      "learning_rate": 1.2625415090153813e-05,
      "loss": 2.3177,
      "step": 122960
    },
    {
      "epoch": 2.529727755934365,
      "grad_norm": 0.42284709215164185,
      "learning_rate": 1.2614615106524797e-05,
      "loss": 2.3989,
      "step": 122970
    },
    {
      "epoch": 2.529933475451233,
      "grad_norm": 0.5084136128425598,
      "learning_rate": 1.2603819433140596e-05,
      "loss": 2.3914,
      "step": 122980
    },
    {
      "epoch": 2.5301391949681005,
      "grad_norm": 0.48808589577674866,
      "learning_rate": 1.2593028070533719e-05,
      "loss": 2.3778,
      "step": 122990
    },
    {
      "epoch": 2.530344914484968,
      "grad_norm": 0.4589013159275055,
      "learning_rate": 1.2582241019236496e-05,
      "loss": 2.349,
      "step": 123000
    },
    {
      "epoch": 2.5305506340018358,
      "grad_norm": 0.4690815508365631,
      "learning_rate": 1.2571458279780934e-05,
      "loss": 2.3131,
      "step": 123010
    },
    {
      "epoch": 2.530756353518704,
      "grad_norm": 0.46077388525009155,
      "learning_rate": 1.2560679852698887e-05,
      "loss": 2.3833,
      "step": 123020
    },
    {
      "epoch": 2.5309620730355715,
      "grad_norm": 0.4289877116680145,
      "learning_rate": 1.254990573852205e-05,
      "loss": 2.3744,
      "step": 123030
    },
    {
      "epoch": 2.531167792552439,
      "grad_norm": 0.45365670323371887,
      "learning_rate": 1.253913593778181e-05,
      "loss": 2.4184,
      "step": 123040
    },
    {
      "epoch": 2.531373512069307,
      "grad_norm": 0.56760573387146,
      "learning_rate": 1.2528370451009374e-05,
      "loss": 2.3254,
      "step": 123050
    },
    {
      "epoch": 2.5315792315861745,
      "grad_norm": 0.508155345916748,
      "learning_rate": 1.2517609278735764e-05,
      "loss": 2.3752,
      "step": 123060
    },
    {
      "epoch": 2.5317849511030426,
      "grad_norm": 0.4885527193546295,
      "learning_rate": 1.2506852421491778e-05,
      "loss": 2.3647,
      "step": 123070
    },
    {
      "epoch": 2.5319906706199102,
      "grad_norm": 0.4827340841293335,
      "learning_rate": 1.2496099879807965e-05,
      "loss": 2.3549,
      "step": 123080
    },
    {
      "epoch": 2.532196390136778,
      "grad_norm": 0.48262476921081543,
      "learning_rate": 1.2485351654214716e-05,
      "loss": 2.3784,
      "step": 123090
    },
    {
      "epoch": 2.5324021096536455,
      "grad_norm": 0.4607744514942169,
      "learning_rate": 1.2474607745242172e-05,
      "loss": 2.4162,
      "step": 123100
    },
    {
      "epoch": 2.532607829170513,
      "grad_norm": 0.4530545771121979,
      "learning_rate": 1.2463868153420266e-05,
      "loss": 2.3821,
      "step": 123110
    },
    {
      "epoch": 2.532813548687381,
      "grad_norm": 0.5142304301261902,
      "learning_rate": 1.245313287927874e-05,
      "loss": 2.3551,
      "step": 123120
    },
    {
      "epoch": 2.5330192682042485,
      "grad_norm": 0.46373653411865234,
      "learning_rate": 1.2442401923347091e-05,
      "loss": 2.3798,
      "step": 123130
    },
    {
      "epoch": 2.533224987721116,
      "grad_norm": 0.4386705756187439,
      "learning_rate": 1.2431675286154621e-05,
      "loss": 2.3078,
      "step": 123140
    },
    {
      "epoch": 2.533430707237984,
      "grad_norm": 0.448796808719635,
      "learning_rate": 1.2420952968230415e-05,
      "loss": 2.3214,
      "step": 123150
    },
    {
      "epoch": 2.533636426754852,
      "grad_norm": 0.5800556540489197,
      "learning_rate": 1.2410234970103363e-05,
      "loss": 2.3275,
      "step": 123160
    },
    {
      "epoch": 2.5338421462717196,
      "grad_norm": 0.47010132670402527,
      "learning_rate": 1.2399521292302096e-05,
      "loss": 2.3687,
      "step": 123170
    },
    {
      "epoch": 2.534047865788587,
      "grad_norm": 0.4720768332481384,
      "learning_rate": 1.2388811935355093e-05,
      "loss": 2.3499,
      "step": 123180
    },
    {
      "epoch": 2.534253585305455,
      "grad_norm": 0.47052887082099915,
      "learning_rate": 1.2378106899790564e-05,
      "loss": 2.3229,
      "step": 123190
    },
    {
      "epoch": 2.5344593048223225,
      "grad_norm": 0.4372888505458832,
      "learning_rate": 1.2367406186136543e-05,
      "loss": 2.3939,
      "step": 123200
    },
    {
      "epoch": 2.5346650243391906,
      "grad_norm": 0.5024739503860474,
      "learning_rate": 1.2356709794920862e-05,
      "loss": 2.4002,
      "step": 123210
    },
    {
      "epoch": 2.5348707438560583,
      "grad_norm": 0.46100422739982605,
      "learning_rate": 1.2346017726671032e-05,
      "loss": 2.3694,
      "step": 123220
    },
    {
      "epoch": 2.535076463372926,
      "grad_norm": 0.6282774806022644,
      "learning_rate": 1.2335329981914523e-05,
      "loss": 2.3216,
      "step": 123230
    },
    {
      "epoch": 2.5352821828897936,
      "grad_norm": 0.46739673614501953,
      "learning_rate": 1.2324646561178487e-05,
      "loss": 2.3449,
      "step": 123240
    },
    {
      "epoch": 2.535487902406661,
      "grad_norm": 0.48443666100502014,
      "learning_rate": 1.2313967464989828e-05,
      "loss": 2.4041,
      "step": 123250
    },
    {
      "epoch": 2.535693621923529,
      "grad_norm": 0.4574666917324066,
      "learning_rate": 1.2303292693875356e-05,
      "loss": 2.3505,
      "step": 123260
    },
    {
      "epoch": 2.5358993414403965,
      "grad_norm": 0.4369382858276367,
      "learning_rate": 1.2292622248361551e-05,
      "loss": 2.3104,
      "step": 123270
    },
    {
      "epoch": 2.536105060957264,
      "grad_norm": 0.4512157440185547,
      "learning_rate": 1.2281956128974715e-05,
      "loss": 2.3787,
      "step": 123280
    },
    {
      "epoch": 2.536310780474132,
      "grad_norm": 0.47333624958992004,
      "learning_rate": 1.2271294336241024e-05,
      "loss": 2.3411,
      "step": 123290
    },
    {
      "epoch": 2.536516499991,
      "grad_norm": 0.4233318269252777,
      "learning_rate": 1.2260636870686292e-05,
      "loss": 2.3542,
      "step": 123300
    },
    {
      "epoch": 2.5367222195078676,
      "grad_norm": 0.4668983519077301,
      "learning_rate": 1.2249983732836212e-05,
      "loss": 2.375,
      "step": 123310
    },
    {
      "epoch": 2.5369279390247352,
      "grad_norm": 0.43832239508628845,
      "learning_rate": 1.2239334923216283e-05,
      "loss": 2.3852,
      "step": 123320
    },
    {
      "epoch": 2.537133658541603,
      "grad_norm": 0.4689239263534546,
      "learning_rate": 1.222869044235171e-05,
      "loss": 2.3674,
      "step": 123330
    },
    {
      "epoch": 2.5373393780584705,
      "grad_norm": 0.5018690228462219,
      "learning_rate": 1.2218050290767536e-05,
      "loss": 2.3571,
      "step": 123340
    },
    {
      "epoch": 2.537545097575338,
      "grad_norm": 0.4534936547279358,
      "learning_rate": 1.2207414468988598e-05,
      "loss": 2.3977,
      "step": 123350
    },
    {
      "epoch": 2.5377508170922063,
      "grad_norm": 0.4251948595046997,
      "learning_rate": 1.2196782977539478e-05,
      "loss": 2.2959,
      "step": 123360
    },
    {
      "epoch": 2.537956536609074,
      "grad_norm": 0.45133060216903687,
      "learning_rate": 1.218615581694459e-05,
      "loss": 2.3206,
      "step": 123370
    },
    {
      "epoch": 2.5381622561259416,
      "grad_norm": 0.47422999143600464,
      "learning_rate": 1.2175532987728111e-05,
      "loss": 2.3668,
      "step": 123380
    },
    {
      "epoch": 2.5383679756428092,
      "grad_norm": 0.4130478799343109,
      "learning_rate": 1.2164914490414003e-05,
      "loss": 2.3385,
      "step": 123390
    },
    {
      "epoch": 2.538573695159677,
      "grad_norm": 0.4689222276210785,
      "learning_rate": 1.2154300325526013e-05,
      "loss": 2.3919,
      "step": 123400
    },
    {
      "epoch": 2.5387794146765446,
      "grad_norm": 0.468880832195282,
      "learning_rate": 1.2143690493587689e-05,
      "loss": 2.3608,
      "step": 123410
    },
    {
      "epoch": 2.538985134193412,
      "grad_norm": 0.45295751094818115,
      "learning_rate": 1.2133084995122357e-05,
      "loss": 2.4039,
      "step": 123420
    },
    {
      "epoch": 2.53919085371028,
      "grad_norm": 0.4337592124938965,
      "learning_rate": 1.212248383065312e-05,
      "loss": 2.3454,
      "step": 123430
    },
    {
      "epoch": 2.5393965732271475,
      "grad_norm": 0.4310101270675659,
      "learning_rate": 1.2111887000702893e-05,
      "loss": 2.3824,
      "step": 123440
    },
    {
      "epoch": 2.5396022927440156,
      "grad_norm": 0.48932909965515137,
      "learning_rate": 1.2101294505794336e-05,
      "loss": 2.3364,
      "step": 123450
    },
    {
      "epoch": 2.5398080122608833,
      "grad_norm": 0.4837198555469513,
      "learning_rate": 1.2090706346449932e-05,
      "loss": 2.3458,
      "step": 123460
    },
    {
      "epoch": 2.540013731777751,
      "grad_norm": 0.5052371621131897,
      "learning_rate": 1.2080122523191928e-05,
      "loss": 2.3471,
      "step": 123470
    },
    {
      "epoch": 2.5402194512946186,
      "grad_norm": 0.4388082027435303,
      "learning_rate": 1.2069543036542385e-05,
      "loss": 2.359,
      "step": 123480
    },
    {
      "epoch": 2.540425170811486,
      "grad_norm": 0.46291545033454895,
      "learning_rate": 1.205896788702312e-05,
      "loss": 2.412,
      "step": 123490
    },
    {
      "epoch": 2.5406308903283543,
      "grad_norm": 0.4666254222393036,
      "learning_rate": 1.2048397075155771e-05,
      "loss": 2.3855,
      "step": 123500
    },
    {
      "epoch": 2.540836609845222,
      "grad_norm": 0.49217021465301514,
      "learning_rate": 1.2037830601461664e-05,
      "loss": 2.3347,
      "step": 123510
    },
    {
      "epoch": 2.5410423293620896,
      "grad_norm": 0.4691261351108551,
      "learning_rate": 1.202726846646206e-05,
      "loss": 2.3835,
      "step": 123520
    },
    {
      "epoch": 2.5412480488789573,
      "grad_norm": 0.486317902803421,
      "learning_rate": 1.2016710670677933e-05,
      "loss": 2.3444,
      "step": 123530
    },
    {
      "epoch": 2.541453768395825,
      "grad_norm": 0.4489159882068634,
      "learning_rate": 1.2006157214629976e-05,
      "loss": 2.3448,
      "step": 123540
    },
    {
      "epoch": 2.5416594879126926,
      "grad_norm": 0.46541398763656616,
      "learning_rate": 1.1995608098838795e-05,
      "loss": 2.414,
      "step": 123550
    },
    {
      "epoch": 2.5418652074295602,
      "grad_norm": 0.4953854978084564,
      "learning_rate": 1.198506332382472e-05,
      "loss": 2.3172,
      "step": 123560
    },
    {
      "epoch": 2.542070926946428,
      "grad_norm": 0.4593313932418823,
      "learning_rate": 1.1974522890107809e-05,
      "loss": 2.276,
      "step": 123570
    },
    {
      "epoch": 2.5422766464632955,
      "grad_norm": 0.46856868267059326,
      "learning_rate": 1.1963986798208038e-05,
      "loss": 2.4011,
      "step": 123580
    },
    {
      "epoch": 2.5424823659801636,
      "grad_norm": 0.48239997029304504,
      "learning_rate": 1.1953455048645046e-05,
      "loss": 2.3666,
      "step": 123590
    },
    {
      "epoch": 2.5426880854970313,
      "grad_norm": 0.44204458594322205,
      "learning_rate": 1.1942927641938296e-05,
      "loss": 2.433,
      "step": 123600
    },
    {
      "epoch": 2.542893805013899,
      "grad_norm": 0.4614591896533966,
      "learning_rate": 1.1932404578607114e-05,
      "loss": 2.3881,
      "step": 123610
    },
    {
      "epoch": 2.5430995245307666,
      "grad_norm": 0.4336698651313782,
      "learning_rate": 1.1921885859170478e-05,
      "loss": 2.341,
      "step": 123620
    },
    {
      "epoch": 2.5433052440476343,
      "grad_norm": 0.43982771039009094,
      "learning_rate": 1.1911371484147227e-05,
      "loss": 2.3928,
      "step": 123630
    },
    {
      "epoch": 2.543510963564502,
      "grad_norm": 0.4224530756473541,
      "learning_rate": 1.1900861454056033e-05,
      "loss": 2.3883,
      "step": 123640
    },
    {
      "epoch": 2.54371668308137,
      "grad_norm": 0.4613513946533203,
      "learning_rate": 1.1890355769415252e-05,
      "loss": 2.4073,
      "step": 123650
    },
    {
      "epoch": 2.5439224025982377,
      "grad_norm": 0.40115025639533997,
      "learning_rate": 1.1879854430743076e-05,
      "loss": 2.3613,
      "step": 123660
    },
    {
      "epoch": 2.5441281221151053,
      "grad_norm": 0.44699665904045105,
      "learning_rate": 1.186935743855747e-05,
      "loss": 2.339,
      "step": 123670
    },
    {
      "epoch": 2.544333841631973,
      "grad_norm": 0.4537298381328583,
      "learning_rate": 1.1858864793376223e-05,
      "loss": 2.342,
      "step": 123680
    },
    {
      "epoch": 2.5445395611488406,
      "grad_norm": 0.45035824179649353,
      "learning_rate": 1.1848376495716862e-05,
      "loss": 2.3605,
      "step": 123690
    },
    {
      "epoch": 2.5447452806657083,
      "grad_norm": 0.5183854699134827,
      "learning_rate": 1.1837892546096719e-05,
      "loss": 2.3901,
      "step": 123700
    },
    {
      "epoch": 2.544951000182576,
      "grad_norm": 0.4845631420612335,
      "learning_rate": 1.18274129450329e-05,
      "loss": 2.3767,
      "step": 123710
    },
    {
      "epoch": 2.5451567196994436,
      "grad_norm": 0.4354318678379059,
      "learning_rate": 1.1816937693042328e-05,
      "loss": 2.4023,
      "step": 123720
    },
    {
      "epoch": 2.5453624392163112,
      "grad_norm": 0.4656652808189392,
      "learning_rate": 1.180646679064168e-05,
      "loss": 2.407,
      "step": 123730
    },
    {
      "epoch": 2.5455681587331793,
      "grad_norm": 0.4499213993549347,
      "learning_rate": 1.1796000238347426e-05,
      "loss": 2.3564,
      "step": 123740
    },
    {
      "epoch": 2.545773878250047,
      "grad_norm": 0.45456308126449585,
      "learning_rate": 1.1785538036675825e-05,
      "loss": 2.3995,
      "step": 123750
    },
    {
      "epoch": 2.5459795977669146,
      "grad_norm": 0.4784628748893738,
      "learning_rate": 1.1775080186142917e-05,
      "loss": 2.3185,
      "step": 123760
    },
    {
      "epoch": 2.5461853172837823,
      "grad_norm": 0.46168386936187744,
      "learning_rate": 1.1764626687264546e-05,
      "loss": 2.3361,
      "step": 123770
    },
    {
      "epoch": 2.54639103680065,
      "grad_norm": 0.44498947262763977,
      "learning_rate": 1.175417754055631e-05,
      "loss": 2.3379,
      "step": 123780
    },
    {
      "epoch": 2.546596756317518,
      "grad_norm": 0.42724618315696716,
      "learning_rate": 1.174373274653361e-05,
      "loss": 2.4028,
      "step": 123790
    },
    {
      "epoch": 2.5468024758343857,
      "grad_norm": 0.4464053809642792,
      "learning_rate": 1.1733292305711641e-05,
      "loss": 2.378,
      "step": 123800
    },
    {
      "epoch": 2.5470081953512533,
      "grad_norm": 0.4800066649913788,
      "learning_rate": 1.172285621860536e-05,
      "loss": 2.3516,
      "step": 123810
    },
    {
      "epoch": 2.547213914868121,
      "grad_norm": 0.398091584444046,
      "learning_rate": 1.1712424485729557e-05,
      "loss": 2.3829,
      "step": 123820
    },
    {
      "epoch": 2.5474196343849886,
      "grad_norm": 0.4886482357978821,
      "learning_rate": 1.1701997107598706e-05,
      "loss": 2.3385,
      "step": 123830
    },
    {
      "epoch": 2.5476253539018563,
      "grad_norm": 0.450693279504776,
      "learning_rate": 1.1691574084727175e-05,
      "loss": 2.3514,
      "step": 123840
    },
    {
      "epoch": 2.547831073418724,
      "grad_norm": 0.5276245474815369,
      "learning_rate": 1.1681155417629108e-05,
      "loss": 2.3728,
      "step": 123850
    },
    {
      "epoch": 2.5480367929355916,
      "grad_norm": 0.44874081015586853,
      "learning_rate": 1.1670741106818317e-05,
      "loss": 2.345,
      "step": 123860
    },
    {
      "epoch": 2.5482425124524593,
      "grad_norm": 0.4186238646507263,
      "learning_rate": 1.1660331152808558e-05,
      "loss": 2.3885,
      "step": 123870
    },
    {
      "epoch": 2.5484482319693273,
      "grad_norm": 0.5797141790390015,
      "learning_rate": 1.1649925556113283e-05,
      "loss": 2.3152,
      "step": 123880
    },
    {
      "epoch": 2.548653951486195,
      "grad_norm": 0.4541051685810089,
      "learning_rate": 1.1639524317245687e-05,
      "loss": 2.412,
      "step": 123890
    },
    {
      "epoch": 2.5488596710030627,
      "grad_norm": 0.4684222638607025,
      "learning_rate": 1.1629127436718889e-05,
      "loss": 2.3795,
      "step": 123900
    },
    {
      "epoch": 2.5490653905199303,
      "grad_norm": 0.44352400302886963,
      "learning_rate": 1.1618734915045658e-05,
      "loss": 2.4056,
      "step": 123910
    },
    {
      "epoch": 2.549271110036798,
      "grad_norm": 0.49303245544433594,
      "learning_rate": 1.1608346752738586e-05,
      "loss": 2.3447,
      "step": 123920
    },
    {
      "epoch": 2.549476829553666,
      "grad_norm": 0.5004188418388367,
      "learning_rate": 1.1597962950310137e-05,
      "loss": 2.3709,
      "step": 123930
    },
    {
      "epoch": 2.5496825490705337,
      "grad_norm": 0.4501166045665741,
      "learning_rate": 1.1587583508272426e-05,
      "loss": 2.3607,
      "step": 123940
    },
    {
      "epoch": 2.5498882685874014,
      "grad_norm": 0.5401654839515686,
      "learning_rate": 1.1577208427137409e-05,
      "loss": 2.3714,
      "step": 123950
    },
    {
      "epoch": 2.550093988104269,
      "grad_norm": 0.45240676403045654,
      "learning_rate": 1.1566837707416888e-05,
      "loss": 2.3693,
      "step": 123960
    },
    {
      "epoch": 2.5502997076211367,
      "grad_norm": 0.47536101937294006,
      "learning_rate": 1.1556471349622343e-05,
      "loss": 2.3602,
      "step": 123970
    },
    {
      "epoch": 2.5505054271380043,
      "grad_norm": 0.516424298286438,
      "learning_rate": 1.154610935426511e-05,
      "loss": 2.3846,
      "step": 123980
    },
    {
      "epoch": 2.550711146654872,
      "grad_norm": 0.45679083466529846,
      "learning_rate": 1.153575172185628e-05,
      "loss": 2.3359,
      "step": 123990
    },
    {
      "epoch": 2.5509168661717396,
      "grad_norm": 0.518876314163208,
      "learning_rate": 1.1525398452906754e-05,
      "loss": 2.3507,
      "step": 124000
    },
    {
      "epoch": 2.5511225856886073,
      "grad_norm": 0.45656687021255493,
      "learning_rate": 1.1515049547927192e-05,
      "loss": 2.3561,
      "step": 124010
    },
    {
      "epoch": 2.5513283052054754,
      "grad_norm": 0.45159849524497986,
      "learning_rate": 1.150470500742804e-05,
      "loss": 2.3515,
      "step": 124020
    },
    {
      "epoch": 2.551534024722343,
      "grad_norm": 0.46997272968292236,
      "learning_rate": 1.1494364831919558e-05,
      "loss": 2.4074,
      "step": 124030
    },
    {
      "epoch": 2.5517397442392107,
      "grad_norm": 0.5654414296150208,
      "learning_rate": 1.1484029021911758e-05,
      "loss": 2.3929,
      "step": 124040
    },
    {
      "epoch": 2.5519454637560783,
      "grad_norm": 0.44736525416374207,
      "learning_rate": 1.1473697577914456e-05,
      "loss": 2.3396,
      "step": 124050
    },
    {
      "epoch": 2.552151183272946,
      "grad_norm": 0.45315271615982056,
      "learning_rate": 1.1463370500437242e-05,
      "loss": 2.2785,
      "step": 124060
    },
    {
      "epoch": 2.5523569027898136,
      "grad_norm": 0.4889028072357178,
      "learning_rate": 1.145304778998949e-05,
      "loss": 2.3608,
      "step": 124070
    },
    {
      "epoch": 2.5525626223066817,
      "grad_norm": 0.4869423508644104,
      "learning_rate": 1.1442729447080369e-05,
      "loss": 2.31,
      "step": 124080
    },
    {
      "epoch": 2.5527683418235494,
      "grad_norm": 0.48373493552207947,
      "learning_rate": 1.1432415472218827e-05,
      "loss": 2.3673,
      "step": 124090
    },
    {
      "epoch": 2.552974061340417,
      "grad_norm": 0.4398442506790161,
      "learning_rate": 1.142210586591359e-05,
      "loss": 2.4108,
      "step": 124100
    },
    {
      "epoch": 2.5531797808572847,
      "grad_norm": 0.47388046979904175,
      "learning_rate": 1.1411800628673186e-05,
      "loss": 2.3666,
      "step": 124110
    },
    {
      "epoch": 2.5533855003741524,
      "grad_norm": 0.5130167007446289,
      "learning_rate": 1.1401499761005896e-05,
      "loss": 2.3609,
      "step": 124120
    },
    {
      "epoch": 2.55359121989102,
      "grad_norm": 0.4887523949146271,
      "learning_rate": 1.1391203263419837e-05,
      "loss": 2.3815,
      "step": 124130
    },
    {
      "epoch": 2.5537969394078877,
      "grad_norm": 0.5004167556762695,
      "learning_rate": 1.1380911136422867e-05,
      "loss": 2.3773,
      "step": 124140
    },
    {
      "epoch": 2.5540026589247553,
      "grad_norm": 0.45609205961227417,
      "learning_rate": 1.1370623380522594e-05,
      "loss": 2.3542,
      "step": 124150
    },
    {
      "epoch": 2.554208378441623,
      "grad_norm": 0.46626365184783936,
      "learning_rate": 1.136033999622652e-05,
      "loss": 2.4558,
      "step": 124160
    },
    {
      "epoch": 2.554414097958491,
      "grad_norm": 0.45296573638916016,
      "learning_rate": 1.1350060984041877e-05,
      "loss": 2.3442,
      "step": 124170
    },
    {
      "epoch": 2.5546198174753587,
      "grad_norm": 0.4730859100818634,
      "learning_rate": 1.1339786344475589e-05,
      "loss": 2.367,
      "step": 124180
    },
    {
      "epoch": 2.5548255369922264,
      "grad_norm": 0.4434303939342499,
      "learning_rate": 1.132951607803453e-05,
      "loss": 2.3709,
      "step": 124190
    },
    {
      "epoch": 2.555031256509094,
      "grad_norm": 0.44300055503845215,
      "learning_rate": 1.131925018522526e-05,
      "loss": 2.3303,
      "step": 124200
    },
    {
      "epoch": 2.5552369760259617,
      "grad_norm": 0.4834122061729431,
      "learning_rate": 1.1308988666554087e-05,
      "loss": 2.4096,
      "step": 124210
    },
    {
      "epoch": 2.5554426955428298,
      "grad_norm": 0.5038412809371948,
      "learning_rate": 1.1298731522527228e-05,
      "loss": 2.3947,
      "step": 124220
    },
    {
      "epoch": 2.5556484150596974,
      "grad_norm": 0.813650906085968,
      "learning_rate": 1.1288478753650567e-05,
      "loss": 2.3288,
      "step": 124230
    },
    {
      "epoch": 2.555854134576565,
      "grad_norm": 0.4685155153274536,
      "learning_rate": 1.1278230360429809e-05,
      "loss": 2.3982,
      "step": 124240
    },
    {
      "epoch": 2.5560598540934327,
      "grad_norm": 0.481984943151474,
      "learning_rate": 1.1267986343370517e-05,
      "loss": 2.3384,
      "step": 124250
    },
    {
      "epoch": 2.5562655736103004,
      "grad_norm": 0.4349423348903656,
      "learning_rate": 1.1257746702977901e-05,
      "loss": 2.3587,
      "step": 124260
    },
    {
      "epoch": 2.556471293127168,
      "grad_norm": 0.7687606811523438,
      "learning_rate": 1.124751143975703e-05,
      "loss": 2.3938,
      "step": 124270
    },
    {
      "epoch": 2.5566770126440357,
      "grad_norm": 0.4696478545665741,
      "learning_rate": 1.1237280554212814e-05,
      "loss": 2.3485,
      "step": 124280
    },
    {
      "epoch": 2.5568827321609033,
      "grad_norm": 0.46504542231559753,
      "learning_rate": 1.1227054046849838e-05,
      "loss": 2.3374,
      "step": 124290
    },
    {
      "epoch": 2.557088451677771,
      "grad_norm": 0.4778827130794525,
      "learning_rate": 1.1216831918172533e-05,
      "loss": 2.3102,
      "step": 124300
    },
    {
      "epoch": 2.557294171194639,
      "grad_norm": 0.4741455316543579,
      "learning_rate": 1.1206614168685092e-05,
      "loss": 2.3484,
      "step": 124310
    },
    {
      "epoch": 2.5574998907115067,
      "grad_norm": 0.46259579062461853,
      "learning_rate": 1.1196400798891515e-05,
      "loss": 2.3737,
      "step": 124320
    },
    {
      "epoch": 2.5577056102283744,
      "grad_norm": 0.47062286734580994,
      "learning_rate": 1.1186191809295554e-05,
      "loss": 2.3227,
      "step": 124330
    },
    {
      "epoch": 2.557911329745242,
      "grad_norm": 0.4634329080581665,
      "learning_rate": 1.1175987200400784e-05,
      "loss": 2.3413,
      "step": 124340
    },
    {
      "epoch": 2.5581170492621097,
      "grad_norm": 0.4254350960254669,
      "learning_rate": 1.1165786972710523e-05,
      "loss": 2.3306,
      "step": 124350
    },
    {
      "epoch": 2.558322768778978,
      "grad_norm": 0.4449460208415985,
      "learning_rate": 1.1155591126727915e-05,
      "loss": 2.3282,
      "step": 124360
    },
    {
      "epoch": 2.5585284882958454,
      "grad_norm": 0.46948447823524475,
      "learning_rate": 1.1145399662955846e-05,
      "loss": 2.4056,
      "step": 124370
    },
    {
      "epoch": 2.558734207812713,
      "grad_norm": 0.44161874055862427,
      "learning_rate": 1.1135212581897014e-05,
      "loss": 2.3303,
      "step": 124380
    },
    {
      "epoch": 2.5589399273295808,
      "grad_norm": 0.42714086174964905,
      "learning_rate": 1.1125029884053883e-05,
      "loss": 2.3339,
      "step": 124390
    },
    {
      "epoch": 2.5591456468464484,
      "grad_norm": 0.47308921813964844,
      "learning_rate": 1.1114851569928731e-05,
      "loss": 2.4176,
      "step": 124400
    },
    {
      "epoch": 2.559351366363316,
      "grad_norm": 0.4357089102268219,
      "learning_rate": 1.1104677640023575e-05,
      "loss": 2.3345,
      "step": 124410
    },
    {
      "epoch": 2.5595570858801837,
      "grad_norm": 0.44878000020980835,
      "learning_rate": 1.109450809484025e-05,
      "loss": 2.3476,
      "step": 124420
    },
    {
      "epoch": 2.5597628053970514,
      "grad_norm": 0.45106297731399536,
      "learning_rate": 1.1084342934880387e-05,
      "loss": 2.3541,
      "step": 124430
    },
    {
      "epoch": 2.559968524913919,
      "grad_norm": 0.5004008412361145,
      "learning_rate": 1.1074182160645308e-05,
      "loss": 2.3265,
      "step": 124440
    },
    {
      "epoch": 2.560174244430787,
      "grad_norm": 0.4572674036026001,
      "learning_rate": 1.1064025772636266e-05,
      "loss": 2.4302,
      "step": 124450
    },
    {
      "epoch": 2.5603799639476548,
      "grad_norm": 0.48212572932243347,
      "learning_rate": 1.1053873771354206e-05,
      "loss": 2.3982,
      "step": 124460
    },
    {
      "epoch": 2.5605856834645224,
      "grad_norm": 0.4861917793750763,
      "learning_rate": 1.1043726157299805e-05,
      "loss": 2.3496,
      "step": 124470
    },
    {
      "epoch": 2.56079140298139,
      "grad_norm": 0.49995312094688416,
      "learning_rate": 1.1033582930973673e-05,
      "loss": 2.3746,
      "step": 124480
    },
    {
      "epoch": 2.5609971224982577,
      "grad_norm": 0.43059512972831726,
      "learning_rate": 1.1023444092876101e-05,
      "loss": 2.3489,
      "step": 124490
    },
    {
      "epoch": 2.5612028420151254,
      "grad_norm": 0.4913744628429413,
      "learning_rate": 1.101330964350712e-05,
      "loss": 2.3665,
      "step": 124500
    },
    {
      "epoch": 2.5614085615319935,
      "grad_norm": 0.46327322721481323,
      "learning_rate": 1.1003179583366708e-05,
      "loss": 2.3927,
      "step": 124510
    },
    {
      "epoch": 2.561614281048861,
      "grad_norm": 0.5162031054496765,
      "learning_rate": 1.0993053912954443e-05,
      "loss": 2.3734,
      "step": 124520
    },
    {
      "epoch": 2.561820000565729,
      "grad_norm": 0.4992488622665405,
      "learning_rate": 1.0982932632769782e-05,
      "loss": 2.323,
      "step": 124530
    },
    {
      "epoch": 2.5620257200825964,
      "grad_norm": 0.4540663957595825,
      "learning_rate": 1.0972815743312015e-05,
      "loss": 2.3716,
      "step": 124540
    },
    {
      "epoch": 2.562231439599464,
      "grad_norm": 0.4244915246963501,
      "learning_rate": 1.0962703245080075e-05,
      "loss": 2.3767,
      "step": 124550
    },
    {
      "epoch": 2.5624371591163317,
      "grad_norm": 0.465962678194046,
      "learning_rate": 1.0952595138572785e-05,
      "loss": 2.3682,
      "step": 124560
    },
    {
      "epoch": 2.5626428786331994,
      "grad_norm": 0.4746280312538147,
      "learning_rate": 1.0942491424288759e-05,
      "loss": 2.3219,
      "step": 124570
    },
    {
      "epoch": 2.562848598150067,
      "grad_norm": 0.41334232687950134,
      "learning_rate": 1.0932392102726296e-05,
      "loss": 2.3659,
      "step": 124580
    },
    {
      "epoch": 2.5630543176669347,
      "grad_norm": 0.43202298879623413,
      "learning_rate": 1.0922297174383578e-05,
      "loss": 2.3969,
      "step": 124590
    },
    {
      "epoch": 2.563260037183803,
      "grad_norm": 0.5110010504722595,
      "learning_rate": 1.0912206639758527e-05,
      "loss": 2.3993,
      "step": 124600
    },
    {
      "epoch": 2.5634657567006705,
      "grad_norm": 0.4308154582977295,
      "learning_rate": 1.0902120499348855e-05,
      "loss": 2.3522,
      "step": 124610
    },
    {
      "epoch": 2.563671476217538,
      "grad_norm": 0.45421984791755676,
      "learning_rate": 1.0892038753652045e-05,
      "loss": 2.3319,
      "step": 124620
    },
    {
      "epoch": 2.5638771957344058,
      "grad_norm": 0.43600666522979736,
      "learning_rate": 1.0881961403165398e-05,
      "loss": 2.3193,
      "step": 124630
    },
    {
      "epoch": 2.5640829152512734,
      "grad_norm": 0.466823011636734,
      "learning_rate": 1.0871888448385948e-05,
      "loss": 2.3998,
      "step": 124640
    },
    {
      "epoch": 2.5642886347681415,
      "grad_norm": 0.46112051606178284,
      "learning_rate": 1.0861819889810554e-05,
      "loss": 2.3126,
      "step": 124650
    },
    {
      "epoch": 2.564494354285009,
      "grad_norm": 0.482403427362442,
      "learning_rate": 1.085175572793583e-05,
      "loss": 2.3474,
      "step": 124660
    },
    {
      "epoch": 2.564700073801877,
      "grad_norm": 0.48007872700691223,
      "learning_rate": 1.0841695963258214e-05,
      "loss": 2.4165,
      "step": 124670
    },
    {
      "epoch": 2.5649057933187445,
      "grad_norm": 0.4479542374610901,
      "learning_rate": 1.0831640596273874e-05,
      "loss": 2.3345,
      "step": 124680
    },
    {
      "epoch": 2.565111512835612,
      "grad_norm": 0.4416768550872803,
      "learning_rate": 1.082158962747879e-05,
      "loss": 2.3776,
      "step": 124690
    },
    {
      "epoch": 2.5653172323524798,
      "grad_norm": 0.4603964388370514,
      "learning_rate": 1.0811543057368722e-05,
      "loss": 2.3688,
      "step": 124700
    },
    {
      "epoch": 2.5655229518693474,
      "grad_norm": 0.44932007789611816,
      "learning_rate": 1.080150088643923e-05,
      "loss": 2.3704,
      "step": 124710
    },
    {
      "epoch": 2.565728671386215,
      "grad_norm": 0.4690653085708618,
      "learning_rate": 1.0791463115185619e-05,
      "loss": 2.4071,
      "step": 124720
    },
    {
      "epoch": 2.5659343909030827,
      "grad_norm": 0.4495779275894165,
      "learning_rate": 1.0781429744103e-05,
      "loss": 2.3368,
      "step": 124730
    },
    {
      "epoch": 2.566140110419951,
      "grad_norm": 0.4647497832775116,
      "learning_rate": 1.0771400773686269e-05,
      "loss": 2.3833,
      "step": 124740
    },
    {
      "epoch": 2.5663458299368185,
      "grad_norm": 0.4597521722316742,
      "learning_rate": 1.0761376204430129e-05,
      "loss": 2.3455,
      "step": 124750
    },
    {
      "epoch": 2.566551549453686,
      "grad_norm": 0.4141136407852173,
      "learning_rate": 1.0751356036828963e-05,
      "loss": 2.3521,
      "step": 124760
    },
    {
      "epoch": 2.566757268970554,
      "grad_norm": 0.4626150131225586,
      "learning_rate": 1.0741340271377075e-05,
      "loss": 2.3404,
      "step": 124770
    },
    {
      "epoch": 2.5669629884874214,
      "grad_norm": 0.453426331281662,
      "learning_rate": 1.0731328908568495e-05,
      "loss": 2.3835,
      "step": 124780
    },
    {
      "epoch": 2.567168708004289,
      "grad_norm": 0.4673781394958496,
      "learning_rate": 1.0721321948896956e-05,
      "loss": 2.3855,
      "step": 124790
    },
    {
      "epoch": 2.567374427521157,
      "grad_norm": 0.4523102641105652,
      "learning_rate": 1.0711319392856123e-05,
      "loss": 2.3643,
      "step": 124800
    },
    {
      "epoch": 2.567580147038025,
      "grad_norm": 0.4864935576915741,
      "learning_rate": 1.0701321240939355e-05,
      "loss": 2.3283,
      "step": 124810
    },
    {
      "epoch": 2.5677858665548925,
      "grad_norm": 0.49322712421417236,
      "learning_rate": 1.0691327493639757e-05,
      "loss": 2.3946,
      "step": 124820
    },
    {
      "epoch": 2.56799158607176,
      "grad_norm": 0.46538978815078735,
      "learning_rate": 1.0681338151450326e-05,
      "loss": 2.3897,
      "step": 124830
    },
    {
      "epoch": 2.568197305588628,
      "grad_norm": 0.44113636016845703,
      "learning_rate": 1.0671353214863755e-05,
      "loss": 2.3895,
      "step": 124840
    },
    {
      "epoch": 2.5684030251054955,
      "grad_norm": 0.4761042296886444,
      "learning_rate": 1.0661372684372517e-05,
      "loss": 2.3602,
      "step": 124850
    },
    {
      "epoch": 2.568608744622363,
      "grad_norm": 0.445231169462204,
      "learning_rate": 1.0651396560468962e-05,
      "loss": 2.3225,
      "step": 124860
    },
    {
      "epoch": 2.5688144641392308,
      "grad_norm": 0.5306617617607117,
      "learning_rate": 1.0641424843645109e-05,
      "loss": 2.3178,
      "step": 124870
    },
    {
      "epoch": 2.5690201836560984,
      "grad_norm": 0.44042184948921204,
      "learning_rate": 1.0631457534392807e-05,
      "loss": 2.3954,
      "step": 124880
    },
    {
      "epoch": 2.5692259031729665,
      "grad_norm": 0.4655842185020447,
      "learning_rate": 1.0621494633203732e-05,
      "loss": 2.3725,
      "step": 124890
    },
    {
      "epoch": 2.569431622689834,
      "grad_norm": 0.43521934747695923,
      "learning_rate": 1.0611536140569266e-05,
      "loss": 2.3215,
      "step": 124900
    },
    {
      "epoch": 2.569637342206702,
      "grad_norm": 0.47145015001296997,
      "learning_rate": 1.0601582056980609e-05,
      "loss": 2.3845,
      "step": 124910
    },
    {
      "epoch": 2.5698430617235695,
      "grad_norm": 0.4407939910888672,
      "learning_rate": 1.059163238292874e-05,
      "loss": 2.3647,
      "step": 124920
    },
    {
      "epoch": 2.570048781240437,
      "grad_norm": 0.4783744513988495,
      "learning_rate": 1.0581687118904437e-05,
      "loss": 2.3716,
      "step": 124930
    },
    {
      "epoch": 2.570254500757305,
      "grad_norm": 0.4546763300895691,
      "learning_rate": 1.057174626539823e-05,
      "loss": 2.3538,
      "step": 124940
    },
    {
      "epoch": 2.570460220274173,
      "grad_norm": 0.46203407645225525,
      "learning_rate": 1.056180982290046e-05,
      "loss": 2.3394,
      "step": 124950
    },
    {
      "epoch": 2.5706659397910405,
      "grad_norm": 0.4245845377445221,
      "learning_rate": 1.0551877791901221e-05,
      "loss": 2.3357,
      "step": 124960
    },
    {
      "epoch": 2.570871659307908,
      "grad_norm": 0.4509775936603546,
      "learning_rate": 1.0541950172890436e-05,
      "loss": 2.3145,
      "step": 124970
    },
    {
      "epoch": 2.571077378824776,
      "grad_norm": 0.43411168456077576,
      "learning_rate": 1.0532026966357744e-05,
      "loss": 2.3067,
      "step": 124980
    },
    {
      "epoch": 2.5712830983416435,
      "grad_norm": 0.5390877723693848,
      "learning_rate": 1.0522108172792621e-05,
      "loss": 2.3798,
      "step": 124990
    },
    {
      "epoch": 2.571488817858511,
      "grad_norm": 0.4228973686695099,
      "learning_rate": 1.0512193792684311e-05,
      "loss": 2.3587,
      "step": 125000
    },
    {
      "epoch": 2.571694537375379,
      "grad_norm": 0.5256722569465637,
      "learning_rate": 1.050228382652183e-05,
      "loss": 2.3223,
      "step": 125010
    },
    {
      "epoch": 2.5719002568922464,
      "grad_norm": 0.46187371015548706,
      "learning_rate": 1.049237827479399e-05,
      "loss": 2.3813,
      "step": 125020
    },
    {
      "epoch": 2.5721059764091145,
      "grad_norm": 0.4384763836860657,
      "learning_rate": 1.0482477137989365e-05,
      "loss": 2.3667,
      "step": 125030
    },
    {
      "epoch": 2.572311695925982,
      "grad_norm": 0.47004178166389465,
      "learning_rate": 1.0472580416596334e-05,
      "loss": 2.3738,
      "step": 125040
    },
    {
      "epoch": 2.57251741544285,
      "grad_norm": 0.4274337887763977,
      "learning_rate": 1.0462688111103036e-05,
      "loss": 2.3993,
      "step": 125050
    },
    {
      "epoch": 2.5727231349597175,
      "grad_norm": 0.4095109701156616,
      "learning_rate": 1.0452800221997428e-05,
      "loss": 2.3209,
      "step": 125060
    },
    {
      "epoch": 2.572928854476585,
      "grad_norm": 0.4853609502315521,
      "learning_rate": 1.0442916749767217e-05,
      "loss": 2.3805,
      "step": 125070
    },
    {
      "epoch": 2.5731345739934532,
      "grad_norm": 0.4727697968482971,
      "learning_rate": 1.0433037694899871e-05,
      "loss": 2.3789,
      "step": 125080
    },
    {
      "epoch": 2.573340293510321,
      "grad_norm": 0.5471944808959961,
      "learning_rate": 1.04231630578827e-05,
      "loss": 2.3296,
      "step": 125090
    },
    {
      "epoch": 2.5735460130271886,
      "grad_norm": 0.4719688892364502,
      "learning_rate": 1.0413292839202782e-05,
      "loss": 2.3293,
      "step": 125100
    },
    {
      "epoch": 2.573751732544056,
      "grad_norm": 0.4523111879825592,
      "learning_rate": 1.0403427039346903e-05,
      "loss": 2.4168,
      "step": 125110
    },
    {
      "epoch": 2.573957452060924,
      "grad_norm": 0.514836847782135,
      "learning_rate": 1.0393565658801751e-05,
      "loss": 2.3602,
      "step": 125120
    },
    {
      "epoch": 2.5741631715777915,
      "grad_norm": 0.4215203821659088,
      "learning_rate": 1.0383708698053718e-05,
      "loss": 2.425,
      "step": 125130
    },
    {
      "epoch": 2.574368891094659,
      "grad_norm": 0.41175660490989685,
      "learning_rate": 1.0373856157588957e-05,
      "loss": 2.428,
      "step": 125140
    },
    {
      "epoch": 2.574574610611527,
      "grad_norm": 0.4620704650878906,
      "learning_rate": 1.036400803789349e-05,
      "loss": 2.3722,
      "step": 125150
    },
    {
      "epoch": 2.5747803301283945,
      "grad_norm": 0.4196467101573944,
      "learning_rate": 1.0354164339453043e-05,
      "loss": 2.4074,
      "step": 125160
    },
    {
      "epoch": 2.5749860496452626,
      "grad_norm": 0.4849624037742615,
      "learning_rate": 1.0344325062753136e-05,
      "loss": 2.2996,
      "step": 125170
    },
    {
      "epoch": 2.57519176916213,
      "grad_norm": 0.4334719777107239,
      "learning_rate": 1.0334490208279146e-05,
      "loss": 2.3227,
      "step": 125180
    },
    {
      "epoch": 2.575397488678998,
      "grad_norm": 0.4493933916091919,
      "learning_rate": 1.0324659776516122e-05,
      "loss": 2.3769,
      "step": 125190
    },
    {
      "epoch": 2.5756032081958655,
      "grad_norm": 0.4543919265270233,
      "learning_rate": 1.0314833767948929e-05,
      "loss": 2.3694,
      "step": 125200
    },
    {
      "epoch": 2.575808927712733,
      "grad_norm": 0.5920528173446655,
      "learning_rate": 1.0305012183062302e-05,
      "loss": 2.3763,
      "step": 125210
    },
    {
      "epoch": 2.576014647229601,
      "grad_norm": 0.4945984184741974,
      "learning_rate": 1.0295195022340632e-05,
      "loss": 2.3506,
      "step": 125220
    },
    {
      "epoch": 2.576220366746469,
      "grad_norm": 0.4648466408252716,
      "learning_rate": 1.0285382286268153e-05,
      "loss": 2.3839,
      "step": 125230
    },
    {
      "epoch": 2.5764260862633366,
      "grad_norm": 0.4672791659832001,
      "learning_rate": 1.0275573975328879e-05,
      "loss": 2.3264,
      "step": 125240
    },
    {
      "epoch": 2.5766318057802042,
      "grad_norm": 0.4494273364543915,
      "learning_rate": 1.026577009000661e-05,
      "loss": 2.3494,
      "step": 125250
    },
    {
      "epoch": 2.576837525297072,
      "grad_norm": 0.4522291123867035,
      "learning_rate": 1.0255970630784905e-05,
      "loss": 2.3143,
      "step": 125260
    },
    {
      "epoch": 2.5770432448139395,
      "grad_norm": 0.4776515066623688,
      "learning_rate": 1.0246175598147112e-05,
      "loss": 2.3217,
      "step": 125270
    },
    {
      "epoch": 2.577248964330807,
      "grad_norm": 0.41236722469329834,
      "learning_rate": 1.0236384992576386e-05,
      "loss": 2.3331,
      "step": 125280
    },
    {
      "epoch": 2.577454683847675,
      "grad_norm": 0.43223437666893005,
      "learning_rate": 1.0226598814555643e-05,
      "loss": 2.3333,
      "step": 125290
    },
    {
      "epoch": 2.5776604033645425,
      "grad_norm": 0.43517225980758667,
      "learning_rate": 1.0216817064567563e-05,
      "loss": 2.3647,
      "step": 125300
    },
    {
      "epoch": 2.57786612288141,
      "grad_norm": 0.4612710177898407,
      "learning_rate": 1.0207039743094638e-05,
      "loss": 2.3801,
      "step": 125310
    },
    {
      "epoch": 2.5780718423982782,
      "grad_norm": 0.5108632445335388,
      "learning_rate": 1.0197266850619136e-05,
      "loss": 2.3446,
      "step": 125320
    },
    {
      "epoch": 2.578277561915146,
      "grad_norm": 0.45961540937423706,
      "learning_rate": 1.0187498387623095e-05,
      "loss": 2.4004,
      "step": 125330
    },
    {
      "epoch": 2.5784832814320136,
      "grad_norm": 0.45267367362976074,
      "learning_rate": 1.0177734354588331e-05,
      "loss": 2.3919,
      "step": 125340
    },
    {
      "epoch": 2.578689000948881,
      "grad_norm": 0.44847211241722107,
      "learning_rate": 1.0167974751996468e-05,
      "loss": 2.3999,
      "step": 125350
    },
    {
      "epoch": 2.578894720465749,
      "grad_norm": 0.4242684245109558,
      "learning_rate": 1.0158219580328877e-05,
      "loss": 2.3166,
      "step": 125360
    },
    {
      "epoch": 2.579100439982617,
      "grad_norm": 0.48855650424957275,
      "learning_rate": 1.0148468840066739e-05,
      "loss": 2.3374,
      "step": 125370
    },
    {
      "epoch": 2.5793061594994846,
      "grad_norm": 0.46895185112953186,
      "learning_rate": 1.0138722531690992e-05,
      "loss": 2.4258,
      "step": 125380
    },
    {
      "epoch": 2.5795118790163523,
      "grad_norm": 0.4778262972831726,
      "learning_rate": 1.0128980655682407e-05,
      "loss": 2.3229,
      "step": 125390
    },
    {
      "epoch": 2.57971759853322,
      "grad_norm": 0.4663955271244049,
      "learning_rate": 1.0119243212521423e-05,
      "loss": 2.3349,
      "step": 125400
    },
    {
      "epoch": 2.5799233180500876,
      "grad_norm": 0.46986129879951477,
      "learning_rate": 1.010951020268841e-05,
      "loss": 2.3137,
      "step": 125410
    },
    {
      "epoch": 2.580129037566955,
      "grad_norm": 0.45236867666244507,
      "learning_rate": 1.0099781626663419e-05,
      "loss": 2.428,
      "step": 125420
    },
    {
      "epoch": 2.580334757083823,
      "grad_norm": 0.4205242693424225,
      "learning_rate": 1.0090057484926273e-05,
      "loss": 2.3779,
      "step": 125430
    },
    {
      "epoch": 2.5805404766006905,
      "grad_norm": 0.4681990146636963,
      "learning_rate": 1.008033777795666e-05,
      "loss": 2.3359,
      "step": 125440
    },
    {
      "epoch": 2.580746196117558,
      "grad_norm": 0.4959106743335724,
      "learning_rate": 1.0070622506233996e-05,
      "loss": 2.3682,
      "step": 125450
    },
    {
      "epoch": 2.5809519156344263,
      "grad_norm": 0.4617862403392792,
      "learning_rate": 1.0060911670237438e-05,
      "loss": 2.3588,
      "step": 125460
    },
    {
      "epoch": 2.581157635151294,
      "grad_norm": 0.42170941829681396,
      "learning_rate": 1.005120527044603e-05,
      "loss": 2.3761,
      "step": 125470
    },
    {
      "epoch": 2.5813633546681616,
      "grad_norm": 0.4746762812137604,
      "learning_rate": 1.0041503307338485e-05,
      "loss": 2.4006,
      "step": 125480
    },
    {
      "epoch": 2.5815690741850292,
      "grad_norm": 0.47841334342956543,
      "learning_rate": 1.0031805781393343e-05,
      "loss": 2.3662,
      "step": 125490
    },
    {
      "epoch": 2.581774793701897,
      "grad_norm": 0.46419161558151245,
      "learning_rate": 1.0022112693089003e-05,
      "loss": 2.3758,
      "step": 125500
    },
    {
      "epoch": 2.581980513218765,
      "grad_norm": 0.5174804329872131,
      "learning_rate": 1.001242404290349e-05,
      "loss": 2.4103,
      "step": 125510
    },
    {
      "epoch": 2.5821862327356326,
      "grad_norm": 0.45395177602767944,
      "learning_rate": 1.0002739831314711e-05,
      "loss": 2.2993,
      "step": 125520
    },
    {
      "epoch": 2.5823919522525003,
      "grad_norm": 0.45611581206321716,
      "learning_rate": 9.993060058800397e-06,
      "loss": 2.3501,
      "step": 125530
    },
    {
      "epoch": 2.582597671769368,
      "grad_norm": 0.4680735468864441,
      "learning_rate": 9.98338472583793e-06,
      "loss": 2.3852,
      "step": 125540
    },
    {
      "epoch": 2.5828033912862356,
      "grad_norm": 0.509242057800293,
      "learning_rate": 9.973713832904553e-06,
      "loss": 2.3736,
      "step": 125550
    },
    {
      "epoch": 2.5830091108031032,
      "grad_norm": 0.4561865031719208,
      "learning_rate": 9.964047380477293e-06,
      "loss": 2.4694,
      "step": 125560
    },
    {
      "epoch": 2.583214830319971,
      "grad_norm": 0.45165562629699707,
      "learning_rate": 9.954385369032948e-06,
      "loss": 2.3694,
      "step": 125570
    },
    {
      "epoch": 2.5834205498368386,
      "grad_norm": 0.5529047250747681,
      "learning_rate": 9.94472779904807e-06,
      "loss": 2.3702,
      "step": 125580
    },
    {
      "epoch": 2.583626269353706,
      "grad_norm": 0.46526819467544556,
      "learning_rate": 9.935074670999034e-06,
      "loss": 2.4087,
      "step": 125590
    },
    {
      "epoch": 2.5838319888705743,
      "grad_norm": 0.47083401679992676,
      "learning_rate": 9.925425985361969e-06,
      "loss": 2.3445,
      "step": 125600
    },
    {
      "epoch": 2.584037708387442,
      "grad_norm": 0.4545951783657074,
      "learning_rate": 9.915781742612795e-06,
      "loss": 2.3376,
      "step": 125610
    },
    {
      "epoch": 2.5842434279043096,
      "grad_norm": 0.4738485515117645,
      "learning_rate": 9.906141943227198e-06,
      "loss": 2.3735,
      "step": 125620
    },
    {
      "epoch": 2.5844491474211773,
      "grad_norm": 0.4415552616119385,
      "learning_rate": 9.896506587680677e-06,
      "loss": 2.3904,
      "step": 125630
    },
    {
      "epoch": 2.584654866938045,
      "grad_norm": 0.4594633877277374,
      "learning_rate": 9.88687567644847e-06,
      "loss": 2.3533,
      "step": 125640
    },
    {
      "epoch": 2.5848605864549126,
      "grad_norm": 0.4702279269695282,
      "learning_rate": 9.877249210005634e-06,
      "loss": 2.4037,
      "step": 125650
    },
    {
      "epoch": 2.5850663059717807,
      "grad_norm": 0.4558553099632263,
      "learning_rate": 9.867627188826989e-06,
      "loss": 2.3541,
      "step": 125660
    },
    {
      "epoch": 2.5852720254886483,
      "grad_norm": 0.4929531514644623,
      "learning_rate": 9.85800961338713e-06,
      "loss": 2.4083,
      "step": 125670
    },
    {
      "epoch": 2.585477745005516,
      "grad_norm": 0.5690403580665588,
      "learning_rate": 9.848396484160461e-06,
      "loss": 2.359,
      "step": 125680
    },
    {
      "epoch": 2.5856834645223836,
      "grad_norm": 0.4484615921974182,
      "learning_rate": 9.838787801621074e-06,
      "loss": 2.3955,
      "step": 125690
    },
    {
      "epoch": 2.5858891840392513,
      "grad_norm": 0.4903780519962311,
      "learning_rate": 9.829183566243005e-06,
      "loss": 2.3543,
      "step": 125700
    },
    {
      "epoch": 2.586094903556119,
      "grad_norm": 0.4740254878997803,
      "learning_rate": 9.819583778499942e-06,
      "loss": 2.369,
      "step": 125710
    },
    {
      "epoch": 2.5863006230729866,
      "grad_norm": 0.526128351688385,
      "learning_rate": 9.80998843886536e-06,
      "loss": 2.3282,
      "step": 125720
    },
    {
      "epoch": 2.5865063425898542,
      "grad_norm": 0.48943671584129333,
      "learning_rate": 9.800397547812589e-06,
      "loss": 2.3152,
      "step": 125730
    },
    {
      "epoch": 2.586712062106722,
      "grad_norm": 0.45395246148109436,
      "learning_rate": 9.79081110581469e-06,
      "loss": 2.3975,
      "step": 125740
    },
    {
      "epoch": 2.58691778162359,
      "grad_norm": 0.44371455907821655,
      "learning_rate": 9.781229113344458e-06,
      "loss": 2.328,
      "step": 125750
    },
    {
      "epoch": 2.5871235011404576,
      "grad_norm": 0.542472243309021,
      "learning_rate": 9.771651570874607e-06,
      "loss": 2.3997,
      "step": 125760
    },
    {
      "epoch": 2.5873292206573253,
      "grad_norm": 0.4655880331993103,
      "learning_rate": 9.762078478877479e-06,
      "loss": 2.4063,
      "step": 125770
    },
    {
      "epoch": 2.587534940174193,
      "grad_norm": 0.46129462122917175,
      "learning_rate": 9.752509837825264e-06,
      "loss": 2.3701,
      "step": 125780
    },
    {
      "epoch": 2.5877406596910606,
      "grad_norm": 0.42015767097473145,
      "learning_rate": 9.742945648189983e-06,
      "loss": 2.342,
      "step": 125790
    },
    {
      "epoch": 2.5879463792079287,
      "grad_norm": 0.44943714141845703,
      "learning_rate": 9.733385910443338e-06,
      "loss": 2.3234,
      "step": 125800
    },
    {
      "epoch": 2.5881520987247963,
      "grad_norm": 0.45155957341194153,
      "learning_rate": 9.72383062505684e-06,
      "loss": 2.3827,
      "step": 125810
    },
    {
      "epoch": 2.588357818241664,
      "grad_norm": 0.4779845178127289,
      "learning_rate": 9.71427979250188e-06,
      "loss": 2.3995,
      "step": 125820
    },
    {
      "epoch": 2.5885635377585317,
      "grad_norm": 0.5875856876373291,
      "learning_rate": 9.704733413249478e-06,
      "loss": 2.4312,
      "step": 125830
    },
    {
      "epoch": 2.5887692572753993,
      "grad_norm": 0.45589691400527954,
      "learning_rate": 9.695191487770528e-06,
      "loss": 2.3699,
      "step": 125840
    },
    {
      "epoch": 2.588974976792267,
      "grad_norm": 0.47616860270500183,
      "learning_rate": 9.685654016535684e-06,
      "loss": 2.3428,
      "step": 125850
    },
    {
      "epoch": 2.5891806963091346,
      "grad_norm": 0.4513411223888397,
      "learning_rate": 9.67612100001536e-06,
      "loss": 2.368,
      "step": 125860
    },
    {
      "epoch": 2.5893864158260023,
      "grad_norm": 0.47229546308517456,
      "learning_rate": 9.66659243867979e-06,
      "loss": 2.3609,
      "step": 125870
    },
    {
      "epoch": 2.58959213534287,
      "grad_norm": 0.45627638697624207,
      "learning_rate": 9.657068332998954e-06,
      "loss": 2.3335,
      "step": 125880
    },
    {
      "epoch": 2.589797854859738,
      "grad_norm": 0.44992443919181824,
      "learning_rate": 9.647548683442632e-06,
      "loss": 2.3929,
      "step": 125890
    },
    {
      "epoch": 2.5900035743766057,
      "grad_norm": 0.47349151968955994,
      "learning_rate": 9.63803349048037e-06,
      "loss": 2.3706,
      "step": 125900
    },
    {
      "epoch": 2.5902092938934733,
      "grad_norm": 0.4746229350566864,
      "learning_rate": 9.628522754581493e-06,
      "loss": 2.4024,
      "step": 125910
    },
    {
      "epoch": 2.590415013410341,
      "grad_norm": 0.4791712462902069,
      "learning_rate": 9.619016476215138e-06,
      "loss": 2.3897,
      "step": 125920
    },
    {
      "epoch": 2.5906207329272086,
      "grad_norm": 0.4553873538970947,
      "learning_rate": 9.60951465585017e-06,
      "loss": 2.3388,
      "step": 125930
    },
    {
      "epoch": 2.5908264524440763,
      "grad_norm": 0.45049992203712463,
      "learning_rate": 9.600017293955288e-06,
      "loss": 2.3998,
      "step": 125940
    },
    {
      "epoch": 2.5910321719609444,
      "grad_norm": 0.4625994861125946,
      "learning_rate": 9.590524390998923e-06,
      "loss": 2.386,
      "step": 125950
    },
    {
      "epoch": 2.591237891477812,
      "grad_norm": 0.42809176445007324,
      "learning_rate": 9.581035947449312e-06,
      "loss": 2.351,
      "step": 125960
    },
    {
      "epoch": 2.5914436109946797,
      "grad_norm": 0.47361135482788086,
      "learning_rate": 9.571551963774483e-06,
      "loss": 2.3783,
      "step": 125970
    },
    {
      "epoch": 2.5916493305115473,
      "grad_norm": 0.4527515172958374,
      "learning_rate": 9.562072440442216e-06,
      "loss": 2.3054,
      "step": 125980
    },
    {
      "epoch": 2.591855050028415,
      "grad_norm": 0.4520817995071411,
      "learning_rate": 9.552597377920091e-06,
      "loss": 2.3395,
      "step": 125990
    },
    {
      "epoch": 2.5920607695452826,
      "grad_norm": 0.4506899118423462,
      "learning_rate": 9.543126776675471e-06,
      "loss": 2.3649,
      "step": 126000
    },
    {
      "epoch": 2.5922664890621503,
      "grad_norm": 0.5248252749443054,
      "learning_rate": 9.533660637175446e-06,
      "loss": 2.3524,
      "step": 126010
    },
    {
      "epoch": 2.592472208579018,
      "grad_norm": 0.4698442816734314,
      "learning_rate": 9.524198959886976e-06,
      "loss": 2.4023,
      "step": 126020
    },
    {
      "epoch": 2.5926779280958856,
      "grad_norm": 0.4195488691329956,
      "learning_rate": 9.514741745276756e-06,
      "loss": 2.3732,
      "step": 126030
    },
    {
      "epoch": 2.5928836476127537,
      "grad_norm": 0.44981685280799866,
      "learning_rate": 9.505288993811201e-06,
      "loss": 2.3762,
      "step": 126040
    },
    {
      "epoch": 2.5930893671296213,
      "grad_norm": 0.4654384255409241,
      "learning_rate": 9.495840705956616e-06,
      "loss": 2.4019,
      "step": 126050
    },
    {
      "epoch": 2.593295086646489,
      "grad_norm": 0.46050962805747986,
      "learning_rate": 9.48639688217905e-06,
      "loss": 2.3848,
      "step": 126060
    },
    {
      "epoch": 2.5935008061633567,
      "grad_norm": 0.4258265495300293,
      "learning_rate": 9.47695752294424e-06,
      "loss": 2.3768,
      "step": 126070
    },
    {
      "epoch": 2.5937065256802243,
      "grad_norm": 0.4574105739593506,
      "learning_rate": 9.467522628717861e-06,
      "loss": 2.3844,
      "step": 126080
    },
    {
      "epoch": 2.5939122451970924,
      "grad_norm": 0.5200092196464539,
      "learning_rate": 9.458092199965241e-06,
      "loss": 2.4044,
      "step": 126090
    },
    {
      "epoch": 2.59411796471396,
      "grad_norm": 0.49816012382507324,
      "learning_rate": 9.448666237151504e-06,
      "loss": 2.35,
      "step": 126100
    },
    {
      "epoch": 2.5943236842308277,
      "grad_norm": 0.4326419532299042,
      "learning_rate": 9.43924474074167e-06,
      "loss": 2.3679,
      "step": 126110
    },
    {
      "epoch": 2.5945294037476954,
      "grad_norm": 0.45863768458366394,
      "learning_rate": 9.429827711200367e-06,
      "loss": 2.3522,
      "step": 126120
    },
    {
      "epoch": 2.594735123264563,
      "grad_norm": 0.4881511926651001,
      "learning_rate": 9.42041514899209e-06,
      "loss": 2.3343,
      "step": 126130
    },
    {
      "epoch": 2.5949408427814307,
      "grad_norm": 0.42817848920822144,
      "learning_rate": 9.411007054581178e-06,
      "loss": 2.3577,
      "step": 126140
    },
    {
      "epoch": 2.5951465622982983,
      "grad_norm": 0.49465057253837585,
      "learning_rate": 9.401603428431626e-06,
      "loss": 2.3998,
      "step": 126150
    },
    {
      "epoch": 2.595352281815166,
      "grad_norm": 0.44675010442733765,
      "learning_rate": 9.392204271007265e-06,
      "loss": 2.3815,
      "step": 126160
    },
    {
      "epoch": 2.5955580013320336,
      "grad_norm": 0.4585547149181366,
      "learning_rate": 9.382809582771723e-06,
      "loss": 2.326,
      "step": 126170
    },
    {
      "epoch": 2.5957637208489017,
      "grad_norm": 0.45517152547836304,
      "learning_rate": 9.373419364188374e-06,
      "loss": 2.4052,
      "step": 126180
    },
    {
      "epoch": 2.5959694403657694,
      "grad_norm": 0.47976407408714294,
      "learning_rate": 9.364033615720391e-06,
      "loss": 2.385,
      "step": 126190
    },
    {
      "epoch": 2.596175159882637,
      "grad_norm": 0.4853626489639282,
      "learning_rate": 9.354652337830726e-06,
      "loss": 2.3445,
      "step": 126200
    },
    {
      "epoch": 2.5963808793995047,
      "grad_norm": 0.4289165437221527,
      "learning_rate": 9.3452755309821e-06,
      "loss": 2.3438,
      "step": 126210
    },
    {
      "epoch": 2.5965865989163723,
      "grad_norm": 0.4527542293071747,
      "learning_rate": 9.33590319563702e-06,
      "loss": 2.3187,
      "step": 126220
    },
    {
      "epoch": 2.5967923184332404,
      "grad_norm": 0.45340001583099365,
      "learning_rate": 9.32653533225778e-06,
      "loss": 2.3617,
      "step": 126230
    },
    {
      "epoch": 2.596998037950108,
      "grad_norm": 0.47883346676826477,
      "learning_rate": 9.317171941306446e-06,
      "loss": 2.3404,
      "step": 126240
    },
    {
      "epoch": 2.5972037574669757,
      "grad_norm": 0.47062674164772034,
      "learning_rate": 9.307813023244849e-06,
      "loss": 2.3881,
      "step": 126250
    },
    {
      "epoch": 2.5974094769838434,
      "grad_norm": 0.4556032717227936,
      "learning_rate": 9.29845857853463e-06,
      "loss": 2.3796,
      "step": 126260
    },
    {
      "epoch": 2.597615196500711,
      "grad_norm": 0.4581291973590851,
      "learning_rate": 9.289108607637176e-06,
      "loss": 2.368,
      "step": 126270
    },
    {
      "epoch": 2.5978209160175787,
      "grad_norm": 0.44256117939949036,
      "learning_rate": 9.279763111013684e-06,
      "loss": 2.4142,
      "step": 126280
    },
    {
      "epoch": 2.5980266355344463,
      "grad_norm": 0.43396589159965515,
      "learning_rate": 9.270422089125119e-06,
      "loss": 2.3379,
      "step": 126290
    },
    {
      "epoch": 2.598232355051314,
      "grad_norm": 0.45163142681121826,
      "learning_rate": 9.261085542432202e-06,
      "loss": 2.3673,
      "step": 126300
    },
    {
      "epoch": 2.5984380745681817,
      "grad_norm": 0.47520318627357483,
      "learning_rate": 9.251753471395475e-06,
      "loss": 2.3759,
      "step": 126310
    },
    {
      "epoch": 2.5986437940850498,
      "grad_norm": 0.47057080268859863,
      "learning_rate": 9.242425876475258e-06,
      "loss": 2.3479,
      "step": 126320
    },
    {
      "epoch": 2.5988495136019174,
      "grad_norm": 0.437004953622818,
      "learning_rate": 9.23310275813155e-06,
      "loss": 2.3502,
      "step": 126330
    },
    {
      "epoch": 2.599055233118785,
      "grad_norm": 0.5063487887382507,
      "learning_rate": 9.223784116824286e-06,
      "loss": 2.3309,
      "step": 126340
    },
    {
      "epoch": 2.5992609526356527,
      "grad_norm": 0.4959179162979126,
      "learning_rate": 9.214469953013105e-06,
      "loss": 2.3379,
      "step": 126350
    },
    {
      "epoch": 2.5994666721525204,
      "grad_norm": 0.45655930042266846,
      "learning_rate": 9.205160267157353e-06,
      "loss": 2.3927,
      "step": 126360
    },
    {
      "epoch": 2.599672391669388,
      "grad_norm": 0.45974811911582947,
      "learning_rate": 9.195855059716297e-06,
      "loss": 2.334,
      "step": 126370
    },
    {
      "epoch": 2.599878111186256,
      "grad_norm": 0.45044177770614624,
      "learning_rate": 9.186554331148912e-06,
      "loss": 2.4253,
      "step": 126380
    },
    {
      "epoch": 2.6000838307031238,
      "grad_norm": 0.44789791107177734,
      "learning_rate": 9.177258081913876e-06,
      "loss": 2.4032,
      "step": 126390
    },
    {
      "epoch": 2.6002895502199914,
      "grad_norm": 0.4753378927707672,
      "learning_rate": 9.167966312469822e-06,
      "loss": 2.2942,
      "step": 126400
    },
    {
      "epoch": 2.600495269736859,
      "grad_norm": 0.43531277775764465,
      "learning_rate": 9.158679023274997e-06,
      "loss": 2.353,
      "step": 126410
    },
    {
      "epoch": 2.6007009892537267,
      "grad_norm": 0.4356014132499695,
      "learning_rate": 9.149396214787487e-06,
      "loss": 2.3653,
      "step": 126420
    },
    {
      "epoch": 2.6009067087705944,
      "grad_norm": 0.4720039963722229,
      "learning_rate": 9.140117887465227e-06,
      "loss": 2.3773,
      "step": 126430
    },
    {
      "epoch": 2.601112428287462,
      "grad_norm": 0.4716629087924957,
      "learning_rate": 9.130844041765796e-06,
      "loss": 2.2968,
      "step": 126440
    },
    {
      "epoch": 2.6013181478043297,
      "grad_norm": 0.47052958607673645,
      "learning_rate": 9.121574678146639e-06,
      "loss": 2.3856,
      "step": 126450
    },
    {
      "epoch": 2.6015238673211973,
      "grad_norm": 0.4676437973976135,
      "learning_rate": 9.11230979706501e-06,
      "loss": 2.3689,
      "step": 126460
    },
    {
      "epoch": 2.6017295868380654,
      "grad_norm": 0.46393442153930664,
      "learning_rate": 9.103049398977848e-06,
      "loss": 2.3757,
      "step": 126470
    },
    {
      "epoch": 2.601935306354933,
      "grad_norm": 0.4730249047279358,
      "learning_rate": 9.093793484341928e-06,
      "loss": 2.3641,
      "step": 126480
    },
    {
      "epoch": 2.6021410258718007,
      "grad_norm": 0.45021671056747437,
      "learning_rate": 9.084542053613786e-06,
      "loss": 2.3824,
      "step": 126490
    },
    {
      "epoch": 2.6023467453886684,
      "grad_norm": 0.5049477815628052,
      "learning_rate": 9.07529510724977e-06,
      "loss": 2.362,
      "step": 126500
    },
    {
      "epoch": 2.602552464905536,
      "grad_norm": 0.4537440836429596,
      "learning_rate": 9.066052645705958e-06,
      "loss": 2.3943,
      "step": 126510
    },
    {
      "epoch": 2.602758184422404,
      "grad_norm": 0.4545523524284363,
      "learning_rate": 9.05681466943824e-06,
      "loss": 2.4268,
      "step": 126520
    },
    {
      "epoch": 2.602963903939272,
      "grad_norm": 0.47443851828575134,
      "learning_rate": 9.047581178902265e-06,
      "loss": 2.3877,
      "step": 126530
    },
    {
      "epoch": 2.6031696234561394,
      "grad_norm": 0.5065099000930786,
      "learning_rate": 9.038352174553489e-06,
      "loss": 2.3371,
      "step": 126540
    },
    {
      "epoch": 2.603375342973007,
      "grad_norm": 0.4143015742301941,
      "learning_rate": 9.029127656847113e-06,
      "loss": 2.3957,
      "step": 126550
    },
    {
      "epoch": 2.6035810624898748,
      "grad_norm": 0.4894148111343384,
      "learning_rate": 9.019907626238145e-06,
      "loss": 2.3555,
      "step": 126560
    },
    {
      "epoch": 2.6037867820067424,
      "grad_norm": 0.4986443519592285,
      "learning_rate": 9.010692083181349e-06,
      "loss": 2.3739,
      "step": 126570
    },
    {
      "epoch": 2.60399250152361,
      "grad_norm": 0.4785400629043579,
      "learning_rate": 9.001481028131275e-06,
      "loss": 2.3271,
      "step": 126580
    },
    {
      "epoch": 2.6041982210404777,
      "grad_norm": 0.507866382598877,
      "learning_rate": 8.992274461542272e-06,
      "loss": 2.374,
      "step": 126590
    },
    {
      "epoch": 2.6044039405573454,
      "grad_norm": 0.5964131951332092,
      "learning_rate": 8.983072383868429e-06,
      "loss": 2.3282,
      "step": 126600
    },
    {
      "epoch": 2.6046096600742135,
      "grad_norm": 0.45628780126571655,
      "learning_rate": 8.973874795563642e-06,
      "loss": 2.363,
      "step": 126610
    },
    {
      "epoch": 2.604815379591081,
      "grad_norm": 0.45163506269454956,
      "learning_rate": 8.964681697081578e-06,
      "loss": 2.3051,
      "step": 126620
    },
    {
      "epoch": 2.6050210991079488,
      "grad_norm": 0.4617767333984375,
      "learning_rate": 8.955493088875677e-06,
      "loss": 2.3107,
      "step": 126630
    },
    {
      "epoch": 2.6052268186248164,
      "grad_norm": 0.4622308313846588,
      "learning_rate": 8.946308971399198e-06,
      "loss": 2.323,
      "step": 126640
    },
    {
      "epoch": 2.605432538141684,
      "grad_norm": 0.4797961115837097,
      "learning_rate": 8.937129345105077e-06,
      "loss": 2.4149,
      "step": 126650
    },
    {
      "epoch": 2.6056382576585517,
      "grad_norm": 0.4550396203994751,
      "learning_rate": 8.927954210446143e-06,
      "loss": 2.4002,
      "step": 126660
    },
    {
      "epoch": 2.60584397717542,
      "grad_norm": 0.4901552200317383,
      "learning_rate": 8.918783567874966e-06,
      "loss": 2.3724,
      "step": 126670
    },
    {
      "epoch": 2.6060496966922875,
      "grad_norm": 0.4504639208316803,
      "learning_rate": 8.90961741784383e-06,
      "loss": 2.3533,
      "step": 126680
    },
    {
      "epoch": 2.606255416209155,
      "grad_norm": 0.4630829989910126,
      "learning_rate": 8.900455760804894e-06,
      "loss": 2.4097,
      "step": 126690
    },
    {
      "epoch": 2.606461135726023,
      "grad_norm": 0.5078029036521912,
      "learning_rate": 8.891298597210063e-06,
      "loss": 2.3505,
      "step": 126700
    },
    {
      "epoch": 2.6066668552428904,
      "grad_norm": 0.44523194432258606,
      "learning_rate": 8.882145927510964e-06,
      "loss": 2.3625,
      "step": 126710
    },
    {
      "epoch": 2.606872574759758,
      "grad_norm": 0.4225601553916931,
      "learning_rate": 8.872997752159096e-06,
      "loss": 2.3589,
      "step": 126720
    },
    {
      "epoch": 2.6070782942766257,
      "grad_norm": 0.47749003767967224,
      "learning_rate": 8.863854071605659e-06,
      "loss": 2.3521,
      "step": 126730
    },
    {
      "epoch": 2.6072840137934934,
      "grad_norm": 0.424203097820282,
      "learning_rate": 8.85471488630164e-06,
      "loss": 2.3415,
      "step": 126740
    },
    {
      "epoch": 2.607489733310361,
      "grad_norm": 0.4398176670074463,
      "learning_rate": 8.8455801966979e-06,
      "loss": 2.3827,
      "step": 126750
    },
    {
      "epoch": 2.607695452827229,
      "grad_norm": 0.42662885785102844,
      "learning_rate": 8.836450003244933e-06,
      "loss": 2.3447,
      "step": 126760
    },
    {
      "epoch": 2.607901172344097,
      "grad_norm": 0.512031078338623,
      "learning_rate": 8.827324306393081e-06,
      "loss": 2.3776,
      "step": 126770
    },
    {
      "epoch": 2.6081068918609644,
      "grad_norm": 0.471351683139801,
      "learning_rate": 8.818203106592538e-06,
      "loss": 2.3384,
      "step": 126780
    },
    {
      "epoch": 2.608312611377832,
      "grad_norm": 0.49261003732681274,
      "learning_rate": 8.809086404293132e-06,
      "loss": 2.3946,
      "step": 126790
    },
    {
      "epoch": 2.6085183308946998,
      "grad_norm": 0.4759126305580139,
      "learning_rate": 8.799974199944561e-06,
      "loss": 2.3857,
      "step": 126800
    },
    {
      "epoch": 2.608724050411568,
      "grad_norm": 0.4920012652873993,
      "learning_rate": 8.790866493996275e-06,
      "loss": 2.39,
      "step": 126810
    },
    {
      "epoch": 2.6089297699284355,
      "grad_norm": 0.4430074095726013,
      "learning_rate": 8.781763286897527e-06,
      "loss": 2.3608,
      "step": 126820
    },
    {
      "epoch": 2.609135489445303,
      "grad_norm": 0.4545305371284485,
      "learning_rate": 8.772664579097312e-06,
      "loss": 2.4169,
      "step": 126830
    },
    {
      "epoch": 2.609341208962171,
      "grad_norm": 0.4734038710594177,
      "learning_rate": 8.763570371044415e-06,
      "loss": 2.3868,
      "step": 126840
    },
    {
      "epoch": 2.6095469284790385,
      "grad_norm": 0.4690905511379242,
      "learning_rate": 8.754480663187425e-06,
      "loss": 2.3888,
      "step": 126850
    },
    {
      "epoch": 2.609752647995906,
      "grad_norm": 0.42963680624961853,
      "learning_rate": 8.74539545597467e-06,
      "loss": 2.3506,
      "step": 126860
    },
    {
      "epoch": 2.6099583675127738,
      "grad_norm": 0.4785630702972412,
      "learning_rate": 8.73631474985429e-06,
      "loss": 2.3811,
      "step": 126870
    },
    {
      "epoch": 2.6101640870296414,
      "grad_norm": 0.45999449491500854,
      "learning_rate": 8.727238545274164e-06,
      "loss": 2.3767,
      "step": 126880
    },
    {
      "epoch": 2.610369806546509,
      "grad_norm": 0.44026023149490356,
      "learning_rate": 8.718166842681986e-06,
      "loss": 2.3851,
      "step": 126890
    },
    {
      "epoch": 2.610575526063377,
      "grad_norm": 0.44207480549812317,
      "learning_rate": 8.709099642525209e-06,
      "loss": 2.3161,
      "step": 126900
    },
    {
      "epoch": 2.610781245580245,
      "grad_norm": 0.48500722646713257,
      "learning_rate": 8.700036945251078e-06,
      "loss": 2.4038,
      "step": 126910
    },
    {
      "epoch": 2.6109869650971125,
      "grad_norm": 0.46668100357055664,
      "learning_rate": 8.69097875130659e-06,
      "loss": 2.3869,
      "step": 126920
    },
    {
      "epoch": 2.61119268461398,
      "grad_norm": 0.4696897566318512,
      "learning_rate": 8.681925061138575e-06,
      "loss": 2.3667,
      "step": 126930
    },
    {
      "epoch": 2.611398404130848,
      "grad_norm": 0.47081148624420166,
      "learning_rate": 8.672875875193531e-06,
      "loss": 2.3559,
      "step": 126940
    },
    {
      "epoch": 2.611604123647716,
      "grad_norm": 0.4488584101200104,
      "learning_rate": 8.66383119391786e-06,
      "loss": 2.3286,
      "step": 126950
    },
    {
      "epoch": 2.6118098431645835,
      "grad_norm": 0.4518350064754486,
      "learning_rate": 8.654791017757701e-06,
      "loss": 2.3426,
      "step": 126960
    },
    {
      "epoch": 2.612015562681451,
      "grad_norm": 0.46962639689445496,
      "learning_rate": 8.645755347158879e-06,
      "loss": 2.3355,
      "step": 126970
    },
    {
      "epoch": 2.612221282198319,
      "grad_norm": 0.49821892380714417,
      "learning_rate": 8.636724182567158e-06,
      "loss": 2.4252,
      "step": 126980
    },
    {
      "epoch": 2.6124270017151865,
      "grad_norm": 0.4322711229324341,
      "learning_rate": 8.62769752442798e-06,
      "loss": 2.3658,
      "step": 126990
    },
    {
      "epoch": 2.612632721232054,
      "grad_norm": 0.531958281993866,
      "learning_rate": 8.618675373186514e-06,
      "loss": 2.4166,
      "step": 127000
    },
    {
      "epoch": 2.612838440748922,
      "grad_norm": 0.5336772203445435,
      "learning_rate": 8.609657729287867e-06,
      "loss": 2.3965,
      "step": 127010
    },
    {
      "epoch": 2.6130441602657895,
      "grad_norm": 0.473825603723526,
      "learning_rate": 8.600644593176777e-06,
      "loss": 2.357,
      "step": 127020
    },
    {
      "epoch": 2.613249879782657,
      "grad_norm": 0.4719783067703247,
      "learning_rate": 8.591635965297795e-06,
      "loss": 2.4703,
      "step": 127030
    },
    {
      "epoch": 2.613455599299525,
      "grad_norm": 0.4722962975502014,
      "learning_rate": 8.582631846095336e-06,
      "loss": 2.337,
      "step": 127040
    },
    {
      "epoch": 2.613661318816393,
      "grad_norm": 0.48871251940727234,
      "learning_rate": 8.573632236013462e-06,
      "loss": 2.347,
      "step": 127050
    },
    {
      "epoch": 2.6138670383332605,
      "grad_norm": 0.5901685357093811,
      "learning_rate": 8.564637135496078e-06,
      "loss": 2.3785,
      "step": 127060
    },
    {
      "epoch": 2.614072757850128,
      "grad_norm": 0.5346975326538086,
      "learning_rate": 8.555646544986916e-06,
      "loss": 2.3436,
      "step": 127070
    },
    {
      "epoch": 2.614278477366996,
      "grad_norm": 0.4410576820373535,
      "learning_rate": 8.54666046492938e-06,
      "loss": 2.366,
      "step": 127080
    },
    {
      "epoch": 2.6144841968838635,
      "grad_norm": 0.4337557852268219,
      "learning_rate": 8.537678895766731e-06,
      "loss": 2.3516,
      "step": 127090
    },
    {
      "epoch": 2.6146899164007316,
      "grad_norm": 0.5195151567459106,
      "learning_rate": 8.528701837941966e-06,
      "loss": 2.3846,
      "step": 127100
    },
    {
      "epoch": 2.614895635917599,
      "grad_norm": 0.4440176486968994,
      "learning_rate": 8.51972929189787e-06,
      "loss": 2.3585,
      "step": 127110
    },
    {
      "epoch": 2.615101355434467,
      "grad_norm": 0.4432169198989868,
      "learning_rate": 8.510761258077037e-06,
      "loss": 2.3497,
      "step": 127120
    },
    {
      "epoch": 2.6153070749513345,
      "grad_norm": 0.4631872773170471,
      "learning_rate": 8.50179773692179e-06,
      "loss": 2.3265,
      "step": 127130
    },
    {
      "epoch": 2.615512794468202,
      "grad_norm": 0.45635199546813965,
      "learning_rate": 8.492838728874253e-06,
      "loss": 2.324,
      "step": 127140
    },
    {
      "epoch": 2.61571851398507,
      "grad_norm": 0.4969034790992737,
      "learning_rate": 8.483884234376327e-06,
      "loss": 2.3075,
      "step": 127150
    },
    {
      "epoch": 2.6159242335019375,
      "grad_norm": 0.47954872250556946,
      "learning_rate": 8.474934253869681e-06,
      "loss": 2.3581,
      "step": 127160
    },
    {
      "epoch": 2.616129953018805,
      "grad_norm": 0.46439605951309204,
      "learning_rate": 8.465988787795787e-06,
      "loss": 2.3039,
      "step": 127170
    },
    {
      "epoch": 2.616335672535673,
      "grad_norm": 0.49582797288894653,
      "learning_rate": 8.457047836595866e-06,
      "loss": 2.3598,
      "step": 127180
    },
    {
      "epoch": 2.616541392052541,
      "grad_norm": 0.4749428629875183,
      "learning_rate": 8.448111400710923e-06,
      "loss": 2.3667,
      "step": 127190
    },
    {
      "epoch": 2.6167471115694085,
      "grad_norm": 0.5008484125137329,
      "learning_rate": 8.439179480581749e-06,
      "loss": 2.2919,
      "step": 127200
    },
    {
      "epoch": 2.616952831086276,
      "grad_norm": 0.4451988935470581,
      "learning_rate": 8.430252076648904e-06,
      "loss": 2.3964,
      "step": 127210
    },
    {
      "epoch": 2.617158550603144,
      "grad_norm": 0.5078921914100647,
      "learning_rate": 8.421329189352722e-06,
      "loss": 2.4074,
      "step": 127220
    },
    {
      "epoch": 2.6173642701200115,
      "grad_norm": 0.44869476556777954,
      "learning_rate": 8.412410819133332e-06,
      "loss": 2.3598,
      "step": 127230
    },
    {
      "epoch": 2.6175699896368796,
      "grad_norm": 0.48089325428009033,
      "learning_rate": 8.403496966430624e-06,
      "loss": 2.3995,
      "step": 127240
    },
    {
      "epoch": 2.6177757091537472,
      "grad_norm": 0.4753996431827545,
      "learning_rate": 8.394587631684292e-06,
      "loss": 2.3719,
      "step": 127250
    },
    {
      "epoch": 2.617981428670615,
      "grad_norm": 0.44122597575187683,
      "learning_rate": 8.385682815333717e-06,
      "loss": 2.3671,
      "step": 127260
    },
    {
      "epoch": 2.6181871481874825,
      "grad_norm": 0.502928614616394,
      "learning_rate": 8.376782517818193e-06,
      "loss": 2.3426,
      "step": 127270
    },
    {
      "epoch": 2.61839286770435,
      "grad_norm": 0.45983394980430603,
      "learning_rate": 8.367886739576725e-06,
      "loss": 2.357,
      "step": 127280
    },
    {
      "epoch": 2.618598587221218,
      "grad_norm": 0.4526363015174866,
      "learning_rate": 8.358995481048026e-06,
      "loss": 2.3682,
      "step": 127290
    },
    {
      "epoch": 2.6188043067380855,
      "grad_norm": 0.48425695300102234,
      "learning_rate": 8.350108742670714e-06,
      "loss": 2.4028,
      "step": 127300
    },
    {
      "epoch": 2.619010026254953,
      "grad_norm": 0.44207629561424255,
      "learning_rate": 8.341226524883128e-06,
      "loss": 2.3759,
      "step": 127310
    },
    {
      "epoch": 2.619215745771821,
      "grad_norm": 0.4421728551387787,
      "learning_rate": 8.332348828123316e-06,
      "loss": 2.4074,
      "step": 127320
    },
    {
      "epoch": 2.619421465288689,
      "grad_norm": 0.4364476799964905,
      "learning_rate": 8.323475652829239e-06,
      "loss": 2.3146,
      "step": 127330
    },
    {
      "epoch": 2.6196271848055566,
      "grad_norm": 0.4216519892215729,
      "learning_rate": 8.314606999438523e-06,
      "loss": 2.4069,
      "step": 127340
    },
    {
      "epoch": 2.619832904322424,
      "grad_norm": 0.4401819705963135,
      "learning_rate": 8.3057428683886e-06,
      "loss": 2.3945,
      "step": 127350
    },
    {
      "epoch": 2.620038623839292,
      "grad_norm": 0.45354321599006653,
      "learning_rate": 8.296883260116739e-06,
      "loss": 2.3487,
      "step": 127360
    },
    {
      "epoch": 2.6202443433561595,
      "grad_norm": 0.4694468379020691,
      "learning_rate": 8.288028175059892e-06,
      "loss": 2.2934,
      "step": 127370
    },
    {
      "epoch": 2.6204500628730276,
      "grad_norm": 0.44368186593055725,
      "learning_rate": 8.27917761365482e-06,
      "loss": 2.3661,
      "step": 127380
    },
    {
      "epoch": 2.6206557823898953,
      "grad_norm": 0.45237502455711365,
      "learning_rate": 8.27033157633813e-06,
      "loss": 2.354,
      "step": 127390
    },
    {
      "epoch": 2.620861501906763,
      "grad_norm": 0.454304963350296,
      "learning_rate": 8.261490063546107e-06,
      "loss": 2.3543,
      "step": 127400
    },
    {
      "epoch": 2.6210672214236306,
      "grad_norm": 0.45235615968704224,
      "learning_rate": 8.252653075714867e-06,
      "loss": 2.375,
      "step": 127410
    },
    {
      "epoch": 2.6212729409404982,
      "grad_norm": 0.4856899082660675,
      "learning_rate": 8.243820613280284e-06,
      "loss": 2.3624,
      "step": 127420
    },
    {
      "epoch": 2.621478660457366,
      "grad_norm": 0.4565854072570801,
      "learning_rate": 8.234992676678021e-06,
      "loss": 2.377,
      "step": 127430
    },
    {
      "epoch": 2.6216843799742335,
      "grad_norm": 0.43894195556640625,
      "learning_rate": 8.22616926634351e-06,
      "loss": 2.3799,
      "step": 127440
    },
    {
      "epoch": 2.621890099491101,
      "grad_norm": 0.41359373927116394,
      "learning_rate": 8.217350382711952e-06,
      "loss": 2.3478,
      "step": 127450
    },
    {
      "epoch": 2.622095819007969,
      "grad_norm": 0.4651411771774292,
      "learning_rate": 8.20853602621835e-06,
      "loss": 2.3316,
      "step": 127460
    },
    {
      "epoch": 2.622301538524837,
      "grad_norm": 0.47636526823043823,
      "learning_rate": 8.199726197297463e-06,
      "loss": 2.3063,
      "step": 127470
    },
    {
      "epoch": 2.6225072580417046,
      "grad_norm": 0.44761621952056885,
      "learning_rate": 8.190920896383836e-06,
      "loss": 2.358,
      "step": 127480
    },
    {
      "epoch": 2.6227129775585722,
      "grad_norm": 0.46058884263038635,
      "learning_rate": 8.182120123911774e-06,
      "loss": 2.3821,
      "step": 127490
    },
    {
      "epoch": 2.62291869707544,
      "grad_norm": 0.4560672640800476,
      "learning_rate": 8.173323880315387e-06,
      "loss": 2.3095,
      "step": 127500
    },
    {
      "epoch": 2.6231244165923076,
      "grad_norm": 0.47003448009490967,
      "learning_rate": 8.164532166028528e-06,
      "loss": 2.3693,
      "step": 127510
    },
    {
      "epoch": 2.623330136109175,
      "grad_norm": 0.45428231358528137,
      "learning_rate": 8.155744981484858e-06,
      "loss": 2.3498,
      "step": 127520
    },
    {
      "epoch": 2.6235358556260433,
      "grad_norm": 0.5252476930618286,
      "learning_rate": 8.146962327117801e-06,
      "loss": 2.3842,
      "step": 127530
    },
    {
      "epoch": 2.623741575142911,
      "grad_norm": 0.4193759858608246,
      "learning_rate": 8.138184203360544e-06,
      "loss": 2.3316,
      "step": 127540
    },
    {
      "epoch": 2.6239472946597786,
      "grad_norm": 0.4814683496952057,
      "learning_rate": 8.129410610646082e-06,
      "loss": 2.3112,
      "step": 127550
    },
    {
      "epoch": 2.6241530141766463,
      "grad_norm": 0.4586070477962494,
      "learning_rate": 8.120641549407159e-06,
      "loss": 2.3881,
      "step": 127560
    },
    {
      "epoch": 2.624358733693514,
      "grad_norm": 0.43976926803588867,
      "learning_rate": 8.111877020076319e-06,
      "loss": 2.3369,
      "step": 127570
    },
    {
      "epoch": 2.6245644532103816,
      "grad_norm": 0.4509052634239197,
      "learning_rate": 8.103117023085827e-06,
      "loss": 2.3526,
      "step": 127580
    },
    {
      "epoch": 2.624770172727249,
      "grad_norm": 0.4761352241039276,
      "learning_rate": 8.094361558867802e-06,
      "loss": 2.4053,
      "step": 127590
    },
    {
      "epoch": 2.624975892244117,
      "grad_norm": 0.47683054208755493,
      "learning_rate": 8.085610627854122e-06,
      "loss": 2.3414,
      "step": 127600
    },
    {
      "epoch": 2.6251816117609845,
      "grad_norm": 0.4589029550552368,
      "learning_rate": 8.076864230476356e-06,
      "loss": 2.3208,
      "step": 127610
    },
    {
      "epoch": 2.6253873312778526,
      "grad_norm": 0.4799659848213196,
      "learning_rate": 8.068122367165975e-06,
      "loss": 2.3416,
      "step": 127620
    },
    {
      "epoch": 2.6255930507947203,
      "grad_norm": 0.4401901066303253,
      "learning_rate": 8.059385038354172e-06,
      "loss": 2.3855,
      "step": 127630
    },
    {
      "epoch": 2.625798770311588,
      "grad_norm": 0.47788873314857483,
      "learning_rate": 8.050652244471856e-06,
      "loss": 2.331,
      "step": 127640
    },
    {
      "epoch": 2.6260044898284556,
      "grad_norm": 0.4634028375148773,
      "learning_rate": 8.041923985949828e-06,
      "loss": 2.4295,
      "step": 127650
    },
    {
      "epoch": 2.6262102093453232,
      "grad_norm": 0.49112367630004883,
      "learning_rate": 8.033200263218554e-06,
      "loss": 2.357,
      "step": 127660
    },
    {
      "epoch": 2.6264159288621913,
      "grad_norm": 0.46338537335395813,
      "learning_rate": 8.024481076708346e-06,
      "loss": 2.3629,
      "step": 127670
    },
    {
      "epoch": 2.626621648379059,
      "grad_norm": 0.6230142712593079,
      "learning_rate": 8.015766426849314e-06,
      "loss": 2.3908,
      "step": 127680
    },
    {
      "epoch": 2.6268273678959266,
      "grad_norm": 0.45654624700546265,
      "learning_rate": 8.007056314071248e-06,
      "loss": 2.3696,
      "step": 127690
    },
    {
      "epoch": 2.6270330874127943,
      "grad_norm": 0.47155439853668213,
      "learning_rate": 7.998350738803784e-06,
      "loss": 2.3584,
      "step": 127700
    },
    {
      "epoch": 2.627238806929662,
      "grad_norm": 0.48374229669570923,
      "learning_rate": 7.989649701476354e-06,
      "loss": 2.3685,
      "step": 127710
    },
    {
      "epoch": 2.6274445264465296,
      "grad_norm": 0.4229883849620819,
      "learning_rate": 7.980953202518093e-06,
      "loss": 2.3751,
      "step": 127720
    },
    {
      "epoch": 2.6276502459633972,
      "grad_norm": 0.44297125935554504,
      "learning_rate": 7.97226124235797e-06,
      "loss": 2.3199,
      "step": 127730
    },
    {
      "epoch": 2.627855965480265,
      "grad_norm": 0.49172574281692505,
      "learning_rate": 7.963573821424707e-06,
      "loss": 2.3663,
      "step": 127740
    },
    {
      "epoch": 2.6280616849971326,
      "grad_norm": 0.4435771405696869,
      "learning_rate": 7.954890940146808e-06,
      "loss": 2.3522,
      "step": 127750
    },
    {
      "epoch": 2.6282674045140006,
      "grad_norm": 0.4476124048233032,
      "learning_rate": 7.946212598952562e-06,
      "loss": 2.3047,
      "step": 127760
    },
    {
      "epoch": 2.6284731240308683,
      "grad_norm": 0.49391254782676697,
      "learning_rate": 7.937538798270006e-06,
      "loss": 2.3985,
      "step": 127770
    },
    {
      "epoch": 2.628678843547736,
      "grad_norm": 0.41394680738449097,
      "learning_rate": 7.928869538526985e-06,
      "loss": 2.3522,
      "step": 127780
    },
    {
      "epoch": 2.6288845630646036,
      "grad_norm": 0.4356254041194916,
      "learning_rate": 7.9202048201511e-06,
      "loss": 2.3536,
      "step": 127790
    },
    {
      "epoch": 2.6290902825814713,
      "grad_norm": 0.45479005575180054,
      "learning_rate": 7.911544643569735e-06,
      "loss": 2.3237,
      "step": 127800
    },
    {
      "epoch": 2.629296002098339,
      "grad_norm": 0.4772929549217224,
      "learning_rate": 7.902889009210047e-06,
      "loss": 2.382,
      "step": 127810
    },
    {
      "epoch": 2.629501721615207,
      "grad_norm": 0.4789828360080719,
      "learning_rate": 7.894237917498981e-06,
      "loss": 2.3679,
      "step": 127820
    },
    {
      "epoch": 2.6297074411320747,
      "grad_norm": 0.4697197675704956,
      "learning_rate": 7.885591368863243e-06,
      "loss": 2.349,
      "step": 127830
    },
    {
      "epoch": 2.6299131606489423,
      "grad_norm": 0.4382692873477936,
      "learning_rate": 7.876949363729314e-06,
      "loss": 2.3577,
      "step": 127840
    },
    {
      "epoch": 2.63011888016581,
      "grad_norm": 0.4424990713596344,
      "learning_rate": 7.868311902523474e-06,
      "loss": 2.4222,
      "step": 127850
    },
    {
      "epoch": 2.6303245996826776,
      "grad_norm": 0.46008384227752686,
      "learning_rate": 7.859678985671738e-06,
      "loss": 2.3186,
      "step": 127860
    },
    {
      "epoch": 2.6305303191995453,
      "grad_norm": 0.4572086036205292,
      "learning_rate": 7.851050613599942e-06,
      "loss": 2.3859,
      "step": 127870
    },
    {
      "epoch": 2.630736038716413,
      "grad_norm": 0.45599988102912903,
      "learning_rate": 7.842426786733658e-06,
      "loss": 2.3413,
      "step": 127880
    },
    {
      "epoch": 2.6309417582332806,
      "grad_norm": 0.4809177815914154,
      "learning_rate": 7.833807505498292e-06,
      "loss": 2.4122,
      "step": 127890
    },
    {
      "epoch": 2.6311474777501482,
      "grad_norm": 0.5066095590591431,
      "learning_rate": 7.825192770318912e-06,
      "loss": 2.3857,
      "step": 127900
    },
    {
      "epoch": 2.6313531972670163,
      "grad_norm": 0.4489573836326599,
      "learning_rate": 7.816582581620503e-06,
      "loss": 2.4066,
      "step": 127910
    },
    {
      "epoch": 2.631558916783884,
      "grad_norm": 0.5128031969070435,
      "learning_rate": 7.807976939827744e-06,
      "loss": 2.3996,
      "step": 127920
    },
    {
      "epoch": 2.6317646363007516,
      "grad_norm": 0.4332870841026306,
      "learning_rate": 7.799375845365075e-06,
      "loss": 2.3799,
      "step": 127930
    },
    {
      "epoch": 2.6319703558176193,
      "grad_norm": 0.5089285373687744,
      "learning_rate": 7.790779298656769e-06,
      "loss": 2.3949,
      "step": 127940
    },
    {
      "epoch": 2.632176075334487,
      "grad_norm": 0.4945037364959717,
      "learning_rate": 7.782187300126864e-06,
      "loss": 2.4006,
      "step": 127950
    },
    {
      "epoch": 2.632381794851355,
      "grad_norm": 0.40277692675590515,
      "learning_rate": 7.773599850199097e-06,
      "loss": 2.3503,
      "step": 127960
    },
    {
      "epoch": 2.6325875143682227,
      "grad_norm": 0.4707382321357727,
      "learning_rate": 7.76501694929711e-06,
      "loss": 2.354,
      "step": 127970
    },
    {
      "epoch": 2.6327932338850903,
      "grad_norm": 0.45002633333206177,
      "learning_rate": 7.756438597844196e-06,
      "loss": 2.3609,
      "step": 127980
    },
    {
      "epoch": 2.632998953401958,
      "grad_norm": 0.4509165585041046,
      "learning_rate": 7.747864796263482e-06,
      "loss": 2.3475,
      "step": 127990
    },
    {
      "epoch": 2.6332046729188257,
      "grad_norm": 0.41803473234176636,
      "learning_rate": 7.73929554497792e-06,
      "loss": 2.3446,
      "step": 128000
    },
    {
      "epoch": 2.6334103924356933,
      "grad_norm": 0.47733038663864136,
      "learning_rate": 7.730730844410127e-06,
      "loss": 2.3744,
      "step": 128010
    },
    {
      "epoch": 2.633616111952561,
      "grad_norm": 0.4275789260864258,
      "learning_rate": 7.722170694982578e-06,
      "loss": 2.4031,
      "step": 128020
    },
    {
      "epoch": 2.6338218314694286,
      "grad_norm": 0.4787115752696991,
      "learning_rate": 7.713615097117499e-06,
      "loss": 2.4009,
      "step": 128030
    },
    {
      "epoch": 2.6340275509862963,
      "grad_norm": 0.5329650044441223,
      "learning_rate": 7.705064051236877e-06,
      "loss": 2.3199,
      "step": 128040
    },
    {
      "epoch": 2.6342332705031644,
      "grad_norm": 0.48141300678253174,
      "learning_rate": 7.696517557762507e-06,
      "loss": 2.3901,
      "step": 128050
    },
    {
      "epoch": 2.634438990020032,
      "grad_norm": 0.5268879532814026,
      "learning_rate": 7.687975617115928e-06,
      "loss": 2.3902,
      "step": 128060
    },
    {
      "epoch": 2.6346447095368997,
      "grad_norm": 0.4554612934589386,
      "learning_rate": 7.679438229718471e-06,
      "loss": 2.3574,
      "step": 128070
    },
    {
      "epoch": 2.6348504290537673,
      "grad_norm": 0.4763593375682831,
      "learning_rate": 7.670905395991245e-06,
      "loss": 2.3954,
      "step": 128080
    },
    {
      "epoch": 2.635056148570635,
      "grad_norm": 0.47310909628868103,
      "learning_rate": 7.662377116355124e-06,
      "loss": 2.3349,
      "step": 128090
    },
    {
      "epoch": 2.635261868087503,
      "grad_norm": 0.4649076759815216,
      "learning_rate": 7.653853391230747e-06,
      "loss": 2.3151,
      "step": 128100
    },
    {
      "epoch": 2.6354675876043707,
      "grad_norm": 0.5250903367996216,
      "learning_rate": 7.64533422103857e-06,
      "loss": 2.239,
      "step": 128110
    },
    {
      "epoch": 2.6356733071212384,
      "grad_norm": 0.4541313946247101,
      "learning_rate": 7.636819606198775e-06,
      "loss": 2.3373,
      "step": 128120
    },
    {
      "epoch": 2.635879026638106,
      "grad_norm": 0.46857449412345886,
      "learning_rate": 7.628309547131351e-06,
      "loss": 2.3994,
      "step": 128130
    },
    {
      "epoch": 2.6360847461549737,
      "grad_norm": 0.45596829056739807,
      "learning_rate": 7.6198040442560514e-06,
      "loss": 2.346,
      "step": 128140
    },
    {
      "epoch": 2.6362904656718413,
      "grad_norm": 0.4506126940250397,
      "learning_rate": 7.611303097992406e-06,
      "loss": 2.3086,
      "step": 128150
    },
    {
      "epoch": 2.636496185188709,
      "grad_norm": 0.45952871441841125,
      "learning_rate": 7.602806708759725e-06,
      "loss": 2.4063,
      "step": 128160
    },
    {
      "epoch": 2.6367019047055766,
      "grad_norm": 0.4088028371334076,
      "learning_rate": 7.594314876977083e-06,
      "loss": 2.377,
      "step": 128170
    },
    {
      "epoch": 2.6369076242224443,
      "grad_norm": 0.47126278281211853,
      "learning_rate": 7.585827603063356e-06,
      "loss": 2.462,
      "step": 128180
    },
    {
      "epoch": 2.6371133437393124,
      "grad_norm": 0.4803087115287781,
      "learning_rate": 7.577344887437121e-06,
      "loss": 2.4413,
      "step": 128190
    },
    {
      "epoch": 2.63731906325618,
      "grad_norm": 0.48568254709243774,
      "learning_rate": 7.568866730516832e-06,
      "loss": 2.4116,
      "step": 128200
    },
    {
      "epoch": 2.6375247827730477,
      "grad_norm": 0.4724530577659607,
      "learning_rate": 7.560393132720678e-06,
      "loss": 2.3717,
      "step": 128210
    },
    {
      "epoch": 2.6377305022899153,
      "grad_norm": 0.4571734666824341,
      "learning_rate": 7.551924094466567e-06,
      "loss": 2.3425,
      "step": 128220
    },
    {
      "epoch": 2.637936221806783,
      "grad_norm": 0.5119550824165344,
      "learning_rate": 7.543459616172266e-06,
      "loss": 2.3004,
      "step": 128230
    },
    {
      "epoch": 2.6381419413236507,
      "grad_norm": 0.5253767967224121,
      "learning_rate": 7.534999698255296e-06,
      "loss": 2.3948,
      "step": 128240
    },
    {
      "epoch": 2.6383476608405187,
      "grad_norm": 0.4379595220088959,
      "learning_rate": 7.526544341132891e-06,
      "loss": 2.3927,
      "step": 128250
    },
    {
      "epoch": 2.6385533803573864,
      "grad_norm": 0.4615073800086975,
      "learning_rate": 7.5180935452221605e-06,
      "loss": 2.3542,
      "step": 128260
    },
    {
      "epoch": 2.638759099874254,
      "grad_norm": 0.4577086567878723,
      "learning_rate": 7.509647310939893e-06,
      "loss": 2.374,
      "step": 128270
    },
    {
      "epoch": 2.6389648193911217,
      "grad_norm": 0.4615890681743622,
      "learning_rate": 7.501205638702691e-06,
      "loss": 2.3583,
      "step": 128280
    },
    {
      "epoch": 2.6391705389079894,
      "grad_norm": 0.4552253782749176,
      "learning_rate": 7.492768528926997e-06,
      "loss": 2.3805,
      "step": 128290
    },
    {
      "epoch": 2.639376258424857,
      "grad_norm": 0.532370924949646,
      "learning_rate": 7.484335982028901e-06,
      "loss": 2.331,
      "step": 128300
    },
    {
      "epoch": 2.6395819779417247,
      "grad_norm": 0.4612615704536438,
      "learning_rate": 7.475907998424348e-06,
      "loss": 2.3457,
      "step": 128310
    },
    {
      "epoch": 2.6397876974585923,
      "grad_norm": 0.48116010427474976,
      "learning_rate": 7.467484578529094e-06,
      "loss": 2.3311,
      "step": 128320
    },
    {
      "epoch": 2.63999341697546,
      "grad_norm": 0.48452967405319214,
      "learning_rate": 7.459065722758573e-06,
      "loss": 2.348,
      "step": 128330
    },
    {
      "epoch": 2.640199136492328,
      "grad_norm": 0.4341559410095215,
      "learning_rate": 7.450651431528044e-06,
      "loss": 2.3629,
      "step": 128340
    },
    {
      "epoch": 2.6404048560091957,
      "grad_norm": 0.4641861319541931,
      "learning_rate": 7.442241705252551e-06,
      "loss": 2.3107,
      "step": 128350
    },
    {
      "epoch": 2.6406105755260634,
      "grad_norm": 0.468112587928772,
      "learning_rate": 7.433836544346895e-06,
      "loss": 2.4151,
      "step": 128360
    },
    {
      "epoch": 2.640816295042931,
      "grad_norm": 0.4816584289073944,
      "learning_rate": 7.425435949225656e-06,
      "loss": 2.3693,
      "step": 128370
    },
    {
      "epoch": 2.6410220145597987,
      "grad_norm": 0.47522425651550293,
      "learning_rate": 7.417039920303181e-06,
      "loss": 2.3835,
      "step": 128380
    },
    {
      "epoch": 2.6412277340766668,
      "grad_norm": 0.5211241245269775,
      "learning_rate": 7.408648457993628e-06,
      "loss": 2.3804,
      "step": 128390
    },
    {
      "epoch": 2.6414334535935344,
      "grad_norm": 0.4903113543987274,
      "learning_rate": 7.400261562710875e-06,
      "loss": 2.3717,
      "step": 128400
    },
    {
      "epoch": 2.641639173110402,
      "grad_norm": 0.4673742353916168,
      "learning_rate": 7.391879234868604e-06,
      "loss": 2.3262,
      "step": 128410
    },
    {
      "epoch": 2.6418448926272697,
      "grad_norm": 0.4596366286277771,
      "learning_rate": 7.383501474880306e-06,
      "loss": 2.3351,
      "step": 128420
    },
    {
      "epoch": 2.6420506121441374,
      "grad_norm": 0.48099979758262634,
      "learning_rate": 7.375128283159138e-06,
      "loss": 2.3615,
      "step": 128430
    },
    {
      "epoch": 2.642256331661005,
      "grad_norm": 0.4601075351238251,
      "learning_rate": 7.366759660118172e-06,
      "loss": 2.409,
      "step": 128440
    },
    {
      "epoch": 2.6424620511778727,
      "grad_norm": 0.5036959648132324,
      "learning_rate": 7.358395606170177e-06,
      "loss": 2.3877,
      "step": 128450
    },
    {
      "epoch": 2.6426677706947403,
      "grad_norm": 0.49438923597335815,
      "learning_rate": 7.350036121727655e-06,
      "loss": 2.4053,
      "step": 128460
    },
    {
      "epoch": 2.642873490211608,
      "grad_norm": 0.4813058078289032,
      "learning_rate": 7.3416812072029775e-06,
      "loss": 2.3705,
      "step": 128470
    },
    {
      "epoch": 2.643079209728476,
      "grad_norm": 0.4807286858558655,
      "learning_rate": 7.333330863008248e-06,
      "loss": 2.334,
      "step": 128480
    },
    {
      "epoch": 2.6432849292453438,
      "grad_norm": 0.45779117941856384,
      "learning_rate": 7.324985089555337e-06,
      "loss": 2.2853,
      "step": 128490
    },
    {
      "epoch": 2.6434906487622114,
      "grad_norm": 0.41952812671661377,
      "learning_rate": 7.316643887255903e-06,
      "loss": 2.3359,
      "step": 128500
    },
    {
      "epoch": 2.643696368279079,
      "grad_norm": 0.47330722212791443,
      "learning_rate": 7.308307256521329e-06,
      "loss": 2.3835,
      "step": 128510
    },
    {
      "epoch": 2.6439020877959467,
      "grad_norm": 0.45848318934440613,
      "learning_rate": 7.299975197762865e-06,
      "loss": 2.3384,
      "step": 128520
    },
    {
      "epoch": 2.6441078073128144,
      "grad_norm": 0.5589292645454407,
      "learning_rate": 7.291647711391492e-06,
      "loss": 2.3854,
      "step": 128530
    },
    {
      "epoch": 2.6443135268296825,
      "grad_norm": 0.44337454438209534,
      "learning_rate": 7.283324797817903e-06,
      "loss": 2.4023,
      "step": 128540
    },
    {
      "epoch": 2.64451924634655,
      "grad_norm": 0.480247437953949,
      "learning_rate": 7.275006457452671e-06,
      "loss": 2.3814,
      "step": 128550
    },
    {
      "epoch": 2.6447249658634178,
      "grad_norm": 0.4439818263053894,
      "learning_rate": 7.266692690706101e-06,
      "loss": 2.4019,
      "step": 128560
    },
    {
      "epoch": 2.6449306853802854,
      "grad_norm": 0.4538324773311615,
      "learning_rate": 7.258383497988208e-06,
      "loss": 2.2896,
      "step": 128570
    },
    {
      "epoch": 2.645136404897153,
      "grad_norm": 0.44061505794525146,
      "learning_rate": 7.25007887970891e-06,
      "loss": 2.3247,
      "step": 128580
    },
    {
      "epoch": 2.6453421244140207,
      "grad_norm": 0.46800497174263,
      "learning_rate": 7.241778836277768e-06,
      "loss": 2.3334,
      "step": 128590
    },
    {
      "epoch": 2.6455478439308884,
      "grad_norm": 0.45186877250671387,
      "learning_rate": 7.2334833681041995e-06,
      "loss": 2.3562,
      "step": 128600
    },
    {
      "epoch": 2.645753563447756,
      "grad_norm": 0.44450289011001587,
      "learning_rate": 7.225192475597409e-06,
      "loss": 2.3633,
      "step": 128610
    },
    {
      "epoch": 2.6459592829646237,
      "grad_norm": 0.48336082696914673,
      "learning_rate": 7.216906159166292e-06,
      "loss": 2.3607,
      "step": 128620
    },
    {
      "epoch": 2.646165002481492,
      "grad_norm": 0.5172832608222961,
      "learning_rate": 7.2086244192195785e-06,
      "loss": 2.3707,
      "step": 128630
    },
    {
      "epoch": 2.6463707219983594,
      "grad_norm": 0.4508272409439087,
      "learning_rate": 7.2003472561657955e-06,
      "loss": 2.373,
      "step": 128640
    },
    {
      "epoch": 2.646576441515227,
      "grad_norm": 0.45496708154678345,
      "learning_rate": 7.192074670413173e-06,
      "loss": 2.3468,
      "step": 128650
    },
    {
      "epoch": 2.6467821610320947,
      "grad_norm": 0.4895736277103424,
      "learning_rate": 7.183806662369763e-06,
      "loss": 2.3399,
      "step": 128660
    },
    {
      "epoch": 2.6469878805489624,
      "grad_norm": 0.45601320266723633,
      "learning_rate": 7.175543232443382e-06,
      "loss": 2.367,
      "step": 128670
    },
    {
      "epoch": 2.6471936000658305,
      "grad_norm": 0.4583655297756195,
      "learning_rate": 7.167284381041628e-06,
      "loss": 2.3224,
      "step": 128680
    },
    {
      "epoch": 2.647399319582698,
      "grad_norm": 0.47745224833488464,
      "learning_rate": 7.159030108571851e-06,
      "loss": 2.3438,
      "step": 128690
    },
    {
      "epoch": 2.647605039099566,
      "grad_norm": 0.4583047330379486,
      "learning_rate": 7.150780415441194e-06,
      "loss": 2.343,
      "step": 128700
    },
    {
      "epoch": 2.6478107586164334,
      "grad_norm": 0.46179136633872986,
      "learning_rate": 7.142535302056574e-06,
      "loss": 2.3767,
      "step": 128710
    },
    {
      "epoch": 2.648016478133301,
      "grad_norm": 0.5029553771018982,
      "learning_rate": 7.134294768824679e-06,
      "loss": 2.3975,
      "step": 128720
    },
    {
      "epoch": 2.6482221976501688,
      "grad_norm": 0.4763062000274658,
      "learning_rate": 7.126058816151959e-06,
      "loss": 2.3667,
      "step": 128730
    },
    {
      "epoch": 2.6484279171670364,
      "grad_norm": 0.5132637619972229,
      "learning_rate": 7.117827444444669e-06,
      "loss": 2.3596,
      "step": 128740
    },
    {
      "epoch": 2.648633636683904,
      "grad_norm": 0.4718583822250366,
      "learning_rate": 7.109600654108783e-06,
      "loss": 2.3232,
      "step": 128750
    },
    {
      "epoch": 2.6488393562007717,
      "grad_norm": 0.4747738838195801,
      "learning_rate": 7.1013784455501106e-06,
      "loss": 2.3781,
      "step": 128760
    },
    {
      "epoch": 2.64904507571764,
      "grad_norm": 0.4689757227897644,
      "learning_rate": 7.0931608191742275e-06,
      "loss": 2.3834,
      "step": 128770
    },
    {
      "epoch": 2.6492507952345075,
      "grad_norm": 0.4394749701023102,
      "learning_rate": 7.084947775386397e-06,
      "loss": 2.3513,
      "step": 128780
    },
    {
      "epoch": 2.649456514751375,
      "grad_norm": 0.45449820160865784,
      "learning_rate": 7.076739314591785e-06,
      "loss": 2.3785,
      "step": 128790
    },
    {
      "epoch": 2.6496622342682428,
      "grad_norm": 0.4999774396419525,
      "learning_rate": 7.068535437195278e-06,
      "loss": 2.3111,
      "step": 128800
    },
    {
      "epoch": 2.6498679537851104,
      "grad_norm": 0.4394708275794983,
      "learning_rate": 7.060336143601465e-06,
      "loss": 2.3739,
      "step": 128810
    },
    {
      "epoch": 2.6500736733019785,
      "grad_norm": 0.4759140908718109,
      "learning_rate": 7.052141434214832e-06,
      "loss": 2.3292,
      "step": 128820
    },
    {
      "epoch": 2.650279392818846,
      "grad_norm": 0.50832200050354,
      "learning_rate": 7.043951309439556e-06,
      "loss": 2.3556,
      "step": 128830
    },
    {
      "epoch": 2.650485112335714,
      "grad_norm": 0.4506639838218689,
      "learning_rate": 7.035765769679581e-06,
      "loss": 2.4073,
      "step": 128840
    },
    {
      "epoch": 2.6506908318525815,
      "grad_norm": 0.506812572479248,
      "learning_rate": 7.027584815338728e-06,
      "loss": 2.3468,
      "step": 128850
    },
    {
      "epoch": 2.650896551369449,
      "grad_norm": 0.4187503755092621,
      "learning_rate": 7.019408446820442e-06,
      "loss": 2.3395,
      "step": 128860
    },
    {
      "epoch": 2.651102270886317,
      "grad_norm": 0.44759300351142883,
      "learning_rate": 7.011236664528076e-06,
      "loss": 2.3894,
      "step": 128870
    },
    {
      "epoch": 2.6513079904031844,
      "grad_norm": 0.4662706255912781,
      "learning_rate": 7.003069468864687e-06,
      "loss": 2.3733,
      "step": 128880
    },
    {
      "epoch": 2.651513709920052,
      "grad_norm": 0.4652387797832489,
      "learning_rate": 6.994906860233075e-06,
      "loss": 2.3608,
      "step": 128890
    },
    {
      "epoch": 2.6517194294369197,
      "grad_norm": 0.5085709095001221,
      "learning_rate": 6.986748839035928e-06,
      "loss": 2.337,
      "step": 128900
    },
    {
      "epoch": 2.651925148953788,
      "grad_norm": 0.462979793548584,
      "learning_rate": 6.9785954056755805e-06,
      "loss": 2.3874,
      "step": 128910
    },
    {
      "epoch": 2.6521308684706555,
      "grad_norm": 0.47475045919418335,
      "learning_rate": 6.9704465605542e-06,
      "loss": 2.3556,
      "step": 128920
    },
    {
      "epoch": 2.652336587987523,
      "grad_norm": 0.44986405968666077,
      "learning_rate": 6.962302304073776e-06,
      "loss": 2.3685,
      "step": 128930
    },
    {
      "epoch": 2.652542307504391,
      "grad_norm": 0.44484055042266846,
      "learning_rate": 6.954162636635963e-06,
      "loss": 2.3542,
      "step": 128940
    },
    {
      "epoch": 2.6527480270212584,
      "grad_norm": 0.44125834107398987,
      "learning_rate": 6.946027558642254e-06,
      "loss": 2.3445,
      "step": 128950
    },
    {
      "epoch": 2.652953746538126,
      "grad_norm": 0.4199475347995758,
      "learning_rate": 6.93789707049396e-06,
      "loss": 2.3972,
      "step": 128960
    },
    {
      "epoch": 2.653159466054994,
      "grad_norm": 0.44787147641181946,
      "learning_rate": 6.929771172592047e-06,
      "loss": 2.3578,
      "step": 128970
    },
    {
      "epoch": 2.653365185571862,
      "grad_norm": 0.4552471339702606,
      "learning_rate": 6.9216498653373655e-06,
      "loss": 2.3439,
      "step": 128980
    },
    {
      "epoch": 2.6535709050887295,
      "grad_norm": 0.4720457196235657,
      "learning_rate": 6.913533149130469e-06,
      "loss": 2.3893,
      "step": 128990
    },
    {
      "epoch": 2.653776624605597,
      "grad_norm": 0.5023858547210693,
      "learning_rate": 6.905421024371728e-06,
      "loss": 2.3347,
      "step": 129000
    },
    {
      "epoch": 2.653982344122465,
      "grad_norm": 0.46469998359680176,
      "learning_rate": 6.897313491461255e-06,
      "loss": 2.3165,
      "step": 129010
    },
    {
      "epoch": 2.6541880636393325,
      "grad_norm": 0.5001580119132996,
      "learning_rate": 6.889210550798963e-06,
      "loss": 2.3785,
      "step": 129020
    },
    {
      "epoch": 2.6543937831562,
      "grad_norm": 0.45287540555000305,
      "learning_rate": 6.881112202784523e-06,
      "loss": 2.3849,
      "step": 129030
    },
    {
      "epoch": 2.6545995026730678,
      "grad_norm": 0.4255165457725525,
      "learning_rate": 6.873018447817381e-06,
      "loss": 2.4188,
      "step": 129040
    },
    {
      "epoch": 2.6548052221899354,
      "grad_norm": 0.4706258773803711,
      "learning_rate": 6.864929286296762e-06,
      "loss": 2.3349,
      "step": 129050
    },
    {
      "epoch": 2.6550109417068035,
      "grad_norm": 0.4420704245567322,
      "learning_rate": 6.856844718621658e-06,
      "loss": 2.3678,
      "step": 129060
    },
    {
      "epoch": 2.655216661223671,
      "grad_norm": 0.5333466529846191,
      "learning_rate": 6.848764745190817e-06,
      "loss": 2.405,
      "step": 129070
    },
    {
      "epoch": 2.655422380740539,
      "grad_norm": 0.42469510436058044,
      "learning_rate": 6.840689366402808e-06,
      "loss": 2.4188,
      "step": 129080
    },
    {
      "epoch": 2.6556281002574065,
      "grad_norm": 0.48811233043670654,
      "learning_rate": 6.832618582655948e-06,
      "loss": 2.3998,
      "step": 129090
    },
    {
      "epoch": 2.655833819774274,
      "grad_norm": 0.4801636040210724,
      "learning_rate": 6.824552394348283e-06,
      "loss": 2.4512,
      "step": 129100
    },
    {
      "epoch": 2.6560395392911422,
      "grad_norm": 0.4642956554889679,
      "learning_rate": 6.8164908018777284e-06,
      "loss": 2.3409,
      "step": 129110
    },
    {
      "epoch": 2.65624525880801,
      "grad_norm": 0.506846010684967,
      "learning_rate": 6.808433805641901e-06,
      "loss": 2.3638,
      "step": 129120
    },
    {
      "epoch": 2.6564509783248775,
      "grad_norm": 0.47728434205055237,
      "learning_rate": 6.80038140603817e-06,
      "loss": 2.3477,
      "step": 129130
    },
    {
      "epoch": 2.656656697841745,
      "grad_norm": 0.45552295446395874,
      "learning_rate": 6.7923336034637745e-06,
      "loss": 2.3822,
      "step": 129140
    },
    {
      "epoch": 2.656862417358613,
      "grad_norm": 0.43467119336128235,
      "learning_rate": 6.7842903983156296e-06,
      "loss": 2.3231,
      "step": 129150
    },
    {
      "epoch": 2.6570681368754805,
      "grad_norm": 0.45373499393463135,
      "learning_rate": 6.776251790990451e-06,
      "loss": 2.3434,
      "step": 129160
    },
    {
      "epoch": 2.657273856392348,
      "grad_norm": 0.48604533076286316,
      "learning_rate": 6.768217781884811e-06,
      "loss": 2.4145,
      "step": 129170
    },
    {
      "epoch": 2.657479575909216,
      "grad_norm": 0.4198109805583954,
      "learning_rate": 6.760188371394904e-06,
      "loss": 2.3609,
      "step": 129180
    },
    {
      "epoch": 2.6576852954260834,
      "grad_norm": 0.4791092574596405,
      "learning_rate": 6.752163559916813e-06,
      "loss": 2.3894,
      "step": 129190
    },
    {
      "epoch": 2.6578910149429515,
      "grad_norm": 0.4403349459171295,
      "learning_rate": 6.744143347846366e-06,
      "loss": 2.3809,
      "step": 129200
    },
    {
      "epoch": 2.658096734459819,
      "grad_norm": 0.49639782309532166,
      "learning_rate": 6.736127735579134e-06,
      "loss": 2.3466,
      "step": 129210
    },
    {
      "epoch": 2.658302453976687,
      "grad_norm": 0.4570555090904236,
      "learning_rate": 6.728116723510491e-06,
      "loss": 2.3287,
      "step": 129220
    },
    {
      "epoch": 2.6585081734935545,
      "grad_norm": 0.48149582743644714,
      "learning_rate": 6.720110312035577e-06,
      "loss": 2.4056,
      "step": 129230
    },
    {
      "epoch": 2.658713893010422,
      "grad_norm": 0.4970163404941559,
      "learning_rate": 6.7121085015492965e-06,
      "loss": 2.3589,
      "step": 129240
    },
    {
      "epoch": 2.6589196125272903,
      "grad_norm": 0.4588348865509033,
      "learning_rate": 6.70411129244638e-06,
      "loss": 2.4184,
      "step": 129250
    },
    {
      "epoch": 2.659125332044158,
      "grad_norm": 0.42396941781044006,
      "learning_rate": 6.696118685121233e-06,
      "loss": 2.3413,
      "step": 129260
    },
    {
      "epoch": 2.6593310515610256,
      "grad_norm": 0.49645939469337463,
      "learning_rate": 6.688130679968107e-06,
      "loss": 2.3861,
      "step": 129270
    },
    {
      "epoch": 2.659536771077893,
      "grad_norm": 0.44470810890197754,
      "learning_rate": 6.680147277380999e-06,
      "loss": 2.3571,
      "step": 129280
    },
    {
      "epoch": 2.659742490594761,
      "grad_norm": 0.4713957905769348,
      "learning_rate": 6.6721684777537045e-06,
      "loss": 2.4033,
      "step": 129290
    },
    {
      "epoch": 2.6599482101116285,
      "grad_norm": 0.4628687798976898,
      "learning_rate": 6.664194281479752e-06,
      "loss": 2.323,
      "step": 129300
    },
    {
      "epoch": 2.660153929628496,
      "grad_norm": 0.4699665606021881,
      "learning_rate": 6.656224688952484e-06,
      "loss": 2.4027,
      "step": 129310
    },
    {
      "epoch": 2.660359649145364,
      "grad_norm": 0.44773146510124207,
      "learning_rate": 6.648259700564996e-06,
      "loss": 2.3491,
      "step": 129320
    },
    {
      "epoch": 2.6605653686622315,
      "grad_norm": 0.4129509925842285,
      "learning_rate": 6.640299316710142e-06,
      "loss": 2.3665,
      "step": 129330
    },
    {
      "epoch": 2.6607710881790996,
      "grad_norm": 0.4838657081127167,
      "learning_rate": 6.632343537780572e-06,
      "loss": 2.3819,
      "step": 129340
    },
    {
      "epoch": 2.6609768076959672,
      "grad_norm": 0.43353232741355896,
      "learning_rate": 6.624392364168719e-06,
      "loss": 2.3531,
      "step": 129350
    },
    {
      "epoch": 2.661182527212835,
      "grad_norm": 0.4876876175403595,
      "learning_rate": 6.6164457962667235e-06,
      "loss": 2.3992,
      "step": 129360
    },
    {
      "epoch": 2.6613882467297025,
      "grad_norm": 0.4581476151943207,
      "learning_rate": 6.608503834466595e-06,
      "loss": 2.3904,
      "step": 129370
    },
    {
      "epoch": 2.66159396624657,
      "grad_norm": 0.4837564527988434,
      "learning_rate": 6.600566479160064e-06,
      "loss": 2.3532,
      "step": 129380
    },
    {
      "epoch": 2.661799685763438,
      "grad_norm": 0.4157671332359314,
      "learning_rate": 6.592633730738595e-06,
      "loss": 2.3555,
      "step": 129390
    },
    {
      "epoch": 2.662005405280306,
      "grad_norm": 0.4633291959762573,
      "learning_rate": 6.58470558959351e-06,
      "loss": 2.3652,
      "step": 129400
    },
    {
      "epoch": 2.6622111247971736,
      "grad_norm": 0.45420151948928833,
      "learning_rate": 6.57678205611586e-06,
      "loss": 2.3923,
      "step": 129410
    },
    {
      "epoch": 2.6624168443140412,
      "grad_norm": 0.46164771914482117,
      "learning_rate": 6.5688631306964235e-06,
      "loss": 2.374,
      "step": 129420
    },
    {
      "epoch": 2.662622563830909,
      "grad_norm": 0.5298276543617249,
      "learning_rate": 6.560948813725865e-06,
      "loss": 2.4022,
      "step": 129430
    },
    {
      "epoch": 2.6628282833477765,
      "grad_norm": 0.46402642130851746,
      "learning_rate": 6.553039105594494e-06,
      "loss": 2.3658,
      "step": 129440
    },
    {
      "epoch": 2.663034002864644,
      "grad_norm": 0.5064341425895691,
      "learning_rate": 6.5451340066924775e-06,
      "loss": 2.3219,
      "step": 129450
    },
    {
      "epoch": 2.663239722381512,
      "grad_norm": 0.48690488934516907,
      "learning_rate": 6.5372335174097575e-06,
      "loss": 2.3641,
      "step": 129460
    },
    {
      "epoch": 2.6634454418983795,
      "grad_norm": 0.4642142057418823,
      "learning_rate": 6.529337638135979e-06,
      "loss": 2.3891,
      "step": 129470
    },
    {
      "epoch": 2.663651161415247,
      "grad_norm": 0.40485435724258423,
      "learning_rate": 6.521446369260609e-06,
      "loss": 2.4045,
      "step": 129480
    },
    {
      "epoch": 2.6638568809321153,
      "grad_norm": 0.4371861517429352,
      "learning_rate": 6.5135597111729115e-06,
      "loss": 2.3426,
      "step": 129490
    },
    {
      "epoch": 2.664062600448983,
      "grad_norm": 0.457701176404953,
      "learning_rate": 6.505677664261878e-06,
      "loss": 2.3822,
      "step": 129500
    },
    {
      "epoch": 2.6642683199658506,
      "grad_norm": 0.44249290227890015,
      "learning_rate": 6.497800228916262e-06,
      "loss": 2.3649,
      "step": 129510
    },
    {
      "epoch": 2.664474039482718,
      "grad_norm": 0.4857548177242279,
      "learning_rate": 6.489927405524654e-06,
      "loss": 2.4189,
      "step": 129520
    },
    {
      "epoch": 2.664679758999586,
      "grad_norm": 0.46911346912384033,
      "learning_rate": 6.482059194475343e-06,
      "loss": 2.3434,
      "step": 129530
    },
    {
      "epoch": 2.664885478516454,
      "grad_norm": 0.4584265947341919,
      "learning_rate": 6.4741955961564515e-06,
      "loss": 2.4185,
      "step": 129540
    },
    {
      "epoch": 2.6650911980333216,
      "grad_norm": 0.5188436508178711,
      "learning_rate": 6.466336610955826e-06,
      "loss": 2.3816,
      "step": 129550
    },
    {
      "epoch": 2.6652969175501893,
      "grad_norm": 0.4586500823497772,
      "learning_rate": 6.458482239261132e-06,
      "loss": 2.3987,
      "step": 129560
    },
    {
      "epoch": 2.665502637067057,
      "grad_norm": 0.466730535030365,
      "learning_rate": 6.450632481459762e-06,
      "loss": 2.3721,
      "step": 129570
    },
    {
      "epoch": 2.6657083565839246,
      "grad_norm": 0.5223210453987122,
      "learning_rate": 6.442787337938905e-06,
      "loss": 2.3717,
      "step": 129580
    },
    {
      "epoch": 2.6659140761007922,
      "grad_norm": 0.4439070522785187,
      "learning_rate": 6.434946809085529e-06,
      "loss": 2.3495,
      "step": 129590
    },
    {
      "epoch": 2.66611979561766,
      "grad_norm": 0.4200740456581116,
      "learning_rate": 6.427110895286348e-06,
      "loss": 2.3284,
      "step": 129600
    },
    {
      "epoch": 2.6663255151345275,
      "grad_norm": 0.4646508991718292,
      "learning_rate": 6.419279596927885e-06,
      "loss": 2.3762,
      "step": 129610
    },
    {
      "epoch": 2.666531234651395,
      "grad_norm": 0.47994887828826904,
      "learning_rate": 6.411452914396409e-06,
      "loss": 2.3407,
      "step": 129620
    },
    {
      "epoch": 2.6667369541682633,
      "grad_norm": 0.4261853098869324,
      "learning_rate": 6.403630848077958e-06,
      "loss": 2.3097,
      "step": 129630
    },
    {
      "epoch": 2.666942673685131,
      "grad_norm": 0.47330138087272644,
      "learning_rate": 6.395813398358352e-06,
      "loss": 2.299,
      "step": 129640
    },
    {
      "epoch": 2.6671483932019986,
      "grad_norm": 0.48689380288124084,
      "learning_rate": 6.388000565623198e-06,
      "loss": 2.3897,
      "step": 129650
    },
    {
      "epoch": 2.6673541127188662,
      "grad_norm": 0.48749691247940063,
      "learning_rate": 6.380192350257852e-06,
      "loss": 2.374,
      "step": 129660
    },
    {
      "epoch": 2.667559832235734,
      "grad_norm": 0.4442450702190399,
      "learning_rate": 6.372388752647463e-06,
      "loss": 2.3609,
      "step": 129670
    },
    {
      "epoch": 2.6677655517526015,
      "grad_norm": 0.4345978796482086,
      "learning_rate": 6.364589773176899e-06,
      "loss": 2.3823,
      "step": 129680
    },
    {
      "epoch": 2.6679712712694696,
      "grad_norm": 0.426382452249527,
      "learning_rate": 6.356795412230887e-06,
      "loss": 2.3492,
      "step": 129690
    },
    {
      "epoch": 2.6681769907863373,
      "grad_norm": 0.45611152052879333,
      "learning_rate": 6.349005670193875e-06,
      "loss": 2.3433,
      "step": 129700
    },
    {
      "epoch": 2.668382710303205,
      "grad_norm": 0.4686994254589081,
      "learning_rate": 6.341220547450044e-06,
      "loss": 2.3201,
      "step": 129710
    },
    {
      "epoch": 2.6685884298200726,
      "grad_norm": 0.4828794002532959,
      "learning_rate": 6.333440044383443e-06,
      "loss": 2.3396,
      "step": 129720
    },
    {
      "epoch": 2.6687941493369403,
      "grad_norm": 0.4761585593223572,
      "learning_rate": 6.325664161377842e-06,
      "loss": 2.3728,
      "step": 129730
    },
    {
      "epoch": 2.668999868853808,
      "grad_norm": 0.4577093720436096,
      "learning_rate": 6.317892898816724e-06,
      "loss": 2.3641,
      "step": 129740
    },
    {
      "epoch": 2.6692055883706756,
      "grad_norm": 0.46586874127388,
      "learning_rate": 6.310126257083482e-06,
      "loss": 2.383,
      "step": 129750
    },
    {
      "epoch": 2.669411307887543,
      "grad_norm": 0.4881041347980499,
      "learning_rate": 6.302364236561143e-06,
      "loss": 2.3729,
      "step": 129760
    },
    {
      "epoch": 2.669617027404411,
      "grad_norm": 0.45514506101608276,
      "learning_rate": 6.294606837632566e-06,
      "loss": 2.3139,
      "step": 129770
    },
    {
      "epoch": 2.669822746921279,
      "grad_norm": 0.46874508261680603,
      "learning_rate": 6.2868540606804475e-06,
      "loss": 2.3349,
      "step": 129780
    },
    {
      "epoch": 2.6700284664381466,
      "grad_norm": 0.46941521763801575,
      "learning_rate": 6.279105906087113e-06,
      "loss": 2.4274,
      "step": 129790
    },
    {
      "epoch": 2.6702341859550143,
      "grad_norm": 0.44718828797340393,
      "learning_rate": 6.271362374234757e-06,
      "loss": 2.3718,
      "step": 129800
    },
    {
      "epoch": 2.670439905471882,
      "grad_norm": 0.5173564553260803,
      "learning_rate": 6.263623465505364e-06,
      "loss": 2.3683,
      "step": 129810
    },
    {
      "epoch": 2.6706456249887496,
      "grad_norm": 0.4716058373451233,
      "learning_rate": 6.255889180280605e-06,
      "loss": 2.3257,
      "step": 129820
    },
    {
      "epoch": 2.6708513445056177,
      "grad_norm": 0.44577649235725403,
      "learning_rate": 6.2481595189419875e-06,
      "loss": 2.3598,
      "step": 129830
    },
    {
      "epoch": 2.6710570640224853,
      "grad_norm": 0.47676435112953186,
      "learning_rate": 6.2404344818707825e-06,
      "loss": 2.3606,
      "step": 129840
    },
    {
      "epoch": 2.671262783539353,
      "grad_norm": 0.4987559914588928,
      "learning_rate": 6.232714069448009e-06,
      "loss": 2.3486,
      "step": 129850
    },
    {
      "epoch": 2.6714685030562206,
      "grad_norm": 0.4945160746574402,
      "learning_rate": 6.224998282054484e-06,
      "loss": 2.3401,
      "step": 129860
    },
    {
      "epoch": 2.6716742225730883,
      "grad_norm": 0.5037990212440491,
      "learning_rate": 6.21728712007078e-06,
      "loss": 2.3383,
      "step": 129870
    },
    {
      "epoch": 2.671879942089956,
      "grad_norm": 0.44908392429351807,
      "learning_rate": 6.20958058387725e-06,
      "loss": 2.3827,
      "step": 129880
    },
    {
      "epoch": 2.6720856616068236,
      "grad_norm": 0.47321146726608276,
      "learning_rate": 6.2018786738540115e-06,
      "loss": 2.3955,
      "step": 129890
    },
    {
      "epoch": 2.6722913811236912,
      "grad_norm": 0.46679219603538513,
      "learning_rate": 6.194181390380959e-06,
      "loss": 2.3526,
      "step": 129900
    },
    {
      "epoch": 2.672497100640559,
      "grad_norm": 0.4803461730480194,
      "learning_rate": 6.1864887338377584e-06,
      "loss": 2.3752,
      "step": 129910
    },
    {
      "epoch": 2.672702820157427,
      "grad_norm": 0.5180032253265381,
      "learning_rate": 6.178800704603849e-06,
      "loss": 2.3923,
      "step": 129920
    },
    {
      "epoch": 2.6729085396742946,
      "grad_norm": 0.42052602767944336,
      "learning_rate": 6.171117303058438e-06,
      "loss": 2.3852,
      "step": 129930
    },
    {
      "epoch": 2.6731142591911623,
      "grad_norm": 0.5004473328590393,
      "learning_rate": 6.163438529580501e-06,
      "loss": 2.3837,
      "step": 129940
    },
    {
      "epoch": 2.67331997870803,
      "grad_norm": 0.5950962901115417,
      "learning_rate": 6.15576438454879e-06,
      "loss": 2.347,
      "step": 129950
    },
    {
      "epoch": 2.6735256982248976,
      "grad_norm": 0.49277761578559875,
      "learning_rate": 6.148094868341836e-06,
      "loss": 2.4368,
      "step": 129960
    },
    {
      "epoch": 2.6737314177417657,
      "grad_norm": 0.442922443151474,
      "learning_rate": 6.140429981337925e-06,
      "loss": 2.3705,
      "step": 129970
    },
    {
      "epoch": 2.6739371372586334,
      "grad_norm": 0.5253792405128479,
      "learning_rate": 6.132769723915133e-06,
      "loss": 2.3626,
      "step": 129980
    },
    {
      "epoch": 2.674142856775501,
      "grad_norm": 0.4549085795879364,
      "learning_rate": 6.125114096451312e-06,
      "loss": 2.3503,
      "step": 129990
    },
    {
      "epoch": 2.6743485762923687,
      "grad_norm": 0.47018346190452576,
      "learning_rate": 6.117463099324028e-06,
      "loss": 2.3503,
      "step": 130000
    },
    {
      "epoch": 2.6745542958092363,
      "grad_norm": 0.49540528655052185,
      "learning_rate": 6.109816732910701e-06,
      "loss": 2.3458,
      "step": 130010
    },
    {
      "epoch": 2.674760015326104,
      "grad_norm": 0.4250156879425049,
      "learning_rate": 6.102174997588494e-06,
      "loss": 2.328,
      "step": 130020
    },
    {
      "epoch": 2.6749657348429716,
      "grad_norm": 0.47589418292045593,
      "learning_rate": 6.094537893734287e-06,
      "loss": 2.3707,
      "step": 130030
    },
    {
      "epoch": 2.6751714543598393,
      "grad_norm": 0.4702780842781067,
      "learning_rate": 6.086905421724809e-06,
      "loss": 2.3653,
      "step": 130040
    },
    {
      "epoch": 2.675377173876707,
      "grad_norm": 0.48508286476135254,
      "learning_rate": 6.0792775819365375e-06,
      "loss": 2.3068,
      "step": 130050
    },
    {
      "epoch": 2.675582893393575,
      "grad_norm": 0.4635004699230194,
      "learning_rate": 6.0716543747456614e-06,
      "loss": 2.3464,
      "step": 130060
    },
    {
      "epoch": 2.6757886129104427,
      "grad_norm": 0.45675531029701233,
      "learning_rate": 6.0640358005282675e-06,
      "loss": 2.402,
      "step": 130070
    },
    {
      "epoch": 2.6759943324273103,
      "grad_norm": 0.46677857637405396,
      "learning_rate": 6.0564218596600775e-06,
      "loss": 2.3324,
      "step": 130080
    },
    {
      "epoch": 2.676200051944178,
      "grad_norm": 0.5133145451545715,
      "learning_rate": 6.048812552516658e-06,
      "loss": 2.3982,
      "step": 130090
    },
    {
      "epoch": 2.6764057714610456,
      "grad_norm": 0.49179285764694214,
      "learning_rate": 6.041207879473365e-06,
      "loss": 2.3225,
      "step": 130100
    },
    {
      "epoch": 2.6766114909779133,
      "grad_norm": 0.45929253101348877,
      "learning_rate": 6.0336078409052535e-06,
      "loss": 2.3549,
      "step": 130110
    },
    {
      "epoch": 2.6768172104947814,
      "grad_norm": 0.415681391954422,
      "learning_rate": 6.026012437187212e-06,
      "loss": 2.3668,
      "step": 130120
    },
    {
      "epoch": 2.677022930011649,
      "grad_norm": 0.4324509799480438,
      "learning_rate": 6.018421668693896e-06,
      "loss": 2.3416,
      "step": 130130
    },
    {
      "epoch": 2.6772286495285167,
      "grad_norm": 0.4957982897758484,
      "learning_rate": 6.010835535799686e-06,
      "loss": 2.4178,
      "step": 130140
    },
    {
      "epoch": 2.6774343690453843,
      "grad_norm": 0.46240469813346863,
      "learning_rate": 6.003254038878792e-06,
      "loss": 2.3931,
      "step": 130150
    },
    {
      "epoch": 2.677640088562252,
      "grad_norm": 0.4557264447212219,
      "learning_rate": 5.995677178305148e-06,
      "loss": 2.3674,
      "step": 130160
    },
    {
      "epoch": 2.6778458080791196,
      "grad_norm": 0.4663335382938385,
      "learning_rate": 5.9881049544524895e-06,
      "loss": 2.3804,
      "step": 130170
    },
    {
      "epoch": 2.6780515275959873,
      "grad_norm": 0.46098002791404724,
      "learning_rate": 5.980537367694306e-06,
      "loss": 2.3844,
      "step": 130180
    },
    {
      "epoch": 2.678257247112855,
      "grad_norm": 0.433092325925827,
      "learning_rate": 5.972974418403865e-06,
      "loss": 2.337,
      "step": 130190
    },
    {
      "epoch": 2.6784629666297226,
      "grad_norm": 0.4935997426509857,
      "learning_rate": 5.965416106954213e-06,
      "loss": 2.4314,
      "step": 130200
    },
    {
      "epoch": 2.6786686861465907,
      "grad_norm": 0.42807653546333313,
      "learning_rate": 5.957862433718164e-06,
      "loss": 2.3807,
      "step": 130210
    },
    {
      "epoch": 2.6788744056634584,
      "grad_norm": 0.48643791675567627,
      "learning_rate": 5.950313399068297e-06,
      "loss": 2.3849,
      "step": 130220
    },
    {
      "epoch": 2.679080125180326,
      "grad_norm": 0.5046836733818054,
      "learning_rate": 5.942769003376947e-06,
      "loss": 2.3942,
      "step": 130230
    },
    {
      "epoch": 2.6792858446971937,
      "grad_norm": 0.43948131799697876,
      "learning_rate": 5.935229247016261e-06,
      "loss": 2.3382,
      "step": 130240
    },
    {
      "epoch": 2.6794915642140613,
      "grad_norm": 0.5294464230537415,
      "learning_rate": 5.927694130358119e-06,
      "loss": 2.3882,
      "step": 130250
    },
    {
      "epoch": 2.6796972837309294,
      "grad_norm": 0.49426937103271484,
      "learning_rate": 5.920163653774191e-06,
      "loss": 2.343,
      "step": 130260
    },
    {
      "epoch": 2.679903003247797,
      "grad_norm": 0.42870014905929565,
      "learning_rate": 5.912637817635925e-06,
      "loss": 2.3391,
      "step": 130270
    },
    {
      "epoch": 2.6801087227646647,
      "grad_norm": 0.4552129805088043,
      "learning_rate": 5.905116622314522e-06,
      "loss": 2.386,
      "step": 130280
    },
    {
      "epoch": 2.6803144422815324,
      "grad_norm": 0.42527300119400024,
      "learning_rate": 5.897600068180953e-06,
      "loss": 2.3354,
      "step": 130290
    },
    {
      "epoch": 2.6805201617984,
      "grad_norm": 0.4772188365459442,
      "learning_rate": 5.890088155605978e-06,
      "loss": 2.36,
      "step": 130300
    },
    {
      "epoch": 2.6807258813152677,
      "grad_norm": 0.46391475200653076,
      "learning_rate": 5.882580884960131e-06,
      "loss": 2.3351,
      "step": 130310
    },
    {
      "epoch": 2.6809316008321353,
      "grad_norm": 0.4595518410205841,
      "learning_rate": 5.875078256613664e-06,
      "loss": 2.2974,
      "step": 130320
    },
    {
      "epoch": 2.681137320349003,
      "grad_norm": 0.4690777063369751,
      "learning_rate": 5.867580270936679e-06,
      "loss": 2.4379,
      "step": 130330
    },
    {
      "epoch": 2.6813430398658706,
      "grad_norm": 0.4727610647678375,
      "learning_rate": 5.860086928299014e-06,
      "loss": 2.3473,
      "step": 130340
    },
    {
      "epoch": 2.6815487593827387,
      "grad_norm": 0.48897334933280945,
      "learning_rate": 5.852598229070227e-06,
      "loss": 2.3445,
      "step": 130350
    },
    {
      "epoch": 2.6817544788996064,
      "grad_norm": 0.48592352867126465,
      "learning_rate": 5.845114173619759e-06,
      "loss": 2.3474,
      "step": 130360
    },
    {
      "epoch": 2.681960198416474,
      "grad_norm": 0.4386325776576996,
      "learning_rate": 5.837634762316713e-06,
      "loss": 2.3318,
      "step": 130370
    },
    {
      "epoch": 2.6821659179333417,
      "grad_norm": 0.4491814374923706,
      "learning_rate": 5.830159995530005e-06,
      "loss": 2.3777,
      "step": 130380
    },
    {
      "epoch": 2.6823716374502093,
      "grad_norm": 0.4611400067806244,
      "learning_rate": 5.822689873628373e-06,
      "loss": 2.2974,
      "step": 130390
    },
    {
      "epoch": 2.682577356967077,
      "grad_norm": 0.42724648118019104,
      "learning_rate": 5.815224396980235e-06,
      "loss": 2.3622,
      "step": 130400
    },
    {
      "epoch": 2.682783076483945,
      "grad_norm": 0.4468308091163635,
      "learning_rate": 5.807763565953805e-06,
      "loss": 2.3973,
      "step": 130410
    },
    {
      "epoch": 2.6829887960008127,
      "grad_norm": 0.4771515429019928,
      "learning_rate": 5.800307380917147e-06,
      "loss": 2.3077,
      "step": 130420
    },
    {
      "epoch": 2.6831945155176804,
      "grad_norm": 0.48742738366127014,
      "learning_rate": 5.792855842237987e-06,
      "loss": 2.3666,
      "step": 130430
    },
    {
      "epoch": 2.683400235034548,
      "grad_norm": 0.4491879642009735,
      "learning_rate": 5.785408950283877e-06,
      "loss": 2.3863,
      "step": 130440
    },
    {
      "epoch": 2.6836059545514157,
      "grad_norm": 0.5365762114524841,
      "learning_rate": 5.7779667054221335e-06,
      "loss": 2.4447,
      "step": 130450
    },
    {
      "epoch": 2.6838116740682834,
      "grad_norm": 0.4367244839668274,
      "learning_rate": 5.770529108019851e-06,
      "loss": 2.3593,
      "step": 130460
    },
    {
      "epoch": 2.684017393585151,
      "grad_norm": 0.4607141315937042,
      "learning_rate": 5.7630961584438814e-06,
      "loss": 2.4212,
      "step": 130470
    },
    {
      "epoch": 2.6842231131020187,
      "grad_norm": 0.4384079873561859,
      "learning_rate": 5.755667857060843e-06,
      "loss": 2.3531,
      "step": 130480
    },
    {
      "epoch": 2.6844288326188863,
      "grad_norm": 0.49513372778892517,
      "learning_rate": 5.7482442042371295e-06,
      "loss": 2.3465,
      "step": 130490
    },
    {
      "epoch": 2.6846345521357544,
      "grad_norm": 0.4627208709716797,
      "learning_rate": 5.7408252003389285e-06,
      "loss": 2.3626,
      "step": 130500
    },
    {
      "epoch": 2.684840271652622,
      "grad_norm": 0.4643903076648712,
      "learning_rate": 5.733410845732168e-06,
      "loss": 2.3501,
      "step": 130510
    },
    {
      "epoch": 2.6850459911694897,
      "grad_norm": 0.5174078941345215,
      "learning_rate": 5.726001140782566e-06,
      "loss": 2.3602,
      "step": 130520
    },
    {
      "epoch": 2.6852517106863574,
      "grad_norm": 0.5111880898475647,
      "learning_rate": 5.718596085855588e-06,
      "loss": 2.388,
      "step": 130530
    },
    {
      "epoch": 2.685457430203225,
      "grad_norm": 0.4686740040779114,
      "learning_rate": 5.711195681316494e-06,
      "loss": 2.3814,
      "step": 130540
    },
    {
      "epoch": 2.685663149720093,
      "grad_norm": 0.4998861849308014,
      "learning_rate": 5.703799927530307e-06,
      "loss": 2.3591,
      "step": 130550
    },
    {
      "epoch": 2.6858688692369608,
      "grad_norm": 0.4881743788719177,
      "learning_rate": 5.6964088248618205e-06,
      "loss": 2.3268,
      "step": 130560
    },
    {
      "epoch": 2.6860745887538284,
      "grad_norm": 0.4440256655216217,
      "learning_rate": 5.68902237367559e-06,
      "loss": 2.3367,
      "step": 130570
    },
    {
      "epoch": 2.686280308270696,
      "grad_norm": 0.4504004418849945,
      "learning_rate": 5.681640574335944e-06,
      "loss": 2.4215,
      "step": 130580
    },
    {
      "epoch": 2.6864860277875637,
      "grad_norm": 0.43686559796333313,
      "learning_rate": 5.674263427207005e-06,
      "loss": 2.4107,
      "step": 130590
    },
    {
      "epoch": 2.6866917473044314,
      "grad_norm": 0.4524196982383728,
      "learning_rate": 5.666890932652646e-06,
      "loss": 2.3861,
      "step": 130600
    },
    {
      "epoch": 2.686897466821299,
      "grad_norm": 0.49356380105018616,
      "learning_rate": 5.659523091036478e-06,
      "loss": 2.3763,
      "step": 130610
    },
    {
      "epoch": 2.6871031863381667,
      "grad_norm": 0.46118319034576416,
      "learning_rate": 5.6521599027219654e-06,
      "loss": 2.3892,
      "step": 130620
    },
    {
      "epoch": 2.6873089058550343,
      "grad_norm": 0.4696023464202881,
      "learning_rate": 5.644801368072272e-06,
      "loss": 2.3171,
      "step": 130630
    },
    {
      "epoch": 2.6875146253719024,
      "grad_norm": 0.4209893047809601,
      "learning_rate": 5.6374474874503314e-06,
      "loss": 2.2979,
      "step": 130640
    },
    {
      "epoch": 2.68772034488877,
      "grad_norm": 0.4699150323867798,
      "learning_rate": 5.630098261218897e-06,
      "loss": 2.3826,
      "step": 130650
    },
    {
      "epoch": 2.6879260644056377,
      "grad_norm": 0.4705483019351959,
      "learning_rate": 5.6227536897404896e-06,
      "loss": 2.3221,
      "step": 130660
    },
    {
      "epoch": 2.6881317839225054,
      "grad_norm": 0.4512860178947449,
      "learning_rate": 5.6154137733773095e-06,
      "loss": 2.3647,
      "step": 130670
    },
    {
      "epoch": 2.688337503439373,
      "grad_norm": 0.4655158221721649,
      "learning_rate": 5.608078512491466e-06,
      "loss": 2.3469,
      "step": 130680
    },
    {
      "epoch": 2.688543222956241,
      "grad_norm": 0.5690358877182007,
      "learning_rate": 5.600747907444714e-06,
      "loss": 2.3974,
      "step": 130690
    },
    {
      "epoch": 2.688748942473109,
      "grad_norm": 0.42804569005966187,
      "learning_rate": 5.593421958598644e-06,
      "loss": 2.333,
      "step": 130700
    },
    {
      "epoch": 2.6889546619899765,
      "grad_norm": 0.4335978329181671,
      "learning_rate": 5.5861006663146535e-06,
      "loss": 2.3195,
      "step": 130710
    },
    {
      "epoch": 2.689160381506844,
      "grad_norm": 0.4696512520313263,
      "learning_rate": 5.578784030953799e-06,
      "loss": 2.2804,
      "step": 130720
    },
    {
      "epoch": 2.6893661010237118,
      "grad_norm": 0.5412485003471375,
      "learning_rate": 5.57147205287698e-06,
      "loss": 2.3273,
      "step": 130730
    },
    {
      "epoch": 2.6895718205405794,
      "grad_norm": 0.46411556005477905,
      "learning_rate": 5.5641647324448985e-06,
      "loss": 2.3467,
      "step": 130740
    },
    {
      "epoch": 2.689777540057447,
      "grad_norm": 0.42780664563179016,
      "learning_rate": 5.556862070017943e-06,
      "loss": 2.3816,
      "step": 130750
    },
    {
      "epoch": 2.6899832595743147,
      "grad_norm": 0.4743292033672333,
      "learning_rate": 5.549564065956325e-06,
      "loss": 2.3761,
      "step": 130760
    },
    {
      "epoch": 2.6901889790911824,
      "grad_norm": 0.4791424572467804,
      "learning_rate": 5.542270720620024e-06,
      "loss": 2.4023,
      "step": 130770
    },
    {
      "epoch": 2.6903946986080505,
      "grad_norm": 0.4944671392440796,
      "learning_rate": 5.534982034368774e-06,
      "loss": 2.3483,
      "step": 130780
    },
    {
      "epoch": 2.690600418124918,
      "grad_norm": 0.44764313101768494,
      "learning_rate": 5.527698007562087e-06,
      "loss": 2.38,
      "step": 130790
    },
    {
      "epoch": 2.690806137641786,
      "grad_norm": 0.46200886368751526,
      "learning_rate": 5.520418640559255e-06,
      "loss": 2.3827,
      "step": 130800
    },
    {
      "epoch": 2.6910118571586534,
      "grad_norm": 0.44345471262931824,
      "learning_rate": 5.513143933719311e-06,
      "loss": 2.3926,
      "step": 130810
    },
    {
      "epoch": 2.691217576675521,
      "grad_norm": 0.45014819502830505,
      "learning_rate": 5.505873887401081e-06,
      "loss": 2.3485,
      "step": 130820
    },
    {
      "epoch": 2.6914232961923887,
      "grad_norm": 0.5626469850540161,
      "learning_rate": 5.498608501963165e-06,
      "loss": 2.3855,
      "step": 130830
    },
    {
      "epoch": 2.691629015709257,
      "grad_norm": 0.4629802405834198,
      "learning_rate": 5.491347777763922e-06,
      "loss": 2.3695,
      "step": 130840
    },
    {
      "epoch": 2.6918347352261245,
      "grad_norm": 0.4451808035373688,
      "learning_rate": 5.484091715161477e-06,
      "loss": 2.2906,
      "step": 130850
    },
    {
      "epoch": 2.692040454742992,
      "grad_norm": 0.47161930799484253,
      "learning_rate": 5.4768403145137426e-06,
      "loss": 2.3898,
      "step": 130860
    },
    {
      "epoch": 2.69224617425986,
      "grad_norm": 0.6050906181335449,
      "learning_rate": 5.469593576178389e-06,
      "loss": 2.3424,
      "step": 130870
    },
    {
      "epoch": 2.6924518937767274,
      "grad_norm": 0.46607354283332825,
      "learning_rate": 5.4623515005128524e-06,
      "loss": 2.4098,
      "step": 130880
    },
    {
      "epoch": 2.692657613293595,
      "grad_norm": 0.49096956849098206,
      "learning_rate": 5.455114087874347e-06,
      "loss": 2.3643,
      "step": 130890
    },
    {
      "epoch": 2.6928633328104628,
      "grad_norm": 0.4872114062309265,
      "learning_rate": 5.4478813386198534e-06,
      "loss": 2.3668,
      "step": 130900
    },
    {
      "epoch": 2.6930690523273304,
      "grad_norm": 0.5124660134315491,
      "learning_rate": 5.4406532531061315e-06,
      "loss": 2.3065,
      "step": 130910
    },
    {
      "epoch": 2.693274771844198,
      "grad_norm": 0.44507697224617004,
      "learning_rate": 5.4334298316897066e-06,
      "loss": 2.3342,
      "step": 130920
    },
    {
      "epoch": 2.693480491361066,
      "grad_norm": 0.4632525146007538,
      "learning_rate": 5.426211074726839e-06,
      "loss": 2.3574,
      "step": 130930
    },
    {
      "epoch": 2.693686210877934,
      "grad_norm": 0.5214082598686218,
      "learning_rate": 5.418996982573621e-06,
      "loss": 2.4537,
      "step": 130940
    },
    {
      "epoch": 2.6938919303948015,
      "grad_norm": 0.4787115752696991,
      "learning_rate": 5.411787555585901e-06,
      "loss": 2.4061,
      "step": 130950
    },
    {
      "epoch": 2.694097649911669,
      "grad_norm": 0.44960638880729675,
      "learning_rate": 5.404582794119217e-06,
      "loss": 2.3851,
      "step": 130960
    },
    {
      "epoch": 2.6943033694285368,
      "grad_norm": 0.46170878410339355,
      "learning_rate": 5.3973826985290076e-06,
      "loss": 2.3812,
      "step": 130970
    },
    {
      "epoch": 2.694509088945405,
      "grad_norm": 0.47110480070114136,
      "learning_rate": 5.390187269170388e-06,
      "loss": 2.3505,
      "step": 130980
    },
    {
      "epoch": 2.6947148084622725,
      "grad_norm": 0.4638212323188782,
      "learning_rate": 5.382996506398253e-06,
      "loss": 2.3437,
      "step": 130990
    },
    {
      "epoch": 2.69492052797914,
      "grad_norm": 0.46496814489364624,
      "learning_rate": 5.375810410567317e-06,
      "loss": 2.3555,
      "step": 131000
    },
    {
      "epoch": 2.695126247496008,
      "grad_norm": 0.4886745512485504,
      "learning_rate": 5.368628982032009e-06,
      "loss": 2.3859,
      "step": 131010
    },
    {
      "epoch": 2.6953319670128755,
      "grad_norm": 0.42431557178497314,
      "learning_rate": 5.361452221146535e-06,
      "loss": 2.3675,
      "step": 131020
    },
    {
      "epoch": 2.695537686529743,
      "grad_norm": 0.5819180011749268,
      "learning_rate": 5.354280128264932e-06,
      "loss": 2.3509,
      "step": 131030
    },
    {
      "epoch": 2.695743406046611,
      "grad_norm": 0.5127250552177429,
      "learning_rate": 5.347112703740919e-06,
      "loss": 2.3677,
      "step": 131040
    },
    {
      "epoch": 2.6959491255634784,
      "grad_norm": 0.5600208044052124,
      "learning_rate": 5.339949947928025e-06,
      "loss": 2.3789,
      "step": 131050
    },
    {
      "epoch": 2.696154845080346,
      "grad_norm": 0.4286767542362213,
      "learning_rate": 5.332791861179598e-06,
      "loss": 2.3688,
      "step": 131060
    },
    {
      "epoch": 2.696360564597214,
      "grad_norm": 0.49173879623413086,
      "learning_rate": 5.325638443848658e-06,
      "loss": 2.3465,
      "step": 131070
    },
    {
      "epoch": 2.696566284114082,
      "grad_norm": 0.48776888847351074,
      "learning_rate": 5.318489696288054e-06,
      "loss": 2.3428,
      "step": 131080
    },
    {
      "epoch": 2.6967720036309495,
      "grad_norm": 0.4638388752937317,
      "learning_rate": 5.311345618850405e-06,
      "loss": 2.3986,
      "step": 131090
    },
    {
      "epoch": 2.696977723147817,
      "grad_norm": 0.44099161028862,
      "learning_rate": 5.304206211888085e-06,
      "loss": 2.394,
      "step": 131100
    },
    {
      "epoch": 2.697183442664685,
      "grad_norm": 0.4206532835960388,
      "learning_rate": 5.297071475753246e-06,
      "loss": 2.3409,
      "step": 131110
    },
    {
      "epoch": 2.697389162181553,
      "grad_norm": 0.4161962568759918,
      "learning_rate": 5.289941410797805e-06,
      "loss": 2.422,
      "step": 131120
    },
    {
      "epoch": 2.6975948816984205,
      "grad_norm": 0.5750868320465088,
      "learning_rate": 5.282816017373438e-06,
      "loss": 2.4083,
      "step": 131130
    },
    {
      "epoch": 2.697800601215288,
      "grad_norm": 0.4466553032398224,
      "learning_rate": 5.2756952958316175e-06,
      "loss": 2.3407,
      "step": 131140
    },
    {
      "epoch": 2.698006320732156,
      "grad_norm": 0.4901137351989746,
      "learning_rate": 5.268579246523553e-06,
      "loss": 2.3967,
      "step": 131150
    },
    {
      "epoch": 2.6982120402490235,
      "grad_norm": 0.4323514401912689,
      "learning_rate": 5.261467869800263e-06,
      "loss": 2.3874,
      "step": 131160
    },
    {
      "epoch": 2.698417759765891,
      "grad_norm": 0.4887642562389374,
      "learning_rate": 5.2543611660124895e-06,
      "loss": 2.3843,
      "step": 131170
    },
    {
      "epoch": 2.698623479282759,
      "grad_norm": 0.4430076479911804,
      "learning_rate": 5.247259135510774e-06,
      "loss": 2.353,
      "step": 131180
    },
    {
      "epoch": 2.6988291987996265,
      "grad_norm": 0.4650104343891144,
      "learning_rate": 5.240161778645436e-06,
      "loss": 2.3711,
      "step": 131190
    },
    {
      "epoch": 2.699034918316494,
      "grad_norm": 0.45543646812438965,
      "learning_rate": 5.233069095766529e-06,
      "loss": 2.3515,
      "step": 131200
    },
    {
      "epoch": 2.699240637833362,
      "grad_norm": 0.44692379236221313,
      "learning_rate": 5.225981087223908e-06,
      "loss": 2.3966,
      "step": 131210
    },
    {
      "epoch": 2.69944635735023,
      "grad_norm": 0.4694293141365051,
      "learning_rate": 5.218897753367191e-06,
      "loss": 2.284,
      "step": 131220
    },
    {
      "epoch": 2.6996520768670975,
      "grad_norm": 0.4586712718009949,
      "learning_rate": 5.211819094545745e-06,
      "loss": 2.3506,
      "step": 131230
    },
    {
      "epoch": 2.699857796383965,
      "grad_norm": 0.4643290936946869,
      "learning_rate": 5.204745111108744e-06,
      "loss": 2.3254,
      "step": 131240
    },
    {
      "epoch": 2.700063515900833,
      "grad_norm": 0.4903799295425415,
      "learning_rate": 5.197675803405078e-06,
      "loss": 2.3821,
      "step": 131250
    },
    {
      "epoch": 2.7002692354177005,
      "grad_norm": 0.45952677726745605,
      "learning_rate": 5.1906111717834545e-06,
      "loss": 2.3551,
      "step": 131260
    },
    {
      "epoch": 2.7004749549345686,
      "grad_norm": 0.4513457119464874,
      "learning_rate": 5.183551216592353e-06,
      "loss": 2.293,
      "step": 131270
    },
    {
      "epoch": 2.7006806744514362,
      "grad_norm": 0.4763336777687073,
      "learning_rate": 5.1764959381799485e-06,
      "loss": 2.4402,
      "step": 131280
    },
    {
      "epoch": 2.700886393968304,
      "grad_norm": 0.5426816344261169,
      "learning_rate": 5.169445336894285e-06,
      "loss": 2.3905,
      "step": 131290
    },
    {
      "epoch": 2.7010921134851715,
      "grad_norm": 0.48917412757873535,
      "learning_rate": 5.162399413083141e-06,
      "loss": 2.3798,
      "step": 131300
    },
    {
      "epoch": 2.701297833002039,
      "grad_norm": 0.4695962071418762,
      "learning_rate": 5.155358167094004e-06,
      "loss": 2.3975,
      "step": 131310
    },
    {
      "epoch": 2.701503552518907,
      "grad_norm": 0.49270063638687134,
      "learning_rate": 5.148321599274231e-06,
      "loss": 2.3308,
      "step": 131320
    },
    {
      "epoch": 2.7017092720357745,
      "grad_norm": 0.4488101303577423,
      "learning_rate": 5.141289709970853e-06,
      "loss": 2.4093,
      "step": 131330
    },
    {
      "epoch": 2.701914991552642,
      "grad_norm": 0.5539028644561768,
      "learning_rate": 5.1342624995307285e-06,
      "loss": 2.295,
      "step": 131340
    },
    {
      "epoch": 2.70212071106951,
      "grad_norm": 0.5259115695953369,
      "learning_rate": 5.127239968300512e-06,
      "loss": 2.3825,
      "step": 131350
    },
    {
      "epoch": 2.702326430586378,
      "grad_norm": 0.49569833278656006,
      "learning_rate": 5.120222116626538e-06,
      "loss": 2.3764,
      "step": 131360
    },
    {
      "epoch": 2.7025321501032455,
      "grad_norm": 0.49551481008529663,
      "learning_rate": 5.1132089448549526e-06,
      "loss": 2.3824,
      "step": 131370
    },
    {
      "epoch": 2.702737869620113,
      "grad_norm": 0.48471131920814514,
      "learning_rate": 5.106200453331733e-06,
      "loss": 2.3397,
      "step": 131380
    },
    {
      "epoch": 2.702943589136981,
      "grad_norm": 0.4397314488887787,
      "learning_rate": 5.099196642402515e-06,
      "loss": 2.3779,
      "step": 131390
    },
    {
      "epoch": 2.7031493086538485,
      "grad_norm": 0.42711299657821655,
      "learning_rate": 5.092197512412788e-06,
      "loss": 2.3306,
      "step": 131400
    },
    {
      "epoch": 2.7033550281707166,
      "grad_norm": 0.4816475212574005,
      "learning_rate": 5.085203063707766e-06,
      "loss": 2.3316,
      "step": 131410
    },
    {
      "epoch": 2.7035607476875843,
      "grad_norm": 0.47447869181632996,
      "learning_rate": 5.078213296632451e-06,
      "loss": 2.3471,
      "step": 131420
    },
    {
      "epoch": 2.703766467204452,
      "grad_norm": 0.4455311596393585,
      "learning_rate": 5.07122821153162e-06,
      "loss": 2.4104,
      "step": 131430
    },
    {
      "epoch": 2.7039721867213196,
      "grad_norm": 0.45787331461906433,
      "learning_rate": 5.0642478087498e-06,
      "loss": 2.3314,
      "step": 131440
    },
    {
      "epoch": 2.704177906238187,
      "grad_norm": 0.4636133015155792,
      "learning_rate": 5.057272088631293e-06,
      "loss": 2.3304,
      "step": 131450
    },
    {
      "epoch": 2.704383625755055,
      "grad_norm": 0.4137677848339081,
      "learning_rate": 5.0503010515201785e-06,
      "loss": 2.3824,
      "step": 131460
    },
    {
      "epoch": 2.7045893452719225,
      "grad_norm": 0.4321557581424713,
      "learning_rate": 5.043334697760305e-06,
      "loss": 2.3651,
      "step": 131470
    },
    {
      "epoch": 2.70479506478879,
      "grad_norm": 0.4355122745037079,
      "learning_rate": 5.036373027695274e-06,
      "loss": 2.4045,
      "step": 131480
    },
    {
      "epoch": 2.705000784305658,
      "grad_norm": 0.4560462534427643,
      "learning_rate": 5.029416041668477e-06,
      "loss": 2.3827,
      "step": 131490
    },
    {
      "epoch": 2.705206503822526,
      "grad_norm": 0.48446959257125854,
      "learning_rate": 5.022463740023054e-06,
      "loss": 2.345,
      "step": 131500
    },
    {
      "epoch": 2.7054122233393936,
      "grad_norm": 0.49062639474868774,
      "learning_rate": 5.015516123101938e-06,
      "loss": 2.4106,
      "step": 131510
    },
    {
      "epoch": 2.7056179428562612,
      "grad_norm": 0.46357378363609314,
      "learning_rate": 5.008573191247801e-06,
      "loss": 2.4281,
      "step": 131520
    },
    {
      "epoch": 2.705823662373129,
      "grad_norm": 0.4638608694076538,
      "learning_rate": 5.001634944803113e-06,
      "loss": 2.3391,
      "step": 131530
    },
    {
      "epoch": 2.7060293818899965,
      "grad_norm": 0.4797007739543915,
      "learning_rate": 4.9947013841100784e-06,
      "loss": 2.3121,
      "step": 131540
    },
    {
      "epoch": 2.706235101406864,
      "grad_norm": 0.4778223931789398,
      "learning_rate": 4.9877725095107245e-06,
      "loss": 2.3769,
      "step": 131550
    },
    {
      "epoch": 2.7064408209237323,
      "grad_norm": 0.45763567090034485,
      "learning_rate": 4.980848321346798e-06,
      "loss": 2.2985,
      "step": 131560
    },
    {
      "epoch": 2.7066465404406,
      "grad_norm": 0.5856661200523376,
      "learning_rate": 4.973928819959817e-06,
      "loss": 2.3172,
      "step": 131570
    },
    {
      "epoch": 2.7068522599574676,
      "grad_norm": 0.5362565517425537,
      "learning_rate": 4.967014005691106e-06,
      "loss": 2.3576,
      "step": 131580
    },
    {
      "epoch": 2.7070579794743352,
      "grad_norm": 0.43262699246406555,
      "learning_rate": 4.9601038788817385e-06,
      "loss": 2.3255,
      "step": 131590
    },
    {
      "epoch": 2.707263698991203,
      "grad_norm": 0.4732236862182617,
      "learning_rate": 4.9531984398725085e-06,
      "loss": 2.3269,
      "step": 131600
    },
    {
      "epoch": 2.7074694185080705,
      "grad_norm": 0.5107091665267944,
      "learning_rate": 4.946297689004098e-06,
      "loss": 2.3456,
      "step": 131610
    },
    {
      "epoch": 2.707675138024938,
      "grad_norm": 0.49804773926734924,
      "learning_rate": 4.939401626616813e-06,
      "loss": 2.3935,
      "step": 131620
    },
    {
      "epoch": 2.707880857541806,
      "grad_norm": 0.5143727660179138,
      "learning_rate": 4.932510253050826e-06,
      "loss": 2.3189,
      "step": 131630
    },
    {
      "epoch": 2.7080865770586735,
      "grad_norm": 0.48028141260147095,
      "learning_rate": 4.925623568646076e-06,
      "loss": 2.3111,
      "step": 131640
    },
    {
      "epoch": 2.7082922965755416,
      "grad_norm": 0.41025227308273315,
      "learning_rate": 4.918741573742213e-06,
      "loss": 2.3449,
      "step": 131650
    },
    {
      "epoch": 2.7084980160924093,
      "grad_norm": 0.4456799626350403,
      "learning_rate": 4.911864268678678e-06,
      "loss": 2.4148,
      "step": 131660
    },
    {
      "epoch": 2.708703735609277,
      "grad_norm": 0.48004215955734253,
      "learning_rate": 4.904991653794733e-06,
      "loss": 2.4345,
      "step": 131670
    },
    {
      "epoch": 2.7089094551261446,
      "grad_norm": 0.42352110147476196,
      "learning_rate": 4.898123729429338e-06,
      "loss": 2.3135,
      "step": 131680
    },
    {
      "epoch": 2.709115174643012,
      "grad_norm": 0.44225552678108215,
      "learning_rate": 4.891260495921257e-06,
      "loss": 2.324,
      "step": 131690
    },
    {
      "epoch": 2.7093208941598803,
      "grad_norm": 0.5086683034896851,
      "learning_rate": 4.884401953609008e-06,
      "loss": 2.3916,
      "step": 131700
    },
    {
      "epoch": 2.709526613676748,
      "grad_norm": 0.4992547929286957,
      "learning_rate": 4.8775481028308975e-06,
      "loss": 2.3282,
      "step": 131710
    },
    {
      "epoch": 2.7097323331936156,
      "grad_norm": 0.461274117231369,
      "learning_rate": 4.8706989439249895e-06,
      "loss": 2.3908,
      "step": 131720
    },
    {
      "epoch": 2.7099380527104833,
      "grad_norm": 0.46947669982910156,
      "learning_rate": 4.863854477229101e-06,
      "loss": 2.3842,
      "step": 131730
    },
    {
      "epoch": 2.710143772227351,
      "grad_norm": 0.4158293902873993,
      "learning_rate": 4.8570147030808424e-06,
      "loss": 2.3528,
      "step": 131740
    },
    {
      "epoch": 2.7103494917442186,
      "grad_norm": 0.42500466108322144,
      "learning_rate": 4.850179621817585e-06,
      "loss": 2.3816,
      "step": 131750
    },
    {
      "epoch": 2.7105552112610862,
      "grad_norm": 0.4563896059989929,
      "learning_rate": 4.843349233776462e-06,
      "loss": 2.4004,
      "step": 131760
    },
    {
      "epoch": 2.710760930777954,
      "grad_norm": 0.43953096866607666,
      "learning_rate": 4.83652353929438e-06,
      "loss": 2.3618,
      "step": 131770
    },
    {
      "epoch": 2.7109666502948215,
      "grad_norm": 0.43080779910087585,
      "learning_rate": 4.829702538708014e-06,
      "loss": 2.3112,
      "step": 131780
    },
    {
      "epoch": 2.7111723698116896,
      "grad_norm": 0.49249663949012756,
      "learning_rate": 4.822886232353807e-06,
      "loss": 2.3711,
      "step": 131790
    },
    {
      "epoch": 2.7113780893285573,
      "grad_norm": 0.44122928380966187,
      "learning_rate": 4.816074620567956e-06,
      "loss": 2.4,
      "step": 131800
    },
    {
      "epoch": 2.711583808845425,
      "grad_norm": 0.5338829159736633,
      "learning_rate": 4.809267703686471e-06,
      "loss": 2.3104,
      "step": 131810
    },
    {
      "epoch": 2.7117895283622926,
      "grad_norm": 0.43097060918807983,
      "learning_rate": 4.802465482045071e-06,
      "loss": 2.4375,
      "step": 131820
    },
    {
      "epoch": 2.7119952478791602,
      "grad_norm": 0.44785064458847046,
      "learning_rate": 4.795667955979288e-06,
      "loss": 2.3847,
      "step": 131830
    },
    {
      "epoch": 2.7122009673960283,
      "grad_norm": 0.4510716199874878,
      "learning_rate": 4.78887512582441e-06,
      "loss": 2.3362,
      "step": 131840
    },
    {
      "epoch": 2.712406686912896,
      "grad_norm": 0.4202408790588379,
      "learning_rate": 4.782086991915491e-06,
      "loss": 2.3204,
      "step": 131850
    },
    {
      "epoch": 2.7126124064297636,
      "grad_norm": 0.4332893192768097,
      "learning_rate": 4.775303554587318e-06,
      "loss": 2.3556,
      "step": 131860
    },
    {
      "epoch": 2.7128181259466313,
      "grad_norm": 0.5765002369880676,
      "learning_rate": 4.768524814174524e-06,
      "loss": 2.3987,
      "step": 131870
    },
    {
      "epoch": 2.713023845463499,
      "grad_norm": 0.4662157893180847,
      "learning_rate": 4.761750771011464e-06,
      "loss": 2.3606,
      "step": 131880
    },
    {
      "epoch": 2.7132295649803666,
      "grad_norm": 0.4792040288448334,
      "learning_rate": 4.754981425432226e-06,
      "loss": 2.2844,
      "step": 131890
    },
    {
      "epoch": 2.7134352844972343,
      "grad_norm": 0.45260289311408997,
      "learning_rate": 4.748216777770742e-06,
      "loss": 2.3738,
      "step": 131900
    },
    {
      "epoch": 2.713641004014102,
      "grad_norm": 0.4702627658843994,
      "learning_rate": 4.74145682836068e-06,
      "loss": 2.2856,
      "step": 131910
    },
    {
      "epoch": 2.7138467235309696,
      "grad_norm": 0.4816310703754425,
      "learning_rate": 4.734701577535427e-06,
      "loss": 2.4189,
      "step": 131920
    },
    {
      "epoch": 2.7140524430478377,
      "grad_norm": 0.4789102077484131,
      "learning_rate": 4.727951025628241e-06,
      "loss": 2.3795,
      "step": 131930
    },
    {
      "epoch": 2.7142581625647053,
      "grad_norm": 0.4723414480686188,
      "learning_rate": 4.721205172972043e-06,
      "loss": 2.348,
      "step": 131940
    },
    {
      "epoch": 2.714463882081573,
      "grad_norm": 0.4426935315132141,
      "learning_rate": 4.714464019899578e-06,
      "loss": 2.3262,
      "step": 131950
    },
    {
      "epoch": 2.7146696015984406,
      "grad_norm": 0.4160332977771759,
      "learning_rate": 4.707727566743381e-06,
      "loss": 2.3171,
      "step": 131960
    },
    {
      "epoch": 2.7148753211153083,
      "grad_norm": 0.4643734097480774,
      "learning_rate": 4.700995813835696e-06,
      "loss": 2.352,
      "step": 131970
    },
    {
      "epoch": 2.715081040632176,
      "grad_norm": 0.49959349632263184,
      "learning_rate": 4.6942687615085575e-06,
      "loss": 2.3359,
      "step": 131980
    },
    {
      "epoch": 2.715286760149044,
      "grad_norm": 0.43152308464050293,
      "learning_rate": 4.687546410093801e-06,
      "loss": 2.3479,
      "step": 131990
    },
    {
      "epoch": 2.7154924796659117,
      "grad_norm": 0.43739861249923706,
      "learning_rate": 4.680828759922984e-06,
      "loss": 2.4228,
      "step": 132000
    },
    {
      "epoch": 2.7156981991827793,
      "grad_norm": 0.46162450313568115,
      "learning_rate": 4.674115811327462e-06,
      "loss": 2.3563,
      "step": 132010
    },
    {
      "epoch": 2.715903918699647,
      "grad_norm": 0.43391233682632446,
      "learning_rate": 4.667407564638338e-06,
      "loss": 2.3812,
      "step": 132020
    },
    {
      "epoch": 2.7161096382165146,
      "grad_norm": 0.4404960870742798,
      "learning_rate": 4.660704020186501e-06,
      "loss": 2.361,
      "step": 132030
    },
    {
      "epoch": 2.7163153577333823,
      "grad_norm": 0.4415140748023987,
      "learning_rate": 4.654005178302601e-06,
      "loss": 2.3269,
      "step": 132040
    },
    {
      "epoch": 2.71652107725025,
      "grad_norm": 0.43794989585876465,
      "learning_rate": 4.647311039317037e-06,
      "loss": 2.4111,
      "step": 132050
    },
    {
      "epoch": 2.7167267967671176,
      "grad_norm": 0.44596150517463684,
      "learning_rate": 4.640621603560025e-06,
      "loss": 2.4026,
      "step": 132060
    },
    {
      "epoch": 2.7169325162839852,
      "grad_norm": 0.43358004093170166,
      "learning_rate": 4.6339368713614995e-06,
      "loss": 2.3173,
      "step": 132070
    },
    {
      "epoch": 2.7171382358008533,
      "grad_norm": 0.4422239065170288,
      "learning_rate": 4.627256843051186e-06,
      "loss": 2.3635,
      "step": 132080
    },
    {
      "epoch": 2.717343955317721,
      "grad_norm": 0.4404597282409668,
      "learning_rate": 4.620581518958566e-06,
      "loss": 2.3839,
      "step": 132090
    },
    {
      "epoch": 2.7175496748345886,
      "grad_norm": 0.4451432526111603,
      "learning_rate": 4.61391089941291e-06,
      "loss": 2.3924,
      "step": 132100
    },
    {
      "epoch": 2.7177553943514563,
      "grad_norm": 0.47581052780151367,
      "learning_rate": 4.607244984743242e-06,
      "loss": 2.3749,
      "step": 132110
    },
    {
      "epoch": 2.717961113868324,
      "grad_norm": 0.4835212826728821,
      "learning_rate": 4.600583775278344e-06,
      "loss": 2.3592,
      "step": 132120
    },
    {
      "epoch": 2.718166833385192,
      "grad_norm": 0.4606187343597412,
      "learning_rate": 4.593927271346788e-06,
      "loss": 2.3692,
      "step": 132130
    },
    {
      "epoch": 2.7183725529020597,
      "grad_norm": 0.4985087513923645,
      "learning_rate": 4.587275473276886e-06,
      "loss": 2.3246,
      "step": 132140
    },
    {
      "epoch": 2.7185782724189274,
      "grad_norm": 0.4451431632041931,
      "learning_rate": 4.580628381396767e-06,
      "loss": 2.3627,
      "step": 132150
    },
    {
      "epoch": 2.718783991935795,
      "grad_norm": 0.43628451228141785,
      "learning_rate": 4.5739859960342694e-06,
      "loss": 2.3891,
      "step": 132160
    },
    {
      "epoch": 2.7189897114526627,
      "grad_norm": 0.46693849563598633,
      "learning_rate": 4.567348317517039e-06,
      "loss": 2.3867,
      "step": 132170
    },
    {
      "epoch": 2.7191954309695303,
      "grad_norm": 0.4433062970638275,
      "learning_rate": 4.560715346172451e-06,
      "loss": 2.3308,
      "step": 132180
    },
    {
      "epoch": 2.719401150486398,
      "grad_norm": 0.4598119258880615,
      "learning_rate": 4.554087082327707e-06,
      "loss": 2.3497,
      "step": 132190
    },
    {
      "epoch": 2.7196068700032656,
      "grad_norm": 0.4673546552658081,
      "learning_rate": 4.547463526309736e-06,
      "loss": 2.3441,
      "step": 132200
    },
    {
      "epoch": 2.7198125895201333,
      "grad_norm": 0.4483422040939331,
      "learning_rate": 4.540844678445222e-06,
      "loss": 2.3137,
      "step": 132210
    },
    {
      "epoch": 2.7200183090370014,
      "grad_norm": 0.4580477774143219,
      "learning_rate": 4.534230539060646e-06,
      "loss": 2.4155,
      "step": 132220
    },
    {
      "epoch": 2.720224028553869,
      "grad_norm": 0.48881447315216064,
      "learning_rate": 4.527621108482283e-06,
      "loss": 2.3712,
      "step": 132230
    },
    {
      "epoch": 2.7204297480707367,
      "grad_norm": 0.46617552638053894,
      "learning_rate": 4.52101638703607e-06,
      "loss": 2.3781,
      "step": 132240
    },
    {
      "epoch": 2.7206354675876043,
      "grad_norm": 0.462885320186615,
      "learning_rate": 4.514416375047847e-06,
      "loss": 2.3247,
      "step": 132250
    },
    {
      "epoch": 2.720841187104472,
      "grad_norm": 0.43257036805152893,
      "learning_rate": 4.507821072843121e-06,
      "loss": 2.3394,
      "step": 132260
    },
    {
      "epoch": 2.7210469066213396,
      "grad_norm": 0.4785431921482086,
      "learning_rate": 4.501230480747198e-06,
      "loss": 2.4165,
      "step": 132270
    },
    {
      "epoch": 2.7212526261382077,
      "grad_norm": 0.46062666177749634,
      "learning_rate": 4.494644599085196e-06,
      "loss": 2.3446,
      "step": 132280
    },
    {
      "epoch": 2.7214583456550754,
      "grad_norm": 0.5106936097145081,
      "learning_rate": 4.488063428181921e-06,
      "loss": 2.3533,
      "step": 132290
    },
    {
      "epoch": 2.721664065171943,
      "grad_norm": 0.4226111173629761,
      "learning_rate": 4.481486968361981e-06,
      "loss": 2.4205,
      "step": 132300
    },
    {
      "epoch": 2.7218697846888107,
      "grad_norm": 0.4641168415546417,
      "learning_rate": 4.474915219949816e-06,
      "loss": 2.3745,
      "step": 132310
    },
    {
      "epoch": 2.7220755042056783,
      "grad_norm": 0.4847273826599121,
      "learning_rate": 4.468348183269511e-06,
      "loss": 2.3155,
      "step": 132320
    },
    {
      "epoch": 2.722281223722546,
      "grad_norm": 0.5375431180000305,
      "learning_rate": 4.461785858644996e-06,
      "loss": 2.3986,
      "step": 132330
    },
    {
      "epoch": 2.7224869432394136,
      "grad_norm": 0.4536339342594147,
      "learning_rate": 4.455228246399979e-06,
      "loss": 2.3198,
      "step": 132340
    },
    {
      "epoch": 2.7226926627562813,
      "grad_norm": 0.5047907829284668,
      "learning_rate": 4.448675346857889e-06,
      "loss": 2.357,
      "step": 132350
    },
    {
      "epoch": 2.7228983822731494,
      "grad_norm": 0.4781618118286133,
      "learning_rate": 4.442127160341947e-06,
      "loss": 2.3216,
      "step": 132360
    },
    {
      "epoch": 2.723104101790017,
      "grad_norm": 0.4853490889072418,
      "learning_rate": 4.43558368717516e-06,
      "loss": 2.3161,
      "step": 132370
    },
    {
      "epoch": 2.7233098213068847,
      "grad_norm": 0.4495362639427185,
      "learning_rate": 4.429044927680248e-06,
      "loss": 2.329,
      "step": 132380
    },
    {
      "epoch": 2.7235155408237524,
      "grad_norm": 0.4454183578491211,
      "learning_rate": 4.422510882179765e-06,
      "loss": 2.3104,
      "step": 132390
    },
    {
      "epoch": 2.72372126034062,
      "grad_norm": 0.48380494117736816,
      "learning_rate": 4.4159815509959735e-06,
      "loss": 2.3388,
      "step": 132400
    },
    {
      "epoch": 2.7239269798574877,
      "grad_norm": 1.231567621231079,
      "learning_rate": 4.409456934450951e-06,
      "loss": 2.3374,
      "step": 132410
    },
    {
      "epoch": 2.7241326993743558,
      "grad_norm": 0.4555851221084595,
      "learning_rate": 4.402937032866506e-06,
      "loss": 2.3918,
      "step": 132420
    },
    {
      "epoch": 2.7243384188912234,
      "grad_norm": 0.4631957411766052,
      "learning_rate": 4.3964218465642355e-06,
      "loss": 2.3378,
      "step": 132430
    },
    {
      "epoch": 2.724544138408091,
      "grad_norm": 0.45107775926589966,
      "learning_rate": 4.389911375865507e-06,
      "loss": 2.4042,
      "step": 132440
    },
    {
      "epoch": 2.7247498579249587,
      "grad_norm": 0.47134390473365784,
      "learning_rate": 4.38340562109143e-06,
      "loss": 2.3697,
      "step": 132450
    },
    {
      "epoch": 2.7249555774418264,
      "grad_norm": 0.48667317628860474,
      "learning_rate": 4.376904582562902e-06,
      "loss": 2.3549,
      "step": 132460
    },
    {
      "epoch": 2.725161296958694,
      "grad_norm": 0.502815306186676,
      "learning_rate": 4.370408260600589e-06,
      "loss": 2.363,
      "step": 132470
    },
    {
      "epoch": 2.7253670164755617,
      "grad_norm": 0.47223401069641113,
      "learning_rate": 4.363916655524925e-06,
      "loss": 2.3541,
      "step": 132480
    },
    {
      "epoch": 2.7255727359924293,
      "grad_norm": 0.4138570725917816,
      "learning_rate": 4.357429767656107e-06,
      "loss": 2.372,
      "step": 132490
    },
    {
      "epoch": 2.725778455509297,
      "grad_norm": 0.44916942715644836,
      "learning_rate": 4.35094759731407e-06,
      "loss": 2.3245,
      "step": 132500
    },
    {
      "epoch": 2.725984175026165,
      "grad_norm": 0.46662014722824097,
      "learning_rate": 4.344470144818569e-06,
      "loss": 2.3382,
      "step": 132510
    },
    {
      "epoch": 2.7261898945430327,
      "grad_norm": 0.4393058121204376,
      "learning_rate": 4.3379974104891034e-06,
      "loss": 2.4192,
      "step": 132520
    },
    {
      "epoch": 2.7263956140599004,
      "grad_norm": 0.5301830172538757,
      "learning_rate": 4.331529394644918e-06,
      "loss": 2.3638,
      "step": 132530
    },
    {
      "epoch": 2.726601333576768,
      "grad_norm": 0.47552353143692017,
      "learning_rate": 4.325066097605057e-06,
      "loss": 2.3021,
      "step": 132540
    },
    {
      "epoch": 2.7268070530936357,
      "grad_norm": 0.44087931513786316,
      "learning_rate": 4.318607519688333e-06,
      "loss": 2.3802,
      "step": 132550
    },
    {
      "epoch": 2.727012772610504,
      "grad_norm": 0.4370546340942383,
      "learning_rate": 4.312153661213281e-06,
      "loss": 2.3599,
      "step": 132560
    },
    {
      "epoch": 2.7272184921273714,
      "grad_norm": 0.47995153069496155,
      "learning_rate": 4.3057045224982775e-06,
      "loss": 2.3623,
      "step": 132570
    },
    {
      "epoch": 2.727424211644239,
      "grad_norm": 0.45987841486930847,
      "learning_rate": 4.29926010386138e-06,
      "loss": 2.3568,
      "step": 132580
    },
    {
      "epoch": 2.7276299311611067,
      "grad_norm": 0.48490434885025024,
      "learning_rate": 4.292820405620468e-06,
      "loss": 2.3631,
      "step": 132590
    },
    {
      "epoch": 2.7278356506779744,
      "grad_norm": 0.45502278208732605,
      "learning_rate": 4.2863854280932094e-06,
      "loss": 2.3851,
      "step": 132600
    },
    {
      "epoch": 2.728041370194842,
      "grad_norm": 0.45274388790130615,
      "learning_rate": 4.279955171596961e-06,
      "loss": 2.3866,
      "step": 132610
    },
    {
      "epoch": 2.7282470897117097,
      "grad_norm": 0.462310791015625,
      "learning_rate": 4.273529636448903e-06,
      "loss": 2.4134,
      "step": 132620
    },
    {
      "epoch": 2.7284528092285774,
      "grad_norm": 0.42173412442207336,
      "learning_rate": 4.267108822966015e-06,
      "loss": 2.3422,
      "step": 132630
    },
    {
      "epoch": 2.728658528745445,
      "grad_norm": 0.4755074381828308,
      "learning_rate": 4.260692731464944e-06,
      "loss": 2.3445,
      "step": 132640
    },
    {
      "epoch": 2.728864248262313,
      "grad_norm": 0.46823936700820923,
      "learning_rate": 4.2542813622621914e-06,
      "loss": 2.3781,
      "step": 132650
    },
    {
      "epoch": 2.7290699677791808,
      "grad_norm": 0.44872912764549255,
      "learning_rate": 4.247874715673983e-06,
      "loss": 2.3431,
      "step": 132660
    },
    {
      "epoch": 2.7292756872960484,
      "grad_norm": 0.5200610160827637,
      "learning_rate": 4.241472792016321e-06,
      "loss": 2.3878,
      "step": 132670
    },
    {
      "epoch": 2.729481406812916,
      "grad_norm": 0.4457962214946747,
      "learning_rate": 4.235075591604998e-06,
      "loss": 2.3304,
      "step": 132680
    },
    {
      "epoch": 2.7296871263297837,
      "grad_norm": 0.5940274596214294,
      "learning_rate": 4.228683114755538e-06,
      "loss": 2.3218,
      "step": 132690
    },
    {
      "epoch": 2.7298928458466514,
      "grad_norm": 0.4493246376514435,
      "learning_rate": 4.222295361783246e-06,
      "loss": 2.3458,
      "step": 132700
    },
    {
      "epoch": 2.7300985653635195,
      "grad_norm": 0.5009530186653137,
      "learning_rate": 4.215912333003191e-06,
      "loss": 2.4118,
      "step": 132710
    },
    {
      "epoch": 2.730304284880387,
      "grad_norm": 0.4419707655906677,
      "learning_rate": 4.209534028730233e-06,
      "loss": 2.3431,
      "step": 132720
    },
    {
      "epoch": 2.7305100043972548,
      "grad_norm": 0.4565768539905548,
      "learning_rate": 4.203160449278953e-06,
      "loss": 2.379,
      "step": 132730
    },
    {
      "epoch": 2.7307157239141224,
      "grad_norm": 0.4790125787258148,
      "learning_rate": 4.196791594963745e-06,
      "loss": 2.4055,
      "step": 132740
    },
    {
      "epoch": 2.73092144343099,
      "grad_norm": 0.5051000714302063,
      "learning_rate": 4.1904274660987454e-06,
      "loss": 2.4335,
      "step": 132750
    },
    {
      "epoch": 2.7311271629478577,
      "grad_norm": 0.4572807252407074,
      "learning_rate": 4.184068062997848e-06,
      "loss": 2.4317,
      "step": 132760
    },
    {
      "epoch": 2.7313328824647254,
      "grad_norm": 0.44688668847084045,
      "learning_rate": 4.1777133859747465e-06,
      "loss": 2.3259,
      "step": 132770
    },
    {
      "epoch": 2.731538601981593,
      "grad_norm": 0.47432518005371094,
      "learning_rate": 4.17136343534289e-06,
      "loss": 2.3118,
      "step": 132780
    },
    {
      "epoch": 2.7317443214984607,
      "grad_norm": 0.5470717549324036,
      "learning_rate": 4.16501821141545e-06,
      "loss": 2.3743,
      "step": 132790
    },
    {
      "epoch": 2.731950041015329,
      "grad_norm": 0.5181742906570435,
      "learning_rate": 4.158677714505421e-06,
      "loss": 2.3851,
      "step": 132800
    },
    {
      "epoch": 2.7321557605321964,
      "grad_norm": 0.44618621468544006,
      "learning_rate": 4.1523419449255755e-06,
      "loss": 2.3503,
      "step": 132810
    },
    {
      "epoch": 2.732361480049064,
      "grad_norm": 0.46964582800865173,
      "learning_rate": 4.146010902988362e-06,
      "loss": 2.365,
      "step": 132820
    },
    {
      "epoch": 2.7325671995659317,
      "grad_norm": 0.4383432865142822,
      "learning_rate": 4.13968458900611e-06,
      "loss": 2.3357,
      "step": 132830
    },
    {
      "epoch": 2.7327729190827994,
      "grad_norm": 0.4712774157524109,
      "learning_rate": 4.133363003290846e-06,
      "loss": 2.4327,
      "step": 132840
    },
    {
      "epoch": 2.7329786385996675,
      "grad_norm": 0.4704616367816925,
      "learning_rate": 4.1270461461543454e-06,
      "loss": 2.4027,
      "step": 132850
    },
    {
      "epoch": 2.733184358116535,
      "grad_norm": 0.4525417685508728,
      "learning_rate": 4.120734017908246e-06,
      "loss": 2.3549,
      "step": 132860
    },
    {
      "epoch": 2.733390077633403,
      "grad_norm": 0.622535228729248,
      "learning_rate": 4.114426618863843e-06,
      "loss": 2.4435,
      "step": 132870
    },
    {
      "epoch": 2.7335957971502705,
      "grad_norm": 0.47616615891456604,
      "learning_rate": 4.108123949332243e-06,
      "loss": 2.3722,
      "step": 132880
    },
    {
      "epoch": 2.733801516667138,
      "grad_norm": 0.5026692152023315,
      "learning_rate": 4.101826009624377e-06,
      "loss": 2.3128,
      "step": 132890
    },
    {
      "epoch": 2.7340072361840058,
      "grad_norm": 0.5107797384262085,
      "learning_rate": 4.0955328000508275e-06,
      "loss": 2.4002,
      "step": 132900
    },
    {
      "epoch": 2.7342129557008734,
      "grad_norm": 0.46367618441581726,
      "learning_rate": 4.0892443209220254e-06,
      "loss": 2.394,
      "step": 132910
    },
    {
      "epoch": 2.734418675217741,
      "grad_norm": 0.47814980149269104,
      "learning_rate": 4.082960572548178e-06,
      "loss": 2.3463,
      "step": 132920
    },
    {
      "epoch": 2.7346243947346087,
      "grad_norm": 0.48324820399284363,
      "learning_rate": 4.076681555239181e-06,
      "loss": 2.3285,
      "step": 132930
    },
    {
      "epoch": 2.734830114251477,
      "grad_norm": 0.4613330066204071,
      "learning_rate": 4.070407269304755e-06,
      "loss": 2.3673,
      "step": 132940
    },
    {
      "epoch": 2.7350358337683445,
      "grad_norm": 0.49067267775535583,
      "learning_rate": 4.064137715054395e-06,
      "loss": 2.3739,
      "step": 132950
    },
    {
      "epoch": 2.735241553285212,
      "grad_norm": 0.4246053397655487,
      "learning_rate": 4.057872892797332e-06,
      "loss": 2.3997,
      "step": 132960
    },
    {
      "epoch": 2.7354472728020798,
      "grad_norm": 0.45011696219444275,
      "learning_rate": 4.051612802842586e-06,
      "loss": 2.3042,
      "step": 132970
    },
    {
      "epoch": 2.7356529923189474,
      "grad_norm": 0.47968754172325134,
      "learning_rate": 4.04535744549891e-06,
      "loss": 2.403,
      "step": 132980
    },
    {
      "epoch": 2.7358587118358155,
      "grad_norm": 0.4574330449104309,
      "learning_rate": 4.039106821074868e-06,
      "loss": 2.3363,
      "step": 132990
    },
    {
      "epoch": 2.736064431352683,
      "grad_norm": 0.45155349373817444,
      "learning_rate": 4.032860929878768e-06,
      "loss": 2.3874,
      "step": 133000
    },
    {
      "epoch": 2.736270150869551,
      "grad_norm": 0.4798465669155121,
      "learning_rate": 4.026619772218665e-06,
      "loss": 2.3414,
      "step": 133010
    },
    {
      "epoch": 2.7364758703864185,
      "grad_norm": 0.4815821349620819,
      "learning_rate": 4.0203833484024235e-06,
      "loss": 2.3797,
      "step": 133020
    },
    {
      "epoch": 2.736681589903286,
      "grad_norm": 0.4698682725429535,
      "learning_rate": 4.014151658737652e-06,
      "loss": 2.3672,
      "step": 133030
    },
    {
      "epoch": 2.736887309420154,
      "grad_norm": 0.5089278221130371,
      "learning_rate": 4.007924703531718e-06,
      "loss": 2.3051,
      "step": 133040
    },
    {
      "epoch": 2.7370930289370214,
      "grad_norm": 0.449572890996933,
      "learning_rate": 4.001702483091751e-06,
      "loss": 2.4255,
      "step": 133050
    },
    {
      "epoch": 2.737298748453889,
      "grad_norm": 0.4201725721359253,
      "learning_rate": 3.995484997724686e-06,
      "loss": 2.4061,
      "step": 133060
    },
    {
      "epoch": 2.7375044679707567,
      "grad_norm": 0.5581843852996826,
      "learning_rate": 3.989272247737185e-06,
      "loss": 2.3389,
      "step": 133070
    },
    {
      "epoch": 2.737710187487625,
      "grad_norm": 0.4238588213920593,
      "learning_rate": 3.983064233435685e-06,
      "loss": 2.3749,
      "step": 133080
    },
    {
      "epoch": 2.7379159070044925,
      "grad_norm": 0.43936797976493835,
      "learning_rate": 3.976860955126393e-06,
      "loss": 2.3294,
      "step": 133090
    },
    {
      "epoch": 2.73812162652136,
      "grad_norm": 0.47287359833717346,
      "learning_rate": 3.9706624131153005e-06,
      "loss": 2.3367,
      "step": 133100
    },
    {
      "epoch": 2.738327346038228,
      "grad_norm": 0.4663439095020294,
      "learning_rate": 3.964468607708105e-06,
      "loss": 2.3502,
      "step": 133110
    },
    {
      "epoch": 2.7385330655550955,
      "grad_norm": 0.45795738697052,
      "learning_rate": 3.9582795392103635e-06,
      "loss": 2.3323,
      "step": 133120
    },
    {
      "epoch": 2.738738785071963,
      "grad_norm": 0.48470252752304077,
      "learning_rate": 3.952095207927331e-06,
      "loss": 2.3413,
      "step": 133130
    },
    {
      "epoch": 2.738944504588831,
      "grad_norm": 0.48445984721183777,
      "learning_rate": 3.94591561416403e-06,
      "loss": 2.3572,
      "step": 133140
    },
    {
      "epoch": 2.739150224105699,
      "grad_norm": 0.4822430908679962,
      "learning_rate": 3.939740758225286e-06,
      "loss": 2.3159,
      "step": 133150
    },
    {
      "epoch": 2.7393559436225665,
      "grad_norm": 0.45868250727653503,
      "learning_rate": 3.933570640415674e-06,
      "loss": 2.3331,
      "step": 133160
    },
    {
      "epoch": 2.739561663139434,
      "grad_norm": 0.4587880074977875,
      "learning_rate": 3.9274052610395075e-06,
      "loss": 2.3596,
      "step": 133170
    },
    {
      "epoch": 2.739767382656302,
      "grad_norm": 0.5155742168426514,
      "learning_rate": 3.921244620400921e-06,
      "loss": 2.4379,
      "step": 133180
    },
    {
      "epoch": 2.7399731021731695,
      "grad_norm": 0.498993456363678,
      "learning_rate": 3.915088718803761e-06,
      "loss": 2.3842,
      "step": 133190
    },
    {
      "epoch": 2.740178821690037,
      "grad_norm": 0.4779358506202698,
      "learning_rate": 3.908937556551662e-06,
      "loss": 2.3574,
      "step": 133200
    },
    {
      "epoch": 2.740384541206905,
      "grad_norm": 0.3964294195175171,
      "learning_rate": 3.90279113394807e-06,
      "loss": 2.3385,
      "step": 133210
    },
    {
      "epoch": 2.7405902607237724,
      "grad_norm": 0.53258216381073,
      "learning_rate": 3.896649451296108e-06,
      "loss": 2.2964,
      "step": 133220
    },
    {
      "epoch": 2.7407959802406405,
      "grad_norm": 0.4456987679004669,
      "learning_rate": 3.890512508898714e-06,
      "loss": 2.4346,
      "step": 133230
    },
    {
      "epoch": 2.741001699757508,
      "grad_norm": 0.4173144996166229,
      "learning_rate": 3.884380307058633e-06,
      "loss": 2.3655,
      "step": 133240
    },
    {
      "epoch": 2.741207419274376,
      "grad_norm": 0.4449504017829895,
      "learning_rate": 3.87825284607829e-06,
      "loss": 2.3134,
      "step": 133250
    },
    {
      "epoch": 2.7414131387912435,
      "grad_norm": 0.4893299341201782,
      "learning_rate": 3.8721301262599315e-06,
      "loss": 2.3584,
      "step": 133260
    },
    {
      "epoch": 2.741618858308111,
      "grad_norm": 0.4781974256038666,
      "learning_rate": 3.866012147905562e-06,
      "loss": 2.4088,
      "step": 133270
    },
    {
      "epoch": 2.7418245778249792,
      "grad_norm": 0.4499754011631012,
      "learning_rate": 3.8598989113169505e-06,
      "loss": 2.336,
      "step": 133280
    },
    {
      "epoch": 2.742030297341847,
      "grad_norm": 0.5477719902992249,
      "learning_rate": 3.853790416795622e-06,
      "loss": 2.4217,
      "step": 133290
    },
    {
      "epoch": 2.7422360168587145,
      "grad_norm": 0.46707868576049805,
      "learning_rate": 3.847686664642869e-06,
      "loss": 2.3339,
      "step": 133300
    },
    {
      "epoch": 2.742441736375582,
      "grad_norm": 0.44806620478630066,
      "learning_rate": 3.841587655159773e-06,
      "loss": 2.3634,
      "step": 133310
    },
    {
      "epoch": 2.74264745589245,
      "grad_norm": 0.4990134835243225,
      "learning_rate": 3.83549338864716e-06,
      "loss": 2.3148,
      "step": 133320
    },
    {
      "epoch": 2.7428531754093175,
      "grad_norm": 0.4310508668422699,
      "learning_rate": 3.829403865405623e-06,
      "loss": 2.3971,
      "step": 133330
    },
    {
      "epoch": 2.743058894926185,
      "grad_norm": 0.4574306309223175,
      "learning_rate": 3.823319085735522e-06,
      "loss": 2.3611,
      "step": 133340
    },
    {
      "epoch": 2.743264614443053,
      "grad_norm": 0.4321199655532837,
      "learning_rate": 3.817239049936994e-06,
      "loss": 2.3902,
      "step": 133350
    },
    {
      "epoch": 2.7434703339599205,
      "grad_norm": 0.4841541647911072,
      "learning_rate": 3.8111637583099324e-06,
      "loss": 2.3551,
      "step": 133360
    },
    {
      "epoch": 2.7436760534767886,
      "grad_norm": 0.4302353262901306,
      "learning_rate": 3.8050932111539985e-06,
      "loss": 2.3743,
      "step": 133370
    },
    {
      "epoch": 2.743881772993656,
      "grad_norm": 0.433919221162796,
      "learning_rate": 3.7990274087686073e-06,
      "loss": 2.4101,
      "step": 133380
    },
    {
      "epoch": 2.744087492510524,
      "grad_norm": 0.4387357532978058,
      "learning_rate": 3.7929663514529644e-06,
      "loss": 2.337,
      "step": 133390
    },
    {
      "epoch": 2.7442932120273915,
      "grad_norm": 0.46960026025772095,
      "learning_rate": 3.786910039506031e-06,
      "loss": 2.3398,
      "step": 133400
    },
    {
      "epoch": 2.744498931544259,
      "grad_norm": 0.449880987405777,
      "learning_rate": 3.780858473226523e-06,
      "loss": 2.4094,
      "step": 133410
    },
    {
      "epoch": 2.744704651061127,
      "grad_norm": 0.44141849875450134,
      "learning_rate": 3.7748116529129573e-06,
      "loss": 2.3884,
      "step": 133420
    },
    {
      "epoch": 2.744910370577995,
      "grad_norm": 0.44632118940353394,
      "learning_rate": 3.768769578863529e-06,
      "loss": 2.3304,
      "step": 133430
    },
    {
      "epoch": 2.7451160900948626,
      "grad_norm": 0.47119203209877014,
      "learning_rate": 3.762732251376322e-06,
      "loss": 2.3824,
      "step": 133440
    },
    {
      "epoch": 2.7453218096117302,
      "grad_norm": 0.563217043876648,
      "learning_rate": 3.7566996707491086e-06,
      "loss": 2.3654,
      "step": 133450
    },
    {
      "epoch": 2.745527529128598,
      "grad_norm": 0.4887397885322571,
      "learning_rate": 3.750671837279418e-06,
      "loss": 2.3833,
      "step": 133460
    },
    {
      "epoch": 2.7457332486454655,
      "grad_norm": 0.43795204162597656,
      "learning_rate": 3.7446487512646013e-06,
      "loss": 2.3347,
      "step": 133470
    },
    {
      "epoch": 2.745938968162333,
      "grad_norm": 0.430961549282074,
      "learning_rate": 3.7386304130017425e-06,
      "loss": 2.3525,
      "step": 133480
    },
    {
      "epoch": 2.746144687679201,
      "grad_norm": 0.5181150436401367,
      "learning_rate": 3.73261682278766e-06,
      "loss": 2.3724,
      "step": 133490
    },
    {
      "epoch": 2.7463504071960685,
      "grad_norm": 0.5080069899559021,
      "learning_rate": 3.726607980919017e-06,
      "loss": 2.3652,
      "step": 133500
    },
    {
      "epoch": 2.746556126712936,
      "grad_norm": 0.4214116036891937,
      "learning_rate": 3.7206038876921646e-06,
      "loss": 2.3568,
      "step": 133510
    },
    {
      "epoch": 2.7467618462298042,
      "grad_norm": 0.5442047715187073,
      "learning_rate": 3.714604543403255e-06,
      "loss": 2.3315,
      "step": 133520
    },
    {
      "epoch": 2.746967565746672,
      "grad_norm": 0.4578222334384918,
      "learning_rate": 3.7086099483482297e-06,
      "loss": 2.3594,
      "step": 133530
    },
    {
      "epoch": 2.7471732852635395,
      "grad_norm": 0.42699283361434937,
      "learning_rate": 3.7026201028227403e-06,
      "loss": 2.3654,
      "step": 133540
    },
    {
      "epoch": 2.747379004780407,
      "grad_norm": 0.4759223163127899,
      "learning_rate": 3.69663500712224e-06,
      "loss": 2.3834,
      "step": 133550
    },
    {
      "epoch": 2.747584724297275,
      "grad_norm": 0.4407432973384857,
      "learning_rate": 3.690654661541959e-06,
      "loss": 2.3992,
      "step": 133560
    },
    {
      "epoch": 2.747790443814143,
      "grad_norm": 0.42453593015670776,
      "learning_rate": 3.6846790663768506e-06,
      "loss": 2.3814,
      "step": 133570
    },
    {
      "epoch": 2.7479961633310106,
      "grad_norm": 0.4512258470058441,
      "learning_rate": 3.6787082219216672e-06,
      "loss": 2.3138,
      "step": 133580
    },
    {
      "epoch": 2.7482018828478783,
      "grad_norm": 0.45647409558296204,
      "learning_rate": 3.6727421284709185e-06,
      "loss": 2.3575,
      "step": 133590
    },
    {
      "epoch": 2.748407602364746,
      "grad_norm": 0.48240819573402405,
      "learning_rate": 3.6667807863188797e-06,
      "loss": 2.3484,
      "step": 133600
    },
    {
      "epoch": 2.7486133218816136,
      "grad_norm": 0.4857844412326813,
      "learning_rate": 3.6608241957595933e-06,
      "loss": 2.4116,
      "step": 133610
    },
    {
      "epoch": 2.748819041398481,
      "grad_norm": 0.4784495234489441,
      "learning_rate": 3.654872357086869e-06,
      "loss": 2.3133,
      "step": 133620
    },
    {
      "epoch": 2.749024760915349,
      "grad_norm": 0.4603634476661682,
      "learning_rate": 3.6489252705942613e-06,
      "loss": 2.3602,
      "step": 133630
    },
    {
      "epoch": 2.7492304804322165,
      "grad_norm": 0.49059075117111206,
      "learning_rate": 3.6429829365751346e-06,
      "loss": 2.3382,
      "step": 133640
    },
    {
      "epoch": 2.749436199949084,
      "grad_norm": 0.4472005069255829,
      "learning_rate": 3.6370453553225658e-06,
      "loss": 2.3606,
      "step": 133650
    },
    {
      "epoch": 2.7496419194659523,
      "grad_norm": 0.4584384858608246,
      "learning_rate": 3.6311125271294322e-06,
      "loss": 2.3367,
      "step": 133660
    },
    {
      "epoch": 2.74984763898282,
      "grad_norm": 0.5458590984344482,
      "learning_rate": 3.625184452288377e-06,
      "loss": 2.3748,
      "step": 133670
    },
    {
      "epoch": 2.7500533584996876,
      "grad_norm": 0.4346886873245239,
      "learning_rate": 3.619261131091789e-06,
      "loss": 2.3907,
      "step": 133680
    },
    {
      "epoch": 2.7502590780165552,
      "grad_norm": 0.4681715965270996,
      "learning_rate": 3.6133425638318453e-06,
      "loss": 2.3305,
      "step": 133690
    },
    {
      "epoch": 2.750464797533423,
      "grad_norm": 0.4344704747200012,
      "learning_rate": 3.6074287508004566e-06,
      "loss": 2.3919,
      "step": 133700
    },
    {
      "epoch": 2.750670517050291,
      "grad_norm": 0.5701724290847778,
      "learning_rate": 3.6015196922893456e-06,
      "loss": 2.3384,
      "step": 133710
    },
    {
      "epoch": 2.7508762365671586,
      "grad_norm": 0.4577575623989105,
      "learning_rate": 3.5956153885899456e-06,
      "loss": 2.3911,
      "step": 133720
    },
    {
      "epoch": 2.7510819560840263,
      "grad_norm": 0.5021424889564514,
      "learning_rate": 3.5897158399935015e-06,
      "loss": 2.323,
      "step": 133730
    },
    {
      "epoch": 2.751287675600894,
      "grad_norm": 0.46790841221809387,
      "learning_rate": 3.583821046791014e-06,
      "loss": 2.4392,
      "step": 133740
    },
    {
      "epoch": 2.7514933951177616,
      "grad_norm": 0.5544679164886475,
      "learning_rate": 3.5779310092732166e-06,
      "loss": 2.3606,
      "step": 133750
    },
    {
      "epoch": 2.7516991146346292,
      "grad_norm": 0.4590207040309906,
      "learning_rate": 3.572045727730644e-06,
      "loss": 2.4098,
      "step": 133760
    },
    {
      "epoch": 2.751904834151497,
      "grad_norm": 0.4511403739452362,
      "learning_rate": 3.5661652024535976e-06,
      "loss": 2.3897,
      "step": 133770
    },
    {
      "epoch": 2.7521105536683645,
      "grad_norm": 0.4741438031196594,
      "learning_rate": 3.5602894337321e-06,
      "loss": 2.4198,
      "step": 133780
    },
    {
      "epoch": 2.752316273185232,
      "grad_norm": 0.4421878755092621,
      "learning_rate": 3.5544184218560094e-06,
      "loss": 2.3564,
      "step": 133790
    },
    {
      "epoch": 2.7525219927021003,
      "grad_norm": 0.4377139210700989,
      "learning_rate": 3.5485521671148937e-06,
      "loss": 2.3542,
      "step": 133800
    },
    {
      "epoch": 2.752727712218968,
      "grad_norm": 0.4806361198425293,
      "learning_rate": 3.542690669798088e-06,
      "loss": 2.381,
      "step": 133810
    },
    {
      "epoch": 2.7529334317358356,
      "grad_norm": 0.45685479044914246,
      "learning_rate": 3.5368339301947386e-06,
      "loss": 2.2891,
      "step": 133820
    },
    {
      "epoch": 2.7531391512527033,
      "grad_norm": 0.5114109516143799,
      "learning_rate": 3.5309819485937035e-06,
      "loss": 2.3636,
      "step": 133830
    },
    {
      "epoch": 2.753344870769571,
      "grad_norm": 0.4542110860347748,
      "learning_rate": 3.525134725283619e-06,
      "loss": 2.3133,
      "step": 133840
    },
    {
      "epoch": 2.7535505902864386,
      "grad_norm": 0.4509691894054413,
      "learning_rate": 3.519292260552942e-06,
      "loss": 2.3145,
      "step": 133850
    },
    {
      "epoch": 2.7537563098033067,
      "grad_norm": 0.44715043902397156,
      "learning_rate": 3.51345455468981e-06,
      "loss": 2.3481,
      "step": 133860
    },
    {
      "epoch": 2.7539620293201743,
      "grad_norm": 0.4589131474494934,
      "learning_rate": 3.507621607982159e-06,
      "loss": 2.4183,
      "step": 133870
    },
    {
      "epoch": 2.754167748837042,
      "grad_norm": 0.4415439963340759,
      "learning_rate": 3.5017934207177363e-06,
      "loss": 2.3369,
      "step": 133880
    },
    {
      "epoch": 2.7543734683539096,
      "grad_norm": 0.45070362091064453,
      "learning_rate": 3.4959699931839895e-06,
      "loss": 2.3583,
      "step": 133890
    },
    {
      "epoch": 2.7545791878707773,
      "grad_norm": 0.5047250986099243,
      "learning_rate": 3.490151325668156e-06,
      "loss": 2.3275,
      "step": 133900
    },
    {
      "epoch": 2.754784907387645,
      "grad_norm": 0.4412177801132202,
      "learning_rate": 3.4843374184572396e-06,
      "loss": 2.3415,
      "step": 133910
    },
    {
      "epoch": 2.7549906269045126,
      "grad_norm": 0.48996809124946594,
      "learning_rate": 3.4785282718380108e-06,
      "loss": 2.41,
      "step": 133920
    },
    {
      "epoch": 2.7551963464213802,
      "grad_norm": 0.4526706039905548,
      "learning_rate": 3.4727238860969957e-06,
      "loss": 2.3358,
      "step": 133930
    },
    {
      "epoch": 2.755402065938248,
      "grad_norm": 0.5162261128425598,
      "learning_rate": 3.46692426152051e-06,
      "loss": 2.3889,
      "step": 133940
    },
    {
      "epoch": 2.755607785455116,
      "grad_norm": 0.42915046215057373,
      "learning_rate": 3.4611293983946135e-06,
      "loss": 2.3098,
      "step": 133950
    },
    {
      "epoch": 2.7558135049719836,
      "grad_norm": 0.49284863471984863,
      "learning_rate": 3.4553392970051223e-06,
      "loss": 2.3942,
      "step": 133960
    },
    {
      "epoch": 2.7560192244888513,
      "grad_norm": 0.44747427105903625,
      "learning_rate": 3.449553957637641e-06,
      "loss": 2.3272,
      "step": 133970
    },
    {
      "epoch": 2.756224944005719,
      "grad_norm": 0.45064109563827515,
      "learning_rate": 3.4437733805775197e-06,
      "loss": 2.3332,
      "step": 133980
    },
    {
      "epoch": 2.7564306635225866,
      "grad_norm": 0.45172223448753357,
      "learning_rate": 3.4379975661098964e-06,
      "loss": 2.346,
      "step": 133990
    },
    {
      "epoch": 2.7566363830394547,
      "grad_norm": 0.44730743765830994,
      "learning_rate": 3.432226514519654e-06,
      "loss": 2.3367,
      "step": 134000
    },
    {
      "epoch": 2.7568421025563223,
      "grad_norm": 0.4417371153831482,
      "learning_rate": 3.4264602260914436e-06,
      "loss": 2.3697,
      "step": 134010
    },
    {
      "epoch": 2.75704782207319,
      "grad_norm": 0.47696876525878906,
      "learning_rate": 3.4206987011096814e-06,
      "loss": 2.34,
      "step": 134020
    },
    {
      "epoch": 2.7572535415900576,
      "grad_norm": 0.4408152997493744,
      "learning_rate": 3.4149419398585737e-06,
      "loss": 2.3967,
      "step": 134030
    },
    {
      "epoch": 2.7574592611069253,
      "grad_norm": 0.5122783184051514,
      "learning_rate": 3.4091899426220373e-06,
      "loss": 2.3707,
      "step": 134040
    },
    {
      "epoch": 2.757664980623793,
      "grad_norm": 0.4455737769603729,
      "learning_rate": 3.403442709683813e-06,
      "loss": 2.4115,
      "step": 134050
    },
    {
      "epoch": 2.7578707001406606,
      "grad_norm": 0.4625685513019562,
      "learning_rate": 3.3977002413273838e-06,
      "loss": 2.3824,
      "step": 134060
    },
    {
      "epoch": 2.7580764196575283,
      "grad_norm": 0.45288363099098206,
      "learning_rate": 3.391962537835958e-06,
      "loss": 2.3536,
      "step": 134070
    },
    {
      "epoch": 2.758282139174396,
      "grad_norm": 0.4908379912376404,
      "learning_rate": 3.386229599492574e-06,
      "loss": 2.4001,
      "step": 134080
    },
    {
      "epoch": 2.758487858691264,
      "grad_norm": 0.4550820589065552,
      "learning_rate": 3.3805014265800185e-06,
      "loss": 2.3511,
      "step": 134090
    },
    {
      "epoch": 2.7586935782081317,
      "grad_norm": 0.46106475591659546,
      "learning_rate": 3.3747780193807975e-06,
      "loss": 2.354,
      "step": 134100
    },
    {
      "epoch": 2.7588992977249993,
      "grad_norm": 0.5069130063056946,
      "learning_rate": 3.369059378177253e-06,
      "loss": 2.3785,
      "step": 134110
    },
    {
      "epoch": 2.759105017241867,
      "grad_norm": 0.4938373863697052,
      "learning_rate": 3.363345503251414e-06,
      "loss": 2.425,
      "step": 134120
    },
    {
      "epoch": 2.7593107367587346,
      "grad_norm": 0.4502023160457611,
      "learning_rate": 3.357636394885133e-06,
      "loss": 2.3576,
      "step": 134130
    },
    {
      "epoch": 2.7595164562756027,
      "grad_norm": 0.505470335483551,
      "learning_rate": 3.3519320533600294e-06,
      "loss": 2.3507,
      "step": 134140
    },
    {
      "epoch": 2.7597221757924704,
      "grad_norm": 0.484346479177475,
      "learning_rate": 3.346232478957445e-06,
      "loss": 2.3407,
      "step": 134150
    },
    {
      "epoch": 2.759927895309338,
      "grad_norm": 0.4572450816631317,
      "learning_rate": 3.340537671958488e-06,
      "loss": 2.3628,
      "step": 134160
    },
    {
      "epoch": 2.7601336148262057,
      "grad_norm": 0.46988362073898315,
      "learning_rate": 3.334847632644111e-06,
      "loss": 2.4164,
      "step": 134170
    },
    {
      "epoch": 2.7603393343430733,
      "grad_norm": 0.4478200674057007,
      "learning_rate": 3.329162361294924e-06,
      "loss": 2.3092,
      "step": 134180
    },
    {
      "epoch": 2.760545053859941,
      "grad_norm": 0.48192548751831055,
      "learning_rate": 3.3234818581913575e-06,
      "loss": 2.3531,
      "step": 134190
    },
    {
      "epoch": 2.7607507733768086,
      "grad_norm": 0.5710325241088867,
      "learning_rate": 3.3178061236136204e-06,
      "loss": 2.4038,
      "step": 134200
    },
    {
      "epoch": 2.7609564928936763,
      "grad_norm": 0.4897690713405609,
      "learning_rate": 3.312135157841645e-06,
      "loss": 2.3446,
      "step": 134210
    },
    {
      "epoch": 2.761162212410544,
      "grad_norm": 0.5128738880157471,
      "learning_rate": 3.3064689611551514e-06,
      "loss": 2.394,
      "step": 134220
    },
    {
      "epoch": 2.761367931927412,
      "grad_norm": 0.4488444924354553,
      "learning_rate": 3.3008075338336273e-06,
      "loss": 2.3463,
      "step": 134230
    },
    {
      "epoch": 2.7615736514442797,
      "grad_norm": 0.4918854236602783,
      "learning_rate": 3.2951508761563275e-06,
      "loss": 2.3136,
      "step": 134240
    },
    {
      "epoch": 2.7617793709611473,
      "grad_norm": 0.4759465157985687,
      "learning_rate": 3.289498988402262e-06,
      "loss": 2.3873,
      "step": 134250
    },
    {
      "epoch": 2.761985090478015,
      "grad_norm": 0.446492999792099,
      "learning_rate": 3.2838518708501853e-06,
      "loss": 2.3698,
      "step": 134260
    },
    {
      "epoch": 2.7621908099948826,
      "grad_norm": 0.45314985513687134,
      "learning_rate": 3.2782095237786746e-06,
      "loss": 2.3157,
      "step": 134270
    },
    {
      "epoch": 2.7623965295117503,
      "grad_norm": 0.42372748255729675,
      "learning_rate": 3.2725719474660077e-06,
      "loss": 2.3311,
      "step": 134280
    },
    {
      "epoch": 2.7626022490286184,
      "grad_norm": 0.4870549738407135,
      "learning_rate": 3.2669391421902617e-06,
      "loss": 2.4147,
      "step": 134290
    },
    {
      "epoch": 2.762807968545486,
      "grad_norm": 0.4498012959957123,
      "learning_rate": 3.2613111082292813e-06,
      "loss": 2.3372,
      "step": 134300
    },
    {
      "epoch": 2.7630136880623537,
      "grad_norm": 0.4742056131362915,
      "learning_rate": 3.2556878458606556e-06,
      "loss": 2.3127,
      "step": 134310
    },
    {
      "epoch": 2.7632194075792214,
      "grad_norm": 0.5908352136611938,
      "learning_rate": 3.250069355361762e-06,
      "loss": 2.3727,
      "step": 134320
    },
    {
      "epoch": 2.763425127096089,
      "grad_norm": 0.4334164559841156,
      "learning_rate": 3.2444556370097247e-06,
      "loss": 2.3526,
      "step": 134330
    },
    {
      "epoch": 2.7636308466129567,
      "grad_norm": 0.4806463420391083,
      "learning_rate": 3.238846691081432e-06,
      "loss": 2.3382,
      "step": 134340
    },
    {
      "epoch": 2.7638365661298243,
      "grad_norm": 0.47996723651885986,
      "learning_rate": 3.233242517853563e-06,
      "loss": 2.3835,
      "step": 134350
    },
    {
      "epoch": 2.764042285646692,
      "grad_norm": 0.48930060863494873,
      "learning_rate": 3.227643117602497e-06,
      "loss": 2.388,
      "step": 134360
    },
    {
      "epoch": 2.7642480051635596,
      "grad_norm": 0.4815007150173187,
      "learning_rate": 3.222048490604457e-06,
      "loss": 2.3335,
      "step": 134370
    },
    {
      "epoch": 2.7644537246804277,
      "grad_norm": 0.4958915114402771,
      "learning_rate": 3.216458637135411e-06,
      "loss": 2.3594,
      "step": 134380
    },
    {
      "epoch": 2.7646594441972954,
      "grad_norm": 0.4777223765850067,
      "learning_rate": 3.210873557471017e-06,
      "loss": 2.4328,
      "step": 134390
    },
    {
      "epoch": 2.764865163714163,
      "grad_norm": 0.44947347044944763,
      "learning_rate": 3.2052932518868204e-06,
      "loss": 2.3435,
      "step": 134400
    },
    {
      "epoch": 2.7650708832310307,
      "grad_norm": 0.43242132663726807,
      "learning_rate": 3.1997177206580353e-06,
      "loss": 2.3706,
      "step": 134410
    },
    {
      "epoch": 2.7652766027478983,
      "grad_norm": 0.5016543865203857,
      "learning_rate": 3.1941469640596635e-06,
      "loss": 2.3703,
      "step": 134420
    },
    {
      "epoch": 2.7654823222647664,
      "grad_norm": 0.5021892786026001,
      "learning_rate": 3.188580982366518e-06,
      "loss": 2.3789,
      "step": 134430
    },
    {
      "epoch": 2.765688041781634,
      "grad_norm": 0.5077970623970032,
      "learning_rate": 3.183019775853102e-06,
      "loss": 2.3859,
      "step": 134440
    },
    {
      "epoch": 2.7658937612985017,
      "grad_norm": 0.49310216307640076,
      "learning_rate": 3.1774633447937076e-06,
      "loss": 2.4089,
      "step": 134450
    },
    {
      "epoch": 2.7660994808153694,
      "grad_norm": 0.45110630989074707,
      "learning_rate": 3.171911689462459e-06,
      "loss": 2.3521,
      "step": 134460
    },
    {
      "epoch": 2.766305200332237,
      "grad_norm": 0.4771694540977478,
      "learning_rate": 3.166364810133149e-06,
      "loss": 2.3261,
      "step": 134470
    },
    {
      "epoch": 2.7665109198491047,
      "grad_norm": 0.4278513789176941,
      "learning_rate": 3.1608227070793693e-06,
      "loss": 2.3505,
      "step": 134480
    },
    {
      "epoch": 2.7667166393659723,
      "grad_norm": 0.48492491245269775,
      "learning_rate": 3.1552853805745132e-06,
      "loss": 2.3359,
      "step": 134490
    },
    {
      "epoch": 2.76692235888284,
      "grad_norm": 0.462360680103302,
      "learning_rate": 3.1497528308916835e-06,
      "loss": 2.3766,
      "step": 134500
    },
    {
      "epoch": 2.7671280783997076,
      "grad_norm": 0.46066415309906006,
      "learning_rate": 3.1442250583037735e-06,
      "loss": 2.3845,
      "step": 134510
    },
    {
      "epoch": 2.7673337979165757,
      "grad_norm": 0.49253159761428833,
      "learning_rate": 3.138702063083443e-06,
      "loss": 2.4266,
      "step": 134520
    },
    {
      "epoch": 2.7675395174334434,
      "grad_norm": 0.4410753846168518,
      "learning_rate": 3.133183845503096e-06,
      "loss": 2.4233,
      "step": 134530
    },
    {
      "epoch": 2.767745236950311,
      "grad_norm": 0.41000357270240784,
      "learning_rate": 3.127670405834948e-06,
      "loss": 2.4145,
      "step": 134540
    },
    {
      "epoch": 2.7679509564671787,
      "grad_norm": 0.5068836808204651,
      "learning_rate": 3.1221617443509157e-06,
      "loss": 2.3855,
      "step": 134550
    },
    {
      "epoch": 2.7681566759840464,
      "grad_norm": 0.4523555636405945,
      "learning_rate": 3.1166578613227247e-06,
      "loss": 2.3167,
      "step": 134560
    },
    {
      "epoch": 2.768362395500914,
      "grad_norm": 0.48016706109046936,
      "learning_rate": 3.111158757021859e-06,
      "loss": 2.3466,
      "step": 134570
    },
    {
      "epoch": 2.768568115017782,
      "grad_norm": 0.4893130660057068,
      "learning_rate": 3.105664431719546e-06,
      "loss": 2.3622,
      "step": 134580
    },
    {
      "epoch": 2.7687738345346498,
      "grad_norm": 0.46276459097862244,
      "learning_rate": 3.1001748856867907e-06,
      "loss": 2.314,
      "step": 134590
    },
    {
      "epoch": 2.7689795540515174,
      "grad_norm": 0.5088648796081543,
      "learning_rate": 3.0946901191943767e-06,
      "loss": 2.3755,
      "step": 134600
    },
    {
      "epoch": 2.769185273568385,
      "grad_norm": 0.42225560545921326,
      "learning_rate": 3.0892101325128208e-06,
      "loss": 2.3269,
      "step": 134610
    },
    {
      "epoch": 2.7693909930852527,
      "grad_norm": 0.4470787048339844,
      "learning_rate": 3.083734925912429e-06,
      "loss": 2.3084,
      "step": 134620
    },
    {
      "epoch": 2.7695967126021204,
      "grad_norm": 0.5015274286270142,
      "learning_rate": 3.0782644996632635e-06,
      "loss": 2.4128,
      "step": 134630
    },
    {
      "epoch": 2.769802432118988,
      "grad_norm": 0.5510170459747314,
      "learning_rate": 3.0727988540351526e-06,
      "loss": 2.3865,
      "step": 134640
    },
    {
      "epoch": 2.7700081516358557,
      "grad_norm": 0.47215190529823303,
      "learning_rate": 3.0673379892976807e-06,
      "loss": 2.3686,
      "step": 134650
    },
    {
      "epoch": 2.7702138711527233,
      "grad_norm": 0.45150548219680786,
      "learning_rate": 3.06188190572021e-06,
      "loss": 2.3929,
      "step": 134660
    },
    {
      "epoch": 2.7704195906695914,
      "grad_norm": 0.47939038276672363,
      "learning_rate": 3.0564306035718694e-06,
      "loss": 2.3619,
      "step": 134670
    },
    {
      "epoch": 2.770625310186459,
      "grad_norm": 0.438568651676178,
      "learning_rate": 3.0509840831214997e-06,
      "loss": 2.3618,
      "step": 134680
    },
    {
      "epoch": 2.7708310297033267,
      "grad_norm": 0.47910335659980774,
      "learning_rate": 3.0455423446377863e-06,
      "loss": 2.3617,
      "step": 134690
    },
    {
      "epoch": 2.7710367492201944,
      "grad_norm": 0.44360285997390747,
      "learning_rate": 3.0401053883891472e-06,
      "loss": 2.3315,
      "step": 134700
    },
    {
      "epoch": 2.771242468737062,
      "grad_norm": 0.49898552894592285,
      "learning_rate": 3.034673214643724e-06,
      "loss": 2.3935,
      "step": 134710
    },
    {
      "epoch": 2.77144818825393,
      "grad_norm": 0.4205223619937897,
      "learning_rate": 3.0292458236694686e-06,
      "loss": 2.4029,
      "step": 134720
    },
    {
      "epoch": 2.771653907770798,
      "grad_norm": 0.46496525406837463,
      "learning_rate": 3.023823215734112e-06,
      "loss": 2.3265,
      "step": 134730
    },
    {
      "epoch": 2.7718596272876654,
      "grad_norm": 0.4284171164035797,
      "learning_rate": 3.0184053911050727e-06,
      "loss": 2.3378,
      "step": 134740
    },
    {
      "epoch": 2.772065346804533,
      "grad_norm": 0.41947323083877563,
      "learning_rate": 3.012992350049637e-06,
      "loss": 2.3187,
      "step": 134750
    },
    {
      "epoch": 2.7722710663214007,
      "grad_norm": 0.46043628454208374,
      "learning_rate": 3.0075840928347587e-06,
      "loss": 2.4003,
      "step": 134760
    },
    {
      "epoch": 2.7724767858382684,
      "grad_norm": 0.49193668365478516,
      "learning_rate": 3.002180619727202e-06,
      "loss": 2.4208,
      "step": 134770
    },
    {
      "epoch": 2.772682505355136,
      "grad_norm": 0.5375012159347534,
      "learning_rate": 2.99678193099352e-06,
      "loss": 2.352,
      "step": 134780
    },
    {
      "epoch": 2.7728882248720037,
      "grad_norm": 0.4602123200893402,
      "learning_rate": 2.991388026899966e-06,
      "loss": 2.3651,
      "step": 134790
    },
    {
      "epoch": 2.7730939443888714,
      "grad_norm": 0.46570080518722534,
      "learning_rate": 2.985998907712606e-06,
      "loss": 2.3418,
      "step": 134800
    },
    {
      "epoch": 2.7732996639057395,
      "grad_norm": 0.44393864274024963,
      "learning_rate": 2.9806145736972713e-06,
      "loss": 2.3175,
      "step": 134810
    },
    {
      "epoch": 2.773505383422607,
      "grad_norm": 0.44798997044563293,
      "learning_rate": 2.975235025119527e-06,
      "loss": 2.313,
      "step": 134820
    },
    {
      "epoch": 2.7737111029394748,
      "grad_norm": 0.47538018226623535,
      "learning_rate": 2.969860262244706e-06,
      "loss": 2.3601,
      "step": 134830
    },
    {
      "epoch": 2.7739168224563424,
      "grad_norm": 0.4803285598754883,
      "learning_rate": 2.9644902853379284e-06,
      "loss": 2.3712,
      "step": 134840
    },
    {
      "epoch": 2.77412254197321,
      "grad_norm": 0.4402746558189392,
      "learning_rate": 2.9591250946640725e-06,
      "loss": 2.4091,
      "step": 134850
    },
    {
      "epoch": 2.774328261490078,
      "grad_norm": 0.4915413558483124,
      "learning_rate": 2.9537646904877593e-06,
      "loss": 2.3989,
      "step": 134860
    },
    {
      "epoch": 2.774533981006946,
      "grad_norm": 0.4741908609867096,
      "learning_rate": 2.9484090730733994e-06,
      "loss": 2.3322,
      "step": 134870
    },
    {
      "epoch": 2.7747397005238135,
      "grad_norm": 0.46713364124298096,
      "learning_rate": 2.943058242685148e-06,
      "loss": 2.3742,
      "step": 134880
    },
    {
      "epoch": 2.774945420040681,
      "grad_norm": 0.45200541615486145,
      "learning_rate": 2.9377121995869394e-06,
      "loss": 2.3522,
      "step": 134890
    },
    {
      "epoch": 2.7751511395575488,
      "grad_norm": 0.4462529718875885,
      "learning_rate": 2.932370944042462e-06,
      "loss": 2.4117,
      "step": 134900
    },
    {
      "epoch": 2.7753568590744164,
      "grad_norm": 0.4370265603065491,
      "learning_rate": 2.9270344763151714e-06,
      "loss": 2.2669,
      "step": 134910
    },
    {
      "epoch": 2.775562578591284,
      "grad_norm": 0.4582159221172333,
      "learning_rate": 2.9217027966682797e-06,
      "loss": 2.4294,
      "step": 134920
    },
    {
      "epoch": 2.7757682981081517,
      "grad_norm": 0.4842384457588196,
      "learning_rate": 2.9163759053647877e-06,
      "loss": 2.3835,
      "step": 134930
    },
    {
      "epoch": 2.7759740176250194,
      "grad_norm": 0.43263623118400574,
      "learning_rate": 2.9110538026674182e-06,
      "loss": 2.3727,
      "step": 134940
    },
    {
      "epoch": 2.7761797371418875,
      "grad_norm": 0.46017518639564514,
      "learning_rate": 2.905736488838695e-06,
      "loss": 2.3634,
      "step": 134950
    },
    {
      "epoch": 2.776385456658755,
      "grad_norm": 0.4571443200111389,
      "learning_rate": 2.900423964140897e-06,
      "loss": 2.3584,
      "step": 134960
    },
    {
      "epoch": 2.776591176175623,
      "grad_norm": 0.4913829565048218,
      "learning_rate": 2.895116228836048e-06,
      "loss": 2.3958,
      "step": 134970
    },
    {
      "epoch": 2.7767968956924904,
      "grad_norm": 0.49120062589645386,
      "learning_rate": 2.8898132831859714e-06,
      "loss": 2.379,
      "step": 134980
    },
    {
      "epoch": 2.777002615209358,
      "grad_norm": 0.4973539113998413,
      "learning_rate": 2.8845151274522144e-06,
      "loss": 2.3446,
      "step": 134990
    },
    {
      "epoch": 2.7772083347262257,
      "grad_norm": 0.4573400616645813,
      "learning_rate": 2.8792217618960892e-06,
      "loss": 2.3276,
      "step": 135000
    },
    {
      "epoch": 2.777414054243094,
      "grad_norm": 0.4536057710647583,
      "learning_rate": 2.873933186778732e-06,
      "loss": 2.3103,
      "step": 135010
    },
    {
      "epoch": 2.7776197737599615,
      "grad_norm": 0.4791255295276642,
      "learning_rate": 2.868649402360979e-06,
      "loss": 2.3635,
      "step": 135020
    },
    {
      "epoch": 2.777825493276829,
      "grad_norm": 0.4861227869987488,
      "learning_rate": 2.863370408903432e-06,
      "loss": 2.3354,
      "step": 135030
    },
    {
      "epoch": 2.778031212793697,
      "grad_norm": 0.6765462160110474,
      "learning_rate": 2.858096206666494e-06,
      "loss": 2.415,
      "step": 135040
    },
    {
      "epoch": 2.7782369323105645,
      "grad_norm": 0.45832008123397827,
      "learning_rate": 2.852826795910324e-06,
      "loss": 2.3768,
      "step": 135050
    },
    {
      "epoch": 2.778442651827432,
      "grad_norm": 0.5198974609375,
      "learning_rate": 2.8475621768948023e-06,
      "loss": 2.4087,
      "step": 135060
    },
    {
      "epoch": 2.7786483713442998,
      "grad_norm": 0.42239800095558167,
      "learning_rate": 2.842302349879644e-06,
      "loss": 2.3697,
      "step": 135070
    },
    {
      "epoch": 2.7788540908611674,
      "grad_norm": 0.44957807660102844,
      "learning_rate": 2.837047315124253e-06,
      "loss": 2.3617,
      "step": 135080
    },
    {
      "epoch": 2.779059810378035,
      "grad_norm": 0.47706976532936096,
      "learning_rate": 2.8317970728878317e-06,
      "loss": 2.3858,
      "step": 135090
    },
    {
      "epoch": 2.779265529894903,
      "grad_norm": 0.48491302132606506,
      "learning_rate": 2.8265516234293743e-06,
      "loss": 2.3703,
      "step": 135100
    },
    {
      "epoch": 2.779471249411771,
      "grad_norm": 0.45772066712379456,
      "learning_rate": 2.8213109670075844e-06,
      "loss": 2.4045,
      "step": 135110
    },
    {
      "epoch": 2.7796769689286385,
      "grad_norm": 0.4098527431488037,
      "learning_rate": 2.8160751038809664e-06,
      "loss": 2.3306,
      "step": 135120
    },
    {
      "epoch": 2.779882688445506,
      "grad_norm": 0.4773007333278656,
      "learning_rate": 2.810844034307769e-06,
      "loss": 2.3557,
      "step": 135130
    },
    {
      "epoch": 2.7800884079623738,
      "grad_norm": 0.44705113768577576,
      "learning_rate": 2.805617758546031e-06,
      "loss": 2.3993,
      "step": 135140
    },
    {
      "epoch": 2.780294127479242,
      "grad_norm": 0.4798934757709503,
      "learning_rate": 2.800396276853512e-06,
      "loss": 2.3259,
      "step": 135150
    },
    {
      "epoch": 2.7804998469961095,
      "grad_norm": 0.4417102336883545,
      "learning_rate": 2.795179589487773e-06,
      "loss": 2.3418,
      "step": 135160
    },
    {
      "epoch": 2.780705566512977,
      "grad_norm": 0.552793025970459,
      "learning_rate": 2.7899676967061194e-06,
      "loss": 2.3395,
      "step": 135170
    },
    {
      "epoch": 2.780911286029845,
      "grad_norm": 0.4413108229637146,
      "learning_rate": 2.7847605987656344e-06,
      "loss": 2.362,
      "step": 135180
    },
    {
      "epoch": 2.7811170055467125,
      "grad_norm": 0.471542626619339,
      "learning_rate": 2.7795582959231457e-06,
      "loss": 2.4221,
      "step": 135190
    },
    {
      "epoch": 2.78132272506358,
      "grad_norm": 0.44828546047210693,
      "learning_rate": 2.7743607884352594e-06,
      "loss": 2.3674,
      "step": 135200
    },
    {
      "epoch": 2.781528444580448,
      "grad_norm": 0.45512300729751587,
      "learning_rate": 2.7691680765583374e-06,
      "loss": 2.3181,
      "step": 135210
    },
    {
      "epoch": 2.7817341640973154,
      "grad_norm": 0.5029584765434265,
      "learning_rate": 2.7639801605485184e-06,
      "loss": 2.3605,
      "step": 135220
    },
    {
      "epoch": 2.781939883614183,
      "grad_norm": 0.4405970871448517,
      "learning_rate": 2.758797040661676e-06,
      "loss": 2.3502,
      "step": 135230
    },
    {
      "epoch": 2.782145603131051,
      "grad_norm": 0.4821436107158661,
      "learning_rate": 2.753618717153472e-06,
      "loss": 2.3837,
      "step": 135240
    },
    {
      "epoch": 2.782351322647919,
      "grad_norm": 0.4291568398475647,
      "learning_rate": 2.7484451902793363e-06,
      "loss": 2.3599,
      "step": 135250
    },
    {
      "epoch": 2.7825570421647865,
      "grad_norm": 0.47051718831062317,
      "learning_rate": 2.7432764602944416e-06,
      "loss": 2.3484,
      "step": 135260
    },
    {
      "epoch": 2.782762761681654,
      "grad_norm": 0.4616003930568695,
      "learning_rate": 2.7381125274537288e-06,
      "loss": 2.3915,
      "step": 135270
    },
    {
      "epoch": 2.782968481198522,
      "grad_norm": 0.44663840532302856,
      "learning_rate": 2.7329533920119167e-06,
      "loss": 2.3989,
      "step": 135280
    },
    {
      "epoch": 2.7831742007153895,
      "grad_norm": 0.503261387348175,
      "learning_rate": 2.7277990542234677e-06,
      "loss": 2.38,
      "step": 135290
    },
    {
      "epoch": 2.7833799202322576,
      "grad_norm": 0.5443208813667297,
      "learning_rate": 2.7226495143426124e-06,
      "loss": 2.3867,
      "step": 135300
    },
    {
      "epoch": 2.783585639749125,
      "grad_norm": 0.4562044143676758,
      "learning_rate": 2.7175047726233804e-06,
      "loss": 2.3013,
      "step": 135310
    },
    {
      "epoch": 2.783791359265993,
      "grad_norm": 0.4807839095592499,
      "learning_rate": 2.7123648293194914e-06,
      "loss": 2.3533,
      "step": 135320
    },
    {
      "epoch": 2.7839970787828605,
      "grad_norm": 0.5403426885604858,
      "learning_rate": 2.707229684684498e-06,
      "loss": 2.3818,
      "step": 135330
    },
    {
      "epoch": 2.784202798299728,
      "grad_norm": 0.4720269441604614,
      "learning_rate": 2.7020993389716974e-06,
      "loss": 2.3927,
      "step": 135340
    },
    {
      "epoch": 2.784408517816596,
      "grad_norm": 0.49431222677230835,
      "learning_rate": 2.696973792434099e-06,
      "loss": 2.3354,
      "step": 135350
    },
    {
      "epoch": 2.7846142373334635,
      "grad_norm": 0.4637879729270935,
      "learning_rate": 2.691853045324566e-06,
      "loss": 2.3776,
      "step": 135360
    },
    {
      "epoch": 2.784819956850331,
      "grad_norm": 0.4361989200115204,
      "learning_rate": 2.686737097895653e-06,
      "loss": 2.3868,
      "step": 135370
    },
    {
      "epoch": 2.7850256763671988,
      "grad_norm": 0.4489336311817169,
      "learning_rate": 2.681625950399691e-06,
      "loss": 2.3363,
      "step": 135380
    },
    {
      "epoch": 2.785231395884067,
      "grad_norm": 0.4633448123931885,
      "learning_rate": 2.676519603088823e-06,
      "loss": 2.3581,
      "step": 135390
    },
    {
      "epoch": 2.7854371154009345,
      "grad_norm": 0.5050004720687866,
      "learning_rate": 2.6714180562148806e-06,
      "loss": 2.3812,
      "step": 135400
    },
    {
      "epoch": 2.785642834917802,
      "grad_norm": 0.42920681834220886,
      "learning_rate": 2.666321310029496e-06,
      "loss": 2.3434,
      "step": 135410
    },
    {
      "epoch": 2.78584855443467,
      "grad_norm": 0.45785343647003174,
      "learning_rate": 2.661229364784101e-06,
      "loss": 2.4636,
      "step": 135420
    },
    {
      "epoch": 2.7860542739515375,
      "grad_norm": 0.46928051114082336,
      "learning_rate": 2.656142220729818e-06,
      "loss": 2.4207,
      "step": 135430
    },
    {
      "epoch": 2.7862599934684056,
      "grad_norm": 0.44745615124702454,
      "learning_rate": 2.6510598781175676e-06,
      "loss": 2.3777,
      "step": 135440
    },
    {
      "epoch": 2.7864657129852732,
      "grad_norm": 0.4497026205062866,
      "learning_rate": 2.645982337198061e-06,
      "loss": 2.2995,
      "step": 135450
    },
    {
      "epoch": 2.786671432502141,
      "grad_norm": 0.4808097779750824,
      "learning_rate": 2.640909598221719e-06,
      "loss": 2.3882,
      "step": 135460
    },
    {
      "epoch": 2.7868771520190085,
      "grad_norm": 0.4561827480792999,
      "learning_rate": 2.635841661438776e-06,
      "loss": 2.4289,
      "step": 135470
    },
    {
      "epoch": 2.787082871535876,
      "grad_norm": 0.44942018389701843,
      "learning_rate": 2.6307785270991757e-06,
      "loss": 2.4086,
      "step": 135480
    },
    {
      "epoch": 2.787288591052744,
      "grad_norm": 0.4627572000026703,
      "learning_rate": 2.6257201954526855e-06,
      "loss": 2.3142,
      "step": 135490
    },
    {
      "epoch": 2.7874943105696115,
      "grad_norm": 0.4670688211917877,
      "learning_rate": 2.6206666667487832e-06,
      "loss": 2.3628,
      "step": 135500
    },
    {
      "epoch": 2.787700030086479,
      "grad_norm": 0.47176632285118103,
      "learning_rate": 2.6156179412367476e-06,
      "loss": 2.3053,
      "step": 135510
    },
    {
      "epoch": 2.787905749603347,
      "grad_norm": 0.47105443477630615,
      "learning_rate": 2.61057401916559e-06,
      "loss": 2.3592,
      "step": 135520
    },
    {
      "epoch": 2.788111469120215,
      "grad_norm": 0.48268193006515503,
      "learning_rate": 2.6055349007841122e-06,
      "loss": 2.364,
      "step": 135530
    },
    {
      "epoch": 2.7883171886370826,
      "grad_norm": 0.44840455055236816,
      "learning_rate": 2.6005005863408703e-06,
      "loss": 2.3798,
      "step": 135540
    },
    {
      "epoch": 2.78852290815395,
      "grad_norm": 0.5265944004058838,
      "learning_rate": 2.595471076084155e-06,
      "loss": 2.3723,
      "step": 135550
    },
    {
      "epoch": 2.788728627670818,
      "grad_norm": 0.4882833957672119,
      "learning_rate": 2.5904463702620674e-06,
      "loss": 2.3419,
      "step": 135560
    },
    {
      "epoch": 2.7889343471876855,
      "grad_norm": 0.4623422622680664,
      "learning_rate": 2.585426469122443e-06,
      "loss": 2.3437,
      "step": 135570
    },
    {
      "epoch": 2.7891400667045536,
      "grad_norm": 0.472425639629364,
      "learning_rate": 2.580411372912883e-06,
      "loss": 2.3658,
      "step": 135580
    },
    {
      "epoch": 2.7893457862214213,
      "grad_norm": 0.4782528281211853,
      "learning_rate": 2.575401081880757e-06,
      "loss": 2.3409,
      "step": 135590
    },
    {
      "epoch": 2.789551505738289,
      "grad_norm": 0.47338294982910156,
      "learning_rate": 2.570395596273212e-06,
      "loss": 2.3925,
      "step": 135600
    },
    {
      "epoch": 2.7897572252551566,
      "grad_norm": 0.43497204780578613,
      "learning_rate": 2.565394916337094e-06,
      "loss": 2.3486,
      "step": 135610
    },
    {
      "epoch": 2.7899629447720242,
      "grad_norm": 0.4702077805995941,
      "learning_rate": 2.560399042319106e-06,
      "loss": 2.3225,
      "step": 135620
    },
    {
      "epoch": 2.790168664288892,
      "grad_norm": 0.4288417100906372,
      "learning_rate": 2.5554079744656623e-06,
      "loss": 2.3697,
      "step": 135630
    },
    {
      "epoch": 2.7903743838057595,
      "grad_norm": 0.5034915208816528,
      "learning_rate": 2.55042171302291e-06,
      "loss": 2.3718,
      "step": 135640
    },
    {
      "epoch": 2.790580103322627,
      "grad_norm": 0.4653318524360657,
      "learning_rate": 2.5454402582368308e-06,
      "loss": 2.3621,
      "step": 135650
    },
    {
      "epoch": 2.790785822839495,
      "grad_norm": 0.4985993206501007,
      "learning_rate": 2.5404636103531276e-06,
      "loss": 2.379,
      "step": 135660
    },
    {
      "epoch": 2.790991542356363,
      "grad_norm": 0.45495906472206116,
      "learning_rate": 2.535491769617249e-06,
      "loss": 2.4028,
      "step": 135670
    },
    {
      "epoch": 2.7911972618732306,
      "grad_norm": 0.4654456675052643,
      "learning_rate": 2.530524736274453e-06,
      "loss": 2.3427,
      "step": 135680
    },
    {
      "epoch": 2.7914029813900982,
      "grad_norm": 0.4607142508029938,
      "learning_rate": 2.5255625105697234e-06,
      "loss": 2.3851,
      "step": 135690
    },
    {
      "epoch": 2.791608700906966,
      "grad_norm": 0.49938422441482544,
      "learning_rate": 2.5206050927478186e-06,
      "loss": 2.3821,
      "step": 135700
    },
    {
      "epoch": 2.7918144204238335,
      "grad_norm": 0.4938366115093231,
      "learning_rate": 2.5156524830532767e-06,
      "loss": 2.4133,
      "step": 135710
    },
    {
      "epoch": 2.792020139940701,
      "grad_norm": 0.4670448899269104,
      "learning_rate": 2.5107046817303693e-06,
      "loss": 2.3891,
      "step": 135720
    },
    {
      "epoch": 2.7922258594575693,
      "grad_norm": 0.49281975626945496,
      "learning_rate": 2.505761689023134e-06,
      "loss": 2.3895,
      "step": 135730
    },
    {
      "epoch": 2.792431578974437,
      "grad_norm": 0.43198585510253906,
      "learning_rate": 2.5008235051754094e-06,
      "loss": 2.39,
      "step": 135740
    },
    {
      "epoch": 2.7926372984913046,
      "grad_norm": 0.4266858398914337,
      "learning_rate": 2.4958901304307447e-06,
      "loss": 2.3505,
      "step": 135750
    },
    {
      "epoch": 2.7928430180081723,
      "grad_norm": 0.4893786907196045,
      "learning_rate": 2.4909615650324902e-06,
      "loss": 2.3592,
      "step": 135760
    },
    {
      "epoch": 2.79304873752504,
      "grad_norm": 0.48030102252960205,
      "learning_rate": 2.48603780922374e-06,
      "loss": 2.3849,
      "step": 135770
    },
    {
      "epoch": 2.7932544570419076,
      "grad_norm": 0.5021320581436157,
      "learning_rate": 2.481118863247367e-06,
      "loss": 2.3375,
      "step": 135780
    },
    {
      "epoch": 2.793460176558775,
      "grad_norm": 0.48272913694381714,
      "learning_rate": 2.4762047273459765e-06,
      "loss": 2.3118,
      "step": 135790
    },
    {
      "epoch": 2.793665896075643,
      "grad_norm": 0.4464736580848694,
      "learning_rate": 2.471295401761964e-06,
      "loss": 2.3655,
      "step": 135800
    },
    {
      "epoch": 2.7938716155925105,
      "grad_norm": 0.5040023922920227,
      "learning_rate": 2.46639088673748e-06,
      "loss": 2.3549,
      "step": 135810
    },
    {
      "epoch": 2.7940773351093786,
      "grad_norm": 0.4663761258125305,
      "learning_rate": 2.4614911825144527e-06,
      "loss": 2.3285,
      "step": 135820
    },
    {
      "epoch": 2.7942830546262463,
      "grad_norm": 0.476459264755249,
      "learning_rate": 2.456596289334534e-06,
      "loss": 2.3695,
      "step": 135830
    },
    {
      "epoch": 2.794488774143114,
      "grad_norm": 0.5102155804634094,
      "learning_rate": 2.451706207439175e-06,
      "loss": 2.2786,
      "step": 135840
    },
    {
      "epoch": 2.7946944936599816,
      "grad_norm": 0.4373381733894348,
      "learning_rate": 2.4468209370695715e-06,
      "loss": 2.3472,
      "step": 135850
    },
    {
      "epoch": 2.7949002131768492,
      "grad_norm": 0.46350032091140747,
      "learning_rate": 2.4419404784666867e-06,
      "loss": 2.3292,
      "step": 135860
    },
    {
      "epoch": 2.7951059326937173,
      "grad_norm": 0.45846930146217346,
      "learning_rate": 2.437064831871261e-06,
      "loss": 2.3451,
      "step": 135870
    },
    {
      "epoch": 2.795311652210585,
      "grad_norm": 0.4619661867618561,
      "learning_rate": 2.4321939975237682e-06,
      "loss": 2.3621,
      "step": 135880
    },
    {
      "epoch": 2.7955173717274526,
      "grad_norm": 0.4546167552471161,
      "learning_rate": 2.4273279756644617e-06,
      "loss": 2.3634,
      "step": 135890
    },
    {
      "epoch": 2.7957230912443203,
      "grad_norm": 0.4810589849948883,
      "learning_rate": 2.42246676653336e-06,
      "loss": 2.3007,
      "step": 135900
    },
    {
      "epoch": 2.795928810761188,
      "grad_norm": 0.47804689407348633,
      "learning_rate": 2.4176103703702378e-06,
      "loss": 2.3472,
      "step": 135910
    },
    {
      "epoch": 2.7961345302780556,
      "grad_norm": 0.4618321359157562,
      "learning_rate": 2.4127587874146486e-06,
      "loss": 2.3689,
      "step": 135920
    },
    {
      "epoch": 2.7963402497949232,
      "grad_norm": 0.4985887408256531,
      "learning_rate": 2.4079120179058667e-06,
      "loss": 2.3835,
      "step": 135930
    },
    {
      "epoch": 2.796545969311791,
      "grad_norm": 0.45911508798599243,
      "learning_rate": 2.403070062082968e-06,
      "loss": 2.3426,
      "step": 135940
    },
    {
      "epoch": 2.7967516888286585,
      "grad_norm": 0.5043246150016785,
      "learning_rate": 2.3982329201848062e-06,
      "loss": 2.4146,
      "step": 135950
    },
    {
      "epoch": 2.7969574083455266,
      "grad_norm": 0.44082167744636536,
      "learning_rate": 2.393400592449924e-06,
      "loss": 2.339,
      "step": 135960
    },
    {
      "epoch": 2.7971631278623943,
      "grad_norm": 0.4701438248157501,
      "learning_rate": 2.3885730791167073e-06,
      "loss": 2.4039,
      "step": 135970
    },
    {
      "epoch": 2.797368847379262,
      "grad_norm": 0.3963086009025574,
      "learning_rate": 2.3837503804232776e-06,
      "loss": 2.331,
      "step": 135980
    },
    {
      "epoch": 2.7975745668961296,
      "grad_norm": 0.4593890607357025,
      "learning_rate": 2.3789324966074665e-06,
      "loss": 2.3551,
      "step": 135990
    },
    {
      "epoch": 2.7977802864129973,
      "grad_norm": 0.4314563274383545,
      "learning_rate": 2.3741194279069734e-06,
      "loss": 2.2939,
      "step": 136000
    },
    {
      "epoch": 2.7979860059298653,
      "grad_norm": 0.44966384768486023,
      "learning_rate": 2.3693111745591633e-06,
      "loss": 2.3388,
      "step": 136010
    },
    {
      "epoch": 2.798191725446733,
      "grad_norm": 0.47541365027427673,
      "learning_rate": 2.3645077368011916e-06,
      "loss": 2.3767,
      "step": 136020
    },
    {
      "epoch": 2.7983974449636007,
      "grad_norm": 0.4032943844795227,
      "learning_rate": 2.3597091148700234e-06,
      "loss": 2.3771,
      "step": 136030
    },
    {
      "epoch": 2.7986031644804683,
      "grad_norm": 0.4518650472164154,
      "learning_rate": 2.3549153090023256e-06,
      "loss": 2.3075,
      "step": 136040
    },
    {
      "epoch": 2.798808883997336,
      "grad_norm": 0.45094648003578186,
      "learning_rate": 2.3501263194345423e-06,
      "loss": 2.4365,
      "step": 136050
    },
    {
      "epoch": 2.7990146035142036,
      "grad_norm": 0.46174898743629456,
      "learning_rate": 2.345342146402918e-06,
      "loss": 2.3953,
      "step": 136060
    },
    {
      "epoch": 2.7992203230310713,
      "grad_norm": 0.42072683572769165,
      "learning_rate": 2.3405627901433967e-06,
      "loss": 2.3379,
      "step": 136070
    },
    {
      "epoch": 2.799426042547939,
      "grad_norm": 0.46246087551116943,
      "learning_rate": 2.335788250891735e-06,
      "loss": 2.3963,
      "step": 136080
    },
    {
      "epoch": 2.7996317620648066,
      "grad_norm": 0.43287429213523865,
      "learning_rate": 2.331018528883444e-06,
      "loss": 2.3273,
      "step": 136090
    },
    {
      "epoch": 2.7998374815816747,
      "grad_norm": 0.5020716786384583,
      "learning_rate": 2.3262536243537692e-06,
      "loss": 2.4433,
      "step": 136100
    },
    {
      "epoch": 2.8000432010985423,
      "grad_norm": 0.5298771262168884,
      "learning_rate": 2.3214935375377443e-06,
      "loss": 2.3606,
      "step": 136110
    },
    {
      "epoch": 2.80024892061541,
      "grad_norm": 0.4577180743217468,
      "learning_rate": 2.31673826867016e-06,
      "loss": 2.3666,
      "step": 136120
    },
    {
      "epoch": 2.8004546401322776,
      "grad_norm": 0.4863218367099762,
      "learning_rate": 2.3119878179855616e-06,
      "loss": 2.3688,
      "step": 136130
    },
    {
      "epoch": 2.8006603596491453,
      "grad_norm": 0.4656012952327728,
      "learning_rate": 2.3072421857182722e-06,
      "loss": 2.3477,
      "step": 136140
    },
    {
      "epoch": 2.800866079166013,
      "grad_norm": 0.47073185443878174,
      "learning_rate": 2.3025013721023613e-06,
      "loss": 2.3419,
      "step": 136150
    },
    {
      "epoch": 2.801071798682881,
      "grad_norm": 0.4603249132633209,
      "learning_rate": 2.2977653773716746e-06,
      "loss": 2.3396,
      "step": 136160
    },
    {
      "epoch": 2.8012775181997487,
      "grad_norm": 0.439425528049469,
      "learning_rate": 2.2930342017598137e-06,
      "loss": 2.3944,
      "step": 136170
    },
    {
      "epoch": 2.8014832377166163,
      "grad_norm": 0.4251366853713989,
      "learning_rate": 2.288307845500126e-06,
      "loss": 2.3846,
      "step": 136180
    },
    {
      "epoch": 2.801688957233484,
      "grad_norm": 0.4496191143989563,
      "learning_rate": 2.283586308825747e-06,
      "loss": 2.3488,
      "step": 136190
    },
    {
      "epoch": 2.8018946767503516,
      "grad_norm": 0.4595211148262024,
      "learning_rate": 2.278869591969568e-06,
      "loss": 2.359,
      "step": 136200
    },
    {
      "epoch": 2.8021003962672193,
      "grad_norm": 0.45596274733543396,
      "learning_rate": 2.2741576951642475e-06,
      "loss": 2.404,
      "step": 136210
    },
    {
      "epoch": 2.802306115784087,
      "grad_norm": 0.45120787620544434,
      "learning_rate": 2.269450618642166e-06,
      "loss": 2.3707,
      "step": 136220
    },
    {
      "epoch": 2.8025118353009546,
      "grad_norm": 0.45441165566444397,
      "learning_rate": 2.264748362635527e-06,
      "loss": 2.3887,
      "step": 136230
    },
    {
      "epoch": 2.8027175548178223,
      "grad_norm": 0.5214175581932068,
      "learning_rate": 2.260050927376267e-06,
      "loss": 2.3302,
      "step": 136240
    },
    {
      "epoch": 2.8029232743346904,
      "grad_norm": 0.5607976317405701,
      "learning_rate": 2.2553583130960674e-06,
      "loss": 2.3095,
      "step": 136250
    },
    {
      "epoch": 2.803128993851558,
      "grad_norm": 0.4640635848045349,
      "learning_rate": 2.2506705200263987e-06,
      "loss": 2.3037,
      "step": 136260
    },
    {
      "epoch": 2.8033347133684257,
      "grad_norm": 0.49686774611473083,
      "learning_rate": 2.245987548398487e-06,
      "loss": 2.3609,
      "step": 136270
    },
    {
      "epoch": 2.8035404328852933,
      "grad_norm": 0.47583189606666565,
      "learning_rate": 2.241309398443303e-06,
      "loss": 2.3531,
      "step": 136280
    },
    {
      "epoch": 2.803746152402161,
      "grad_norm": 0.4562561810016632,
      "learning_rate": 2.2366360703916177e-06,
      "loss": 2.3412,
      "step": 136290
    },
    {
      "epoch": 2.803951871919029,
      "grad_norm": 0.44540974497795105,
      "learning_rate": 2.2319675644739247e-06,
      "loss": 2.3591,
      "step": 136300
    },
    {
      "epoch": 2.8041575914358967,
      "grad_norm": 0.4891704022884369,
      "learning_rate": 2.2273038809204838e-06,
      "loss": 2.3782,
      "step": 136310
    },
    {
      "epoch": 2.8043633109527644,
      "grad_norm": 0.4961966276168823,
      "learning_rate": 2.2226450199613668e-06,
      "loss": 2.336,
      "step": 136320
    },
    {
      "epoch": 2.804569030469632,
      "grad_norm": 0.4575332999229431,
      "learning_rate": 2.2179909818263454e-06,
      "loss": 2.3445,
      "step": 136330
    },
    {
      "epoch": 2.8047747499864997,
      "grad_norm": 0.461029052734375,
      "learning_rate": 2.2133417667449694e-06,
      "loss": 2.3212,
      "step": 136340
    },
    {
      "epoch": 2.8049804695033673,
      "grad_norm": 0.4916096031665802,
      "learning_rate": 2.208697374946589e-06,
      "loss": 2.3653,
      "step": 136350
    },
    {
      "epoch": 2.805186189020235,
      "grad_norm": 0.5699676275253296,
      "learning_rate": 2.2040578066602535e-06,
      "loss": 2.4082,
      "step": 136360
    },
    {
      "epoch": 2.8053919085371026,
      "grad_norm": 0.47072431445121765,
      "learning_rate": 2.1994230621148247e-06,
      "loss": 2.3162,
      "step": 136370
    },
    {
      "epoch": 2.8055976280539703,
      "grad_norm": 0.4607298970222473,
      "learning_rate": 2.1947931415389088e-06,
      "loss": 2.3675,
      "step": 136380
    },
    {
      "epoch": 2.8058033475708384,
      "grad_norm": 0.4647156596183777,
      "learning_rate": 2.1901680451608675e-06,
      "loss": 2.3651,
      "step": 136390
    },
    {
      "epoch": 2.806009067087706,
      "grad_norm": 0.4421117901802063,
      "learning_rate": 2.1855477732088403e-06,
      "loss": 2.3507,
      "step": 136400
    },
    {
      "epoch": 2.8062147866045737,
      "grad_norm": 0.4797156751155853,
      "learning_rate": 2.1809323259107226e-06,
      "loss": 2.3716,
      "step": 136410
    },
    {
      "epoch": 2.8064205061214413,
      "grad_norm": 0.4222188889980316,
      "learning_rate": 2.176321703494155e-06,
      "loss": 2.3677,
      "step": 136420
    },
    {
      "epoch": 2.806626225638309,
      "grad_norm": 0.48872047662734985,
      "learning_rate": 2.1717159061865664e-06,
      "loss": 2.356,
      "step": 136430
    },
    {
      "epoch": 2.8068319451551766,
      "grad_norm": 0.4679293632507324,
      "learning_rate": 2.167114934215131e-06,
      "loss": 2.3402,
      "step": 136440
    },
    {
      "epoch": 2.8070376646720447,
      "grad_norm": 0.434352308511734,
      "learning_rate": 2.162518787806789e-06,
      "loss": 2.405,
      "step": 136450
    },
    {
      "epoch": 2.8072433841889124,
      "grad_norm": 0.4570375978946686,
      "learning_rate": 2.1579274671882475e-06,
      "loss": 2.2933,
      "step": 136460
    },
    {
      "epoch": 2.80744910370578,
      "grad_norm": 0.46753883361816406,
      "learning_rate": 2.1533409725859712e-06,
      "loss": 2.3415,
      "step": 136470
    },
    {
      "epoch": 2.8076548232226477,
      "grad_norm": 0.5427582859992981,
      "learning_rate": 2.148759304226178e-06,
      "loss": 2.3646,
      "step": 136480
    },
    {
      "epoch": 2.8078605427395154,
      "grad_norm": 0.5189752578735352,
      "learning_rate": 2.144182462334865e-06,
      "loss": 2.3648,
      "step": 136490
    },
    {
      "epoch": 2.808066262256383,
      "grad_norm": 0.45621824264526367,
      "learning_rate": 2.1396104471377853e-06,
      "loss": 2.3439,
      "step": 136500
    },
    {
      "epoch": 2.8082719817732507,
      "grad_norm": 0.47455254197120667,
      "learning_rate": 2.1350432588604473e-06,
      "loss": 2.4126,
      "step": 136510
    },
    {
      "epoch": 2.8084777012901183,
      "grad_norm": 0.4550417959690094,
      "learning_rate": 2.1304808977281153e-06,
      "loss": 2.3633,
      "step": 136520
    },
    {
      "epoch": 2.808683420806986,
      "grad_norm": 0.45038801431655884,
      "learning_rate": 2.125923363965854e-06,
      "loss": 2.3904,
      "step": 136530
    },
    {
      "epoch": 2.808889140323854,
      "grad_norm": 0.5055866241455078,
      "learning_rate": 2.121370657798427e-06,
      "loss": 2.3347,
      "step": 136540
    },
    {
      "epoch": 2.8090948598407217,
      "grad_norm": 0.42805027961730957,
      "learning_rate": 2.1168227794504114e-06,
      "loss": 2.3772,
      "step": 136550
    },
    {
      "epoch": 2.8093005793575894,
      "grad_norm": 0.45975354313850403,
      "learning_rate": 2.1122797291461493e-06,
      "loss": 2.3124,
      "step": 136560
    },
    {
      "epoch": 2.809506298874457,
      "grad_norm": 0.46270662546157837,
      "learning_rate": 2.107741507109673e-06,
      "loss": 2.33,
      "step": 136570
    },
    {
      "epoch": 2.8097120183913247,
      "grad_norm": 0.5468450784683228,
      "learning_rate": 2.1032081135648805e-06,
      "loss": 2.3252,
      "step": 136580
    },
    {
      "epoch": 2.8099177379081928,
      "grad_norm": 0.4629180431365967,
      "learning_rate": 2.0986795487353604e-06,
      "loss": 2.371,
      "step": 136590
    },
    {
      "epoch": 2.8101234574250604,
      "grad_norm": 0.45187073945999146,
      "learning_rate": 2.094155812844456e-06,
      "loss": 2.3494,
      "step": 136600
    },
    {
      "epoch": 2.810329176941928,
      "grad_norm": 0.4319993555545807,
      "learning_rate": 2.0896369061153444e-06,
      "loss": 2.3927,
      "step": 136610
    },
    {
      "epoch": 2.8105348964587957,
      "grad_norm": 0.44734877347946167,
      "learning_rate": 2.0851228287708914e-06,
      "loss": 2.3695,
      "step": 136620
    },
    {
      "epoch": 2.8107406159756634,
      "grad_norm": 0.5371663570404053,
      "learning_rate": 2.0806135810337522e-06,
      "loss": 2.4051,
      "step": 136630
    },
    {
      "epoch": 2.810946335492531,
      "grad_norm": 0.4455861449241638,
      "learning_rate": 2.0761091631263606e-06,
      "loss": 2.3335,
      "step": 136640
    },
    {
      "epoch": 2.8111520550093987,
      "grad_norm": 0.4412520229816437,
      "learning_rate": 2.071609575270872e-06,
      "loss": 2.311,
      "step": 136650
    },
    {
      "epoch": 2.8113577745262663,
      "grad_norm": 0.44862526655197144,
      "learning_rate": 2.0671148176892306e-06,
      "loss": 2.3492,
      "step": 136660
    },
    {
      "epoch": 2.811563494043134,
      "grad_norm": 0.46540817618370056,
      "learning_rate": 2.0626248906031597e-06,
      "loss": 2.3956,
      "step": 136670
    },
    {
      "epoch": 2.811769213560002,
      "grad_norm": 0.5052000880241394,
      "learning_rate": 2.058139794234104e-06,
      "loss": 2.3975,
      "step": 136680
    },
    {
      "epoch": 2.8119749330768697,
      "grad_norm": 0.4718113839626312,
      "learning_rate": 2.0536595288032977e-06,
      "loss": 2.3591,
      "step": 136690
    },
    {
      "epoch": 2.8121806525937374,
      "grad_norm": 0.48563122749328613,
      "learning_rate": 2.0491840945317087e-06,
      "loss": 2.3519,
      "step": 136700
    },
    {
      "epoch": 2.812386372110605,
      "grad_norm": 0.4483073055744171,
      "learning_rate": 2.0447134916401155e-06,
      "loss": 2.3683,
      "step": 136710
    },
    {
      "epoch": 2.8125920916274727,
      "grad_norm": 0.5129187703132629,
      "learning_rate": 2.0402477203490088e-06,
      "loss": 2.3972,
      "step": 136720
    },
    {
      "epoch": 2.812797811144341,
      "grad_norm": 0.46657851338386536,
      "learning_rate": 2.035786780878657e-06,
      "loss": 2.3475,
      "step": 136730
    },
    {
      "epoch": 2.8130035306612085,
      "grad_norm": 0.5049504637718201,
      "learning_rate": 2.0313306734491165e-06,
      "loss": 2.2824,
      "step": 136740
    },
    {
      "epoch": 2.813209250178076,
      "grad_norm": 0.49931812286376953,
      "learning_rate": 2.026879398280157e-06,
      "loss": 2.3852,
      "step": 136750
    },
    {
      "epoch": 2.8134149696949438,
      "grad_norm": 0.4629375636577606,
      "learning_rate": 2.022432955591347e-06,
      "loss": 2.3527,
      "step": 136760
    },
    {
      "epoch": 2.8136206892118114,
      "grad_norm": 0.5371803641319275,
      "learning_rate": 2.0179913456020107e-06,
      "loss": 2.328,
      "step": 136770
    },
    {
      "epoch": 2.813826408728679,
      "grad_norm": 0.46911320090293884,
      "learning_rate": 2.0135545685312286e-06,
      "loss": 2.3636,
      "step": 136780
    },
    {
      "epoch": 2.8140321282455467,
      "grad_norm": 0.4400094449520111,
      "learning_rate": 2.0091226245978256e-06,
      "loss": 2.3576,
      "step": 136790
    },
    {
      "epoch": 2.8142378477624144,
      "grad_norm": 0.4888538420200348,
      "learning_rate": 2.004695514020427e-06,
      "loss": 2.4092,
      "step": 136800
    },
    {
      "epoch": 2.814443567279282,
      "grad_norm": 0.4417964816093445,
      "learning_rate": 2.0002732370173806e-06,
      "loss": 2.3709,
      "step": 136810
    },
    {
      "epoch": 2.81464928679615,
      "grad_norm": 0.4714033007621765,
      "learning_rate": 1.9958557938068335e-06,
      "loss": 2.3558,
      "step": 136820
    },
    {
      "epoch": 2.8148550063130178,
      "grad_norm": 0.46479785442352295,
      "learning_rate": 1.991443184606645e-06,
      "loss": 2.3763,
      "step": 136830
    },
    {
      "epoch": 2.8150607258298854,
      "grad_norm": 0.44294852018356323,
      "learning_rate": 1.987035409634497e-06,
      "loss": 2.3435,
      "step": 136840
    },
    {
      "epoch": 2.815266445346753,
      "grad_norm": 0.4929523766040802,
      "learning_rate": 1.9826324691077814e-06,
      "loss": 2.3457,
      "step": 136850
    },
    {
      "epoch": 2.8154721648636207,
      "grad_norm": 0.5556228160858154,
      "learning_rate": 1.9782343632436696e-06,
      "loss": 2.4097,
      "step": 136860
    },
    {
      "epoch": 2.8156778843804884,
      "grad_norm": 0.5015366673469543,
      "learning_rate": 1.973841092259099e-06,
      "loss": 2.363,
      "step": 136870
    },
    {
      "epoch": 2.8158836038973565,
      "grad_norm": 0.4537535309791565,
      "learning_rate": 1.969452656370785e-06,
      "loss": 2.3402,
      "step": 136880
    },
    {
      "epoch": 2.816089323414224,
      "grad_norm": 0.45308777689933777,
      "learning_rate": 1.9650690557951544e-06,
      "loss": 2.3318,
      "step": 136890
    },
    {
      "epoch": 2.816295042931092,
      "grad_norm": 0.4500259459018707,
      "learning_rate": 1.9606902907484458e-06,
      "loss": 2.3841,
      "step": 136900
    },
    {
      "epoch": 2.8165007624479594,
      "grad_norm": 0.4958084225654602,
      "learning_rate": 1.956316361446642e-06,
      "loss": 2.3407,
      "step": 136910
    },
    {
      "epoch": 2.816706481964827,
      "grad_norm": 0.479544073343277,
      "learning_rate": 1.9519472681054584e-06,
      "loss": 2.3509,
      "step": 136920
    },
    {
      "epoch": 2.8169122014816947,
      "grad_norm": 0.46059224009513855,
      "learning_rate": 1.9475830109404346e-06,
      "loss": 2.3703,
      "step": 136930
    },
    {
      "epoch": 2.8171179209985624,
      "grad_norm": 0.5107743144035339,
      "learning_rate": 1.9432235901668207e-06,
      "loss": 2.3931,
      "step": 136940
    },
    {
      "epoch": 2.81732364051543,
      "grad_norm": 0.4885035455226898,
      "learning_rate": 1.9388690059996216e-06,
      "loss": 2.3675,
      "step": 136950
    },
    {
      "epoch": 2.8175293600322977,
      "grad_norm": 0.5070815682411194,
      "learning_rate": 1.9345192586536774e-06,
      "loss": 2.3157,
      "step": 136960
    },
    {
      "epoch": 2.817735079549166,
      "grad_norm": 0.4156777560710907,
      "learning_rate": 1.930174348343483e-06,
      "loss": 2.3382,
      "step": 136970
    },
    {
      "epoch": 2.8179407990660335,
      "grad_norm": 0.4285385310649872,
      "learning_rate": 1.925834275283367e-06,
      "loss": 2.324,
      "step": 136980
    },
    {
      "epoch": 2.818146518582901,
      "grad_norm": 0.46768927574157715,
      "learning_rate": 1.9214990396874246e-06,
      "loss": 2.2789,
      "step": 136990
    },
    {
      "epoch": 2.8183522380997688,
      "grad_norm": 0.45269694924354553,
      "learning_rate": 1.917168641769462e-06,
      "loss": 2.3311,
      "step": 137000
    },
    {
      "epoch": 2.8185579576166364,
      "grad_norm": 0.5611335635185242,
      "learning_rate": 1.9128430817430876e-06,
      "loss": 2.3882,
      "step": 137010
    },
    {
      "epoch": 2.8187636771335045,
      "grad_norm": 0.538161039352417,
      "learning_rate": 1.9085223598216405e-06,
      "loss": 2.3616,
      "step": 137020
    },
    {
      "epoch": 2.818969396650372,
      "grad_norm": 0.4507203698158264,
      "learning_rate": 1.904206476218262e-06,
      "loss": 2.3874,
      "step": 137030
    },
    {
      "epoch": 2.81917511616724,
      "grad_norm": 0.43340736627578735,
      "learning_rate": 1.899895431145815e-06,
      "loss": 2.3801,
      "step": 137040
    },
    {
      "epoch": 2.8193808356841075,
      "grad_norm": 0.4533270299434662,
      "learning_rate": 1.8955892248169516e-06,
      "loss": 2.3385,
      "step": 137050
    },
    {
      "epoch": 2.819586555200975,
      "grad_norm": 0.48793113231658936,
      "learning_rate": 1.8912878574440573e-06,
      "loss": 2.3992,
      "step": 137060
    },
    {
      "epoch": 2.8197922747178428,
      "grad_norm": 0.4808937609195709,
      "learning_rate": 1.886991329239307e-06,
      "loss": 2.3932,
      "step": 137070
    },
    {
      "epoch": 2.8199979942347104,
      "grad_norm": 0.48520204424858093,
      "learning_rate": 1.8826996404146313e-06,
      "loss": 2.4088,
      "step": 137080
    },
    {
      "epoch": 2.820203713751578,
      "grad_norm": 0.467679888010025,
      "learning_rate": 1.878412791181694e-06,
      "loss": 2.4437,
      "step": 137090
    },
    {
      "epoch": 2.8204094332684457,
      "grad_norm": 0.45388421416282654,
      "learning_rate": 1.8741307817519704e-06,
      "loss": 2.3778,
      "step": 137100
    },
    {
      "epoch": 2.820615152785314,
      "grad_norm": 0.4640144407749176,
      "learning_rate": 1.8698536123366473e-06,
      "loss": 2.3659,
      "step": 137110
    },
    {
      "epoch": 2.8208208723021815,
      "grad_norm": 0.47193408012390137,
      "learning_rate": 1.865581283146689e-06,
      "loss": 2.3322,
      "step": 137120
    },
    {
      "epoch": 2.821026591819049,
      "grad_norm": 0.45703360438346863,
      "learning_rate": 1.8613137943928494e-06,
      "loss": 2.3692,
      "step": 137130
    },
    {
      "epoch": 2.821232311335917,
      "grad_norm": 0.4645111858844757,
      "learning_rate": 1.8570511462856045e-06,
      "loss": 2.3861,
      "step": 137140
    },
    {
      "epoch": 2.8214380308527844,
      "grad_norm": 0.488889217376709,
      "learning_rate": 1.8527933390352192e-06,
      "loss": 2.3496,
      "step": 137150
    },
    {
      "epoch": 2.821643750369652,
      "grad_norm": 0.468070924282074,
      "learning_rate": 1.8485403728516926e-06,
      "loss": 2.3833,
      "step": 137160
    },
    {
      "epoch": 2.82184946988652,
      "grad_norm": 0.4796920120716095,
      "learning_rate": 1.844292247944812e-06,
      "loss": 2.3596,
      "step": 137170
    },
    {
      "epoch": 2.822055189403388,
      "grad_norm": 0.4680531620979309,
      "learning_rate": 1.8400489645240992e-06,
      "loss": 2.4102,
      "step": 137180
    },
    {
      "epoch": 2.8222609089202555,
      "grad_norm": 0.4392673671245575,
      "learning_rate": 1.8358105227988754e-06,
      "loss": 2.3309,
      "step": 137190
    },
    {
      "epoch": 2.822466628437123,
      "grad_norm": 0.5039032101631165,
      "learning_rate": 1.8315769229781955e-06,
      "loss": 2.4427,
      "step": 137200
    },
    {
      "epoch": 2.822672347953991,
      "grad_norm": 0.4878830015659332,
      "learning_rate": 1.8273481652708479e-06,
      "loss": 2.3632,
      "step": 137210
    },
    {
      "epoch": 2.8228780674708585,
      "grad_norm": 0.449110209941864,
      "learning_rate": 1.8231242498854551e-06,
      "loss": 2.3919,
      "step": 137220
    },
    {
      "epoch": 2.823083786987726,
      "grad_norm": 0.4872220754623413,
      "learning_rate": 1.81890517703035e-06,
      "loss": 2.3673,
      "step": 137230
    },
    {
      "epoch": 2.8232895065045938,
      "grad_norm": 0.4716971814632416,
      "learning_rate": 1.814690946913611e-06,
      "loss": 2.3476,
      "step": 137240
    },
    {
      "epoch": 2.8234952260214614,
      "grad_norm": 0.4478614628314972,
      "learning_rate": 1.810481559743138e-06,
      "loss": 2.4082,
      "step": 137250
    },
    {
      "epoch": 2.8237009455383295,
      "grad_norm": 0.5619714856147766,
      "learning_rate": 1.8062770157265318e-06,
      "loss": 2.3633,
      "step": 137260
    },
    {
      "epoch": 2.823906665055197,
      "grad_norm": 0.47161343693733215,
      "learning_rate": 1.8020773150711824e-06,
      "loss": 2.3939,
      "step": 137270
    },
    {
      "epoch": 2.824112384572065,
      "grad_norm": 0.481618732213974,
      "learning_rate": 1.797882457984268e-06,
      "loss": 2.3492,
      "step": 137280
    },
    {
      "epoch": 2.8243181040889325,
      "grad_norm": 0.47836410999298096,
      "learning_rate": 1.7936924446726566e-06,
      "loss": 2.3042,
      "step": 137290
    },
    {
      "epoch": 2.8245238236058,
      "grad_norm": 0.4782892167568207,
      "learning_rate": 1.7895072753430276e-06,
      "loss": 2.3733,
      "step": 137300
    },
    {
      "epoch": 2.824729543122668,
      "grad_norm": 0.48628175258636475,
      "learning_rate": 1.7853269502018378e-06,
      "loss": 2.3273,
      "step": 137310
    },
    {
      "epoch": 2.824935262639536,
      "grad_norm": 0.4603440761566162,
      "learning_rate": 1.7811514694552667e-06,
      "loss": 2.3963,
      "step": 137320
    },
    {
      "epoch": 2.8251409821564035,
      "grad_norm": 0.49666133522987366,
      "learning_rate": 1.7769808333092608e-06,
      "loss": 2.3225,
      "step": 137330
    },
    {
      "epoch": 2.825346701673271,
      "grad_norm": 0.4466976821422577,
      "learning_rate": 1.7728150419695332e-06,
      "loss": 2.3301,
      "step": 137340
    },
    {
      "epoch": 2.825552421190139,
      "grad_norm": 0.4623068571090698,
      "learning_rate": 1.7686540956415643e-06,
      "loss": 2.3704,
      "step": 137350
    },
    {
      "epoch": 2.8257581407070065,
      "grad_norm": 0.4640543460845947,
      "learning_rate": 1.7644979945306006e-06,
      "loss": 2.3572,
      "step": 137360
    },
    {
      "epoch": 2.825963860223874,
      "grad_norm": 0.440069317817688,
      "learning_rate": 1.7603467388416228e-06,
      "loss": 2.353,
      "step": 137370
    },
    {
      "epoch": 2.826169579740742,
      "grad_norm": 0.48679691553115845,
      "learning_rate": 1.7562003287794005e-06,
      "loss": 2.3445,
      "step": 137380
    },
    {
      "epoch": 2.8263752992576094,
      "grad_norm": 0.4629081189632416,
      "learning_rate": 1.7520587645484588e-06,
      "loss": 2.2771,
      "step": 137390
    },
    {
      "epoch": 2.8265810187744775,
      "grad_norm": 0.4532046616077423,
      "learning_rate": 1.7479220463530676e-06,
      "loss": 2.3968,
      "step": 137400
    },
    {
      "epoch": 2.826786738291345,
      "grad_norm": 0.45006299018859863,
      "learning_rate": 1.743790174397275e-06,
      "loss": 2.3698,
      "step": 137410
    },
    {
      "epoch": 2.826992457808213,
      "grad_norm": 0.4600507318973541,
      "learning_rate": 1.7396631488848735e-06,
      "loss": 2.3226,
      "step": 137420
    },
    {
      "epoch": 2.8271981773250805,
      "grad_norm": 0.44615933299064636,
      "learning_rate": 1.7355409700194336e-06,
      "loss": 2.3719,
      "step": 137430
    },
    {
      "epoch": 2.827403896841948,
      "grad_norm": 0.4172113537788391,
      "learning_rate": 1.7314236380042926e-06,
      "loss": 2.4151,
      "step": 137440
    },
    {
      "epoch": 2.8276096163588162,
      "grad_norm": 0.463750422000885,
      "learning_rate": 1.7273111530425101e-06,
      "loss": 2.3681,
      "step": 137450
    },
    {
      "epoch": 2.827815335875684,
      "grad_norm": 0.45036405324935913,
      "learning_rate": 1.7232035153369573e-06,
      "loss": 2.3588,
      "step": 137460
    },
    {
      "epoch": 2.8280210553925516,
      "grad_norm": 0.44865986704826355,
      "learning_rate": 1.719100725090228e-06,
      "loss": 2.3671,
      "step": 137470
    },
    {
      "epoch": 2.828226774909419,
      "grad_norm": 0.4477628469467163,
      "learning_rate": 1.7150027825046822e-06,
      "loss": 2.3843,
      "step": 137480
    },
    {
      "epoch": 2.828432494426287,
      "grad_norm": 0.4505928158760071,
      "learning_rate": 1.7109096877824804e-06,
      "loss": 2.3371,
      "step": 137490
    },
    {
      "epoch": 2.8286382139431545,
      "grad_norm": 0.5134786367416382,
      "learning_rate": 1.7068214411254723e-06,
      "loss": 2.3752,
      "step": 137500
    },
    {
      "epoch": 2.828843933460022,
      "grad_norm": 0.5154908299446106,
      "learning_rate": 1.7027380427353412e-06,
      "loss": 2.3921,
      "step": 137510
    },
    {
      "epoch": 2.82904965297689,
      "grad_norm": 0.4718588590621948,
      "learning_rate": 1.6986594928134925e-06,
      "loss": 2.4003,
      "step": 137520
    },
    {
      "epoch": 2.8292553724937575,
      "grad_norm": 0.4542061686515808,
      "learning_rate": 1.6945857915610764e-06,
      "loss": 2.3363,
      "step": 137530
    },
    {
      "epoch": 2.8294610920106256,
      "grad_norm": 0.4408033788204193,
      "learning_rate": 1.6905169391790544e-06,
      "loss": 2.3941,
      "step": 137540
    },
    {
      "epoch": 2.829666811527493,
      "grad_norm": 0.5294923186302185,
      "learning_rate": 1.6864529358681102e-06,
      "loss": 2.2996,
      "step": 137550
    },
    {
      "epoch": 2.829872531044361,
      "grad_norm": 0.4445653557777405,
      "learning_rate": 1.6823937818286839e-06,
      "loss": 2.3771,
      "step": 137560
    },
    {
      "epoch": 2.8300782505612285,
      "grad_norm": 0.49400317668914795,
      "learning_rate": 1.678339477261015e-06,
      "loss": 2.3413,
      "step": 137570
    },
    {
      "epoch": 2.830283970078096,
      "grad_norm": 1.1415703296661377,
      "learning_rate": 1.674290022365077e-06,
      "loss": 2.3743,
      "step": 137580
    },
    {
      "epoch": 2.830489689594964,
      "grad_norm": 0.4432336091995239,
      "learning_rate": 1.670245417340588e-06,
      "loss": 2.3556,
      "step": 137590
    },
    {
      "epoch": 2.830695409111832,
      "grad_norm": 0.4663785994052887,
      "learning_rate": 1.666205662387077e-06,
      "loss": 2.3442,
      "step": 137600
    },
    {
      "epoch": 2.8309011286286996,
      "grad_norm": 0.42714762687683105,
      "learning_rate": 1.6621707577037737e-06,
      "loss": 2.4273,
      "step": 137610
    },
    {
      "epoch": 2.8311068481455672,
      "grad_norm": 0.47970932722091675,
      "learning_rate": 1.6581407034897079e-06,
      "loss": 2.3928,
      "step": 137620
    },
    {
      "epoch": 2.831312567662435,
      "grad_norm": 0.4349296987056732,
      "learning_rate": 1.6541154999436648e-06,
      "loss": 2.3489,
      "step": 137630
    },
    {
      "epoch": 2.8315182871793025,
      "grad_norm": 0.4859103262424469,
      "learning_rate": 1.6500951472641857e-06,
      "loss": 2.3158,
      "step": 137640
    },
    {
      "epoch": 2.83172400669617,
      "grad_norm": 0.4676016867160797,
      "learning_rate": 1.6460796456495675e-06,
      "loss": 2.3782,
      "step": 137650
    },
    {
      "epoch": 2.831929726213038,
      "grad_norm": 0.4606371521949768,
      "learning_rate": 1.6420689952978852e-06,
      "loss": 2.4133,
      "step": 137660
    },
    {
      "epoch": 2.8321354457299055,
      "grad_norm": 0.5113137364387512,
      "learning_rate": 1.638063196406936e-06,
      "loss": 2.4102,
      "step": 137670
    },
    {
      "epoch": 2.832341165246773,
      "grad_norm": 0.4021419882774353,
      "learning_rate": 1.6340622491743396e-06,
      "loss": 2.326,
      "step": 137680
    },
    {
      "epoch": 2.8325468847636412,
      "grad_norm": 0.4455432593822479,
      "learning_rate": 1.6300661537974049e-06,
      "loss": 2.291,
      "step": 137690
    },
    {
      "epoch": 2.832752604280509,
      "grad_norm": 0.5781937837600708,
      "learning_rate": 1.6260749104732742e-06,
      "loss": 2.3611,
      "step": 137700
    },
    {
      "epoch": 2.8329583237973766,
      "grad_norm": 0.513550341129303,
      "learning_rate": 1.622088519398779e-06,
      "loss": 2.3989,
      "step": 137710
    },
    {
      "epoch": 2.833164043314244,
      "grad_norm": 0.4851379692554474,
      "learning_rate": 1.6181069807705617e-06,
      "loss": 2.2748,
      "step": 137720
    },
    {
      "epoch": 2.833369762831112,
      "grad_norm": 0.4510120153427124,
      "learning_rate": 1.6141302947850212e-06,
      "loss": 2.3482,
      "step": 137730
    },
    {
      "epoch": 2.83357548234798,
      "grad_norm": 0.4558963477611542,
      "learning_rate": 1.6101584616382891e-06,
      "loss": 2.359,
      "step": 137740
    },
    {
      "epoch": 2.8337812018648476,
      "grad_norm": 0.4679548144340515,
      "learning_rate": 1.6061914815262758e-06,
      "loss": 2.3265,
      "step": 137750
    },
    {
      "epoch": 2.8339869213817153,
      "grad_norm": 0.43323078751564026,
      "learning_rate": 1.6022293546446576e-06,
      "loss": 2.3579,
      "step": 137760
    },
    {
      "epoch": 2.834192640898583,
      "grad_norm": 0.44359391927719116,
      "learning_rate": 1.5982720811888562e-06,
      "loss": 2.3125,
      "step": 137770
    },
    {
      "epoch": 2.8343983604154506,
      "grad_norm": 0.485588401556015,
      "learning_rate": 1.5943196613540822e-06,
      "loss": 2.3756,
      "step": 137780
    },
    {
      "epoch": 2.834604079932318,
      "grad_norm": 0.4945228397846222,
      "learning_rate": 1.5903720953352575e-06,
      "loss": 2.3833,
      "step": 137790
    },
    {
      "epoch": 2.834809799449186,
      "grad_norm": 0.4601564407348633,
      "learning_rate": 1.586429383327115e-06,
      "loss": 2.3273,
      "step": 137800
    },
    {
      "epoch": 2.8350155189660535,
      "grad_norm": 0.4713762700557709,
      "learning_rate": 1.5824915255241323e-06,
      "loss": 2.3593,
      "step": 137810
    },
    {
      "epoch": 2.835221238482921,
      "grad_norm": 0.4759816527366638,
      "learning_rate": 1.5785585221205102e-06,
      "loss": 2.375,
      "step": 137820
    },
    {
      "epoch": 2.8354269579997893,
      "grad_norm": 0.4510265588760376,
      "learning_rate": 1.5746303733102818e-06,
      "loss": 2.3294,
      "step": 137830
    },
    {
      "epoch": 2.835632677516657,
      "grad_norm": 0.4560340344905853,
      "learning_rate": 1.5707070792871814e-06,
      "loss": 2.4019,
      "step": 137840
    },
    {
      "epoch": 2.8358383970335246,
      "grad_norm": 0.4511900246143341,
      "learning_rate": 1.5667886402447096e-06,
      "loss": 2.3958,
      "step": 137850
    },
    {
      "epoch": 2.8360441165503922,
      "grad_norm": 0.4649132490158081,
      "learning_rate": 1.5628750563761784e-06,
      "loss": 2.3549,
      "step": 137860
    },
    {
      "epoch": 2.83624983606726,
      "grad_norm": 0.4456939697265625,
      "learning_rate": 1.5589663278745892e-06,
      "loss": 2.3602,
      "step": 137870
    },
    {
      "epoch": 2.836455555584128,
      "grad_norm": 0.472077876329422,
      "learning_rate": 1.555062454932754e-06,
      "loss": 2.3607,
      "step": 137880
    },
    {
      "epoch": 2.8366612751009956,
      "grad_norm": 0.4825619161128998,
      "learning_rate": 1.5511634377432304e-06,
      "loss": 2.4378,
      "step": 137890
    },
    {
      "epoch": 2.8368669946178633,
      "grad_norm": 0.42556998133659363,
      "learning_rate": 1.547269276498331e-06,
      "loss": 2.3882,
      "step": 137900
    },
    {
      "epoch": 2.837072714134731,
      "grad_norm": 0.485320121049881,
      "learning_rate": 1.5433799713901243e-06,
      "loss": 2.3921,
      "step": 137910
    },
    {
      "epoch": 2.8372784336515986,
      "grad_norm": 0.4327295422554016,
      "learning_rate": 1.539495522610479e-06,
      "loss": 2.4522,
      "step": 137920
    },
    {
      "epoch": 2.8374841531684662,
      "grad_norm": 0.43349823355674744,
      "learning_rate": 1.5356159303509642e-06,
      "loss": 2.3836,
      "step": 137930
    },
    {
      "epoch": 2.837689872685334,
      "grad_norm": 0.5487793684005737,
      "learning_rate": 1.5317411948029491e-06,
      "loss": 2.364,
      "step": 137940
    },
    {
      "epoch": 2.8378955922022016,
      "grad_norm": 0.45378515124320984,
      "learning_rate": 1.5278713161575587e-06,
      "loss": 2.3553,
      "step": 137950
    },
    {
      "epoch": 2.838101311719069,
      "grad_norm": 0.45345520973205566,
      "learning_rate": 1.5240062946056621e-06,
      "loss": 2.3759,
      "step": 137960
    },
    {
      "epoch": 2.8383070312359373,
      "grad_norm": 0.448334664106369,
      "learning_rate": 1.5201461303379072e-06,
      "loss": 2.3704,
      "step": 137970
    },
    {
      "epoch": 2.838512750752805,
      "grad_norm": 0.44827404618263245,
      "learning_rate": 1.516290823544686e-06,
      "loss": 2.312,
      "step": 137980
    },
    {
      "epoch": 2.8387184702696726,
      "grad_norm": 0.4492737054824829,
      "learning_rate": 1.5124403744161797e-06,
      "loss": 2.399,
      "step": 137990
    },
    {
      "epoch": 2.8389241897865403,
      "grad_norm": 0.49680808186531067,
      "learning_rate": 1.5085947831422809e-06,
      "loss": 2.3621,
      "step": 138000
    },
    {
      "epoch": 2.839129909303408,
      "grad_norm": 0.4831252694129944,
      "learning_rate": 1.5047540499127043e-06,
      "loss": 2.365,
      "step": 138010
    },
    {
      "epoch": 2.8393356288202756,
      "grad_norm": 0.4600580334663391,
      "learning_rate": 1.5009181749168655e-06,
      "loss": 2.3583,
      "step": 138020
    },
    {
      "epoch": 2.8395413483371437,
      "grad_norm": 0.49927160143852234,
      "learning_rate": 1.4970871583439795e-06,
      "loss": 2.3676,
      "step": 138030
    },
    {
      "epoch": 2.8397470678540113,
      "grad_norm": 0.49128270149230957,
      "learning_rate": 1.493261000383006e-06,
      "loss": 2.4161,
      "step": 138040
    },
    {
      "epoch": 2.839952787370879,
      "grad_norm": 0.4732552170753479,
      "learning_rate": 1.4894397012226725e-06,
      "loss": 2.3913,
      "step": 138050
    },
    {
      "epoch": 2.8401585068877466,
      "grad_norm": 0.4623381197452545,
      "learning_rate": 1.4856232610514609e-06,
      "loss": 2.3816,
      "step": 138060
    },
    {
      "epoch": 2.8403642264046143,
      "grad_norm": 0.45456433296203613,
      "learning_rate": 1.4818116800576098e-06,
      "loss": 2.3892,
      "step": 138070
    },
    {
      "epoch": 2.840569945921482,
      "grad_norm": 0.43970435857772827,
      "learning_rate": 1.4780049584291355e-06,
      "loss": 2.42,
      "step": 138080
    },
    {
      "epoch": 2.8407756654383496,
      "grad_norm": 0.462199330329895,
      "learning_rate": 1.4742030963537879e-06,
      "loss": 2.4201,
      "step": 138090
    },
    {
      "epoch": 2.8409813849552172,
      "grad_norm": 0.4487190246582031,
      "learning_rate": 1.4704060940191167e-06,
      "loss": 2.372,
      "step": 138100
    },
    {
      "epoch": 2.841187104472085,
      "grad_norm": 0.4803616404533386,
      "learning_rate": 1.4666139516123722e-06,
      "loss": 2.3702,
      "step": 138110
    },
    {
      "epoch": 2.841392823988953,
      "grad_norm": 0.49419069290161133,
      "learning_rate": 1.4628266693206272e-06,
      "loss": 2.3853,
      "step": 138120
    },
    {
      "epoch": 2.8415985435058206,
      "grad_norm": 0.5087405443191528,
      "learning_rate": 1.4590442473306764e-06,
      "loss": 2.4086,
      "step": 138130
    },
    {
      "epoch": 2.8418042630226883,
      "grad_norm": 0.39622899889945984,
      "learning_rate": 1.455266685829093e-06,
      "loss": 2.3471,
      "step": 138140
    },
    {
      "epoch": 2.842009982539556,
      "grad_norm": 0.4116247892379761,
      "learning_rate": 1.4514939850021948e-06,
      "loss": 2.3757,
      "step": 138150
    },
    {
      "epoch": 2.8422157020564236,
      "grad_norm": 0.4435361325740814,
      "learning_rate": 1.4477261450360768e-06,
      "loss": 2.3615,
      "step": 138160
    },
    {
      "epoch": 2.8424214215732917,
      "grad_norm": 0.45218345522880554,
      "learning_rate": 1.4439631661165798e-06,
      "loss": 2.2806,
      "step": 138170
    },
    {
      "epoch": 2.8426271410901593,
      "grad_norm": 0.4392378330230713,
      "learning_rate": 1.4402050484293217e-06,
      "loss": 2.428,
      "step": 138180
    },
    {
      "epoch": 2.842832860607027,
      "grad_norm": 0.5155420303344727,
      "learning_rate": 1.4364517921596542e-06,
      "loss": 2.3471,
      "step": 138190
    },
    {
      "epoch": 2.8430385801238947,
      "grad_norm": 0.4432387948036194,
      "learning_rate": 1.4327033974927074e-06,
      "loss": 2.3818,
      "step": 138200
    },
    {
      "epoch": 2.8432442996407623,
      "grad_norm": 0.5060977935791016,
      "learning_rate": 1.4289598646133995e-06,
      "loss": 2.3604,
      "step": 138210
    },
    {
      "epoch": 2.84345001915763,
      "grad_norm": 0.4562665820121765,
      "learning_rate": 1.4252211937063275e-06,
      "loss": 2.3672,
      "step": 138220
    },
    {
      "epoch": 2.8436557386744976,
      "grad_norm": 0.4706569015979767,
      "learning_rate": 1.4214873849559328e-06,
      "loss": 2.3939,
      "step": 138230
    },
    {
      "epoch": 2.8438614581913653,
      "grad_norm": 0.47924938797950745,
      "learning_rate": 1.4177584385463905e-06,
      "loss": 2.3692,
      "step": 138240
    },
    {
      "epoch": 2.844067177708233,
      "grad_norm": 0.5127305388450623,
      "learning_rate": 1.4140343546615976e-06,
      "loss": 2.3542,
      "step": 138250
    },
    {
      "epoch": 2.844272897225101,
      "grad_norm": 0.46413418650627136,
      "learning_rate": 1.4103151334852739e-06,
      "loss": 2.3505,
      "step": 138260
    },
    {
      "epoch": 2.8444786167419687,
      "grad_norm": 0.476503849029541,
      "learning_rate": 1.4066007752008393e-06,
      "loss": 2.3503,
      "step": 138270
    },
    {
      "epoch": 2.8446843362588363,
      "grad_norm": 0.4645349383354187,
      "learning_rate": 1.402891279991536e-06,
      "loss": 2.3163,
      "step": 138280
    },
    {
      "epoch": 2.844890055775704,
      "grad_norm": 0.47138622403144836,
      "learning_rate": 1.399186648040307e-06,
      "loss": 2.4128,
      "step": 138290
    },
    {
      "epoch": 2.8450957752925716,
      "grad_norm": 0.46494826674461365,
      "learning_rate": 1.3954868795298947e-06,
      "loss": 2.373,
      "step": 138300
    },
    {
      "epoch": 2.8453014948094393,
      "grad_norm": 0.422081857919693,
      "learning_rate": 1.3917919746427755e-06,
      "loss": 2.3501,
      "step": 138310
    },
    {
      "epoch": 2.8455072143263074,
      "grad_norm": 0.5002273321151733,
      "learning_rate": 1.3881019335612144e-06,
      "loss": 2.329,
      "step": 138320
    },
    {
      "epoch": 2.845712933843175,
      "grad_norm": 0.520298182964325,
      "learning_rate": 1.3844167564672217e-06,
      "loss": 2.339,
      "step": 138330
    },
    {
      "epoch": 2.8459186533600427,
      "grad_norm": 0.48518627882003784,
      "learning_rate": 1.3807364435425518e-06,
      "loss": 2.3799,
      "step": 138340
    },
    {
      "epoch": 2.8461243728769103,
      "grad_norm": 0.4985494911670685,
      "learning_rate": 1.3770609949687375e-06,
      "loss": 2.3557,
      "step": 138350
    },
    {
      "epoch": 2.846330092393778,
      "grad_norm": 0.4994904100894928,
      "learning_rate": 1.373390410927078e-06,
      "loss": 2.4134,
      "step": 138360
    },
    {
      "epoch": 2.8465358119106456,
      "grad_norm": 0.4834095537662506,
      "learning_rate": 1.3697246915986283e-06,
      "loss": 2.381,
      "step": 138370
    },
    {
      "epoch": 2.8467415314275133,
      "grad_norm": 0.521153450012207,
      "learning_rate": 1.3660638371641776e-06,
      "loss": 2.322,
      "step": 138380
    },
    {
      "epoch": 2.846947250944381,
      "grad_norm": 0.5585054159164429,
      "learning_rate": 1.3624078478043146e-06,
      "loss": 2.3393,
      "step": 138390
    },
    {
      "epoch": 2.8471529704612486,
      "grad_norm": 0.42996007204055786,
      "learning_rate": 1.3587567236993615e-06,
      "loss": 2.3994,
      "step": 138400
    },
    {
      "epoch": 2.8473586899781167,
      "grad_norm": 0.4304613173007965,
      "learning_rate": 1.3551104650293966e-06,
      "loss": 2.3079,
      "step": 138410
    },
    {
      "epoch": 2.8475644094949843,
      "grad_norm": 0.5040642023086548,
      "learning_rate": 1.3514690719742984e-06,
      "loss": 2.3199,
      "step": 138420
    },
    {
      "epoch": 2.847770129011852,
      "grad_norm": 0.4342094659805298,
      "learning_rate": 1.3478325447136563e-06,
      "loss": 2.3936,
      "step": 138430
    },
    {
      "epoch": 2.8479758485287197,
      "grad_norm": 0.4563833773136139,
      "learning_rate": 1.3442008834268382e-06,
      "loss": 2.3575,
      "step": 138440
    },
    {
      "epoch": 2.8481815680455873,
      "grad_norm": 0.43655362725257874,
      "learning_rate": 1.3405740882929895e-06,
      "loss": 2.3545,
      "step": 138450
    },
    {
      "epoch": 2.8483872875624554,
      "grad_norm": 0.43450164794921875,
      "learning_rate": 1.3369521594909895e-06,
      "loss": 2.3488,
      "step": 138460
    },
    {
      "epoch": 2.848593007079323,
      "grad_norm": 0.44398772716522217,
      "learning_rate": 1.333335097199484e-06,
      "loss": 2.3106,
      "step": 138470
    },
    {
      "epoch": 2.8487987265961907,
      "grad_norm": 0.4791564345359802,
      "learning_rate": 1.329722901596897e-06,
      "loss": 2.3507,
      "step": 138480
    },
    {
      "epoch": 2.8490044461130584,
      "grad_norm": 0.4761345386505127,
      "learning_rate": 1.3261155728613862e-06,
      "loss": 2.4177,
      "step": 138490
    },
    {
      "epoch": 2.849210165629926,
      "grad_norm": 0.49296361207962036,
      "learning_rate": 1.3225131111708866e-06,
      "loss": 2.336,
      "step": 138500
    },
    {
      "epoch": 2.8494158851467937,
      "grad_norm": 0.467387318611145,
      "learning_rate": 1.3189155167030898e-06,
      "loss": 2.3558,
      "step": 138510
    },
    {
      "epoch": 2.8496216046636613,
      "grad_norm": 0.4412582814693451,
      "learning_rate": 1.3153227896354315e-06,
      "loss": 2.3929,
      "step": 138520
    },
    {
      "epoch": 2.849827324180529,
      "grad_norm": 0.4525454640388489,
      "learning_rate": 1.3117349301451587e-06,
      "loss": 2.3977,
      "step": 138530
    },
    {
      "epoch": 2.8500330436973966,
      "grad_norm": 0.44749686121940613,
      "learning_rate": 1.3081519384091967e-06,
      "loss": 2.3205,
      "step": 138540
    },
    {
      "epoch": 2.8502387632142647,
      "grad_norm": 0.47284603118896484,
      "learning_rate": 1.304573814604293e-06,
      "loss": 2.3472,
      "step": 138550
    },
    {
      "epoch": 2.8504444827311324,
      "grad_norm": 0.4166888892650604,
      "learning_rate": 1.3010005589069396e-06,
      "loss": 2.3389,
      "step": 138560
    },
    {
      "epoch": 2.850650202248,
      "grad_norm": 0.4382351040840149,
      "learning_rate": 1.2974321714933846e-06,
      "loss": 2.3273,
      "step": 138570
    },
    {
      "epoch": 2.8508559217648677,
      "grad_norm": 0.4321709871292114,
      "learning_rate": 1.2938686525396426e-06,
      "loss": 2.3401,
      "step": 138580
    },
    {
      "epoch": 2.8510616412817353,
      "grad_norm": 0.45977774262428284,
      "learning_rate": 1.290310002221473e-06,
      "loss": 2.3475,
      "step": 138590
    },
    {
      "epoch": 2.8512673607986034,
      "grad_norm": 0.4429568648338318,
      "learning_rate": 1.2867562207144023e-06,
      "loss": 2.3516,
      "step": 138600
    },
    {
      "epoch": 2.851473080315471,
      "grad_norm": 0.46095049381256104,
      "learning_rate": 1.2832073081937235e-06,
      "loss": 2.3763,
      "step": 138610
    },
    {
      "epoch": 2.8516787998323387,
      "grad_norm": 0.44616299867630005,
      "learning_rate": 1.2796632648344854e-06,
      "loss": 2.4379,
      "step": 138620
    },
    {
      "epoch": 2.8518845193492064,
      "grad_norm": 0.42849311232566833,
      "learning_rate": 1.2761240908115146e-06,
      "loss": 2.3164,
      "step": 138630
    },
    {
      "epoch": 2.852090238866074,
      "grad_norm": 0.44007250666618347,
      "learning_rate": 1.2725897862993386e-06,
      "loss": 2.3494,
      "step": 138640
    },
    {
      "epoch": 2.8522959583829417,
      "grad_norm": 0.44748666882514954,
      "learning_rate": 1.2690603514723175e-06,
      "loss": 2.3859,
      "step": 138650
    },
    {
      "epoch": 2.8525016778998094,
      "grad_norm": 0.46672332286834717,
      "learning_rate": 1.2655357865045348e-06,
      "loss": 2.3502,
      "step": 138660
    },
    {
      "epoch": 2.852707397416677,
      "grad_norm": 0.4677947163581848,
      "learning_rate": 1.2620160915698287e-06,
      "loss": 2.3533,
      "step": 138670
    },
    {
      "epoch": 2.8529131169335447,
      "grad_norm": 0.48879873752593994,
      "learning_rate": 1.258501266841805e-06,
      "loss": 2.3495,
      "step": 138680
    },
    {
      "epoch": 2.8531188364504128,
      "grad_norm": 0.47429367899894714,
      "learning_rate": 1.2549913124938584e-06,
      "loss": 2.3342,
      "step": 138690
    },
    {
      "epoch": 2.8533245559672804,
      "grad_norm": 0.50951087474823,
      "learning_rate": 1.2514862286990724e-06,
      "loss": 2.3767,
      "step": 138700
    },
    {
      "epoch": 2.853530275484148,
      "grad_norm": 0.44684451818466187,
      "learning_rate": 1.2479860156303646e-06,
      "loss": 2.3888,
      "step": 138710
    },
    {
      "epoch": 2.8537359950010157,
      "grad_norm": 0.46255648136138916,
      "learning_rate": 1.2444906734603745e-06,
      "loss": 2.3482,
      "step": 138720
    },
    {
      "epoch": 2.8539417145178834,
      "grad_norm": 0.4471789002418518,
      "learning_rate": 1.2410002023614975e-06,
      "loss": 2.3897,
      "step": 138730
    },
    {
      "epoch": 2.854147434034751,
      "grad_norm": 0.4785407483577728,
      "learning_rate": 1.2375146025059292e-06,
      "loss": 2.3516,
      "step": 138740
    },
    {
      "epoch": 2.854353153551619,
      "grad_norm": 0.5256245136260986,
      "learning_rate": 1.2340338740655544e-06,
      "loss": 2.3451,
      "step": 138750
    },
    {
      "epoch": 2.8545588730684868,
      "grad_norm": 0.4340929687023163,
      "learning_rate": 1.23055801721208e-06,
      "loss": 2.419,
      "step": 138760
    },
    {
      "epoch": 2.8547645925853544,
      "grad_norm": 0.48827943205833435,
      "learning_rate": 1.2270870321169581e-06,
      "loss": 2.367,
      "step": 138770
    },
    {
      "epoch": 2.854970312102222,
      "grad_norm": 0.4647378921508789,
      "learning_rate": 1.2236209189513847e-06,
      "loss": 2.3824,
      "step": 138780
    },
    {
      "epoch": 2.8551760316190897,
      "grad_norm": 0.4851781725883484,
      "learning_rate": 1.2201596778863233e-06,
      "loss": 2.3491,
      "step": 138790
    },
    {
      "epoch": 2.8553817511359574,
      "grad_norm": 0.4668146073818207,
      "learning_rate": 1.2167033090924928e-06,
      "loss": 2.3386,
      "step": 138800
    },
    {
      "epoch": 2.855587470652825,
      "grad_norm": 0.47595563530921936,
      "learning_rate": 1.2132518127403902e-06,
      "loss": 2.3251,
      "step": 138810
    },
    {
      "epoch": 2.8557931901696927,
      "grad_norm": 0.5327784419059753,
      "learning_rate": 1.209805189000246e-06,
      "loss": 2.3119,
      "step": 138820
    },
    {
      "epoch": 2.8559989096865603,
      "grad_norm": 0.4920935332775116,
      "learning_rate": 1.2063634380420797e-06,
      "loss": 2.3734,
      "step": 138830
    },
    {
      "epoch": 2.8562046292034284,
      "grad_norm": 0.46006014943122864,
      "learning_rate": 1.2029265600356331e-06,
      "loss": 2.3882,
      "step": 138840
    },
    {
      "epoch": 2.856410348720296,
      "grad_norm": 0.451599657535553,
      "learning_rate": 1.1994945551504489e-06,
      "loss": 2.2923,
      "step": 138850
    },
    {
      "epoch": 2.8566160682371637,
      "grad_norm": 0.49405398964881897,
      "learning_rate": 1.1960674235558022e-06,
      "loss": 2.3538,
      "step": 138860
    },
    {
      "epoch": 2.8568217877540314,
      "grad_norm": 0.4753682315349579,
      "learning_rate": 1.192645165420725e-06,
      "loss": 2.3724,
      "step": 138870
    },
    {
      "epoch": 2.857027507270899,
      "grad_norm": 0.48021724820137024,
      "learning_rate": 1.1892277809140262e-06,
      "loss": 2.4187,
      "step": 138880
    },
    {
      "epoch": 2.857233226787767,
      "grad_norm": 0.43738195300102234,
      "learning_rate": 1.1858152702042714e-06,
      "loss": 2.3719,
      "step": 138890
    },
    {
      "epoch": 2.857438946304635,
      "grad_norm": 0.45637303590774536,
      "learning_rate": 1.18240763345977e-06,
      "loss": 2.3682,
      "step": 138900
    },
    {
      "epoch": 2.8576446658215024,
      "grad_norm": 0.44214123487472534,
      "learning_rate": 1.1790048708486213e-06,
      "loss": 2.3442,
      "step": 138910
    },
    {
      "epoch": 2.85785038533837,
      "grad_norm": 0.4999985098838806,
      "learning_rate": 1.1756069825386352e-06,
      "loss": 2.3348,
      "step": 138920
    },
    {
      "epoch": 2.8580561048552378,
      "grad_norm": 0.47177156805992126,
      "learning_rate": 1.1722139686974442e-06,
      "loss": 2.3442,
      "step": 138930
    },
    {
      "epoch": 2.8582618243721054,
      "grad_norm": 0.572887122631073,
      "learning_rate": 1.1688258294923814e-06,
      "loss": 2.327,
      "step": 138940
    },
    {
      "epoch": 2.858467543888973,
      "grad_norm": 0.4107469916343689,
      "learning_rate": 1.1654425650905908e-06,
      "loss": 2.3611,
      "step": 138950
    },
    {
      "epoch": 2.8586732634058407,
      "grad_norm": 0.4900713562965393,
      "learning_rate": 1.1620641756589057e-06,
      "loss": 2.3985,
      "step": 138960
    },
    {
      "epoch": 2.8588789829227084,
      "grad_norm": 0.4371042847633362,
      "learning_rate": 1.1586906613640147e-06,
      "loss": 2.3437,
      "step": 138970
    },
    {
      "epoch": 2.8590847024395765,
      "grad_norm": 0.4210217595100403,
      "learning_rate": 1.155322022372296e-06,
      "loss": 2.3797,
      "step": 138980
    },
    {
      "epoch": 2.859290421956444,
      "grad_norm": 0.5087864995002747,
      "learning_rate": 1.1519582588498834e-06,
      "loss": 2.3823,
      "step": 138990
    },
    {
      "epoch": 2.8594961414733118,
      "grad_norm": 0.5084867477416992,
      "learning_rate": 1.148599370962722e-06,
      "loss": 2.4206,
      "step": 139000
    },
    {
      "epoch": 2.8597018609901794,
      "grad_norm": 0.4431869387626648,
      "learning_rate": 1.14524535887649e-06,
      "loss": 2.3728,
      "step": 139010
    },
    {
      "epoch": 2.859907580507047,
      "grad_norm": 0.47526460886001587,
      "learning_rate": 1.1418962227565888e-06,
      "loss": 2.3683,
      "step": 139020
    },
    {
      "epoch": 2.8601133000239147,
      "grad_norm": 0.44718676805496216,
      "learning_rate": 1.1385519627682528e-06,
      "loss": 2.3617,
      "step": 139030
    },
    {
      "epoch": 2.860319019540783,
      "grad_norm": 0.5270054936408997,
      "learning_rate": 1.1352125790764167e-06,
      "loss": 2.3829,
      "step": 139040
    },
    {
      "epoch": 2.8605247390576505,
      "grad_norm": 0.4496273100376129,
      "learning_rate": 1.1318780718457822e-06,
      "loss": 2.333,
      "step": 139050
    },
    {
      "epoch": 2.860730458574518,
      "grad_norm": 0.44893166422843933,
      "learning_rate": 1.128548441240851e-06,
      "loss": 2.3256,
      "step": 139060
    },
    {
      "epoch": 2.860936178091386,
      "grad_norm": 0.4886355698108673,
      "learning_rate": 1.125223687425836e-06,
      "loss": 2.3886,
      "step": 139070
    },
    {
      "epoch": 2.8611418976082534,
      "grad_norm": 0.46939536929130554,
      "learning_rate": 1.1219038105647174e-06,
      "loss": 2.3521,
      "step": 139080
    },
    {
      "epoch": 2.861347617125121,
      "grad_norm": 0.532241702079773,
      "learning_rate": 1.1185888108212861e-06,
      "loss": 2.3262,
      "step": 139090
    },
    {
      "epoch": 2.8615533366419887,
      "grad_norm": 0.6760038733482361,
      "learning_rate": 1.1152786883590227e-06,
      "loss": 2.307,
      "step": 139100
    },
    {
      "epoch": 2.8617590561588564,
      "grad_norm": 0.44652485847473145,
      "learning_rate": 1.1119734433411965e-06,
      "loss": 2.3445,
      "step": 139110
    },
    {
      "epoch": 2.861964775675724,
      "grad_norm": 0.5067551732063293,
      "learning_rate": 1.108673075930855e-06,
      "loss": 2.3804,
      "step": 139120
    },
    {
      "epoch": 2.862170495192592,
      "grad_norm": 0.4798645079135895,
      "learning_rate": 1.1053775862907788e-06,
      "loss": 2.392,
      "step": 139130
    },
    {
      "epoch": 2.86237621470946,
      "grad_norm": 0.4632417559623718,
      "learning_rate": 1.1020869745835049e-06,
      "loss": 2.3974,
      "step": 139140
    },
    {
      "epoch": 2.8625819342263275,
      "grad_norm": 0.4620550274848938,
      "learning_rate": 1.0988012409713588e-06,
      "loss": 2.3459,
      "step": 139150
    },
    {
      "epoch": 2.862787653743195,
      "grad_norm": 0.4800007939338684,
      "learning_rate": 1.0955203856163997e-06,
      "loss": 2.3681,
      "step": 139160
    },
    {
      "epoch": 2.8629933732600628,
      "grad_norm": 0.4260427951812744,
      "learning_rate": 1.092244408680454e-06,
      "loss": 2.3106,
      "step": 139170
    },
    {
      "epoch": 2.863199092776931,
      "grad_norm": 0.46050575375556946,
      "learning_rate": 1.0889733103251031e-06,
      "loss": 2.3342,
      "step": 139180
    },
    {
      "epoch": 2.8634048122937985,
      "grad_norm": 0.46213868260383606,
      "learning_rate": 1.0857070907117073e-06,
      "loss": 2.3578,
      "step": 139190
    },
    {
      "epoch": 2.863610531810666,
      "grad_norm": 0.41840338706970215,
      "learning_rate": 1.0824457500013596e-06,
      "loss": 2.3425,
      "step": 139200
    },
    {
      "epoch": 2.863816251327534,
      "grad_norm": 0.6094666719436646,
      "learning_rate": 1.0791892883549204e-06,
      "loss": 2.3886,
      "step": 139210
    },
    {
      "epoch": 2.8640219708444015,
      "grad_norm": 0.4570571184158325,
      "learning_rate": 1.0759377059330278e-06,
      "loss": 2.4024,
      "step": 139220
    },
    {
      "epoch": 2.864227690361269,
      "grad_norm": 0.45290860533714294,
      "learning_rate": 1.0726910028960536e-06,
      "loss": 2.3797,
      "step": 139230
    },
    {
      "epoch": 2.8644334098781368,
      "grad_norm": 0.4258558452129364,
      "learning_rate": 1.0694491794041472e-06,
      "loss": 2.3878,
      "step": 139240
    },
    {
      "epoch": 2.8646391293950044,
      "grad_norm": 0.5250780582427979,
      "learning_rate": 1.0662122356172033e-06,
      "loss": 2.4181,
      "step": 139250
    },
    {
      "epoch": 2.864844848911872,
      "grad_norm": 0.495256632566452,
      "learning_rate": 1.0629801716948827e-06,
      "loss": 2.3679,
      "step": 139260
    },
    {
      "epoch": 2.86505056842874,
      "grad_norm": 0.4276735186576843,
      "learning_rate": 1.059752987796614e-06,
      "loss": 2.3399,
      "step": 139270
    },
    {
      "epoch": 2.865256287945608,
      "grad_norm": 0.4598468840122223,
      "learning_rate": 1.0565306840815692e-06,
      "loss": 2.3811,
      "step": 139280
    },
    {
      "epoch": 2.8654620074624755,
      "grad_norm": 0.4887397885322571,
      "learning_rate": 1.053313260708677e-06,
      "loss": 2.3573,
      "step": 139290
    },
    {
      "epoch": 2.865667726979343,
      "grad_norm": 0.45537588000297546,
      "learning_rate": 1.0501007178366662e-06,
      "loss": 2.3287,
      "step": 139300
    },
    {
      "epoch": 2.865873446496211,
      "grad_norm": 0.4374350905418396,
      "learning_rate": 1.0468930556239653e-06,
      "loss": 2.3567,
      "step": 139310
    },
    {
      "epoch": 2.866079166013079,
      "grad_norm": 0.4912155866622925,
      "learning_rate": 1.0436902742287924e-06,
      "loss": 2.3247,
      "step": 139320
    },
    {
      "epoch": 2.8662848855299465,
      "grad_norm": 0.44822120666503906,
      "learning_rate": 1.0404923738091433e-06,
      "loss": 2.3399,
      "step": 139330
    },
    {
      "epoch": 2.866490605046814,
      "grad_norm": 0.4272337257862091,
      "learning_rate": 1.037299354522736e-06,
      "loss": 2.3389,
      "step": 139340
    },
    {
      "epoch": 2.866696324563682,
      "grad_norm": 0.4464018940925598,
      "learning_rate": 1.0341112165270673e-06,
      "loss": 2.3794,
      "step": 139350
    },
    {
      "epoch": 2.8669020440805495,
      "grad_norm": 0.4316677153110504,
      "learning_rate": 1.0309279599793997e-06,
      "loss": 2.3793,
      "step": 139360
    },
    {
      "epoch": 2.867107763597417,
      "grad_norm": 0.4891965985298157,
      "learning_rate": 1.0277495850367302e-06,
      "loss": 2.3643,
      "step": 139370
    },
    {
      "epoch": 2.867313483114285,
      "grad_norm": 0.446719765663147,
      "learning_rate": 1.0245760918558445e-06,
      "loss": 2.3538,
      "step": 139380
    },
    {
      "epoch": 2.8675192026311525,
      "grad_norm": 0.4907807409763336,
      "learning_rate": 1.0214074805932728e-06,
      "loss": 2.3722,
      "step": 139390
    },
    {
      "epoch": 2.86772492214802,
      "grad_norm": 0.49043941497802734,
      "learning_rate": 1.01824375140529e-06,
      "loss": 2.3584,
      "step": 139400
    },
    {
      "epoch": 2.867930641664888,
      "grad_norm": 0.5210883617401123,
      "learning_rate": 1.0150849044479715e-06,
      "loss": 2.3229,
      "step": 139410
    },
    {
      "epoch": 2.868136361181756,
      "grad_norm": 0.445481538772583,
      "learning_rate": 1.0119309398771038e-06,
      "loss": 2.3469,
      "step": 139420
    },
    {
      "epoch": 2.8683420806986235,
      "grad_norm": 0.4255385994911194,
      "learning_rate": 1.0087818578482621e-06,
      "loss": 2.3565,
      "step": 139430
    },
    {
      "epoch": 2.868547800215491,
      "grad_norm": 0.5011543035507202,
      "learning_rate": 1.0056376585167671e-06,
      "loss": 2.3355,
      "step": 139440
    },
    {
      "epoch": 2.868753519732359,
      "grad_norm": 0.49251702427864075,
      "learning_rate": 1.0024983420377275e-06,
      "loss": 2.3605,
      "step": 139450
    },
    {
      "epoch": 2.8689592392492265,
      "grad_norm": 0.4552159309387207,
      "learning_rate": 9.993639085659646e-07,
      "loss": 2.3735,
      "step": 139460
    },
    {
      "epoch": 2.8691649587660946,
      "grad_norm": 0.47133225202560425,
      "learning_rate": 9.962343582560873e-07,
      "loss": 2.3248,
      "step": 139470
    },
    {
      "epoch": 2.869370678282962,
      "grad_norm": 0.48657122254371643,
      "learning_rate": 9.931096912624726e-07,
      "loss": 2.4025,
      "step": 139480
    },
    {
      "epoch": 2.86957639779983,
      "grad_norm": 0.4425615668296814,
      "learning_rate": 9.899899077392194e-07,
      "loss": 2.3787,
      "step": 139490
    },
    {
      "epoch": 2.8697821173166975,
      "grad_norm": 0.4360886812210083,
      "learning_rate": 9.868750078402377e-07,
      "loss": 2.3603,
      "step": 139500
    },
    {
      "epoch": 2.869987836833565,
      "grad_norm": 0.445050984621048,
      "learning_rate": 9.837649917191494e-07,
      "loss": 2.3385,
      "step": 139510
    },
    {
      "epoch": 2.870193556350433,
      "grad_norm": 0.4890873432159424,
      "learning_rate": 9.806598595293648e-07,
      "loss": 2.3967,
      "step": 139520
    },
    {
      "epoch": 2.8703992758673005,
      "grad_norm": 0.4661839008331299,
      "learning_rate": 9.775596114240282e-07,
      "loss": 2.3886,
      "step": 139530
    },
    {
      "epoch": 2.870604995384168,
      "grad_norm": 0.4260009527206421,
      "learning_rate": 9.744642475560839e-07,
      "loss": 2.3471,
      "step": 139540
    },
    {
      "epoch": 2.870810714901036,
      "grad_norm": 0.46525657176971436,
      "learning_rate": 9.713737680781765e-07,
      "loss": 2.3827,
      "step": 139550
    },
    {
      "epoch": 2.871016434417904,
      "grad_norm": 0.48674625158309937,
      "learning_rate": 9.682881731427728e-07,
      "loss": 2.3602,
      "step": 139560
    },
    {
      "epoch": 2.8712221539347715,
      "grad_norm": 0.4596725404262543,
      "learning_rate": 9.652074629020514e-07,
      "loss": 2.3534,
      "step": 139570
    },
    {
      "epoch": 2.871427873451639,
      "grad_norm": 0.4962163269519806,
      "learning_rate": 9.621316375079682e-07,
      "loss": 2.3543,
      "step": 139580
    },
    {
      "epoch": 2.871633592968507,
      "grad_norm": 0.4986100494861603,
      "learning_rate": 9.590606971122574e-07,
      "loss": 2.3816,
      "step": 139590
    },
    {
      "epoch": 2.8718393124853745,
      "grad_norm": 0.4282931387424469,
      "learning_rate": 9.559946418663535e-07,
      "loss": 2.3585,
      "step": 139600
    },
    {
      "epoch": 2.8720450320022426,
      "grad_norm": 0.46867066621780396,
      "learning_rate": 9.529334719215133e-07,
      "loss": 2.3021,
      "step": 139610
    },
    {
      "epoch": 2.8722507515191102,
      "grad_norm": 0.48147156834602356,
      "learning_rate": 9.498771874287271e-07,
      "loss": 2.36,
      "step": 139620
    },
    {
      "epoch": 2.872456471035978,
      "grad_norm": 0.48384758830070496,
      "learning_rate": 9.468257885387299e-07,
      "loss": 2.369,
      "step": 139630
    },
    {
      "epoch": 2.8726621905528456,
      "grad_norm": 0.43346330523490906,
      "learning_rate": 9.437792754020458e-07,
      "loss": 2.3135,
      "step": 139640
    },
    {
      "epoch": 2.872867910069713,
      "grad_norm": 0.472079873085022,
      "learning_rate": 9.407376481689545e-07,
      "loss": 2.4238,
      "step": 139650
    },
    {
      "epoch": 2.873073629586581,
      "grad_norm": 0.48688435554504395,
      "learning_rate": 9.377009069894471e-07,
      "loss": 2.3934,
      "step": 139660
    },
    {
      "epoch": 2.8732793491034485,
      "grad_norm": 0.44137996435165405,
      "learning_rate": 9.346690520133483e-07,
      "loss": 2.3646,
      "step": 139670
    },
    {
      "epoch": 2.873485068620316,
      "grad_norm": 0.4779088795185089,
      "learning_rate": 9.316420833901717e-07,
      "loss": 2.2614,
      "step": 139680
    },
    {
      "epoch": 2.873690788137184,
      "grad_norm": 0.47137534618377686,
      "learning_rate": 9.286200012692203e-07,
      "loss": 2.3596,
      "step": 139690
    },
    {
      "epoch": 2.873896507654052,
      "grad_norm": 0.44727006554603577,
      "learning_rate": 9.256028057995969e-07,
      "loss": 2.3436,
      "step": 139700
    },
    {
      "epoch": 2.8741022271709196,
      "grad_norm": 0.4408703148365021,
      "learning_rate": 9.225904971300825e-07,
      "loss": 2.3453,
      "step": 139710
    },
    {
      "epoch": 2.874307946687787,
      "grad_norm": 0.468793660402298,
      "learning_rate": 9.195830754092582e-07,
      "loss": 2.4073,
      "step": 139720
    },
    {
      "epoch": 2.874513666204655,
      "grad_norm": 0.43082794547080994,
      "learning_rate": 9.165805407854944e-07,
      "loss": 2.346,
      "step": 139730
    },
    {
      "epoch": 2.8747193857215225,
      "grad_norm": 0.43799322843551636,
      "learning_rate": 9.135828934068724e-07,
      "loss": 2.2834,
      "step": 139740
    },
    {
      "epoch": 2.8749251052383906,
      "grad_norm": 0.4636014997959137,
      "learning_rate": 9.105901334212518e-07,
      "loss": 2.4,
      "step": 139750
    },
    {
      "epoch": 2.8751308247552583,
      "grad_norm": 0.4548775255680084,
      "learning_rate": 9.076022609762369e-07,
      "loss": 2.3734,
      "step": 139760
    },
    {
      "epoch": 2.875336544272126,
      "grad_norm": 0.427167147397995,
      "learning_rate": 9.046192762192207e-07,
      "loss": 2.328,
      "step": 139770
    },
    {
      "epoch": 2.8755422637889936,
      "grad_norm": 0.4857172966003418,
      "learning_rate": 9.0164117929733e-07,
      "loss": 2.3682,
      "step": 139780
    },
    {
      "epoch": 2.8757479833058612,
      "grad_norm": 0.4278903603553772,
      "learning_rate": 8.986679703574585e-07,
      "loss": 2.3288,
      "step": 139790
    },
    {
      "epoch": 2.875953702822729,
      "grad_norm": 0.483659029006958,
      "learning_rate": 8.956996495462555e-07,
      "loss": 2.3439,
      "step": 139800
    },
    {
      "epoch": 2.8761594223395965,
      "grad_norm": 0.4308435916900635,
      "learning_rate": 8.927362170101372e-07,
      "loss": 2.3759,
      "step": 139810
    },
    {
      "epoch": 2.876365141856464,
      "grad_norm": 0.4690185785293579,
      "learning_rate": 8.897776728952755e-07,
      "loss": 2.3301,
      "step": 139820
    },
    {
      "epoch": 2.876570861373332,
      "grad_norm": 0.45102834701538086,
      "learning_rate": 8.868240173475873e-07,
      "loss": 2.3633,
      "step": 139830
    },
    {
      "epoch": 2.8767765808902,
      "grad_norm": 0.4882557690143585,
      "learning_rate": 8.838752505127779e-07,
      "loss": 2.3956,
      "step": 139840
    },
    {
      "epoch": 2.8769823004070676,
      "grad_norm": 0.4927583932876587,
      "learning_rate": 8.809313725362756e-07,
      "loss": 2.3603,
      "step": 139850
    },
    {
      "epoch": 2.8771880199239352,
      "grad_norm": 0.49706318974494934,
      "learning_rate": 8.779923835632975e-07,
      "loss": 2.345,
      "step": 139860
    },
    {
      "epoch": 2.877393739440803,
      "grad_norm": 0.45669880509376526,
      "learning_rate": 8.750582837388055e-07,
      "loss": 2.3429,
      "step": 139870
    },
    {
      "epoch": 2.8775994589576706,
      "grad_norm": 0.48295286297798157,
      "learning_rate": 8.721290732075282e-07,
      "loss": 2.389,
      "step": 139880
    },
    {
      "epoch": 2.877805178474538,
      "grad_norm": 0.45506080985069275,
      "learning_rate": 8.692047521139279e-07,
      "loss": 2.3226,
      "step": 139890
    },
    {
      "epoch": 2.8780108979914063,
      "grad_norm": 0.43708479404449463,
      "learning_rate": 8.66285320602267e-07,
      "loss": 2.349,
      "step": 139900
    },
    {
      "epoch": 2.878216617508274,
      "grad_norm": 0.4238399863243103,
      "learning_rate": 8.633707788165413e-07,
      "loss": 2.3219,
      "step": 139910
    },
    {
      "epoch": 2.8784223370251416,
      "grad_norm": 0.4424816071987152,
      "learning_rate": 8.604611269005025e-07,
      "loss": 2.3847,
      "step": 139920
    },
    {
      "epoch": 2.8786280565420093,
      "grad_norm": 0.46976953744888306,
      "learning_rate": 8.575563649976692e-07,
      "loss": 2.3574,
      "step": 139930
    },
    {
      "epoch": 2.878833776058877,
      "grad_norm": 0.4782489240169525,
      "learning_rate": 8.546564932513157e-07,
      "loss": 2.4111,
      "step": 139940
    },
    {
      "epoch": 2.8790394955757446,
      "grad_norm": 0.4989811182022095,
      "learning_rate": 8.517615118044831e-07,
      "loss": 2.3612,
      "step": 139950
    },
    {
      "epoch": 2.879245215092612,
      "grad_norm": 0.4417342245578766,
      "learning_rate": 8.488714207999682e-07,
      "loss": 2.3829,
      "step": 139960
    },
    {
      "epoch": 2.87945093460948,
      "grad_norm": 0.4567752480506897,
      "learning_rate": 8.459862203803126e-07,
      "loss": 2.3908,
      "step": 139970
    },
    {
      "epoch": 2.8796566541263475,
      "grad_norm": 0.4339388608932495,
      "learning_rate": 8.431059106878247e-07,
      "loss": 2.371,
      "step": 139980
    },
    {
      "epoch": 2.8798623736432156,
      "grad_norm": 0.4878592789173126,
      "learning_rate": 8.402304918645909e-07,
      "loss": 2.3784,
      "step": 139990
    },
    {
      "epoch": 2.8800680931600833,
      "grad_norm": 0.5131596922874451,
      "learning_rate": 8.37359964052431e-07,
      "loss": 2.4041,
      "step": 140000
    },
    {
      "epoch": 2.880273812676951,
      "grad_norm": 0.4642390310764313,
      "learning_rate": 8.344943273929207e-07,
      "loss": 2.3814,
      "step": 140010
    },
    {
      "epoch": 2.8804795321938186,
      "grad_norm": 0.5085402131080627,
      "learning_rate": 8.316335820274357e-07,
      "loss": 2.4275,
      "step": 140020
    },
    {
      "epoch": 2.8806852517106862,
      "grad_norm": 0.47166410088539124,
      "learning_rate": 8.287777280970521e-07,
      "loss": 2.3761,
      "step": 140030
    },
    {
      "epoch": 2.8808909712275543,
      "grad_norm": 0.43378525972366333,
      "learning_rate": 8.259267657426462e-07,
      "loss": 2.3393,
      "step": 140040
    },
    {
      "epoch": 2.881096690744422,
      "grad_norm": 0.4439232349395752,
      "learning_rate": 8.230806951048386e-07,
      "loss": 2.3227,
      "step": 140050
    },
    {
      "epoch": 2.8813024102612896,
      "grad_norm": 0.46591249108314514,
      "learning_rate": 8.202395163240173e-07,
      "loss": 2.3363,
      "step": 140060
    },
    {
      "epoch": 2.8815081297781573,
      "grad_norm": 0.48978516459465027,
      "learning_rate": 8.174032295403033e-07,
      "loss": 2.3258,
      "step": 140070
    },
    {
      "epoch": 2.881713849295025,
      "grad_norm": 0.4709138870239258,
      "learning_rate": 8.145718348936182e-07,
      "loss": 2.4512,
      "step": 140080
    },
    {
      "epoch": 2.8819195688118926,
      "grad_norm": 0.47342991828918457,
      "learning_rate": 8.117453325236057e-07,
      "loss": 2.3574,
      "step": 140090
    },
    {
      "epoch": 2.8821252883287602,
      "grad_norm": 0.46869587898254395,
      "learning_rate": 8.089237225696878e-07,
      "loss": 2.3618,
      "step": 140100
    },
    {
      "epoch": 2.882331007845628,
      "grad_norm": 0.4703579843044281,
      "learning_rate": 8.061070051710307e-07,
      "loss": 2.3857,
      "step": 140110
    },
    {
      "epoch": 2.8825367273624956,
      "grad_norm": 0.4564613103866577,
      "learning_rate": 8.032951804665789e-07,
      "loss": 2.3978,
      "step": 140120
    },
    {
      "epoch": 2.8827424468793637,
      "grad_norm": 0.4811039865016937,
      "learning_rate": 8.004882485950105e-07,
      "loss": 2.3491,
      "step": 140130
    },
    {
      "epoch": 2.8829481663962313,
      "grad_norm": 0.5084524750709534,
      "learning_rate": 7.976862096947924e-07,
      "loss": 2.401,
      "step": 140140
    },
    {
      "epoch": 2.883153885913099,
      "grad_norm": 0.49523600935935974,
      "learning_rate": 7.948890639041251e-07,
      "loss": 2.3653,
      "step": 140150
    },
    {
      "epoch": 2.8833596054299666,
      "grad_norm": 0.453462153673172,
      "learning_rate": 7.920968113609651e-07,
      "loss": 2.3373,
      "step": 140160
    },
    {
      "epoch": 2.8835653249468343,
      "grad_norm": 0.4802464246749878,
      "learning_rate": 7.893094522030686e-07,
      "loss": 2.3558,
      "step": 140170
    },
    {
      "epoch": 2.883771044463702,
      "grad_norm": 0.4797999858856201,
      "learning_rate": 7.865269865678926e-07,
      "loss": 2.3708,
      "step": 140180
    },
    {
      "epoch": 2.88397676398057,
      "grad_norm": 0.4545745849609375,
      "learning_rate": 7.837494145926938e-07,
      "loss": 2.3666,
      "step": 140190
    },
    {
      "epoch": 2.8841824834974377,
      "grad_norm": 0.47389060258865356,
      "learning_rate": 7.809767364144738e-07,
      "loss": 2.348,
      "step": 140200
    },
    {
      "epoch": 2.8843882030143053,
      "grad_norm": 0.5543164610862732,
      "learning_rate": 7.782089521699898e-07,
      "loss": 2.3403,
      "step": 140210
    },
    {
      "epoch": 2.884593922531173,
      "grad_norm": 0.4835107624530792,
      "learning_rate": 7.754460619957659e-07,
      "loss": 2.3946,
      "step": 140220
    },
    {
      "epoch": 2.8847996420480406,
      "grad_norm": 0.5241580605506897,
      "learning_rate": 7.726880660280822e-07,
      "loss": 2.383,
      "step": 140230
    },
    {
      "epoch": 2.8850053615649083,
      "grad_norm": 0.4813081920146942,
      "learning_rate": 7.69934964402974e-07,
      "loss": 2.2773,
      "step": 140240
    },
    {
      "epoch": 2.885211081081776,
      "grad_norm": 0.41962653398513794,
      "learning_rate": 7.671867572562219e-07,
      "loss": 2.4092,
      "step": 140250
    },
    {
      "epoch": 2.8854168005986436,
      "grad_norm": 0.42787134647369385,
      "learning_rate": 7.644434447234061e-07,
      "loss": 2.4041,
      "step": 140260
    },
    {
      "epoch": 2.8856225201155112,
      "grad_norm": 0.41195765137672424,
      "learning_rate": 7.617050269398185e-07,
      "loss": 2.3594,
      "step": 140270
    },
    {
      "epoch": 2.8858282396323793,
      "grad_norm": 0.4941430985927582,
      "learning_rate": 7.58971504040551e-07,
      "loss": 2.4074,
      "step": 140280
    },
    {
      "epoch": 2.886033959149247,
      "grad_norm": 0.509609043598175,
      "learning_rate": 7.562428761604068e-07,
      "loss": 2.3333,
      "step": 140290
    },
    {
      "epoch": 2.8862396786661146,
      "grad_norm": 0.48154643177986145,
      "learning_rate": 7.535191434339895e-07,
      "loss": 2.3067,
      "step": 140300
    },
    {
      "epoch": 2.8864453981829823,
      "grad_norm": 0.4330214560031891,
      "learning_rate": 7.50800305995647e-07,
      "loss": 2.3298,
      "step": 140310
    },
    {
      "epoch": 2.88665111769985,
      "grad_norm": 0.46345263719558716,
      "learning_rate": 7.480863639794832e-07,
      "loss": 2.3823,
      "step": 140320
    },
    {
      "epoch": 2.886856837216718,
      "grad_norm": 0.4745519757270813,
      "learning_rate": 7.453773175193579e-07,
      "loss": 2.3192,
      "step": 140330
    },
    {
      "epoch": 2.8870625567335857,
      "grad_norm": 0.4977683126926422,
      "learning_rate": 7.426731667489084e-07,
      "loss": 2.3599,
      "step": 140340
    },
    {
      "epoch": 2.8872682762504533,
      "grad_norm": 0.460222452878952,
      "learning_rate": 7.399739118014948e-07,
      "loss": 2.3439,
      "step": 140350
    },
    {
      "epoch": 2.887473995767321,
      "grad_norm": 0.47892266511917114,
      "learning_rate": 7.372795528102661e-07,
      "loss": 2.345,
      "step": 140360
    },
    {
      "epoch": 2.8876797152841887,
      "grad_norm": 0.49365079402923584,
      "learning_rate": 7.34590089908116e-07,
      "loss": 2.3402,
      "step": 140370
    },
    {
      "epoch": 2.8878854348010563,
      "grad_norm": 0.46084049344062805,
      "learning_rate": 7.31905523227705e-07,
      "loss": 2.3309,
      "step": 140380
    },
    {
      "epoch": 2.888091154317924,
      "grad_norm": 0.4837969243526459,
      "learning_rate": 7.292258529014495e-07,
      "loss": 2.3627,
      "step": 140390
    },
    {
      "epoch": 2.8882968738347916,
      "grad_norm": 0.4404747784137726,
      "learning_rate": 7.265510790615327e-07,
      "loss": 2.3814,
      "step": 140400
    },
    {
      "epoch": 2.8885025933516593,
      "grad_norm": 0.5171846747398376,
      "learning_rate": 7.2388120183986e-07,
      "loss": 2.3705,
      "step": 140410
    },
    {
      "epoch": 2.8887083128685274,
      "grad_norm": 0.4407489597797394,
      "learning_rate": 7.212162213681484e-07,
      "loss": 2.3631,
      "step": 140420
    },
    {
      "epoch": 2.888914032385395,
      "grad_norm": 0.4858487546443939,
      "learning_rate": 7.18556137777826e-07,
      "loss": 2.4119,
      "step": 140430
    },
    {
      "epoch": 2.8891197519022627,
      "grad_norm": 0.4338114559650421,
      "learning_rate": 7.159009512001213e-07,
      "loss": 2.3438,
      "step": 140440
    },
    {
      "epoch": 2.8893254714191303,
      "grad_norm": 0.5940195918083191,
      "learning_rate": 7.132506617659851e-07,
      "loss": 2.3585,
      "step": 140450
    },
    {
      "epoch": 2.889531190935998,
      "grad_norm": 0.4453425109386444,
      "learning_rate": 7.106052696061461e-07,
      "loss": 2.3472,
      "step": 140460
    },
    {
      "epoch": 2.889736910452866,
      "grad_norm": 0.5123056769371033,
      "learning_rate": 7.079647748510887e-07,
      "loss": 2.4078,
      "step": 140470
    },
    {
      "epoch": 2.8899426299697337,
      "grad_norm": 0.44846659898757935,
      "learning_rate": 7.053291776310422e-07,
      "loss": 2.4027,
      "step": 140480
    },
    {
      "epoch": 2.8901483494866014,
      "grad_norm": 0.44752562046051025,
      "learning_rate": 7.026984780760249e-07,
      "loss": 2.344,
      "step": 140490
    },
    {
      "epoch": 2.890354069003469,
      "grad_norm": 0.4871360957622528,
      "learning_rate": 7.000726763157883e-07,
      "loss": 2.3954,
      "step": 140500
    },
    {
      "epoch": 2.8905597885203367,
      "grad_norm": 0.4406784474849701,
      "learning_rate": 6.974517724798402e-07,
      "loss": 2.3767,
      "step": 140510
    },
    {
      "epoch": 2.8907655080372043,
      "grad_norm": 0.4565179944038391,
      "learning_rate": 6.948357666974769e-07,
      "loss": 2.3444,
      "step": 140520
    },
    {
      "epoch": 2.890971227554072,
      "grad_norm": 0.4364759922027588,
      "learning_rate": 6.922246590976955e-07,
      "loss": 2.3332,
      "step": 140530
    },
    {
      "epoch": 2.8911769470709396,
      "grad_norm": 0.462256520986557,
      "learning_rate": 6.896184498093262e-07,
      "loss": 2.3545,
      "step": 140540
    },
    {
      "epoch": 2.8913826665878073,
      "grad_norm": 0.49034392833709717,
      "learning_rate": 6.870171389608992e-07,
      "loss": 2.3622,
      "step": 140550
    },
    {
      "epoch": 2.8915883861046754,
      "grad_norm": 0.5099607110023499,
      "learning_rate": 6.844207266807123e-07,
      "loss": 2.341,
      "step": 140560
    },
    {
      "epoch": 2.891794105621543,
      "grad_norm": 0.4891018569469452,
      "learning_rate": 6.818292130968628e-07,
      "loss": 2.3586,
      "step": 140570
    },
    {
      "epoch": 2.8919998251384107,
      "grad_norm": 0.4968859851360321,
      "learning_rate": 6.792425983371487e-07,
      "loss": 2.3967,
      "step": 140580
    },
    {
      "epoch": 2.8922055446552783,
      "grad_norm": 0.47072741389274597,
      "learning_rate": 6.766608825291676e-07,
      "loss": 2.3703,
      "step": 140590
    },
    {
      "epoch": 2.892411264172146,
      "grad_norm": 0.49504515528678894,
      "learning_rate": 6.740840658002623e-07,
      "loss": 2.3156,
      "step": 140600
    },
    {
      "epoch": 2.8926169836890137,
      "grad_norm": 0.4662308990955353,
      "learning_rate": 6.715121482775199e-07,
      "loss": 2.3707,
      "step": 140610
    },
    {
      "epoch": 2.8928227032058818,
      "grad_norm": 0.4533642530441284,
      "learning_rate": 6.689451300878057e-07,
      "loss": 2.3439,
      "step": 140620
    },
    {
      "epoch": 2.8930284227227494,
      "grad_norm": 0.424428790807724,
      "learning_rate": 6.663830113577519e-07,
      "loss": 2.3375,
      "step": 140630
    },
    {
      "epoch": 2.893234142239617,
      "grad_norm": 0.45512428879737854,
      "learning_rate": 6.638257922137125e-07,
      "loss": 2.3217,
      "step": 140640
    },
    {
      "epoch": 2.8934398617564847,
      "grad_norm": 0.462415874004364,
      "learning_rate": 6.612734727818315e-07,
      "loss": 2.2953,
      "step": 140650
    },
    {
      "epoch": 2.8936455812733524,
      "grad_norm": 0.43609148263931274,
      "learning_rate": 6.58726053188008e-07,
      "loss": 2.3737,
      "step": 140660
    },
    {
      "epoch": 2.89385130079022,
      "grad_norm": 0.47612830996513367,
      "learning_rate": 6.561835335578748e-07,
      "loss": 2.3512,
      "step": 140670
    },
    {
      "epoch": 2.8940570203070877,
      "grad_norm": 0.4866754710674286,
      "learning_rate": 6.536459140168427e-07,
      "loss": 2.366,
      "step": 140680
    },
    {
      "epoch": 2.8942627398239553,
      "grad_norm": 0.45985913276672363,
      "learning_rate": 6.511131946900895e-07,
      "loss": 2.4034,
      "step": 140690
    },
    {
      "epoch": 2.894468459340823,
      "grad_norm": 0.5299413204193115,
      "learning_rate": 6.485853757025373e-07,
      "loss": 2.3698,
      "step": 140700
    },
    {
      "epoch": 2.894674178857691,
      "grad_norm": 0.5973715782165527,
      "learning_rate": 6.460624571788643e-07,
      "loss": 2.362,
      "step": 140710
    },
    {
      "epoch": 2.8948798983745587,
      "grad_norm": 0.4404872953891754,
      "learning_rate": 6.435444392435263e-07,
      "loss": 2.4056,
      "step": 140720
    },
    {
      "epoch": 2.8950856178914264,
      "grad_norm": 0.44565549492836,
      "learning_rate": 6.41031322020702e-07,
      "loss": 2.3457,
      "step": 140730
    },
    {
      "epoch": 2.895291337408294,
      "grad_norm": 0.490224689245224,
      "learning_rate": 6.385231056343699e-07,
      "loss": 2.3911,
      "step": 140740
    },
    {
      "epoch": 2.8954970569251617,
      "grad_norm": 0.4480449855327606,
      "learning_rate": 6.36019790208231e-07,
      "loss": 2.3648,
      "step": 140750
    },
    {
      "epoch": 2.89570277644203,
      "grad_norm": 0.46709513664245605,
      "learning_rate": 6.335213758657754e-07,
      "loss": 2.352,
      "step": 140760
    },
    {
      "epoch": 2.8959084959588974,
      "grad_norm": 0.4826113283634186,
      "learning_rate": 6.310278627302157e-07,
      "loss": 2.3639,
      "step": 140770
    },
    {
      "epoch": 2.896114215475765,
      "grad_norm": 0.477984219789505,
      "learning_rate": 6.285392509245647e-07,
      "loss": 2.347,
      "step": 140780
    },
    {
      "epoch": 2.8963199349926327,
      "grad_norm": 0.474168598651886,
      "learning_rate": 6.260555405715574e-07,
      "loss": 2.3185,
      "step": 140790
    },
    {
      "epoch": 2.8965256545095004,
      "grad_norm": 0.45555421710014343,
      "learning_rate": 6.23576731793718e-07,
      "loss": 2.3599,
      "step": 140800
    },
    {
      "epoch": 2.896731374026368,
      "grad_norm": 0.4422806203365326,
      "learning_rate": 6.211028247132933e-07,
      "loss": 2.3697,
      "step": 140810
    },
    {
      "epoch": 2.8969370935432357,
      "grad_norm": 0.46046414971351624,
      "learning_rate": 6.186338194523189e-07,
      "loss": 2.392,
      "step": 140820
    },
    {
      "epoch": 2.8971428130601034,
      "grad_norm": 0.4866913855075836,
      "learning_rate": 6.161697161325641e-07,
      "loss": 2.3558,
      "step": 140830
    },
    {
      "epoch": 2.897348532576971,
      "grad_norm": 0.4921033978462219,
      "learning_rate": 6.137105148755984e-07,
      "loss": 2.3315,
      "step": 140840
    },
    {
      "epoch": 2.897554252093839,
      "grad_norm": 0.4733615815639496,
      "learning_rate": 6.112562158026914e-07,
      "loss": 2.34,
      "step": 140850
    },
    {
      "epoch": 2.8977599716107068,
      "grad_norm": 0.4280947148799896,
      "learning_rate": 6.088068190349128e-07,
      "loss": 2.3681,
      "step": 140860
    },
    {
      "epoch": 2.8979656911275744,
      "grad_norm": 0.43441250920295715,
      "learning_rate": 6.063623246930771e-07,
      "loss": 2.4138,
      "step": 140870
    },
    {
      "epoch": 2.898171410644442,
      "grad_norm": 0.5061637163162231,
      "learning_rate": 6.039227328977548e-07,
      "loss": 2.3707,
      "step": 140880
    },
    {
      "epoch": 2.8983771301613097,
      "grad_norm": 0.4380579888820648,
      "learning_rate": 6.014880437692826e-07,
      "loss": 2.3397,
      "step": 140890
    },
    {
      "epoch": 2.8985828496781774,
      "grad_norm": 0.46906596422195435,
      "learning_rate": 5.990582574277536e-07,
      "loss": 2.3676,
      "step": 140900
    },
    {
      "epoch": 2.8987885691950455,
      "grad_norm": 0.453529953956604,
      "learning_rate": 5.966333739930053e-07,
      "loss": 2.3463,
      "step": 140910
    },
    {
      "epoch": 2.898994288711913,
      "grad_norm": 0.45608291029930115,
      "learning_rate": 5.942133935846528e-07,
      "loss": 2.359,
      "step": 140920
    },
    {
      "epoch": 2.8992000082287808,
      "grad_norm": 0.4461309313774109,
      "learning_rate": 5.917983163220564e-07,
      "loss": 2.3642,
      "step": 140930
    },
    {
      "epoch": 2.8994057277456484,
      "grad_norm": 0.46756094694137573,
      "learning_rate": 5.893881423243319e-07,
      "loss": 2.4019,
      "step": 140940
    },
    {
      "epoch": 2.899611447262516,
      "grad_norm": 0.43833762407302856,
      "learning_rate": 5.869828717103843e-07,
      "loss": 2.3422,
      "step": 140950
    },
    {
      "epoch": 2.8998171667793837,
      "grad_norm": 0.4436391592025757,
      "learning_rate": 5.845825045988185e-07,
      "loss": 2.3225,
      "step": 140960
    },
    {
      "epoch": 2.9000228862962514,
      "grad_norm": 0.4408552348613739,
      "learning_rate": 5.82187041108051e-07,
      "loss": 2.3676,
      "step": 140970
    },
    {
      "epoch": 2.900228605813119,
      "grad_norm": 0.465096116065979,
      "learning_rate": 5.797964813562429e-07,
      "loss": 2.3853,
      "step": 140980
    },
    {
      "epoch": 2.900434325329987,
      "grad_norm": 0.47189685702323914,
      "learning_rate": 5.774108254612998e-07,
      "loss": 2.4788,
      "step": 140990
    },
    {
      "epoch": 2.900640044846855,
      "grad_norm": 0.4528108835220337,
      "learning_rate": 5.750300735408831e-07,
      "loss": 2.3946,
      "step": 141000
    },
    {
      "epoch": 2.9008457643637224,
      "grad_norm": 0.44015368819236755,
      "learning_rate": 5.726542257124323e-07,
      "loss": 2.3023,
      "step": 141010
    },
    {
      "epoch": 2.90105148388059,
      "grad_norm": 0.48421594500541687,
      "learning_rate": 5.702832820931425e-07,
      "loss": 2.3565,
      "step": 141020
    },
    {
      "epoch": 2.9012572033974577,
      "grad_norm": 0.5087523460388184,
      "learning_rate": 5.679172427999424e-07,
      "loss": 2.352,
      "step": 141030
    },
    {
      "epoch": 2.9014629229143254,
      "grad_norm": 0.4548226594924927,
      "learning_rate": 5.655561079495497e-07,
      "loss": 2.364,
      "step": 141040
    },
    {
      "epoch": 2.9016686424311935,
      "grad_norm": 0.4358845055103302,
      "learning_rate": 5.631998776584268e-07,
      "loss": 2.3744,
      "step": 141050
    },
    {
      "epoch": 2.901874361948061,
      "grad_norm": 0.4898936152458191,
      "learning_rate": 5.608485520427808e-07,
      "loss": 2.3864,
      "step": 141060
    },
    {
      "epoch": 2.902080081464929,
      "grad_norm": 0.47740408778190613,
      "learning_rate": 5.585021312185967e-07,
      "loss": 2.3439,
      "step": 141070
    },
    {
      "epoch": 2.9022858009817964,
      "grad_norm": 0.43214476108551025,
      "learning_rate": 5.561606153016152e-07,
      "loss": 2.3964,
      "step": 141080
    },
    {
      "epoch": 2.902491520498664,
      "grad_norm": 0.4521822929382324,
      "learning_rate": 5.538240044073329e-07,
      "loss": 2.3523,
      "step": 141090
    },
    {
      "epoch": 2.9026972400155318,
      "grad_norm": 0.4287356436252594,
      "learning_rate": 5.514922986509796e-07,
      "loss": 2.3513,
      "step": 141100
    },
    {
      "epoch": 2.9029029595323994,
      "grad_norm": 0.5033363699913025,
      "learning_rate": 5.491654981475968e-07,
      "loss": 2.3628,
      "step": 141110
    },
    {
      "epoch": 2.903108679049267,
      "grad_norm": 0.4747604429721832,
      "learning_rate": 5.468436030119262e-07,
      "loss": 2.3657,
      "step": 141120
    },
    {
      "epoch": 2.9033143985661347,
      "grad_norm": 0.45123395323753357,
      "learning_rate": 5.445266133585203e-07,
      "loss": 2.3531,
      "step": 141130
    },
    {
      "epoch": 2.903520118083003,
      "grad_norm": 0.43506401777267456,
      "learning_rate": 5.422145293016323e-07,
      "loss": 2.3869,
      "step": 141140
    },
    {
      "epoch": 2.9037258375998705,
      "grad_norm": 0.4854562282562256,
      "learning_rate": 5.399073509553376e-07,
      "loss": 2.3906,
      "step": 141150
    },
    {
      "epoch": 2.903931557116738,
      "grad_norm": 0.4424612522125244,
      "learning_rate": 5.376050784334119e-07,
      "loss": 2.3127,
      "step": 141160
    },
    {
      "epoch": 2.9041372766336058,
      "grad_norm": 0.46713340282440186,
      "learning_rate": 5.353077118494199e-07,
      "loss": 2.3567,
      "step": 141170
    },
    {
      "epoch": 2.9043429961504734,
      "grad_norm": 0.4916759729385376,
      "learning_rate": 5.33015251316682e-07,
      "loss": 2.355,
      "step": 141180
    },
    {
      "epoch": 2.9045487156673415,
      "grad_norm": 0.4776442050933838,
      "learning_rate": 5.307276969482855e-07,
      "loss": 2.3415,
      "step": 141190
    },
    {
      "epoch": 2.904754435184209,
      "grad_norm": 0.4417591989040375,
      "learning_rate": 5.284450488570291e-07,
      "loss": 2.3859,
      "step": 141200
    },
    {
      "epoch": 2.904960154701077,
      "grad_norm": 0.47106197476387024,
      "learning_rate": 5.26167307155534e-07,
      "loss": 2.3598,
      "step": 141210
    },
    {
      "epoch": 2.9051658742179445,
      "grad_norm": 0.45977234840393066,
      "learning_rate": 5.238944719561434e-07,
      "loss": 2.304,
      "step": 141220
    },
    {
      "epoch": 2.905371593734812,
      "grad_norm": 0.43250200152397156,
      "learning_rate": 5.216265433709455e-07,
      "loss": 2.3852,
      "step": 141230
    },
    {
      "epoch": 2.90557731325168,
      "grad_norm": 0.5191914439201355,
      "learning_rate": 5.193635215118286e-07,
      "loss": 2.4012,
      "step": 141240
    },
    {
      "epoch": 2.9057830327685474,
      "grad_norm": 0.4461299479007721,
      "learning_rate": 5.171054064904035e-07,
      "loss": 2.402,
      "step": 141250
    },
    {
      "epoch": 2.905988752285415,
      "grad_norm": 0.461341917514801,
      "learning_rate": 5.148521984180366e-07,
      "loss": 2.326,
      "step": 141260
    },
    {
      "epoch": 2.9061944718022827,
      "grad_norm": 0.42535141110420227,
      "learning_rate": 5.126038974059055e-07,
      "loss": 2.3107,
      "step": 141270
    },
    {
      "epoch": 2.906400191319151,
      "grad_norm": 0.4716908633708954,
      "learning_rate": 5.103605035648662e-07,
      "loss": 2.43,
      "step": 141280
    },
    {
      "epoch": 2.9066059108360185,
      "grad_norm": 0.44432902336120605,
      "learning_rate": 5.081220170055966e-07,
      "loss": 2.3474,
      "step": 141290
    },
    {
      "epoch": 2.906811630352886,
      "grad_norm": 0.4888745844364166,
      "learning_rate": 5.058884378384976e-07,
      "loss": 2.3411,
      "step": 141300
    },
    {
      "epoch": 2.907017349869754,
      "grad_norm": 0.5123066902160645,
      "learning_rate": 5.036597661737474e-07,
      "loss": 2.3205,
      "step": 141310
    },
    {
      "epoch": 2.9072230693866215,
      "grad_norm": 0.4868089258670807,
      "learning_rate": 5.014360021212694e-07,
      "loss": 2.3646,
      "step": 141320
    },
    {
      "epoch": 2.907428788903489,
      "grad_norm": 0.4540618062019348,
      "learning_rate": 4.992171457907535e-07,
      "loss": 2.3171,
      "step": 141330
    },
    {
      "epoch": 2.907634508420357,
      "grad_norm": 0.48015493154525757,
      "learning_rate": 4.970031972916455e-07,
      "loss": 2.3664,
      "step": 141340
    },
    {
      "epoch": 2.907840227937225,
      "grad_norm": 0.5201729536056519,
      "learning_rate": 4.94794156733136e-07,
      "loss": 2.3142,
      "step": 141350
    },
    {
      "epoch": 2.9080459474540925,
      "grad_norm": 0.48769259452819824,
      "learning_rate": 4.925900242241932e-07,
      "loss": 2.3583,
      "step": 141360
    },
    {
      "epoch": 2.90825166697096,
      "grad_norm": 0.4844367802143097,
      "learning_rate": 4.903907998735413e-07,
      "loss": 2.3229,
      "step": 141370
    },
    {
      "epoch": 2.908457386487828,
      "grad_norm": 0.46155503392219543,
      "learning_rate": 4.88196483789638e-07,
      "loss": 2.354,
      "step": 141380
    },
    {
      "epoch": 2.9086631060046955,
      "grad_norm": 0.47225096821784973,
      "learning_rate": 4.860070760807411e-07,
      "loss": 2.3976,
      "step": 141390
    },
    {
      "epoch": 2.908868825521563,
      "grad_norm": 0.45959848165512085,
      "learning_rate": 4.838225768548199e-07,
      "loss": 2.3392,
      "step": 141400
    },
    {
      "epoch": 2.9090745450384308,
      "grad_norm": 0.43144866824150085,
      "learning_rate": 4.816429862196325e-07,
      "loss": 2.328,
      "step": 141410
    },
    {
      "epoch": 2.9092802645552984,
      "grad_norm": 0.44882944226264954,
      "learning_rate": 4.79468304282693e-07,
      "loss": 2.3159,
      "step": 141420
    },
    {
      "epoch": 2.9094859840721665,
      "grad_norm": 0.4712306559085846,
      "learning_rate": 4.77298531151249e-07,
      "loss": 2.3971,
      "step": 141430
    },
    {
      "epoch": 2.909691703589034,
      "grad_norm": 0.46142181754112244,
      "learning_rate": 4.7513366693234807e-07,
      "loss": 2.3527,
      "step": 141440
    },
    {
      "epoch": 2.909897423105902,
      "grad_norm": 0.46439021825790405,
      "learning_rate": 4.729737117327493e-07,
      "loss": 2.3562,
      "step": 141450
    },
    {
      "epoch": 2.9101031426227695,
      "grad_norm": 0.4895435571670532,
      "learning_rate": 4.708186656590008e-07,
      "loss": 2.378,
      "step": 141460
    },
    {
      "epoch": 2.910308862139637,
      "grad_norm": 0.43555811047554016,
      "learning_rate": 4.686685288173953e-07,
      "loss": 2.3816,
      "step": 141470
    },
    {
      "epoch": 2.9105145816565052,
      "grad_norm": 0.48830446600914,
      "learning_rate": 4.6652330131400357e-07,
      "loss": 2.346,
      "step": 141480
    },
    {
      "epoch": 2.910720301173373,
      "grad_norm": 0.46276870369911194,
      "learning_rate": 4.643829832546076e-07,
      "loss": 2.3745,
      "step": 141490
    },
    {
      "epoch": 2.9109260206902405,
      "grad_norm": 0.47166112065315247,
      "learning_rate": 4.622475747448007e-07,
      "loss": 2.4416,
      "step": 141500
    },
    {
      "epoch": 2.911131740207108,
      "grad_norm": 0.5104331374168396,
      "learning_rate": 4.601170758899209e-07,
      "loss": 2.4075,
      "step": 141510
    },
    {
      "epoch": 2.911337459723976,
      "grad_norm": 0.45409366488456726,
      "learning_rate": 4.579914867950175e-07,
      "loss": 2.42,
      "step": 141520
    },
    {
      "epoch": 2.9115431792408435,
      "grad_norm": 0.44345787167549133,
      "learning_rate": 4.5587080756497313e-07,
      "loss": 2.3844,
      "step": 141530
    },
    {
      "epoch": 2.911748898757711,
      "grad_norm": 0.47665154933929443,
      "learning_rate": 4.5375503830435986e-07,
      "loss": 2.3712,
      "step": 141540
    },
    {
      "epoch": 2.911954618274579,
      "grad_norm": 0.49807506799697876,
      "learning_rate": 4.5164417911753853e-07,
      "loss": 2.3694,
      "step": 141550
    },
    {
      "epoch": 2.9121603377914465,
      "grad_norm": 0.474746972322464,
      "learning_rate": 4.4953823010865926e-07,
      "loss": 2.3569,
      "step": 141560
    },
    {
      "epoch": 2.9123660573083145,
      "grad_norm": 0.48093676567077637,
      "learning_rate": 4.4743719138156115e-07,
      "loss": 2.4163,
      "step": 141570
    },
    {
      "epoch": 2.912571776825182,
      "grad_norm": 0.43446290493011475,
      "learning_rate": 4.4534106303988353e-07,
      "loss": 2.3866,
      "step": 141580
    },
    {
      "epoch": 2.91277749634205,
      "grad_norm": 0.4615776538848877,
      "learning_rate": 4.432498451870437e-07,
      "loss": 2.3688,
      "step": 141590
    },
    {
      "epoch": 2.9129832158589175,
      "grad_norm": 0.4706137180328369,
      "learning_rate": 4.411635379261481e-07,
      "loss": 2.38,
      "step": 141600
    },
    {
      "epoch": 2.913188935375785,
      "grad_norm": 0.4625829756259918,
      "learning_rate": 4.3908214136013646e-07,
      "loss": 2.4046,
      "step": 141610
    },
    {
      "epoch": 2.9133946548926533,
      "grad_norm": 0.4673462510108948,
      "learning_rate": 4.3700565559166017e-07,
      "loss": 2.352,
      "step": 141620
    },
    {
      "epoch": 2.913600374409521,
      "grad_norm": 0.4309409558773041,
      "learning_rate": 4.3493408072313725e-07,
      "loss": 2.3521,
      "step": 141630
    },
    {
      "epoch": 2.9138060939263886,
      "grad_norm": 0.4479353725910187,
      "learning_rate": 4.328674168567637e-07,
      "loss": 2.4089,
      "step": 141640
    },
    {
      "epoch": 2.914011813443256,
      "grad_norm": 0.46139487624168396,
      "learning_rate": 4.308056640944469e-07,
      "loss": 2.3165,
      "step": 141650
    },
    {
      "epoch": 2.914217532960124,
      "grad_norm": 0.46922406554222107,
      "learning_rate": 4.2874882253791657e-07,
      "loss": 2.3945,
      "step": 141660
    },
    {
      "epoch": 2.9144232524769915,
      "grad_norm": 0.4926619529724121,
      "learning_rate": 4.2669689228859167e-07,
      "loss": 2.3042,
      "step": 141670
    },
    {
      "epoch": 2.914628971993859,
      "grad_norm": 0.45418402552604675,
      "learning_rate": 4.2464987344771334e-07,
      "loss": 2.3717,
      "step": 141680
    },
    {
      "epoch": 2.914834691510727,
      "grad_norm": 0.46621546149253845,
      "learning_rate": 4.226077661162342e-07,
      "loss": 2.3628,
      "step": 141690
    },
    {
      "epoch": 2.9150404110275945,
      "grad_norm": 0.48105013370513916,
      "learning_rate": 4.205705703948737e-07,
      "loss": 2.4019,
      "step": 141700
    },
    {
      "epoch": 2.9152461305444626,
      "grad_norm": 0.4432009756565094,
      "learning_rate": 4.185382863841292e-07,
      "loss": 2.3482,
      "step": 141710
    },
    {
      "epoch": 2.9154518500613302,
      "grad_norm": 0.4877321124076843,
      "learning_rate": 4.165109141842427e-07,
      "loss": 2.3406,
      "step": 141720
    },
    {
      "epoch": 2.915657569578198,
      "grad_norm": 0.47850659489631653,
      "learning_rate": 4.1448845389520096e-07,
      "loss": 2.3383,
      "step": 141730
    },
    {
      "epoch": 2.9158632890950655,
      "grad_norm": 0.46070531010627747,
      "learning_rate": 4.124709056167686e-07,
      "loss": 2.3352,
      "step": 141740
    },
    {
      "epoch": 2.916069008611933,
      "grad_norm": 0.4700389504432678,
      "learning_rate": 4.104582694484549e-07,
      "loss": 2.3308,
      "step": 141750
    },
    {
      "epoch": 2.916274728128801,
      "grad_norm": 0.49983587861061096,
      "learning_rate": 4.084505454895471e-07,
      "loss": 2.3314,
      "step": 141760
    },
    {
      "epoch": 2.916480447645669,
      "grad_norm": 0.4470980167388916,
      "learning_rate": 4.06447733839066e-07,
      "loss": 2.3391,
      "step": 141770
    },
    {
      "epoch": 2.9166861671625366,
      "grad_norm": 0.4902914762496948,
      "learning_rate": 4.0444983459578813e-07,
      "loss": 2.4242,
      "step": 141780
    },
    {
      "epoch": 2.9168918866794042,
      "grad_norm": 0.4709501266479492,
      "learning_rate": 4.02456847858268e-07,
      "loss": 2.3869,
      "step": 141790
    },
    {
      "epoch": 2.917097606196272,
      "grad_norm": 0.46643152832984924,
      "learning_rate": 4.0046877372482693e-07,
      "loss": 2.3259,
      "step": 141800
    },
    {
      "epoch": 2.9173033257131396,
      "grad_norm": 0.4580658972263336,
      "learning_rate": 3.984856122934977e-07,
      "loss": 2.3405,
      "step": 141810
    },
    {
      "epoch": 2.917509045230007,
      "grad_norm": 0.4750669300556183,
      "learning_rate": 3.9650736366210195e-07,
      "loss": 2.3237,
      "step": 141820
    },
    {
      "epoch": 2.917714764746875,
      "grad_norm": 0.47601795196533203,
      "learning_rate": 3.9453402792823945e-07,
      "loss": 2.3161,
      "step": 141830
    },
    {
      "epoch": 2.9179204842637425,
      "grad_norm": 0.433750182390213,
      "learning_rate": 3.9256560518922125e-07,
      "loss": 2.3568,
      "step": 141840
    },
    {
      "epoch": 2.91812620378061,
      "grad_norm": 0.4451906383037567,
      "learning_rate": 3.9060209554214745e-07,
      "loss": 2.4114,
      "step": 141850
    },
    {
      "epoch": 2.9183319232974783,
      "grad_norm": 0.47860151529312134,
      "learning_rate": 3.886434990838628e-07,
      "loss": 2.3188,
      "step": 141860
    },
    {
      "epoch": 2.918537642814346,
      "grad_norm": 0.46336302161216736,
      "learning_rate": 3.8668981591097885e-07,
      "loss": 2.34,
      "step": 141870
    },
    {
      "epoch": 2.9187433623312136,
      "grad_norm": 0.46934375166893005,
      "learning_rate": 3.847410461198631e-07,
      "loss": 2.3206,
      "step": 141880
    },
    {
      "epoch": 2.918949081848081,
      "grad_norm": 0.46536654233932495,
      "learning_rate": 3.827971898066274e-07,
      "loss": 2.3619,
      "step": 141890
    },
    {
      "epoch": 2.919154801364949,
      "grad_norm": 0.48098036646842957,
      "learning_rate": 3.808582470671618e-07,
      "loss": 2.3725,
      "step": 141900
    },
    {
      "epoch": 2.919360520881817,
      "grad_norm": 0.5644034147262573,
      "learning_rate": 3.789242179971009e-07,
      "loss": 2.3371,
      "step": 141910
    },
    {
      "epoch": 2.9195662403986846,
      "grad_norm": 0.47543221712112427,
      "learning_rate": 3.7699510269183504e-07,
      "loss": 2.3805,
      "step": 141920
    },
    {
      "epoch": 2.9197719599155523,
      "grad_norm": 0.4556676149368286,
      "learning_rate": 3.7507090124652144e-07,
      "loss": 2.3428,
      "step": 141930
    },
    {
      "epoch": 2.91997767943242,
      "grad_norm": 0.49748915433883667,
      "learning_rate": 3.731516137560731e-07,
      "loss": 2.3842,
      "step": 141940
    },
    {
      "epoch": 2.9201833989492876,
      "grad_norm": 0.47121867537498474,
      "learning_rate": 3.7123724031514764e-07,
      "loss": 2.4094,
      "step": 141950
    },
    {
      "epoch": 2.9203891184661552,
      "grad_norm": 0.4452071785926819,
      "learning_rate": 3.6932778101818054e-07,
      "loss": 2.3465,
      "step": 141960
    },
    {
      "epoch": 2.920594837983023,
      "grad_norm": 0.5115879774093628,
      "learning_rate": 3.6742323595935214e-07,
      "loss": 2.3635,
      "step": 141970
    },
    {
      "epoch": 2.9208005574998905,
      "grad_norm": 0.40909436345100403,
      "learning_rate": 3.6552360523260944e-07,
      "loss": 2.3374,
      "step": 141980
    },
    {
      "epoch": 2.921006277016758,
      "grad_norm": 0.4481450021266937,
      "learning_rate": 3.6362888893163303e-07,
      "loss": 2.3949,
      "step": 141990
    },
    {
      "epoch": 2.9212119965336263,
      "grad_norm": 0.49655261635780334,
      "learning_rate": 3.617390871498927e-07,
      "loss": 2.3522,
      "step": 142000
    },
    {
      "epoch": 2.921417716050494,
      "grad_norm": 0.4541606307029724,
      "learning_rate": 3.5985419998060265e-07,
      "loss": 2.3768,
      "step": 142010
    },
    {
      "epoch": 2.9216234355673616,
      "grad_norm": 0.46453067660331726,
      "learning_rate": 3.5797422751673303e-07,
      "loss": 2.3378,
      "step": 142020
    },
    {
      "epoch": 2.9218291550842292,
      "grad_norm": 0.4383447766304016,
      "learning_rate": 3.560991698510097e-07,
      "loss": 2.3924,
      "step": 142030
    },
    {
      "epoch": 2.922034874601097,
      "grad_norm": 0.44190075993537903,
      "learning_rate": 3.542290270759252e-07,
      "loss": 2.3486,
      "step": 142040
    },
    {
      "epoch": 2.9222405941179646,
      "grad_norm": 0.45256751775741577,
      "learning_rate": 3.523637992837059e-07,
      "loss": 2.3398,
      "step": 142050
    },
    {
      "epoch": 2.9224463136348326,
      "grad_norm": 0.47252678871154785,
      "learning_rate": 3.505034865663781e-07,
      "loss": 2.3657,
      "step": 142060
    },
    {
      "epoch": 2.9226520331517003,
      "grad_norm": 0.48835664987564087,
      "learning_rate": 3.486480890156796e-07,
      "loss": 2.3931,
      "step": 142070
    },
    {
      "epoch": 2.922857752668568,
      "grad_norm": 0.48450741171836853,
      "learning_rate": 3.467976067231482e-07,
      "loss": 2.3482,
      "step": 142080
    },
    {
      "epoch": 2.9230634721854356,
      "grad_norm": 0.5278814435005188,
      "learning_rate": 3.449520397800332e-07,
      "loss": 2.3576,
      "step": 142090
    },
    {
      "epoch": 2.9232691917023033,
      "grad_norm": 0.4888968765735626,
      "learning_rate": 3.43111388277384e-07,
      "loss": 2.3549,
      "step": 142100
    },
    {
      "epoch": 2.923474911219171,
      "grad_norm": 0.46739664673805237,
      "learning_rate": 3.4127565230598345e-07,
      "loss": 2.3067,
      "step": 142110
    },
    {
      "epoch": 2.9236806307360386,
      "grad_norm": 0.44520092010498047,
      "learning_rate": 3.3944483195638144e-07,
      "loss": 2.3456,
      "step": 142120
    },
    {
      "epoch": 2.923886350252906,
      "grad_norm": 0.44650766253471375,
      "learning_rate": 3.3761892731888345e-07,
      "loss": 2.3338,
      "step": 142130
    },
    {
      "epoch": 2.924092069769774,
      "grad_norm": 0.47760361433029175,
      "learning_rate": 3.357979384835508e-07,
      "loss": 2.3218,
      "step": 142140
    },
    {
      "epoch": 2.924297789286642,
      "grad_norm": 0.45516082644462585,
      "learning_rate": 3.339818655402005e-07,
      "loss": 2.3628,
      "step": 142150
    },
    {
      "epoch": 2.9245035088035096,
      "grad_norm": 0.4858625829219818,
      "learning_rate": 3.321707085783943e-07,
      "loss": 2.3322,
      "step": 142160
    },
    {
      "epoch": 2.9247092283203773,
      "grad_norm": 0.449931800365448,
      "learning_rate": 3.3036446768750504e-07,
      "loss": 2.32,
      "step": 142170
    },
    {
      "epoch": 2.924914947837245,
      "grad_norm": 0.44469761848449707,
      "learning_rate": 3.2856314295658384e-07,
      "loss": 2.3799,
      "step": 142180
    },
    {
      "epoch": 2.9251206673541126,
      "grad_norm": 0.47158512473106384,
      "learning_rate": 3.26766734474504e-07,
      "loss": 2.3544,
      "step": 142190
    },
    {
      "epoch": 2.9253263868709807,
      "grad_norm": 0.4844718873500824,
      "learning_rate": 3.249752423298724e-07,
      "loss": 2.3259,
      "step": 142200
    },
    {
      "epoch": 2.9255321063878483,
      "grad_norm": 0.5599868297576904,
      "learning_rate": 3.2318866661105174e-07,
      "loss": 2.3728,
      "step": 142210
    },
    {
      "epoch": 2.925737825904716,
      "grad_norm": 0.4526265263557434,
      "learning_rate": 3.2140700740614924e-07,
      "loss": 2.3865,
      "step": 142220
    },
    {
      "epoch": 2.9259435454215836,
      "grad_norm": 0.4662554860115051,
      "learning_rate": 3.1963026480306136e-07,
      "loss": 2.3463,
      "step": 142230
    },
    {
      "epoch": 2.9261492649384513,
      "grad_norm": 0.5194844603538513,
      "learning_rate": 3.17858438889429e-07,
      "loss": 2.3154,
      "step": 142240
    },
    {
      "epoch": 2.926354984455319,
      "grad_norm": 0.4690355062484741,
      "learning_rate": 3.160915297526268e-07,
      "loss": 2.3681,
      "step": 142250
    },
    {
      "epoch": 2.9265607039721866,
      "grad_norm": 0.43010443449020386,
      "learning_rate": 3.1432953747981833e-07,
      "loss": 2.4283,
      "step": 142260
    },
    {
      "epoch": 2.9267664234890542,
      "grad_norm": 0.48392581939697266,
      "learning_rate": 3.125724621579118e-07,
      "loss": 2.3582,
      "step": 142270
    },
    {
      "epoch": 2.926972143005922,
      "grad_norm": 0.45826950669288635,
      "learning_rate": 3.108203038735713e-07,
      "loss": 2.2735,
      "step": 142280
    },
    {
      "epoch": 2.92717786252279,
      "grad_norm": 0.4515700042247772,
      "learning_rate": 3.0907306271322767e-07,
      "loss": 2.3658,
      "step": 142290
    },
    {
      "epoch": 2.9273835820396577,
      "grad_norm": 0.4758731722831726,
      "learning_rate": 3.073307387630564e-07,
      "loss": 2.3334,
      "step": 142300
    },
    {
      "epoch": 2.9275893015565253,
      "grad_norm": 0.4581719636917114,
      "learning_rate": 3.0559333210901096e-07,
      "loss": 2.3405,
      "step": 142310
    },
    {
      "epoch": 2.927795021073393,
      "grad_norm": 0.4721880555152893,
      "learning_rate": 3.038608428367673e-07,
      "loss": 2.4342,
      "step": 142320
    },
    {
      "epoch": 2.9280007405902606,
      "grad_norm": 0.46932828426361084,
      "learning_rate": 3.0213327103179034e-07,
      "loss": 2.3847,
      "step": 142330
    },
    {
      "epoch": 2.9282064601071287,
      "grad_norm": 0.4729105234146118,
      "learning_rate": 3.004106167792897e-07,
      "loss": 2.3918,
      "step": 142340
    },
    {
      "epoch": 2.9284121796239964,
      "grad_norm": 0.4394031763076782,
      "learning_rate": 2.986928801642419e-07,
      "loss": 2.3636,
      "step": 142350
    },
    {
      "epoch": 2.928617899140864,
      "grad_norm": 0.412779837846756,
      "learning_rate": 2.969800612713569e-07,
      "loss": 2.3685,
      "step": 142360
    },
    {
      "epoch": 2.9288236186577317,
      "grad_norm": 0.4625925123691559,
      "learning_rate": 2.952721601851338e-07,
      "loss": 2.353,
      "step": 142370
    },
    {
      "epoch": 2.9290293381745993,
      "grad_norm": 0.43548330664634705,
      "learning_rate": 2.9356917698981637e-07,
      "loss": 2.3651,
      "step": 142380
    },
    {
      "epoch": 2.929235057691467,
      "grad_norm": 0.47934386134147644,
      "learning_rate": 2.9187111176939283e-07,
      "loss": 2.3362,
      "step": 142390
    },
    {
      "epoch": 2.9294407772083346,
      "grad_norm": 0.44471099972724915,
      "learning_rate": 2.901779646076186e-07,
      "loss": 2.3733,
      "step": 142400
    },
    {
      "epoch": 2.9296464967252023,
      "grad_norm": 0.4356106221675873,
      "learning_rate": 2.884897355880156e-07,
      "loss": 2.3941,
      "step": 142410
    },
    {
      "epoch": 2.92985221624207,
      "grad_norm": 0.49626919627189636,
      "learning_rate": 2.8680642479385067e-07,
      "loss": 2.3556,
      "step": 142420
    },
    {
      "epoch": 2.930057935758938,
      "grad_norm": 0.43172740936279297,
      "learning_rate": 2.8512803230815733e-07,
      "loss": 2.3509,
      "step": 142430
    },
    {
      "epoch": 2.9302636552758057,
      "grad_norm": 0.4742237329483032,
      "learning_rate": 2.834545582137138e-07,
      "loss": 2.3969,
      "step": 142440
    },
    {
      "epoch": 2.9304693747926733,
      "grad_norm": 0.5296658277511597,
      "learning_rate": 2.817860025930652e-07,
      "loss": 2.3916,
      "step": 142450
    },
    {
      "epoch": 2.930675094309541,
      "grad_norm": 0.4565454125404358,
      "learning_rate": 2.8012236552851235e-07,
      "loss": 2.3483,
      "step": 142460
    },
    {
      "epoch": 2.9308808138264086,
      "grad_norm": 0.44264692068099976,
      "learning_rate": 2.784636471021229e-07,
      "loss": 2.3859,
      "step": 142470
    },
    {
      "epoch": 2.9310865333432763,
      "grad_norm": 0.4634633958339691,
      "learning_rate": 2.7680984739569814e-07,
      "loss": 2.379,
      "step": 142480
    },
    {
      "epoch": 2.9312922528601444,
      "grad_norm": 0.4510975480079651,
      "learning_rate": 2.7516096649081726e-07,
      "loss": 2.4021,
      "step": 142490
    },
    {
      "epoch": 2.931497972377012,
      "grad_norm": 0.5032587647438049,
      "learning_rate": 2.735170044688151e-07,
      "loss": 2.338,
      "step": 142500
    },
    {
      "epoch": 2.9317036918938797,
      "grad_norm": 0.44618988037109375,
      "learning_rate": 2.7187796141077137e-07,
      "loss": 2.3471,
      "step": 142510
    },
    {
      "epoch": 2.9319094114107473,
      "grad_norm": 0.45135143399238586,
      "learning_rate": 2.7024383739753243e-07,
      "loss": 2.3479,
      "step": 142520
    },
    {
      "epoch": 2.932115130927615,
      "grad_norm": 0.45972737669944763,
      "learning_rate": 2.6861463250971165e-07,
      "loss": 2.4025,
      "step": 142530
    },
    {
      "epoch": 2.9323208504444827,
      "grad_norm": 0.4610637426376343,
      "learning_rate": 2.6699034682765577e-07,
      "loss": 2.3967,
      "step": 142540
    },
    {
      "epoch": 2.9325265699613503,
      "grad_norm": 0.4665975868701935,
      "learning_rate": 2.653709804314786e-07,
      "loss": 2.3533,
      "step": 142550
    },
    {
      "epoch": 2.932732289478218,
      "grad_norm": 0.48125818371772766,
      "learning_rate": 2.6375653340107163e-07,
      "loss": 2.3705,
      "step": 142560
    },
    {
      "epoch": 2.9329380089950856,
      "grad_norm": 0.47044503688812256,
      "learning_rate": 2.621470058160602e-07,
      "loss": 2.3206,
      "step": 142570
    },
    {
      "epoch": 2.9331437285119537,
      "grad_norm": 0.46551334857940674,
      "learning_rate": 2.605423977558252e-07,
      "loss": 2.3771,
      "step": 142580
    },
    {
      "epoch": 2.9333494480288214,
      "grad_norm": 0.47196754813194275,
      "learning_rate": 2.589427092995145e-07,
      "loss": 2.3443,
      "step": 142590
    },
    {
      "epoch": 2.933555167545689,
      "grad_norm": 0.46335193514823914,
      "learning_rate": 2.573479405260426e-07,
      "loss": 2.3695,
      "step": 142600
    },
    {
      "epoch": 2.9337608870625567,
      "grad_norm": 0.4756091833114624,
      "learning_rate": 2.5575809151405786e-07,
      "loss": 2.3476,
      "step": 142610
    },
    {
      "epoch": 2.9339666065794243,
      "grad_norm": 0.5272289514541626,
      "learning_rate": 2.541731623419974e-07,
      "loss": 2.4135,
      "step": 142620
    },
    {
      "epoch": 2.9341723260962924,
      "grad_norm": 0.42159080505371094,
      "learning_rate": 2.52593153088021e-07,
      "loss": 2.41,
      "step": 142630
    },
    {
      "epoch": 2.93437804561316,
      "grad_norm": 0.46591684222221375,
      "learning_rate": 2.510180638300552e-07,
      "loss": 2.3724,
      "step": 142640
    },
    {
      "epoch": 2.9345837651300277,
      "grad_norm": 0.4894174039363861,
      "learning_rate": 2.4944789464581565e-07,
      "loss": 2.4049,
      "step": 142650
    },
    {
      "epoch": 2.9347894846468954,
      "grad_norm": 0.4786854386329651,
      "learning_rate": 2.478826456127292e-07,
      "loss": 2.3714,
      "step": 142660
    },
    {
      "epoch": 2.934995204163763,
      "grad_norm": 0.41323980689048767,
      "learning_rate": 2.46322316808012e-07,
      "loss": 2.333,
      "step": 142670
    },
    {
      "epoch": 2.9352009236806307,
      "grad_norm": 0.5153698325157166,
      "learning_rate": 2.447669083086135e-07,
      "loss": 2.3691,
      "step": 142680
    },
    {
      "epoch": 2.9354066431974983,
      "grad_norm": 0.4649405777454376,
      "learning_rate": 2.4321642019127234e-07,
      "loss": 2.3735,
      "step": 142690
    },
    {
      "epoch": 2.935612362714366,
      "grad_norm": 0.44665810465812683,
      "learning_rate": 2.416708525324496e-07,
      "loss": 2.37,
      "step": 142700
    },
    {
      "epoch": 2.9358180822312336,
      "grad_norm": 0.42639392614364624,
      "learning_rate": 2.4013020540839537e-07,
      "loss": 2.3628,
      "step": 142710
    },
    {
      "epoch": 2.9360238017481017,
      "grad_norm": 0.46040335297584534,
      "learning_rate": 2.3859447889508223e-07,
      "loss": 2.3649,
      "step": 142720
    },
    {
      "epoch": 2.9362295212649694,
      "grad_norm": 0.49173736572265625,
      "learning_rate": 2.3706367306827182e-07,
      "loss": 2.4213,
      "step": 142730
    },
    {
      "epoch": 2.936435240781837,
      "grad_norm": 0.4571290910243988,
      "learning_rate": 2.3553778800347037e-07,
      "loss": 2.3913,
      "step": 142740
    },
    {
      "epoch": 2.9366409602987047,
      "grad_norm": 0.47347545623779297,
      "learning_rate": 2.340168237759288e-07,
      "loss": 2.3073,
      "step": 142750
    },
    {
      "epoch": 2.9368466798155723,
      "grad_norm": 0.46929502487182617,
      "learning_rate": 2.3250078046069822e-07,
      "loss": 2.4249,
      "step": 142760
    },
    {
      "epoch": 2.9370523993324404,
      "grad_norm": 0.48518067598342896,
      "learning_rate": 2.3098965813251882e-07,
      "loss": 2.3791,
      "step": 142770
    },
    {
      "epoch": 2.937258118849308,
      "grad_norm": 0.48362889885902405,
      "learning_rate": 2.2948345686595318e-07,
      "loss": 2.378,
      "step": 142780
    },
    {
      "epoch": 2.9374638383661758,
      "grad_norm": 0.4573483467102051,
      "learning_rate": 2.2798217673527522e-07,
      "loss": 2.4075,
      "step": 142790
    },
    {
      "epoch": 2.9376695578830434,
      "grad_norm": 0.4694688618183136,
      "learning_rate": 2.2648581781454792e-07,
      "loss": 2.3436,
      "step": 142800
    },
    {
      "epoch": 2.937875277399911,
      "grad_norm": 0.45527294278144836,
      "learning_rate": 2.249943801775789e-07,
      "loss": 2.3397,
      "step": 142810
    },
    {
      "epoch": 2.9380809969167787,
      "grad_norm": 0.47078508138656616,
      "learning_rate": 2.2350786389792043e-07,
      "loss": 2.4082,
      "step": 142820
    },
    {
      "epoch": 2.9382867164336464,
      "grad_norm": 0.5050182938575745,
      "learning_rate": 2.220262690489028e-07,
      "loss": 2.3403,
      "step": 142830
    },
    {
      "epoch": 2.938492435950514,
      "grad_norm": 0.4862819314002991,
      "learning_rate": 2.2054959570361189e-07,
      "loss": 2.3787,
      "step": 142840
    },
    {
      "epoch": 2.9386981554673817,
      "grad_norm": 0.4645703136920929,
      "learning_rate": 2.190778439348784e-07,
      "loss": 2.3298,
      "step": 142850
    },
    {
      "epoch": 2.9389038749842498,
      "grad_norm": 0.44079914689064026,
      "learning_rate": 2.176110138152887e-07,
      "loss": 2.3628,
      "step": 142860
    },
    {
      "epoch": 2.9391095945011174,
      "grad_norm": 0.46505501866340637,
      "learning_rate": 2.1614910541719602e-07,
      "loss": 2.3592,
      "step": 142870
    },
    {
      "epoch": 2.939315314017985,
      "grad_norm": 0.442311555147171,
      "learning_rate": 2.1469211881272044e-07,
      "loss": 2.3639,
      "step": 142880
    },
    {
      "epoch": 2.9395210335348527,
      "grad_norm": 0.5497655272483826,
      "learning_rate": 2.1324005407371562e-07,
      "loss": 2.329,
      "step": 142890
    },
    {
      "epoch": 2.9397267530517204,
      "grad_norm": 0.49669212102890015,
      "learning_rate": 2.11792911271802e-07,
      "loss": 2.3837,
      "step": 142900
    },
    {
      "epoch": 2.939932472568588,
      "grad_norm": 0.41494351625442505,
      "learning_rate": 2.1035069047836698e-07,
      "loss": 2.3096,
      "step": 142910
    },
    {
      "epoch": 2.940138192085456,
      "grad_norm": 0.47495537996292114,
      "learning_rate": 2.0891339176453138e-07,
      "loss": 2.3945,
      "step": 142920
    },
    {
      "epoch": 2.940343911602324,
      "grad_norm": 0.4469020366668701,
      "learning_rate": 2.0748101520121633e-07,
      "loss": 2.3657,
      "step": 142930
    },
    {
      "epoch": 2.9405496311191914,
      "grad_norm": 0.43053367733955383,
      "learning_rate": 2.0605356085905414e-07,
      "loss": 2.3957,
      "step": 142940
    },
    {
      "epoch": 2.940755350636059,
      "grad_norm": 0.44569486379623413,
      "learning_rate": 2.046310288084552e-07,
      "loss": 2.3448,
      "step": 142950
    },
    {
      "epoch": 2.9409610701529267,
      "grad_norm": 0.5045263171195984,
      "learning_rate": 2.0321341911959668e-07,
      "loss": 2.3703,
      "step": 142960
    },
    {
      "epoch": 2.9411667896697944,
      "grad_norm": 0.44839924573898315,
      "learning_rate": 2.0180073186238934e-07,
      "loss": 2.3967,
      "step": 142970
    },
    {
      "epoch": 2.941372509186662,
      "grad_norm": 0.48224127292633057,
      "learning_rate": 2.003929671065108e-07,
      "loss": 2.3815,
      "step": 142980
    },
    {
      "epoch": 2.9415782287035297,
      "grad_norm": 0.4289180636405945,
      "learning_rate": 1.9899012492141656e-07,
      "loss": 2.3108,
      "step": 142990
    },
    {
      "epoch": 2.9417839482203973,
      "grad_norm": 0.5039428472518921,
      "learning_rate": 1.9759220537627354e-07,
      "loss": 2.3277,
      "step": 143000
    },
    {
      "epoch": 2.9419896677372654,
      "grad_norm": 0.45822975039482117,
      "learning_rate": 1.961992085400599e-07,
      "loss": 2.357,
      "step": 143010
    },
    {
      "epoch": 2.942195387254133,
      "grad_norm": 0.48426374793052673,
      "learning_rate": 1.9481113448147626e-07,
      "loss": 2.3646,
      "step": 143020
    },
    {
      "epoch": 2.9424011067710008,
      "grad_norm": 0.4450277090072632,
      "learning_rate": 1.9342798326897893e-07,
      "loss": 2.3875,
      "step": 143030
    },
    {
      "epoch": 2.9426068262878684,
      "grad_norm": 0.45185577869415283,
      "learning_rate": 1.9204975497080223e-07,
      "loss": 2.3055,
      "step": 143040
    },
    {
      "epoch": 2.942812545804736,
      "grad_norm": 0.45410770177841187,
      "learning_rate": 1.9067644965492515e-07,
      "loss": 2.3597,
      "step": 143050
    },
    {
      "epoch": 2.943018265321604,
      "grad_norm": 0.47156766057014465,
      "learning_rate": 1.8930806738907127e-07,
      "loss": 2.355,
      "step": 143060
    },
    {
      "epoch": 2.943223984838472,
      "grad_norm": 0.4410509765148163,
      "learning_rate": 1.8794460824075322e-07,
      "loss": 2.3897,
      "step": 143070
    },
    {
      "epoch": 2.9434297043553395,
      "grad_norm": 0.49854665994644165,
      "learning_rate": 1.8658607227722835e-07,
      "loss": 2.305,
      "step": 143080
    },
    {
      "epoch": 2.943635423872207,
      "grad_norm": 0.42847391963005066,
      "learning_rate": 1.8523245956547641e-07,
      "loss": 2.4032,
      "step": 143090
    },
    {
      "epoch": 2.9438411433890748,
      "grad_norm": 0.49302923679351807,
      "learning_rate": 1.8388377017229952e-07,
      "loss": 2.3218,
      "step": 143100
    },
    {
      "epoch": 2.9440468629059424,
      "grad_norm": 0.4663166105747223,
      "learning_rate": 1.8254000416420002e-07,
      "loss": 2.3752,
      "step": 143110
    },
    {
      "epoch": 2.94425258242281,
      "grad_norm": 0.4634183347225189,
      "learning_rate": 1.8120116160744715e-07,
      "loss": 2.4229,
      "step": 143120
    },
    {
      "epoch": 2.9444583019396777,
      "grad_norm": 0.5486686825752258,
      "learning_rate": 1.7986724256811027e-07,
      "loss": 2.3373,
      "step": 143130
    },
    {
      "epoch": 2.9446640214565454,
      "grad_norm": 0.5096981525421143,
      "learning_rate": 1.78538247111959e-07,
      "loss": 2.3562,
      "step": 143140
    },
    {
      "epoch": 2.9448697409734135,
      "grad_norm": 0.46839842200279236,
      "learning_rate": 1.7721417530456307e-07,
      "loss": 2.3957,
      "step": 143150
    },
    {
      "epoch": 2.945075460490281,
      "grad_norm": 0.48707035183906555,
      "learning_rate": 1.7589502721121475e-07,
      "loss": 2.3861,
      "step": 143160
    },
    {
      "epoch": 2.945281180007149,
      "grad_norm": 0.50172358751297,
      "learning_rate": 1.745808028969953e-07,
      "loss": 2.3302,
      "step": 143170
    },
    {
      "epoch": 2.9454868995240164,
      "grad_norm": 0.4729141294956207,
      "learning_rate": 1.7327150242670842e-07,
      "loss": 2.4053,
      "step": 143180
    },
    {
      "epoch": 2.945692619040884,
      "grad_norm": 0.5135312676429749,
      "learning_rate": 1.71967125864958e-07,
      "loss": 2.2944,
      "step": 143190
    },
    {
      "epoch": 2.9458983385577517,
      "grad_norm": 0.45690643787384033,
      "learning_rate": 1.7066767327607036e-07,
      "loss": 2.3676,
      "step": 143200
    },
    {
      "epoch": 2.94610405807462,
      "grad_norm": 0.4908083975315094,
      "learning_rate": 1.6937314472414978e-07,
      "loss": 2.3582,
      "step": 143210
    },
    {
      "epoch": 2.9463097775914875,
      "grad_norm": 0.4443076252937317,
      "learning_rate": 1.6808354027303407e-07,
      "loss": 2.4136,
      "step": 143220
    },
    {
      "epoch": 2.946515497108355,
      "grad_norm": 0.4996422529220581,
      "learning_rate": 1.6679885998632794e-07,
      "loss": 2.3821,
      "step": 143230
    },
    {
      "epoch": 2.946721216625223,
      "grad_norm": 0.474168598651886,
      "learning_rate": 1.655191039274251e-07,
      "loss": 2.348,
      "step": 143240
    },
    {
      "epoch": 2.9469269361420904,
      "grad_norm": 0.4289022982120514,
      "learning_rate": 1.6424427215941952e-07,
      "loss": 2.3902,
      "step": 143250
    },
    {
      "epoch": 2.947132655658958,
      "grad_norm": 0.42885202169418335,
      "learning_rate": 1.6297436474520533e-07,
      "loss": 2.4084,
      "step": 143260
    },
    {
      "epoch": 2.9473383751758258,
      "grad_norm": 0.46750229597091675,
      "learning_rate": 1.6170938174743245e-07,
      "loss": 2.3638,
      "step": 143270
    },
    {
      "epoch": 2.9475440946926934,
      "grad_norm": 0.4578826427459717,
      "learning_rate": 1.6044932322847318e-07,
      "loss": 2.3934,
      "step": 143280
    },
    {
      "epoch": 2.947749814209561,
      "grad_norm": 0.45062029361724854,
      "learning_rate": 1.591941892504889e-07,
      "loss": 2.3549,
      "step": 143290
    },
    {
      "epoch": 2.947955533726429,
      "grad_norm": 0.44390323758125305,
      "learning_rate": 1.5794397987538569e-07,
      "loss": 2.4042,
      "step": 143300
    },
    {
      "epoch": 2.948161253243297,
      "grad_norm": 0.4698913097381592,
      "learning_rate": 1.5669869516483637e-07,
      "loss": 2.3342,
      "step": 143310
    },
    {
      "epoch": 2.9483669727601645,
      "grad_norm": 0.5222078561782837,
      "learning_rate": 1.5545833518024743e-07,
      "loss": 2.4187,
      "step": 143320
    },
    {
      "epoch": 2.948572692277032,
      "grad_norm": 0.4713209569454193,
      "learning_rate": 1.5422289998282545e-07,
      "loss": 2.3584,
      "step": 143330
    },
    {
      "epoch": 2.9487784117938998,
      "grad_norm": 0.4696792960166931,
      "learning_rate": 1.5299238963348838e-07,
      "loss": 2.3219,
      "step": 143340
    },
    {
      "epoch": 2.948984131310768,
      "grad_norm": 0.47988682985305786,
      "learning_rate": 1.517668041929432e-07,
      "loss": 2.3721,
      "step": 143350
    },
    {
      "epoch": 2.9491898508276355,
      "grad_norm": 0.48600471019744873,
      "learning_rate": 1.5054614372161934e-07,
      "loss": 2.3287,
      "step": 143360
    },
    {
      "epoch": 2.949395570344503,
      "grad_norm": 0.4268166124820709,
      "learning_rate": 1.4933040827975754e-07,
      "loss": 2.3599,
      "step": 143370
    },
    {
      "epoch": 2.949601289861371,
      "grad_norm": 0.47655653953552246,
      "learning_rate": 1.4811959792728757e-07,
      "loss": 2.3286,
      "step": 143380
    },
    {
      "epoch": 2.9498070093782385,
      "grad_norm": 0.5474603772163391,
      "learning_rate": 1.469137127239617e-07,
      "loss": 2.3506,
      "step": 143390
    },
    {
      "epoch": 2.950012728895106,
      "grad_norm": 0.4276818335056305,
      "learning_rate": 1.4571275272924346e-07,
      "loss": 2.3137,
      "step": 143400
    },
    {
      "epoch": 2.950218448411974,
      "grad_norm": 0.44897693395614624,
      "learning_rate": 1.4451671800237432e-07,
      "loss": 2.3705,
      "step": 143410
    },
    {
      "epoch": 2.9504241679288414,
      "grad_norm": 0.49039414525032043,
      "learning_rate": 1.4332560860236267e-07,
      "loss": 2.3309,
      "step": 143420
    },
    {
      "epoch": 2.950629887445709,
      "grad_norm": 0.47472628951072693,
      "learning_rate": 1.4213942458792817e-07,
      "loss": 2.4529,
      "step": 143430
    },
    {
      "epoch": 2.950835606962577,
      "grad_norm": 0.4590994417667389,
      "learning_rate": 1.409581660176018e-07,
      "loss": 2.3704,
      "step": 143440
    },
    {
      "epoch": 2.951041326479445,
      "grad_norm": 0.4537927806377411,
      "learning_rate": 1.3978183294963699e-07,
      "loss": 2.3376,
      "step": 143450
    },
    {
      "epoch": 2.9512470459963125,
      "grad_norm": 0.5048815011978149,
      "learning_rate": 1.3861042544206504e-07,
      "loss": 2.3482,
      "step": 143460
    },
    {
      "epoch": 2.95145276551318,
      "grad_norm": 0.49760836362838745,
      "learning_rate": 1.37443943552662e-07,
      "loss": 2.3481,
      "step": 143470
    },
    {
      "epoch": 2.951658485030048,
      "grad_norm": 0.4651324450969696,
      "learning_rate": 1.362823873389707e-07,
      "loss": 2.3644,
      "step": 143480
    },
    {
      "epoch": 2.951864204546916,
      "grad_norm": 0.45291438698768616,
      "learning_rate": 1.3512575685826757e-07,
      "loss": 2.3203,
      "step": 143490
    },
    {
      "epoch": 2.9520699240637835,
      "grad_norm": 0.45978063344955444,
      "learning_rate": 1.3397405216761804e-07,
      "loss": 2.3222,
      "step": 143500
    },
    {
      "epoch": 2.952275643580651,
      "grad_norm": 0.4522794485092163,
      "learning_rate": 1.3282727332382116e-07,
      "loss": 2.3468,
      "step": 143510
    },
    {
      "epoch": 2.952481363097519,
      "grad_norm": 0.4500011205673218,
      "learning_rate": 1.3168542038344278e-07,
      "loss": 2.4001,
      "step": 143520
    },
    {
      "epoch": 2.9526870826143865,
      "grad_norm": 0.5056714415550232,
      "learning_rate": 1.305484934028045e-07,
      "loss": 2.3656,
      "step": 143530
    },
    {
      "epoch": 2.952892802131254,
      "grad_norm": 0.4462154805660248,
      "learning_rate": 1.2941649243799482e-07,
      "loss": 2.3411,
      "step": 143540
    },
    {
      "epoch": 2.953098521648122,
      "grad_norm": 0.455870121717453,
      "learning_rate": 1.282894175448357e-07,
      "loss": 2.373,
      "step": 143550
    },
    {
      "epoch": 2.9533042411649895,
      "grad_norm": 0.4731610417366028,
      "learning_rate": 1.2716726877891605e-07,
      "loss": 2.3658,
      "step": 143560
    },
    {
      "epoch": 2.953509960681857,
      "grad_norm": 0.5101927518844604,
      "learning_rate": 1.2605004619560267e-07,
      "loss": 2.3981,
      "step": 143570
    },
    {
      "epoch": 2.953715680198725,
      "grad_norm": 0.4827072322368622,
      "learning_rate": 1.2493774984998485e-07,
      "loss": 2.3771,
      "step": 143580
    },
    {
      "epoch": 2.953921399715593,
      "grad_norm": 0.4865795075893402,
      "learning_rate": 1.238303797969409e-07,
      "loss": 2.3708,
      "step": 143590
    },
    {
      "epoch": 2.9541271192324605,
      "grad_norm": 0.49068403244018555,
      "learning_rate": 1.227279360910827e-07,
      "loss": 2.3384,
      "step": 143600
    },
    {
      "epoch": 2.954332838749328,
      "grad_norm": 0.4783215820789337,
      "learning_rate": 1.2163041878678894e-07,
      "loss": 2.3501,
      "step": 143610
    },
    {
      "epoch": 2.954538558266196,
      "grad_norm": 0.48330366611480713,
      "learning_rate": 1.2053782793819413e-07,
      "loss": 2.3849,
      "step": 143620
    },
    {
      "epoch": 2.9547442777830635,
      "grad_norm": 0.48466455936431885,
      "learning_rate": 1.194501635991996e-07,
      "loss": 2.3735,
      "step": 143630
    },
    {
      "epoch": 2.9549499972999316,
      "grad_norm": 0.4528336226940155,
      "learning_rate": 1.1836742582342907e-07,
      "loss": 2.3816,
      "step": 143640
    },
    {
      "epoch": 2.9551557168167992,
      "grad_norm": 0.4707290530204773,
      "learning_rate": 1.1728961466430654e-07,
      "loss": 2.3744,
      "step": 143650
    },
    {
      "epoch": 2.955361436333667,
      "grad_norm": 0.4726145565509796,
      "learning_rate": 1.1621673017500057e-07,
      "loss": 2.3109,
      "step": 143660
    },
    {
      "epoch": 2.9555671558505345,
      "grad_norm": 0.4937063753604889,
      "learning_rate": 1.1514877240842436e-07,
      "loss": 2.3802,
      "step": 143670
    },
    {
      "epoch": 2.955772875367402,
      "grad_norm": 0.46506834030151367,
      "learning_rate": 1.1408574141723582e-07,
      "loss": 2.3488,
      "step": 143680
    },
    {
      "epoch": 2.95597859488427,
      "grad_norm": 0.4415646195411682,
      "learning_rate": 1.1302763725390409e-07,
      "loss": 2.3288,
      "step": 143690
    },
    {
      "epoch": 2.9561843144011375,
      "grad_norm": 0.4681372344493866,
      "learning_rate": 1.1197445997058742e-07,
      "loss": 2.3606,
      "step": 143700
    },
    {
      "epoch": 2.956390033918005,
      "grad_norm": 0.42853012681007385,
      "learning_rate": 1.1092620961925537e-07,
      "loss": 2.3601,
      "step": 143710
    },
    {
      "epoch": 2.956595753434873,
      "grad_norm": 0.4944375157356262,
      "learning_rate": 1.0988288625158882e-07,
      "loss": 2.3822,
      "step": 143720
    },
    {
      "epoch": 2.956801472951741,
      "grad_norm": 0.4649340510368347,
      "learning_rate": 1.0884448991906882e-07,
      "loss": 2.3051,
      "step": 143730
    },
    {
      "epoch": 2.9570071924686085,
      "grad_norm": 0.4824336767196655,
      "learning_rate": 1.0781102067292104e-07,
      "loss": 2.3681,
      "step": 143740
    },
    {
      "epoch": 2.957212911985476,
      "grad_norm": 0.4638754427433014,
      "learning_rate": 1.0678247856409362e-07,
      "loss": 2.3849,
      "step": 143750
    },
    {
      "epoch": 2.957418631502344,
      "grad_norm": 0.4528196454048157,
      "learning_rate": 1.0575886364333487e-07,
      "loss": 2.4256,
      "step": 143760
    },
    {
      "epoch": 2.9576243510192115,
      "grad_norm": 0.4842585623264313,
      "learning_rate": 1.0474017596114882e-07,
      "loss": 2.3487,
      "step": 143770
    },
    {
      "epoch": 2.9578300705360796,
      "grad_norm": 0.4214416742324829,
      "learning_rate": 1.0372641556775087e-07,
      "loss": 2.417,
      "step": 143780
    },
    {
      "epoch": 2.9580357900529473,
      "grad_norm": 0.49573859572410583,
      "learning_rate": 1.0271758251316766e-07,
      "loss": 2.3129,
      "step": 143790
    },
    {
      "epoch": 2.958241509569815,
      "grad_norm": 0.47693291306495667,
      "learning_rate": 1.0171367684714827e-07,
      "loss": 2.3225,
      "step": 143800
    },
    {
      "epoch": 2.9584472290866826,
      "grad_norm": 0.49105826020240784,
      "learning_rate": 1.0071469861920868e-07,
      "loss": 2.3394,
      "step": 143810
    },
    {
      "epoch": 2.95865294860355,
      "grad_norm": 0.42694881558418274,
      "learning_rate": 9.972064787863167e-08,
      "loss": 2.3427,
      "step": 143820
    },
    {
      "epoch": 2.958858668120418,
      "grad_norm": 0.46167463064193726,
      "learning_rate": 9.873152467443358e-08,
      "loss": 2.3391,
      "step": 143830
    },
    {
      "epoch": 2.9590643876372855,
      "grad_norm": 0.46895289421081543,
      "learning_rate": 9.774732905543093e-08,
      "loss": 2.4298,
      "step": 143840
    },
    {
      "epoch": 2.959270107154153,
      "grad_norm": 0.4444231688976288,
      "learning_rate": 9.676806107014048e-08,
      "loss": 2.3614,
      "step": 143850
    },
    {
      "epoch": 2.959475826671021,
      "grad_norm": 0.4975166618824005,
      "learning_rate": 9.579372076686798e-08,
      "loss": 2.3645,
      "step": 143860
    },
    {
      "epoch": 2.959681546187889,
      "grad_norm": 0.4662024974822998,
      "learning_rate": 9.482430819368615e-08,
      "loss": 2.3424,
      "step": 143870
    },
    {
      "epoch": 2.9598872657047566,
      "grad_norm": 0.49387380480766296,
      "learning_rate": 9.385982339840115e-08,
      "loss": 2.3284,
      "step": 143880
    },
    {
      "epoch": 2.9600929852216242,
      "grad_norm": 0.4559798538684845,
      "learning_rate": 9.290026642859717e-08,
      "loss": 2.3393,
      "step": 143890
    },
    {
      "epoch": 2.960298704738492,
      "grad_norm": 0.5012820363044739,
      "learning_rate": 9.194563733158079e-08,
      "loss": 2.3645,
      "step": 143900
    },
    {
      "epoch": 2.9605044242553595,
      "grad_norm": 0.4545367956161499,
      "learning_rate": 9.099593615445878e-08,
      "loss": 2.3647,
      "step": 143910
    },
    {
      "epoch": 2.960710143772227,
      "grad_norm": 0.44919025897979736,
      "learning_rate": 9.005116294407146e-08,
      "loss": 2.3918,
      "step": 143920
    },
    {
      "epoch": 2.9609158632890953,
      "grad_norm": 0.4912453889846802,
      "learning_rate": 8.911131774701487e-08,
      "loss": 2.3862,
      "step": 143930
    },
    {
      "epoch": 2.961121582805963,
      "grad_norm": 0.46260514855384827,
      "learning_rate": 8.817640060964084e-08,
      "loss": 2.3313,
      "step": 143940
    },
    {
      "epoch": 2.9613273023228306,
      "grad_norm": 0.46130335330963135,
      "learning_rate": 8.724641157807911e-08,
      "loss": 2.3804,
      "step": 143950
    },
    {
      "epoch": 2.9615330218396982,
      "grad_norm": 0.44974300265312195,
      "learning_rate": 8.632135069818192e-08,
      "loss": 2.3503,
      "step": 143960
    },
    {
      "epoch": 2.961738741356566,
      "grad_norm": 0.4632992148399353,
      "learning_rate": 8.540121801560164e-08,
      "loss": 2.3811,
      "step": 143970
    },
    {
      "epoch": 2.9619444608734335,
      "grad_norm": 0.4705449640750885,
      "learning_rate": 8.448601357570196e-08,
      "loss": 2.3349,
      "step": 143980
    },
    {
      "epoch": 2.962150180390301,
      "grad_norm": 0.4442708194255829,
      "learning_rate": 8.357573742363567e-08,
      "loss": 2.382,
      "step": 143990
    },
    {
      "epoch": 2.962355899907169,
      "grad_norm": 0.48020073771476746,
      "learning_rate": 8.267038960430018e-08,
      "loss": 2.3774,
      "step": 144000
    },
    {
      "epoch": 2.9625616194240365,
      "grad_norm": 0.4199093282222748,
      "learning_rate": 8.176997016234867e-08,
      "loss": 2.4118,
      "step": 144010
    },
    {
      "epoch": 2.9627673389409046,
      "grad_norm": 0.4463285803794861,
      "learning_rate": 8.087447914220114e-08,
      "loss": 2.3704,
      "step": 144020
    },
    {
      "epoch": 2.9629730584577723,
      "grad_norm": 0.46550455689430237,
      "learning_rate": 7.998391658801118e-08,
      "loss": 2.3574,
      "step": 144030
    },
    {
      "epoch": 2.96317877797464,
      "grad_norm": 0.44310474395751953,
      "learning_rate": 7.909828254372142e-08,
      "loss": 2.3294,
      "step": 144040
    },
    {
      "epoch": 2.9633844974915076,
      "grad_norm": 0.4699956774711609,
      "learning_rate": 7.821757705300803e-08,
      "loss": 2.3899,
      "step": 144050
    },
    {
      "epoch": 2.963590217008375,
      "grad_norm": 0.44391483068466187,
      "learning_rate": 7.734180015931403e-08,
      "loss": 2.3853,
      "step": 144060
    },
    {
      "epoch": 2.9637959365252433,
      "grad_norm": 0.46617233753204346,
      "learning_rate": 7.64709519058382e-08,
      "loss": 2.3948,
      "step": 144070
    },
    {
      "epoch": 2.964001656042111,
      "grad_norm": 0.5027040243148804,
      "learning_rate": 7.560503233552396e-08,
      "loss": 2.4201,
      "step": 144080
    },
    {
      "epoch": 2.9642073755589786,
      "grad_norm": 0.48207414150238037,
      "learning_rate": 7.474404149110381e-08,
      "loss": 2.4302,
      "step": 144090
    },
    {
      "epoch": 2.9644130950758463,
      "grad_norm": 0.46136340498924255,
      "learning_rate": 7.388797941502156e-08,
      "loss": 2.4271,
      "step": 144100
    },
    {
      "epoch": 2.964618814592714,
      "grad_norm": 0.4716230630874634,
      "learning_rate": 7.303684614952123e-08,
      "loss": 2.3313,
      "step": 144110
    },
    {
      "epoch": 2.9648245341095816,
      "grad_norm": 0.4793241322040558,
      "learning_rate": 7.219064173656919e-08,
      "loss": 2.3559,
      "step": 144120
    },
    {
      "epoch": 2.9650302536264492,
      "grad_norm": 0.4415951073169708,
      "learning_rate": 7.134936621792099e-08,
      "loss": 2.3386,
      "step": 144130
    },
    {
      "epoch": 2.965235973143317,
      "grad_norm": 0.4402081370353699,
      "learning_rate": 7.051301963505453e-08,
      "loss": 2.4113,
      "step": 144140
    },
    {
      "epoch": 2.9654416926601845,
      "grad_norm": 0.49992436170578003,
      "learning_rate": 6.96816020292368e-08,
      "loss": 2.3842,
      "step": 144150
    },
    {
      "epoch": 2.9656474121770526,
      "grad_norm": 0.44578465819358826,
      "learning_rate": 6.885511344146833e-08,
      "loss": 2.3567,
      "step": 144160
    },
    {
      "epoch": 2.9658531316939203,
      "grad_norm": 0.44437021017074585,
      "learning_rate": 6.803355391251654e-08,
      "loss": 2.3726,
      "step": 144170
    },
    {
      "epoch": 2.966058851210788,
      "grad_norm": 0.46682626008987427,
      "learning_rate": 6.721692348290453e-08,
      "loss": 2.4214,
      "step": 144180
    },
    {
      "epoch": 2.9662645707276556,
      "grad_norm": 0.4554235637187958,
      "learning_rate": 6.640522219291123e-08,
      "loss": 2.3365,
      "step": 144190
    },
    {
      "epoch": 2.9664702902445232,
      "grad_norm": 0.4654240012168884,
      "learning_rate": 6.559845008258236e-08,
      "loss": 2.3264,
      "step": 144200
    },
    {
      "epoch": 2.9666760097613913,
      "grad_norm": 0.5146041512489319,
      "learning_rate": 6.479660719169723e-08,
      "loss": 2.3412,
      "step": 144210
    },
    {
      "epoch": 2.966881729278259,
      "grad_norm": 0.44760873913764954,
      "learning_rate": 6.399969355981305e-08,
      "loss": 2.4043,
      "step": 144220
    },
    {
      "epoch": 2.9670874487951266,
      "grad_norm": 0.46866798400878906,
      "learning_rate": 6.320770922624287e-08,
      "loss": 2.3337,
      "step": 144230
    },
    {
      "epoch": 2.9672931683119943,
      "grad_norm": 0.49328258633613586,
      "learning_rate": 6.242065423005538e-08,
      "loss": 2.3924,
      "step": 144240
    },
    {
      "epoch": 2.967498887828862,
      "grad_norm": 0.47426480054855347,
      "learning_rate": 6.163852861005292e-08,
      "loss": 2.3911,
      "step": 144250
    },
    {
      "epoch": 2.9677046073457296,
      "grad_norm": 0.5001989006996155,
      "learning_rate": 6.086133240481572e-08,
      "loss": 2.3073,
      "step": 144260
    },
    {
      "epoch": 2.9679103268625973,
      "grad_norm": 0.49415260553359985,
      "learning_rate": 6.0089065652702e-08,
      "loss": 2.3294,
      "step": 144270
    },
    {
      "epoch": 2.968116046379465,
      "grad_norm": 0.4838047921657562,
      "learning_rate": 5.9321728391770194e-08,
      "loss": 2.3182,
      "step": 144280
    },
    {
      "epoch": 2.9683217658963326,
      "grad_norm": 0.546605110168457,
      "learning_rate": 5.8559320659901106e-08,
      "loss": 2.3339,
      "step": 144290
    },
    {
      "epoch": 2.9685274854132007,
      "grad_norm": 0.5043715834617615,
      "learning_rate": 5.78018424946758e-08,
      "loss": 2.41,
      "step": 144300
    },
    {
      "epoch": 2.9687332049300683,
      "grad_norm": 0.46739840507507324,
      "learning_rate": 5.704929393346436e-08,
      "loss": 2.3846,
      "step": 144310
    },
    {
      "epoch": 2.968938924446936,
      "grad_norm": 0.42158377170562744,
      "learning_rate": 5.630167501339267e-08,
      "loss": 2.3184,
      "step": 144320
    },
    {
      "epoch": 2.9691446439638036,
      "grad_norm": 0.4422881305217743,
      "learning_rate": 5.5558985771342334e-08,
      "loss": 2.3652,
      "step": 144330
    },
    {
      "epoch": 2.9693503634806713,
      "grad_norm": 0.4450667202472687,
      "learning_rate": 5.482122624391739e-08,
      "loss": 2.3794,
      "step": 144340
    },
    {
      "epoch": 2.969556082997539,
      "grad_norm": 0.44733479619026184,
      "learning_rate": 5.4088396467533164e-08,
      "loss": 2.4125,
      "step": 144350
    },
    {
      "epoch": 2.969761802514407,
      "grad_norm": 0.45868125557899475,
      "learning_rate": 5.3360496478327415e-08,
      "loss": 2.3951,
      "step": 144360
    },
    {
      "epoch": 2.9699675220312747,
      "grad_norm": 0.4655247926712036,
      "learning_rate": 5.2637526312204756e-08,
      "loss": 2.3837,
      "step": 144370
    },
    {
      "epoch": 2.9701732415481423,
      "grad_norm": 0.5072098970413208,
      "learning_rate": 5.1919486004825546e-08,
      "loss": 2.3924,
      "step": 144380
    },
    {
      "epoch": 2.97037896106501,
      "grad_norm": 0.43882128596305847,
      "learning_rate": 5.1206375591605906e-08,
      "loss": 2.405,
      "step": 144390
    },
    {
      "epoch": 2.9705846805818776,
      "grad_norm": 0.49949967861175537,
      "learning_rate": 5.0498195107717695e-08,
      "loss": 2.3442,
      "step": 144400
    },
    {
      "epoch": 2.9707904000987453,
      "grad_norm": 0.4716167747974396,
      "learning_rate": 4.979494458809963e-08,
      "loss": 2.356,
      "step": 144410
    },
    {
      "epoch": 2.970996119615613,
      "grad_norm": 0.49952781200408936,
      "learning_rate": 4.909662406743509e-08,
      "loss": 2.3651,
      "step": 144420
    },
    {
      "epoch": 2.9712018391324806,
      "grad_norm": 0.4492522180080414,
      "learning_rate": 4.840323358015209e-08,
      "loss": 2.3414,
      "step": 144430
    },
    {
      "epoch": 2.9714075586493482,
      "grad_norm": 0.4730432331562042,
      "learning_rate": 4.771477316047879e-08,
      "loss": 2.3363,
      "step": 144440
    },
    {
      "epoch": 2.9716132781662163,
      "grad_norm": 0.4544196128845215,
      "learning_rate": 4.703124284236582e-08,
      "loss": 2.3627,
      "step": 144450
    },
    {
      "epoch": 2.971818997683084,
      "grad_norm": 0.4578357934951782,
      "learning_rate": 4.635264265950845e-08,
      "loss": 2.3934,
      "step": 144460
    },
    {
      "epoch": 2.9720247171999516,
      "grad_norm": 0.4696299731731415,
      "learning_rate": 4.567897264540211e-08,
      "loss": 2.3962,
      "step": 144470
    },
    {
      "epoch": 2.9722304367168193,
      "grad_norm": 0.46320652961730957,
      "learning_rate": 4.5010232833264663e-08,
      "loss": 2.3934,
      "step": 144480
    },
    {
      "epoch": 2.972436156233687,
      "grad_norm": 0.4738888740539551,
      "learning_rate": 4.434642325609195e-08,
      "loss": 2.3518,
      "step": 144490
    },
    {
      "epoch": 2.972641875750555,
      "grad_norm": 0.43689194321632385,
      "learning_rate": 4.3687543946602236e-08,
      "loss": 2.3512,
      "step": 144500
    },
    {
      "epoch": 2.9728475952674227,
      "grad_norm": 0.47058728337287903,
      "learning_rate": 4.303359493732506e-08,
      "loss": 2.3427,
      "step": 144510
    },
    {
      "epoch": 2.9730533147842904,
      "grad_norm": 0.44766175746917725,
      "learning_rate": 4.2384576260490195e-08,
      "loss": 2.3566,
      "step": 144520
    },
    {
      "epoch": 2.973259034301158,
      "grad_norm": 0.4741303324699402,
      "learning_rate": 4.174048794811647e-08,
      "loss": 2.3243,
      "step": 144530
    },
    {
      "epoch": 2.9734647538180257,
      "grad_norm": 0.43031737208366394,
      "learning_rate": 4.1101330031989574e-08,
      "loss": 2.3276,
      "step": 144540
    },
    {
      "epoch": 2.9736704733348933,
      "grad_norm": 0.4352733790874481,
      "learning_rate": 4.046710254361763e-08,
      "loss": 2.322,
      "step": 144550
    },
    {
      "epoch": 2.973876192851761,
      "grad_norm": 0.4383809566497803,
      "learning_rate": 3.983780551428673e-08,
      "loss": 2.3764,
      "step": 144560
    },
    {
      "epoch": 2.9740819123686286,
      "grad_norm": 0.46124324202537537,
      "learning_rate": 3.9213438975038706e-08,
      "loss": 2.355,
      "step": 144570
    },
    {
      "epoch": 2.9742876318854963,
      "grad_norm": 0.4807758331298828,
      "learning_rate": 3.859400295667115e-08,
      "loss": 2.3433,
      "step": 144580
    },
    {
      "epoch": 2.9744933514023644,
      "grad_norm": 0.43334197998046875,
      "learning_rate": 3.797949748973739e-08,
      "loss": 2.3389,
      "step": 144590
    },
    {
      "epoch": 2.974699070919232,
      "grad_norm": 0.4563528001308441,
      "learning_rate": 3.736992260455763e-08,
      "loss": 2.3586,
      "step": 144600
    },
    {
      "epoch": 2.9749047904360997,
      "grad_norm": 0.453620582818985,
      "learning_rate": 3.676527833117449e-08,
      "loss": 2.3696,
      "step": 144610
    },
    {
      "epoch": 2.9751105099529673,
      "grad_norm": 0.4394499659538269,
      "learning_rate": 3.6165564699430774e-08,
      "loss": 2.3763,
      "step": 144620
    },
    {
      "epoch": 2.975316229469835,
      "grad_norm": 0.462820827960968,
      "learning_rate": 3.5570781738902827e-08,
      "loss": 2.3281,
      "step": 144630
    },
    {
      "epoch": 2.975521948986703,
      "grad_norm": 0.5588144659996033,
      "learning_rate": 3.498092947893383e-08,
      "loss": 2.4088,
      "step": 144640
    },
    {
      "epoch": 2.9757276685035707,
      "grad_norm": 0.45001092553138733,
      "learning_rate": 3.439600794860054e-08,
      "loss": 2.3738,
      "step": 144650
    },
    {
      "epoch": 2.9759333880204384,
      "grad_norm": 0.4714902937412262,
      "learning_rate": 3.381601717676875e-08,
      "loss": 2.3297,
      "step": 144660
    },
    {
      "epoch": 2.976139107537306,
      "grad_norm": 0.4345015287399292,
      "learning_rate": 3.3240957192048894e-08,
      "loss": 2.329,
      "step": 144670
    },
    {
      "epoch": 2.9763448270541737,
      "grad_norm": 0.468938946723938,
      "learning_rate": 3.267082802278498e-08,
      "loss": 2.3633,
      "step": 144680
    },
    {
      "epoch": 2.9765505465710413,
      "grad_norm": 0.48762598633766174,
      "learning_rate": 3.2105629697121166e-08,
      "loss": 2.3185,
      "step": 144690
    },
    {
      "epoch": 2.976756266087909,
      "grad_norm": 0.44432058930397034,
      "learning_rate": 3.154536224293514e-08,
      "loss": 2.3574,
      "step": 144700
    },
    {
      "epoch": 2.9769619856047767,
      "grad_norm": 0.5556313991546631,
      "learning_rate": 3.0990025687838154e-08,
      "loss": 2.4032,
      "step": 144710
    },
    {
      "epoch": 2.9771677051216443,
      "grad_norm": 0.48039308190345764,
      "learning_rate": 3.043962005925272e-08,
      "loss": 2.381,
      "step": 144720
    },
    {
      "epoch": 2.9773734246385124,
      "grad_norm": 0.47952598333358765,
      "learning_rate": 2.9894145384301574e-08,
      "loss": 2.3506,
      "step": 144730
    },
    {
      "epoch": 2.97757914415538,
      "grad_norm": 0.4252755343914032,
      "learning_rate": 2.9353601689896538e-08,
      "loss": 2.3769,
      "step": 144740
    },
    {
      "epoch": 2.9777848636722477,
      "grad_norm": 0.5202186107635498,
      "learning_rate": 2.8817989002716263e-08,
      "loss": 2.3882,
      "step": 144750
    },
    {
      "epoch": 2.9779905831891154,
      "grad_norm": 0.46399974822998047,
      "learning_rate": 2.8287307349161853e-08,
      "loss": 2.3288,
      "step": 144760
    },
    {
      "epoch": 2.978196302705983,
      "grad_norm": 0.45675674080848694,
      "learning_rate": 2.7761556755412366e-08,
      "loss": 2.2924,
      "step": 144770
    },
    {
      "epoch": 2.9784020222228507,
      "grad_norm": 0.4813114404678345,
      "learning_rate": 2.7240737247402615e-08,
      "loss": 2.3192,
      "step": 144780
    },
    {
      "epoch": 2.9786077417397188,
      "grad_norm": 0.43795862793922424,
      "learning_rate": 2.672484885082316e-08,
      "loss": 2.3524,
      "step": 144790
    },
    {
      "epoch": 2.9788134612565864,
      "grad_norm": 0.4945181906223297,
      "learning_rate": 2.621389159110921e-08,
      "loss": 2.3576,
      "step": 144800
    },
    {
      "epoch": 2.979019180773454,
      "grad_norm": 0.4546615183353424,
      "learning_rate": 2.570786549348503e-08,
      "loss": 2.3356,
      "step": 144810
    },
    {
      "epoch": 2.9792249002903217,
      "grad_norm": 0.45657670497894287,
      "learning_rate": 2.520677058288623e-08,
      "loss": 2.3807,
      "step": 144820
    },
    {
      "epoch": 2.9794306198071894,
      "grad_norm": 0.4692959785461426,
      "learning_rate": 2.471060688404858e-08,
      "loss": 2.3596,
      "step": 144830
    },
    {
      "epoch": 2.979636339324057,
      "grad_norm": 0.4757175147533417,
      "learning_rate": 2.4219374421430298e-08,
      "loss": 2.3262,
      "step": 144840
    },
    {
      "epoch": 2.9798420588409247,
      "grad_norm": 0.6050140261650085,
      "learning_rate": 2.3733073219267543e-08,
      "loss": 2.3086,
      "step": 144850
    },
    {
      "epoch": 2.9800477783577923,
      "grad_norm": 0.47141456604003906,
      "learning_rate": 2.3251703301552243e-08,
      "loss": 2.3708,
      "step": 144860
    },
    {
      "epoch": 2.98025349787466,
      "grad_norm": 0.43913957476615906,
      "learning_rate": 2.277526469202096e-08,
      "loss": 2.3329,
      "step": 144870
    },
    {
      "epoch": 2.980459217391528,
      "grad_norm": 0.4725152850151062,
      "learning_rate": 2.230375741417712e-08,
      "loss": 2.3749,
      "step": 144880
    },
    {
      "epoch": 2.9806649369083957,
      "grad_norm": 0.47796720266342163,
      "learning_rate": 2.1837181491268787e-08,
      "loss": 2.3742,
      "step": 144890
    },
    {
      "epoch": 2.9808706564252634,
      "grad_norm": 0.47783973813056946,
      "learning_rate": 2.1375536946321994e-08,
      "loss": 2.3474,
      "step": 144900
    },
    {
      "epoch": 2.981076375942131,
      "grad_norm": 0.554448127746582,
      "learning_rate": 2.091882380209631e-08,
      "loss": 2.3874,
      "step": 144910
    },
    {
      "epoch": 2.9812820954589987,
      "grad_norm": 0.4591868221759796,
      "learning_rate": 2.0467042081129263e-08,
      "loss": 2.3326,
      "step": 144920
    },
    {
      "epoch": 2.981487814975867,
      "grad_norm": 0.4549461007118225,
      "learning_rate": 2.0020191805703025e-08,
      "loss": 2.3942,
      "step": 144930
    },
    {
      "epoch": 2.9816935344927344,
      "grad_norm": 0.47359317541122437,
      "learning_rate": 1.957827299785553e-08,
      "loss": 2.2729,
      "step": 144940
    },
    {
      "epoch": 2.981899254009602,
      "grad_norm": 0.515109121799469,
      "learning_rate": 1.9141285679369347e-08,
      "loss": 2.3365,
      "step": 144950
    },
    {
      "epoch": 2.9821049735264697,
      "grad_norm": 0.4410363435745239,
      "learning_rate": 1.8709229871827215e-08,
      "loss": 2.3893,
      "step": 144960
    },
    {
      "epoch": 2.9823106930433374,
      "grad_norm": 0.4724857807159424,
      "learning_rate": 1.8282105596523213e-08,
      "loss": 2.3383,
      "step": 144970
    },
    {
      "epoch": 2.982516412560205,
      "grad_norm": 0.4389799237251282,
      "learning_rate": 1.785991287452937e-08,
      "loss": 2.3776,
      "step": 144980
    },
    {
      "epoch": 2.9827221320770727,
      "grad_norm": 0.4711870551109314,
      "learning_rate": 1.7442651726662373e-08,
      "loss": 2.4064,
      "step": 144990
    },
    {
      "epoch": 2.9829278515939404,
      "grad_norm": 0.4370627999305725,
      "learning_rate": 1.7030322173516854e-08,
      "loss": 2.3722,
      "step": 145000
    },
    {
      "epoch": 2.983133571110808,
      "grad_norm": 0.48546507954597473,
      "learning_rate": 1.6622924235421e-08,
      "loss": 2.3969,
      "step": 145010
    },
    {
      "epoch": 2.983339290627676,
      "grad_norm": 0.4971044063568115,
      "learning_rate": 1.6220457932469844e-08,
      "loss": 2.3551,
      "step": 145020
    },
    {
      "epoch": 2.9835450101445438,
      "grad_norm": 0.452223002910614,
      "learning_rate": 1.5822923284525283e-08,
      "loss": 2.3877,
      "step": 145030
    },
    {
      "epoch": 2.9837507296614114,
      "grad_norm": 0.4476369321346283,
      "learning_rate": 1.5430320311182745e-08,
      "loss": 2.386,
      "step": 145040
    },
    {
      "epoch": 2.983956449178279,
      "grad_norm": 0.5378942489624023,
      "learning_rate": 1.5042649031804524e-08,
      "loss": 2.3872,
      "step": 145050
    },
    {
      "epoch": 2.9841621686951467,
      "grad_norm": 0.40965428948402405,
      "learning_rate": 1.4659909465530864e-08,
      "loss": 2.3282,
      "step": 145060
    },
    {
      "epoch": 2.9843678882120144,
      "grad_norm": 0.5174803137779236,
      "learning_rate": 1.4282101631224455e-08,
      "loss": 2.4163,
      "step": 145070
    },
    {
      "epoch": 2.9845736077288825,
      "grad_norm": 0.46544864773750305,
      "learning_rate": 1.390922554752594e-08,
      "loss": 2.3709,
      "step": 145080
    },
    {
      "epoch": 2.98477932724575,
      "grad_norm": 0.45971769094467163,
      "learning_rate": 1.3541281232820613e-08,
      "loss": 2.4165,
      "step": 145090
    },
    {
      "epoch": 2.9849850467626178,
      "grad_norm": 0.4371240735054016,
      "learning_rate": 1.3178268705271723e-08,
      "loss": 2.3268,
      "step": 145100
    },
    {
      "epoch": 2.9851907662794854,
      "grad_norm": 0.5044957399368286,
      "learning_rate": 1.2820187982776067e-08,
      "loss": 2.3639,
      "step": 145110
    },
    {
      "epoch": 2.985396485796353,
      "grad_norm": 0.4719308912754059,
      "learning_rate": 1.2467039082986188e-08,
      "loss": 2.3786,
      "step": 145120
    },
    {
      "epoch": 2.9856022053132207,
      "grad_norm": 0.436094731092453,
      "learning_rate": 1.2118822023332588e-08,
      "loss": 2.3485,
      "step": 145130
    },
    {
      "epoch": 2.9858079248300884,
      "grad_norm": 0.44758495688438416,
      "learning_rate": 1.177553682100152e-08,
      "loss": 2.3359,
      "step": 145140
    },
    {
      "epoch": 2.986013644346956,
      "grad_norm": 0.4671967625617981,
      "learning_rate": 1.1437183492901683e-08,
      "loss": 2.3743,
      "step": 145150
    },
    {
      "epoch": 2.9862193638638237,
      "grad_norm": 0.42803245782852173,
      "learning_rate": 1.1103762055730827e-08,
      "loss": 2.4347,
      "step": 145160
    },
    {
      "epoch": 2.986425083380692,
      "grad_norm": 0.44120728969573975,
      "learning_rate": 1.077527252594246e-08,
      "loss": 2.3092,
      "step": 145170
    },
    {
      "epoch": 2.9866308028975594,
      "grad_norm": 0.46984660625457764,
      "learning_rate": 1.0451714919734734e-08,
      "loss": 2.3385,
      "step": 145180
    },
    {
      "epoch": 2.986836522414427,
      "grad_norm": 0.4681161046028137,
      "learning_rate": 1.0133089253061556e-08,
      "loss": 2.3331,
      "step": 145190
    },
    {
      "epoch": 2.9870422419312948,
      "grad_norm": 0.5543679594993591,
      "learning_rate": 9.819395541654786e-09,
      "loss": 2.3974,
      "step": 145200
    },
    {
      "epoch": 2.9872479614481624,
      "grad_norm": 0.5050954818725586,
      "learning_rate": 9.510633800968727e-09,
      "loss": 2.4111,
      "step": 145210
    },
    {
      "epoch": 2.9874536809650305,
      "grad_norm": 0.5138043165206909,
      "learning_rate": 9.20680404623564e-09,
      "loss": 2.3862,
      "step": 145220
    },
    {
      "epoch": 2.987659400481898,
      "grad_norm": 0.4858221411705017,
      "learning_rate": 8.907906292454638e-09,
      "loss": 2.3578,
      "step": 145230
    },
    {
      "epoch": 2.987865119998766,
      "grad_norm": 0.4747403562068939,
      "learning_rate": 8.61394055435838e-09,
      "loss": 2.3326,
      "step": 145240
    },
    {
      "epoch": 2.9880708395156335,
      "grad_norm": 0.4616795778274536,
      "learning_rate": 8.324906846446379e-09,
      "loss": 2.3573,
      "step": 145250
    },
    {
      "epoch": 2.988276559032501,
      "grad_norm": 0.4567955434322357,
      "learning_rate": 8.040805182973898e-09,
      "loss": 2.352,
      "step": 145260
    },
    {
      "epoch": 2.9884822785493688,
      "grad_norm": 0.46182599663734436,
      "learning_rate": 7.761635577963056e-09,
      "loss": 2.3908,
      "step": 145270
    },
    {
      "epoch": 2.9886879980662364,
      "grad_norm": 0.4740355908870697,
      "learning_rate": 7.487398045180616e-09,
      "loss": 2.3642,
      "step": 145280
    },
    {
      "epoch": 2.988893717583104,
      "grad_norm": 0.4618457555770874,
      "learning_rate": 7.2180925981379935e-09,
      "loss": 2.4101,
      "step": 145290
    },
    {
      "epoch": 2.9890994370999717,
      "grad_norm": 0.43106818199157715,
      "learning_rate": 6.953719250146762e-09,
      "loss": 2.396,
      "step": 145300
    },
    {
      "epoch": 2.98930515661684,
      "grad_norm": 0.5018125176429749,
      "learning_rate": 6.694278014229838e-09,
      "loss": 2.3856,
      "step": 145310
    },
    {
      "epoch": 2.9895108761337075,
      "grad_norm": 0.44803982973098755,
      "learning_rate": 6.439768903176991e-09,
      "loss": 2.341,
      "step": 145320
    },
    {
      "epoch": 2.989716595650575,
      "grad_norm": 0.4183904826641083,
      "learning_rate": 6.1901919295559444e-09,
      "loss": 2.3742,
      "step": 145330
    },
    {
      "epoch": 2.989922315167443,
      "grad_norm": 0.4503621459007263,
      "learning_rate": 5.94554710566797e-09,
      "loss": 2.329,
      "step": 145340
    },
    {
      "epoch": 2.9901280346843104,
      "grad_norm": 0.425751268863678,
      "learning_rate": 5.705834443592295e-09,
      "loss": 2.3764,
      "step": 145350
    },
    {
      "epoch": 2.9903337542011785,
      "grad_norm": 0.4695843458175659,
      "learning_rate": 5.4710539551416915e-09,
      "loss": 2.401,
      "step": 145360
    },
    {
      "epoch": 2.990539473718046,
      "grad_norm": 0.442145973443985,
      "learning_rate": 5.241205651895786e-09,
      "loss": 2.3625,
      "step": 145370
    },
    {
      "epoch": 2.990745193234914,
      "grad_norm": 0.4137899875640869,
      "learning_rate": 5.016289545189956e-09,
      "loss": 2.2752,
      "step": 145380
    },
    {
      "epoch": 2.9909509127517815,
      "grad_norm": 0.4905254542827606,
      "learning_rate": 4.796305646137533e-09,
      "loss": 2.3626,
      "step": 145390
    },
    {
      "epoch": 2.991156632268649,
      "grad_norm": 0.4539943337440491,
      "learning_rate": 4.581253965563193e-09,
      "loss": 2.3376,
      "step": 145400
    },
    {
      "epoch": 2.991362351785517,
      "grad_norm": 0.4648827910423279,
      "learning_rate": 4.3711345140917684e-09,
      "loss": 2.3855,
      "step": 145410
    },
    {
      "epoch": 2.9915680713023844,
      "grad_norm": 0.4630221128463745,
      "learning_rate": 4.165947302081641e-09,
      "loss": 2.3069,
      "step": 145420
    },
    {
      "epoch": 2.991773790819252,
      "grad_norm": 0.5028098821640015,
      "learning_rate": 3.965692339646942e-09,
      "loss": 2.3695,
      "step": 145430
    },
    {
      "epoch": 2.9919795103361198,
      "grad_norm": 0.46511900424957275,
      "learning_rate": 3.7703696366797604e-09,
      "loss": 2.3099,
      "step": 145440
    },
    {
      "epoch": 2.992185229852988,
      "grad_norm": 0.47615647315979004,
      "learning_rate": 3.5799792028057276e-09,
      "loss": 2.4208,
      "step": 145450
    },
    {
      "epoch": 2.9923909493698555,
      "grad_norm": 0.5629293918609619,
      "learning_rate": 3.3945210474062295e-09,
      "loss": 2.3265,
      "step": 145460
    },
    {
      "epoch": 2.992596668886723,
      "grad_norm": 0.4256020486354828,
      "learning_rate": 3.2139951796517076e-09,
      "loss": 2.4002,
      "step": 145470
    },
    {
      "epoch": 2.992802388403591,
      "grad_norm": 0.4874100387096405,
      "learning_rate": 3.0384016084239464e-09,
      "loss": 2.4283,
      "step": 145480
    },
    {
      "epoch": 2.9930081079204585,
      "grad_norm": 0.49509456753730774,
      "learning_rate": 2.8677403423937876e-09,
      "loss": 2.2998,
      "step": 145490
    },
    {
      "epoch": 2.993213827437326,
      "grad_norm": 0.49131470918655396,
      "learning_rate": 2.702011389987824e-09,
      "loss": 2.3692,
      "step": 145500
    },
    {
      "epoch": 2.993419546954194,
      "grad_norm": 0.5182375907897949,
      "learning_rate": 2.541214759366195e-09,
      "loss": 2.3755,
      "step": 145510
    },
    {
      "epoch": 2.993625266471062,
      "grad_norm": 0.4506970942020416,
      "learning_rate": 2.385350458466995e-09,
      "loss": 2.3673,
      "step": 145520
    },
    {
      "epoch": 2.9938309859879295,
      "grad_norm": 0.43536993861198425,
      "learning_rate": 2.2344184949729676e-09,
      "loss": 2.4043,
      "step": 145530
    },
    {
      "epoch": 2.994036705504797,
      "grad_norm": 0.4504726231098175,
      "learning_rate": 2.0884188763337086e-09,
      "loss": 2.3926,
      "step": 145540
    },
    {
      "epoch": 2.994242425021665,
      "grad_norm": 0.4895833134651184,
      "learning_rate": 1.9473516097545662e-09,
      "loss": 2.4356,
      "step": 145550
    },
    {
      "epoch": 2.9944481445385325,
      "grad_norm": 0.4708227515220642,
      "learning_rate": 1.8112167021855364e-09,
      "loss": 2.3928,
      "step": 145560
    },
    {
      "epoch": 2.9946538640554,
      "grad_norm": 0.4881365895271301,
      "learning_rate": 1.6800141603434683e-09,
      "loss": 2.3609,
      "step": 145570
    },
    {
      "epoch": 2.994859583572268,
      "grad_norm": 0.42221957445144653,
      "learning_rate": 1.5537439907009622e-09,
      "loss": 2.3352,
      "step": 145580
    },
    {
      "epoch": 2.9950653030891354,
      "grad_norm": 0.47473829984664917,
      "learning_rate": 1.432406199486369e-09,
      "loss": 2.3231,
      "step": 145590
    },
    {
      "epoch": 2.9952710226060035,
      "grad_norm": 0.49315157532691956,
      "learning_rate": 1.3160007926837914e-09,
      "loss": 2.3563,
      "step": 145600
    },
    {
      "epoch": 2.995476742122871,
      "grad_norm": 0.43219244480133057,
      "learning_rate": 1.2045277760330819e-09,
      "loss": 2.3556,
      "step": 145610
    },
    {
      "epoch": 2.995682461639739,
      "grad_norm": 0.5278927683830261,
      "learning_rate": 1.097987155040947e-09,
      "loss": 2.3615,
      "step": 145620
    },
    {
      "epoch": 2.9958881811566065,
      "grad_norm": 0.4577803313732147,
      "learning_rate": 9.963789349476394e-10,
      "loss": 2.3319,
      "step": 145630
    },
    {
      "epoch": 2.996093900673474,
      "grad_norm": 0.4708586037158966,
      "learning_rate": 8.997031207824691e-10,
      "loss": 2.3341,
      "step": 145640
    },
    {
      "epoch": 2.9962996201903422,
      "grad_norm": 0.4457750618457794,
      "learning_rate": 8.079597172971909e-10,
      "loss": 2.3864,
      "step": 145650
    },
    {
      "epoch": 2.99650533970721,
      "grad_norm": 0.42603281140327454,
      "learning_rate": 7.211487290326169e-10,
      "loss": 2.3452,
      "step": 145660
    },
    {
      "epoch": 2.9967110592240775,
      "grad_norm": 0.4781898856163025,
      "learning_rate": 6.392701602631057e-10,
      "loss": 2.3738,
      "step": 145670
    },
    {
      "epoch": 2.996916778740945,
      "grad_norm": 0.5251858234405518,
      "learning_rate": 5.623240150187669e-10,
      "loss": 2.3757,
      "step": 145680
    },
    {
      "epoch": 2.997122498257813,
      "grad_norm": 0.4711991250514984,
      "learning_rate": 4.903102971076656e-10,
      "loss": 2.4052,
      "step": 145690
    },
    {
      "epoch": 2.9973282177746805,
      "grad_norm": 0.4780920147895813,
      "learning_rate": 4.232290100825154e-10,
      "loss": 2.3585,
      "step": 145700
    },
    {
      "epoch": 2.997533937291548,
      "grad_norm": 0.4567960500717163,
      "learning_rate": 3.6108015724067855e-10,
      "loss": 2.3339,
      "step": 145710
    },
    {
      "epoch": 2.997739656808416,
      "grad_norm": 0.5502168536186218,
      "learning_rate": 3.03863741657473e-10,
      "loss": 2.3439,
      "step": 145720
    },
    {
      "epoch": 2.9979453763252835,
      "grad_norm": 0.5020416378974915,
      "learning_rate": 2.5157976615286515e-10,
      "loss": 2.348,
      "step": 145730
    },
    {
      "epoch": 2.9981510958421516,
      "grad_norm": 0.47895094752311707,
      "learning_rate": 2.0422823330257247e-10,
      "loss": 2.3641,
      "step": 145740
    },
    {
      "epoch": 2.998356815359019,
      "grad_norm": 0.6060206890106201,
      "learning_rate": 1.6180914544916548e-10,
      "loss": 2.3836,
      "step": 145750
    },
    {
      "epoch": 2.998562534875887,
      "grad_norm": 0.4554010033607483,
      "learning_rate": 1.2432250466876127e-10,
      "loss": 2.3497,
      "step": 145760
    },
    {
      "epoch": 2.9987682543927545,
      "grad_norm": 0.477864146232605,
      "learning_rate": 9.176831283763676e-11,
      "loss": 2.359,
      "step": 145770
    },
    {
      "epoch": 2.998973973909622,
      "grad_norm": 0.4244038164615631,
      "learning_rate": 6.414657153230864e-11,
      "loss": 2.336,
      "step": 145780
    },
    {
      "epoch": 2.99917969342649,
      "grad_norm": 0.4590964913368225,
      "learning_rate": 4.1457282129453436e-11,
      "loss": 2.3952,
      "step": 145790
    },
    {
      "epoch": 2.999385412943358,
      "grad_norm": 0.46445903182029724,
      "learning_rate": 2.3700445750396426e-11,
      "loss": 2.3735,
      "step": 145800
    }
  ],
  "logging_steps": 10,
  "max_steps": 145830,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.3286493792973095e+19,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
